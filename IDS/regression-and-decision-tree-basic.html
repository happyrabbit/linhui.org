<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>11.4 Regression and Decision Tree Basic | Practitioner’s Guide to Data Science</title>
  <meta name="description" content="Introduction to Data Science" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="11.4 Regression and Decision Tree Basic | Practitioner’s Guide to Data Science" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="http://scientistcafe.com/IDS/" />
  
  <meta property="og:description" content="Introduction to Data Science" />
  <meta name="github-repo" content="happyrabbit/IntroDataScience" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="11.4 Regression and Decision Tree Basic | Practitioner’s Guide to Data Science" />
  
  <meta name="twitter:description" content="Introduction to Data Science" />
  

<meta name="author" content="Hui Lin and Ming Li" />


<meta name="date" content="2023-02-26" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="tree-pruning.html"/>
<link rel="next" href="bagging-tree-1.html"/>
<script src="libs/header-attrs/header-attrs.js"></script>
<script src="libs/jquery/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="http://scientistcafe.com">Homepage</a></li>

<li class="divider"></li>
<li><a href="index.html#preface" id="toc-preface">Preface</a>
<ul>
<li><a href="goal-of-the-book.html#goal-of-the-book" id="toc-goal-of-the-book">Goal of the Book</a></li>
<li><a href="what-this-book-covers.html#what-this-book-covers" id="toc-what-this-book-covers">What This Book Covers</a></li>
<li><a href="who-this-book-is-for.html#who-this-book-is-for" id="toc-who-this-book-is-for">Who This Book Is For</a></li>
<li><a href="how-to-use-this-book.html#how-to-use-this-book" id="toc-how-to-use-this-book">How to Use This Book</a>
<ul>
<li><a href="how-to-use-this-book.html#what-the-book-assumes" id="toc-what-the-book-assumes">What the Book Assumes</a></li>
<li><a href="how-to-use-this-book.html#how-to-run-r-and-python-code" id="toc-how-to-run-r-and-python-code">How to Run R and Python Code</a></li>
</ul></li>
<li><a href="complementary-reading.html#complementary-reading" id="toc-complementary-reading">Complementary Reading</a></li>
</ul></li>
<li><a href="about-the-authors.html#about-the-authors" id="toc-about-the-authors">About the Authors</a></li>
<li><a href="introduction.html#introduction" id="toc-introduction"><span class="toc-section-number">1</span> Introduction</a>
<ul>
<li><a href="a-brief-history-of-data-science.html#a-brief-history-of-data-science" id="toc-a-brief-history-of-data-science"><span class="toc-section-number">1.1</span> A Brief History of Data Science</a></li>
<li><a href="data-science-role-and-skill-tracks.html#data-science-role-and-skill-tracks" id="toc-data-science-role-and-skill-tracks"><span class="toc-section-number">1.2</span> Data Science Role and Skill Tracks</a>
<ul>
<li><a href="data-science-role-and-skill-tracks.html#engineering" id="toc-engineering"><span class="toc-section-number">1.2.1</span> Engineering</a></li>
<li><a href="data-science-role-and-skill-tracks.html#analysis" id="toc-analysis"><span class="toc-section-number">1.2.2</span> Analysis</a></li>
<li><a href="data-science-role-and-skill-tracks.html#modelinginference" id="toc-modelinginference"><span class="toc-section-number">1.2.3</span> Modeling/Inference</a></li>
</ul></li>
<li><a href="what-kind-of-questions-can-data-science-solve.html#what-kind-of-questions-can-data-science-solve" id="toc-what-kind-of-questions-can-data-science-solve"><span class="toc-section-number">1.3</span> What Kind of Questions Can Data Science Solve?</a>
<ul>
<li><a href="what-kind-of-questions-can-data-science-solve.html#prerequisites" id="toc-prerequisites"><span class="toc-section-number">1.3.1</span> Prerequisites</a></li>
<li><a href="what-kind-of-questions-can-data-science-solve.html#problem-type" id="toc-problem-type"><span class="toc-section-number">1.3.2</span> Problem Type</a></li>
</ul></li>
<li><a href="structure-of-data-science-team.html#structure-of-data-science-team" id="toc-structure-of-data-science-team"><span class="toc-section-number">1.4</span> Structure of Data Science Team</a></li>
<li><a href="data-science-roles.html#data-science-roles" id="toc-data-science-roles"><span class="toc-section-number">1.5</span> Data Science Roles</a></li>
</ul></li>
<li><a href="SoftSkillsforDataScientists.html#SoftSkillsforDataScientists" id="toc-SoftSkillsforDataScientists"><span class="toc-section-number">2</span> Soft Skills for Data Scientists</a>
<ul>
<li><a href="comparison-between-statistician-and-data-scientist.html#comparison-between-statistician-and-data-scientist" id="toc-comparison-between-statistician-and-data-scientist"><span class="toc-section-number">2.1</span> Comparison between Statistician and Data Scientist</a></li>
<li><a href="beyond-data-and-analytics.html#beyond-data-and-analytics" id="toc-beyond-data-and-analytics"><span class="toc-section-number">2.2</span> Beyond Data and Analytics</a></li>
<li><a href="three-pillars-of-knowledge.html#three-pillars-of-knowledge" id="toc-three-pillars-of-knowledge"><span class="toc-section-number">2.3</span> Three Pillars of Knowledge</a></li>
<li><a href="data-science-project-cycle.html#data-science-project-cycle" id="toc-data-science-project-cycle"><span class="toc-section-number">2.4</span> Data Science Project Cycle</a>
<ul>
<li><a href="data-science-project-cycle.html#types-of-data-science-projects" id="toc-types-of-data-science-projects"><span class="toc-section-number">2.4.1</span> Types of Data Science Projects</a></li>
<li><a href="data-science-project-cycle.html#ProblemFormulationandProjectPlanningStage" id="toc-ProblemFormulationandProjectPlanningStage"><span class="toc-section-number">2.4.2</span> Problem Formulation and Project Planning Stage</a></li>
<li><a href="data-science-project-cycle.html#project-modeling-stage" id="toc-project-modeling-stage"><span class="toc-section-number">2.4.3</span> Project Modeling Stage</a></li>
<li><a href="data-science-project-cycle.html#ModelImplementationandPostProductionStage" id="toc-ModelImplementationandPostProductionStage"><span class="toc-section-number">2.4.4</span> Model Implementation and Post Production Stage</a></li>
<li><a href="data-science-project-cycle.html#ProjectCycleSummary" id="toc-ProjectCycleSummary"><span class="toc-section-number">2.4.5</span> Project Cycle Summary</a></li>
</ul></li>
<li><a href="common-mistakes-in-data-science.html#common-mistakes-in-data-science" id="toc-common-mistakes-in-data-science"><span class="toc-section-number">2.5</span> Common Mistakes in Data Science</a>
<ul>
<li><a href="common-mistakes-in-data-science.html#problem-formulation-stage" id="toc-problem-formulation-stage"><span class="toc-section-number">2.5.1</span> Problem Formulation Stage</a></li>
<li><a href="common-mistakes-in-data-science.html#project-planning-stage" id="toc-project-planning-stage"><span class="toc-section-number">2.5.2</span> Project Planning Stage</a></li>
<li><a href="common-mistakes-in-data-science.html#project-modeling-stage-1" id="toc-project-modeling-stage-1"><span class="toc-section-number">2.5.3</span> Project Modeling Stage</a></li>
<li><a href="common-mistakes-in-data-science.html#model-implementation-and-post-production-stage" id="toc-model-implementation-and-post-production-stage"><span class="toc-section-number">2.5.4</span> Model Implementation and Post Production Stage</a></li>
<li><a href="common-mistakes-in-data-science.html#summary-of-common-mistakes" id="toc-summary-of-common-mistakes"><span class="toc-section-number">2.5.5</span> Summary of Common Mistakes</a></li>
</ul></li>
</ul></li>
<li><a href="introduction-to-the-data.html#introduction-to-the-data" id="toc-introduction-to-the-data"><span class="toc-section-number">3</span> Introduction to The Data</a>
<ul>
<li><a href="customer-data-for-a-clothing-company.html#customer-data-for-a-clothing-company" id="toc-customer-data-for-a-clothing-company"><span class="toc-section-number">3.1</span> Customer Data for A Clothing Company</a></li>
<li><a href="swinediseasedata.html#swinediseasedata" id="toc-swinediseasedata"><span class="toc-section-number">3.2</span> Swine Disease Breakout Data</a></li>
<li><a href="mnist-dataset.html#mnist-dataset" id="toc-mnist-dataset"><span class="toc-section-number">3.3</span> MNIST Dataset</a></li>
<li><a href="imdb-dataset.html#imdb-dataset" id="toc-imdb-dataset"><span class="toc-section-number">3.4</span> IMDB Dataset</a></li>
</ul></li>
<li><a href="bigdatacloudplatform.html#bigdatacloudplatform" id="toc-bigdatacloudplatform"><span class="toc-section-number">4</span> Big Data Cloud Platform</a>
<ul>
<li><a href="power-of-cluster-of-computers.html#power-of-cluster-of-computers" id="toc-power-of-cluster-of-computers"><span class="toc-section-number">4.1</span> Power of Cluster of Computers</a></li>
<li><a href="evolution-of-cluster-computing.html#evolution-of-cluster-computing" id="toc-evolution-of-cluster-computing"><span class="toc-section-number">4.2</span> Evolution of Cluster Computing</a>
<ul>
<li><a href="evolution-of-cluster-computing.html#hadoop" id="toc-hadoop"><span class="toc-section-number">4.2.1</span> Hadoop</a></li>
<li><a href="evolution-of-cluster-computing.html#spark" id="toc-spark"><span class="toc-section-number">4.2.2</span> Spark</a></li>
</ul></li>
<li><a href="CloudEnvironment.html#CloudEnvironment" id="toc-CloudEnvironment"><span class="toc-section-number">4.3</span> Introduction of Cloud Environment</a>
<ul>
<li><a href="CloudEnvironment.html#open-account-and-create-a-cluster" id="toc-open-account-and-create-a-cluster"><span class="toc-section-number">4.3.1</span> Open Account and Create a Cluster</a></li>
<li><a href="CloudEnvironment.html#r-notebook" id="toc-r-notebook"><span class="toc-section-number">4.3.2</span> R Notebook</a></li>
<li><a href="CloudEnvironment.html#markdown-cells" id="toc-markdown-cells"><span class="toc-section-number">4.3.3</span> Markdown Cells</a></li>
</ul></li>
<li><a href="leveragesparkr.html#leveragesparkr" id="toc-leveragesparkr"><span class="toc-section-number">4.4</span> Leverage Spark Using R Notebook</a></li>
<li><a href="databases-and-sql.html#databases-and-sql" id="toc-databases-and-sql"><span class="toc-section-number">4.5</span> Databases and SQL</a>
<ul>
<li><a href="databases-and-sql.html#history" id="toc-history"><span class="toc-section-number">4.5.1</span> History</a></li>
<li><a href="databases-and-sql.html#database-table-and-view" id="toc-database-table-and-view"><span class="toc-section-number">4.5.2</span> Database, Table and View</a></li>
<li><a href="databases-and-sql.html#basic-sql-statement" id="toc-basic-sql-statement"><span class="toc-section-number">4.5.3</span> Basic SQL Statement</a></li>
<li><a href="databases-and-sql.html#advanced-topics-in-database" id="toc-advanced-topics-in-database"><span class="toc-section-number">4.5.4</span> Advanced Topics in Database</a></li>
</ul></li>
</ul></li>
<li><a href="datapreprocessing.html#datapreprocessing" id="toc-datapreprocessing"><span class="toc-section-number">5</span> Data Pre-processing</a>
<ul>
<li><a href="data-cleaning.html#data-cleaning" id="toc-data-cleaning"><span class="toc-section-number">5.1</span> Data Cleaning</a></li>
<li><a href="missing-values.html#missing-values" id="toc-missing-values"><span class="toc-section-number">5.2</span> Missing Values</a>
<ul>
<li><a href="missing-values.html#impute-missing-values-with-medianmode" id="toc-impute-missing-values-with-medianmode"><span class="toc-section-number">5.2.1</span> Impute missing values with median/mode</a></li>
<li><a href="missing-values.html#k-nearest-neighbors" id="toc-k-nearest-neighbors"><span class="toc-section-number">5.2.2</span> K-nearest neighbors</a></li>
<li><a href="missing-values.html#bagging-tree" id="toc-bagging-tree"><span class="toc-section-number">5.2.3</span> Bagging Tree</a></li>
</ul></li>
<li><a href="centering-and-scaling.html#centering-and-scaling" id="toc-centering-and-scaling"><span class="toc-section-number">5.3</span> Centering and Scaling</a></li>
<li><a href="resolve-skewness.html#resolve-skewness" id="toc-resolve-skewness"><span class="toc-section-number">5.4</span> Resolve Skewness</a></li>
<li><a href="outliers.html#outliers" id="toc-outliers"><span class="toc-section-number">5.5</span> Resolve Outliers</a></li>
<li><a href="collinearity.html#collinearity" id="toc-collinearity"><span class="toc-section-number">5.6</span> Collinearity</a></li>
<li><a href="sparse-variables.html#sparse-variables" id="toc-sparse-variables"><span class="toc-section-number">5.7</span> Sparse Variables</a></li>
<li><a href="re-encode-dummy-variables.html#re-encode-dummy-variables" id="toc-re-encode-dummy-variables"><span class="toc-section-number">5.8</span> Re-encode Dummy Variables</a></li>
</ul></li>
<li><a href="datawrangline.html#datawrangline" id="toc-datawrangline"><span class="toc-section-number">6</span> Data Wrangling</a>
<ul>
<li><a href="summarize-data.html#summarize-data" id="toc-summarize-data"><span class="toc-section-number">6.1</span> Summarize Data</a>
<ul>
<li><a href="summarize-data.html#dplyr-package" id="toc-dplyr-package"><span class="toc-section-number">6.1.1</span> <code>dplyr</code> package</a></li>
<li><a href="summarize-data.html#applyfamilyinbaser" id="toc-applyfamilyinbaser"><span class="toc-section-number">6.1.2</span> <code>apply()</code>, <code>lapply()</code> and <code>sapply()</code> in base R</a></li>
</ul></li>
<li><a href="tidy-and-reshape-data.html#tidy-and-reshape-data" id="toc-tidy-and-reshape-data"><span class="toc-section-number">6.2</span> Tidy and Reshape Data</a></li>
</ul></li>
<li><a href="modeltuningstrategy.html#modeltuningstrategy" id="toc-modeltuningstrategy"><span class="toc-section-number">7</span> Model Tuning Strategy</a>
<ul>
<li><a href="vbtradeoff.html#vbtradeoff" id="toc-vbtradeoff"><span class="toc-section-number">7.1</span> Variance-Bias Trade-Off</a></li>
<li><a href="datasplittingresampling.html#datasplittingresampling" id="toc-datasplittingresampling"><span class="toc-section-number">7.2</span> Data Splitting and Resampling</a>
<ul>
<li><a href="datasplittingresampling.html#datasplitting" id="toc-datasplitting"><span class="toc-section-number">7.2.1</span> Data Splitting</a></li>
<li><a href="datasplittingresampling.html#resampling" id="toc-resampling"><span class="toc-section-number">7.2.2</span> Resampling</a></li>
</ul></li>
</ul></li>
<li><a href="measuring-performance.html#measuring-performance" id="toc-measuring-performance"><span class="toc-section-number">8</span> Measuring Performance</a>
<ul>
<li><a href="regression-model-performance.html#regression-model-performance" id="toc-regression-model-performance"><span class="toc-section-number">8.1</span> Regression Model Performance</a></li>
<li><a href="classification-model-performance.html#classification-model-performance" id="toc-classification-model-performance"><span class="toc-section-number">8.2</span> Classification Model Performance</a>
<ul>
<li><a href="classification-model-performance.html#confusion-matrix" id="toc-confusion-matrix"><span class="toc-section-number">8.2.1</span> Confusion Matrix</a></li>
<li><a href="classification-model-performance.html#kappa-statistic" id="toc-kappa-statistic"><span class="toc-section-number">8.2.2</span> Kappa Statistic</a></li>
<li><a href="classification-model-performance.html#roc" id="toc-roc"><span class="toc-section-number">8.2.3</span> ROC</a></li>
<li><a href="classification-model-performance.html#gain-and-lift-charts" id="toc-gain-and-lift-charts"><span class="toc-section-number">8.2.4</span> Gain and Lift Charts</a></li>
</ul></li>
</ul></li>
<li><a href="regression-models.html#regression-models" id="toc-regression-models"><span class="toc-section-number">9</span> Regression Models</a>
<ul>
<li><a href="ordinary-least-square.html#ordinary-least-square" id="toc-ordinary-least-square"><span class="toc-section-number">9.1</span> Ordinary Least Square</a>
<ul>
<li><a href="ordinary-least-square.html#the-magic-p-value" id="toc-the-magic-p-value"><span class="toc-section-number">9.1.1</span> The Magic P-value</a></li>
<li><a href="ordinary-least-square.html#diagnostics-for-linear-regression" id="toc-diagnostics-for-linear-regression"><span class="toc-section-number">9.1.2</span> Diagnostics for Linear Regression</a></li>
</ul></li>
<li><a href="principal-component-regression-and-partial-least-square.html#principal-component-regression-and-partial-least-square" id="toc-principal-component-regression-and-partial-least-square"><span class="toc-section-number">9.2</span> Principal Component Regression and Partial Least Square</a></li>
</ul></li>
<li><a href="regularization-methods.html#regularization-methods" id="toc-regularization-methods"><span class="toc-section-number">10</span> Regularization Methods</a>
<ul>
<li><a href="ridge-regression.html#ridge-regression" id="toc-ridge-regression"><span class="toc-section-number">10.1</span> Ridge Regression</a></li>
<li><a href="lasso.html#lasso" id="toc-lasso"><span class="toc-section-number">10.2</span> LASSO</a></li>
<li><a href="elastic-net.html#elastic-net" id="toc-elastic-net"><span class="toc-section-number">10.3</span> Elastic Net</a></li>
<li><a href="penalized-generalized-linear-model.html#penalized-generalized-linear-model" id="toc-penalized-generalized-linear-model"><span class="toc-section-number">10.4</span> Penalized Generalized Linear Model</a>
<ul>
<li><a href="penalized-generalized-linear-model.html#introduction-to-glmnet-package" id="toc-introduction-to-glmnet-package"><span class="toc-section-number">10.4.1</span> Introduction to <code>glmnet</code> package</a></li>
<li><a href="penalized-generalized-linear-model.html#penalized-logistic-regression" id="toc-penalized-logistic-regression"><span class="toc-section-number">10.4.2</span> Penalized logistic regression</a></li>
</ul></li>
</ul></li>
<li><a href="treemodel.html#treemodel" id="toc-treemodel"><span class="toc-section-number">11</span> Tree-Based Methods</a>
<ul>
<li><a href="tree-basics.html#tree-basics" id="toc-tree-basics"><span class="toc-section-number">11.1</span> Tree Basics</a></li>
<li><a href="splitting-criteria.html#splitting-criteria" id="toc-splitting-criteria"><span class="toc-section-number">11.2</span> Splitting Criteria</a>
<ul>
<li><a href="splitting-criteria.html#gini-impurity" id="toc-gini-impurity"><span class="toc-section-number">11.2.1</span> Gini impurity</a></li>
<li><a href="splitting-criteria.html#information-gain-ig" id="toc-information-gain-ig"><span class="toc-section-number">11.2.2</span> Information Gain (IG)</a></li>
<li><a href="splitting-criteria.html#information-gain-ratio-igr" id="toc-information-gain-ratio-igr"><span class="toc-section-number">11.2.3</span> Information Gain Ratio (IGR)</a></li>
<li><a href="splitting-criteria.html#sum-of-squared-error-sse" id="toc-sum-of-squared-error-sse"><span class="toc-section-number">11.2.4</span> Sum of Squared Error (SSE)</a></li>
</ul></li>
<li><a href="tree-pruning.html#tree-pruning" id="toc-tree-pruning"><span class="toc-section-number">11.3</span> Tree Pruning</a></li>
<li><a href="regression-and-decision-tree-basic.html#regression-and-decision-tree-basic" id="toc-regression-and-decision-tree-basic"><span class="toc-section-number">11.4</span> Regression and Decision Tree Basic</a>
<ul>
<li><a href="regression-and-decision-tree-basic.html#regression-tree" id="toc-regression-tree"><span class="toc-section-number">11.4.1</span> Regression Tree</a></li>
<li><a href="regression-and-decision-tree-basic.html#decision-tree" id="toc-decision-tree"><span class="toc-section-number">11.4.2</span> Decision Tree</a></li>
</ul></li>
<li><a href="bagging-tree-1.html#bagging-tree-1" id="toc-bagging-tree-1"><span class="toc-section-number">11.5</span> Bagging Tree</a></li>
<li><a href="random-forest.html#random-forest" id="toc-random-forest"><span class="toc-section-number">11.6</span> Random Forest</a></li>
<li><a href="gradient-boosted-machine.html#gradient-boosted-machine" id="toc-gradient-boosted-machine"><span class="toc-section-number">11.7</span> Gradient Boosted Machine</a>
<ul>
<li><a href="gradient-boosted-machine.html#adaptive-boosting" id="toc-adaptive-boosting"><span class="toc-section-number">11.7.1</span> Adaptive Boosting</a></li>
<li><a href="gradient-boosted-machine.html#stochastic-gradient-boosting" id="toc-stochastic-gradient-boosting"><span class="toc-section-number">11.7.2</span> Stochastic Gradient Boosting</a></li>
</ul></li>
</ul></li>
<li><a href="deeplearning.html#deeplearning" id="toc-deeplearning"><span class="toc-section-number">12</span> Deep Learning</a>
<ul>
<li><a href="feedforward-neural-network.html#feedforward-neural-network" id="toc-feedforward-neural-network"><span class="toc-section-number">12.1</span> Feedforward Neural Network</a>
<ul>
<li><a href="feedforward-neural-network.html#logisticregasneuralnetwork" id="toc-logisticregasneuralnetwork"><span class="toc-section-number">12.1.1</span> Logistic Regression as Neural Network</a></li>
<li><a href="feedforward-neural-network.html#stochastic-gradient-descent" id="toc-stochastic-gradient-descent"><span class="toc-section-number">12.1.2</span> Stochastic Gradient Descent</a></li>
<li><a href="feedforward-neural-network.html#deepneuralnetwork" id="toc-deepneuralnetwork"><span class="toc-section-number">12.1.3</span> Deep Neural Network</a></li>
<li><a href="feedforward-neural-network.html#activationfunction" id="toc-activationfunction"><span class="toc-section-number">12.1.4</span> Activation Function</a></li>
<li><a href="feedforward-neural-network.html#optimization" id="toc-optimization"><span class="toc-section-number">12.1.5</span> Optimization</a></li>
<li><a href="feedforward-neural-network.html#deal-with-overfitting" id="toc-deal-with-overfitting"><span class="toc-section-number">12.1.6</span> Deal with Overfitting</a></li>
<li><a href="feedforward-neural-network.html#ffnnexample" id="toc-ffnnexample"><span class="toc-section-number">12.1.7</span> Image Recognition Using FFNN</a></li>
</ul></li>
<li><a href="convolutional-neural-network.html#convolutional-neural-network" id="toc-convolutional-neural-network"><span class="toc-section-number">12.2</span> Convolutional Neural Network</a>
<ul>
<li><a href="convolutional-neural-network.html#convolution-layer" id="toc-convolution-layer"><span class="toc-section-number">12.2.1</span> Convolution Layer</a></li>
<li><a href="convolutional-neural-network.html#padding-layer" id="toc-padding-layer"><span class="toc-section-number">12.2.2</span> Padding Layer</a></li>
<li><a href="convolutional-neural-network.html#pooling-layer" id="toc-pooling-layer"><span class="toc-section-number">12.2.3</span> Pooling Layer</a></li>
<li><a href="convolutional-neural-network.html#convolution-over-volume" id="toc-convolution-over-volume"><span class="toc-section-number">12.2.4</span> Convolution Over Volume</a></li>
<li><a href="convolutional-neural-network.html#cnnexample" id="toc-cnnexample"><span class="toc-section-number">12.2.5</span> Image Recognition Using CNN</a></li>
</ul></li>
<li><a href="recurrent-neural-network.html#recurrent-neural-network" id="toc-recurrent-neural-network"><span class="toc-section-number">12.3</span> Recurrent Neural Network</a>
<ul>
<li><a href="recurrent-neural-network.html#rnn-model" id="toc-rnn-model"><span class="toc-section-number">12.3.1</span> RNN Model</a></li>
<li><a href="recurrent-neural-network.html#lstm" id="toc-lstm"><span class="toc-section-number">12.3.2</span> Long Short Term Memory</a></li>
<li><a href="recurrent-neural-network.html#embedding" id="toc-embedding"><span class="toc-section-number">12.3.3</span> Word Embedding</a></li>
<li><a href="recurrent-neural-network.html#rnnexample" id="toc-rnnexample"><span class="toc-section-number">12.3.4</span> Sentiment Analysis Using RNN</a></li>
</ul></li>
</ul></li>
<li><a href="appendix.html#appendix" id="toc-appendix">Appendix</a></li>
<li><a href="largelocaldata.html#largelocaldata" id="toc-largelocaldata"><span class="toc-section-number">13</span> Handling Large Local Data</a>
<ul>
<li><a href="readr.html#readr" id="toc-readr"><span class="toc-section-number">13.1</span> <code>readr</code></a></li>
<li><a href="data.table-enhanced-data.html#data.table-enhanced-data.frame" id="toc-data.table-enhanced-data.frame"><span class="toc-section-number">13.2</span> <code>data.table</code>— enhanced <code>data.frame</code></a></li>
</ul></li>
<li><a href="r-code-for-data-simulation.html#r-code-for-data-simulation" id="toc-r-code-for-data-simulation"><span class="toc-section-number">14</span> R code for data simulation</a>
<ul>
<li><a href="appendixdata1.html#appendixdata1" id="toc-appendixdata1"><span class="toc-section-number">14.1</span> Customer Data for Clothing Company</a></li>
<li><a href="appendixdata3.html#appendixdata3" id="toc-appendixdata3"><span class="toc-section-number">14.2</span> Swine Disease Breakout Data</a></li>
</ul></li>
<li><a href="references.html#references" id="toc-references">References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Practitioner’s Guide to Data Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="regression-and-decision-tree-basic" class="section level2 hasAnchor" number="11.4">
<h2><span class="header-section-number">11.4</span> Regression and Decision Tree Basic<a href="regression-and-decision-tree-basic.html#regression-and-decision-tree-basic" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="regression-tree" class="section level3 hasAnchor" number="11.4.1">
<h3><span class="header-section-number">11.4.1</span> Regression Tree<a href="regression-and-decision-tree-basic.html#regression-tree" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let’s look at the process of building a regression tree <span class="citation">(<a href="#ref-ISLR15" role="doc-biblioref">Gareth James and Tibshirani 2015</a>)</span>. There are two steps:</p>
<ol style="list-style-type: decimal">
<li>Divide predictors space — that is a set of possible values of <span class="math inline">\(X_1,X_2,\dots,X_p\)</span>— into <span class="math inline">\(J\)</span> distinct and non-overlapping regions: <span class="math inline">\(R_1,R_2,\dots,R_J\)</span></li>
<li>For every observation that falls into the region <span class="math inline">\(R_j\)</span>, the prediction is the mean of the response values for the training observations in <span class="math inline">\(R_j\)</span></li>
</ol>
<p>Let’s go back to the previous simple example. If we use the variable “Gender” to divide the observations, we obtain two regions <span class="math inline">\(R_1\)</span> (female) and <span class="math inline">\(R_2\)</span> (male).</p>
<center>
<img src="images/varEN.png" />
</center>
<div class="sourceCode" id="cb295"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb295-1"><a href="regression-and-decision-tree-basic.html#cb295-1" aria-hidden="true" tabindex="-1"></a>y1 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">156</span>, <span class="dv">167</span>, <span class="dv">165</span>, <span class="dv">163</span>, <span class="dv">160</span>, <span class="dv">170</span>, <span class="dv">160</span>)</span>
<span id="cb295-2"><a href="regression-and-decision-tree-basic.html#cb295-2" aria-hidden="true" tabindex="-1"></a>y2 <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">172</span>, <span class="dv">180</span>, <span class="dv">176</span>)</span></code></pre></div>
<p>The sample average for region <span class="math inline">\(R_1\)</span> is 163, for region <span class="math inline">\(R_2\)</span> is 176. For a new observation, if it is female, the model predicts the height to be 163, if it is male, the predicted height is 176. Calculating the mean is easy. Let’s look at the first step in more detail which is to divide the space into <span class="math inline">\(R_1, R_2, \dots, R_J\)</span>.</p>
<p>In theory, the region can be any shape. However, to simplify the problem, we divide the predictor space into high-dimensional rectangles. The goal is to divide the space in a way that minimize RSS. Practically, it is nearly impossible to consider all possible partitions of the feature space. So we use an approach named recursive binary splitting, a top-down, greedy algorithm. The process starts from the top of the tree (root node) and then successively splits the predictor space. Each split produces two branches (hence binary). At each step of the process, it chooses the best split at that particular step, rather than looking ahead and picking a split that leads to a better tree in general (hence greedy).</p>
<p><span class="math display">\[R_{1}(j, s)=\{X|X_j&lt;s\}\ and\ R_{2}(j, s)=\{X|X_j\geq s\}\]</span></p>
<p>Calculate the RSS decrease after the split. For different <span class="math inline">\((j,s)\)</span>, search for the combination that minimizes the RSS, that is to minimize the following:</p>
<p><span class="math display">\[\Sigma_{i:x_i\in R_1(j,s)}(y_i-\hat{y}_{R_{1}})^2+\Sigma_{i:x_i\in R_2(j,s)}(y_i-\hat{y}_{R_{2}})^2\]</span></p>
<p>where <span class="math inline">\(\hat{y}_{R_1}\)</span> is the mean of all samples in <span class="math inline">\(R_1\)</span>, <span class="math inline">\(\hat{y}_{R_2}\)</span> is the mean of samples in <span class="math inline">\(R_2\)</span>. It can be quick to optimize the equation above. Especially when <span class="math inline">\(p\)</span> is not too large.</p>
<p>Next, we continue to search for the split that optimize the RSS. Note that the optimization is limited in the sub-region. The process keeps going until a stopping criterion is reaches. For example, continue until no region contains more than 5 samples or the RSS decreases less than 1%. The process is like a tree growing.</p>
<center>
<img src="images/BinaryTree.png" />
</center>
<p>There are multiple R packages for building regression tree, such as <code>ctree</code>, <code>rpart</code> and <code>tree</code>. <code>rpart</code> is widely used for building a single tree. The split is based on CART algorithm, using <code>rpart()</code> function from the package. There are some parameters that controls the model fitting, such as the minimum number of observations that must exist in a node in order for a split to be attempted, the minimum number of observations in any leaf node etc. You can set those parameter using <code>rpart.control</code>.</p>
<p>A more convenient way is to use <code>train()</code> function in <code>caret</code> package. The package can call <code>rpart()</code> function and train the model through cross-validation. In this case, the most common parameters are <code>cp</code> (complexity parameter) and <code>maxdepth</code> (the maximum depth of any node of the final tree). To tune the complexity parameter, set <code>method = "rpart"</code>. To tune the maximum tree depth, set <code>method = "rpart2"</code>. Now let us use the customer expenditure regression example to illustrate:</p>
<div class="sourceCode" id="cb296"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb296-1"><a href="regression-and-decision-tree-basic.html#cb296-1" aria-hidden="true" tabindex="-1"></a>dat <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;http://bit.ly/2P5gTw4&quot;</span>)</span>
<span id="cb296-2"><a href="regression-and-decision-tree-basic.html#cb296-2" aria-hidden="true" tabindex="-1"></a><span class="co"># data cleaning: delete wrong observations</span></span>
<span id="cb296-3"><a href="regression-and-decision-tree-basic.html#cb296-3" aria-hidden="true" tabindex="-1"></a>dat <span class="ot">&lt;-</span> <span class="fu">subset</span>(dat, store_exp <span class="sc">&gt;</span> <span class="dv">0</span> <span class="sc">&amp;</span> online_exp <span class="sc">&gt;</span> <span class="dv">0</span>)</span>
<span id="cb296-4"><a href="regression-and-decision-tree-basic.html#cb296-4" aria-hidden="true" tabindex="-1"></a><span class="co"># use the 10 survey questions as predictors</span></span>
<span id="cb296-5"><a href="regression-and-decision-tree-basic.html#cb296-5" aria-hidden="true" tabindex="-1"></a>trainx <span class="ot">&lt;-</span> dat[, <span class="fu">grep</span>(<span class="st">&quot;Q&quot;</span>, <span class="fu">names</span>(dat))]</span>
<span id="cb296-6"><a href="regression-and-decision-tree-basic.html#cb296-6" aria-hidden="true" tabindex="-1"></a><span class="co"># use the sum of store and online expenditure as response variable</span></span>
<span id="cb296-7"><a href="regression-and-decision-tree-basic.html#cb296-7" aria-hidden="true" tabindex="-1"></a><span class="co"># total expenditure = store expenditure + online expenditure</span></span>
<span id="cb296-8"><a href="regression-and-decision-tree-basic.html#cb296-8" aria-hidden="true" tabindex="-1"></a>trainy <span class="ot">&lt;-</span> dat<span class="sc">$</span>store_exp <span class="sc">+</span> dat<span class="sc">$</span>online_exp</span>
<span id="cb296-9"><a href="regression-and-decision-tree-basic.html#cb296-9" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">100</span>)</span>
<span id="cb296-10"><a href="regression-and-decision-tree-basic.html#cb296-10" aria-hidden="true" tabindex="-1"></a>rpartTune <span class="ot">&lt;-</span> <span class="fu">train</span>(trainx, trainy, </span>
<span id="cb296-11"><a href="regression-and-decision-tree-basic.html#cb296-11" aria-hidden="true" tabindex="-1"></a>                   <span class="at">method =</span> <span class="st">&quot;rpart2&quot;</span>, </span>
<span id="cb296-12"><a href="regression-and-decision-tree-basic.html#cb296-12" aria-hidden="true" tabindex="-1"></a>                   <span class="at">tuneLength =</span> <span class="dv">10</span>, </span>
<span id="cb296-13"><a href="regression-and-decision-tree-basic.html#cb296-13" aria-hidden="true" tabindex="-1"></a>    <span class="at">trControl =</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">&quot;cv&quot;</span>))</span>
<span id="cb296-14"><a href="regression-and-decision-tree-basic.html#cb296-14" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(rpartTune)</span></code></pre></div>
<p><img src="IDS_files/figure-html/unnamed-chunk-139-1.svg" width="672" /></p>
<p>RMSE doesn’t change much when the maximum is larger than 2. So we set the maximum depth to be 2 and refit the model:</p>
<div class="sourceCode" id="cb297"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb297-1"><a href="regression-and-decision-tree-basic.html#cb297-1" aria-hidden="true" tabindex="-1"></a>rpartTree <span class="ot">&lt;-</span> <span class="fu">rpart</span>(trainy <span class="sc">~</span> ., <span class="at">data =</span> trainx, <span class="at">maxdepth =</span> <span class="dv">2</span>)</span></code></pre></div>
<p>You can check the result using <code>print()</code>:</p>
<div class="sourceCode" id="cb298"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb298-1"><a href="regression-and-decision-tree-basic.html#cb298-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(rpartTree)</span></code></pre></div>
<pre><code>## n= 999 
## 
## node), split, n, deviance, yval
##       * denotes terminal node
## 
## 1) root 999 1.581e+10  3479.0  
##   2) Q3&lt; 3.5 799 2.374e+09  1819.0  
##     4) Q5&lt; 1.5 250 3.534e+06   705.2 *
##     5) Q5&gt;=1.5 549 1.919e+09  2326.0 *
##   3) Q3&gt;=3.5 200 2.436e+09 10110.0 *</code></pre>
<p>You can see that the final model picks <code>Q3</code> and <code>Q5</code> to predict total expenditure. To visualize the tree, you can convert <code>rpart</code> object to <code>party</code> object using <code>partykit</code> then use <code>plot()</code> function:</p>
<div class="sourceCode" id="cb300"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb300-1"><a href="regression-and-decision-tree-basic.html#cb300-1" aria-hidden="true" tabindex="-1"></a>rpartTree2 <span class="ot">&lt;-</span> <span class="fu">as.party</span>(rpartTree)</span>
<span id="cb300-2"><a href="regression-and-decision-tree-basic.html#cb300-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(rpartTree2)</span></code></pre></div>
<p><img src="IDS_files/figure-html/unnamed-chunk-142-1.svg" width="672" /></p>
</div>
<div id="decision-tree" class="section level3 hasAnchor" number="11.4.2">
<h3><span class="header-section-number">11.4.2</span> Decision Tree<a href="regression-and-decision-tree-basic.html#decision-tree" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Similar to a regression tree, the goal of a classification tree is to stratifying the predictor space into a number of sub-regions that are more homogeneous. The difference is that a classification tree is used to predict a categorical response rather than a continuous one. For a classification tree, the prediction is the most commonly occurring class of training observations in the region to which an observation belongs. The splitting criteria for a classification tree are different. The most common criteria are entropy and Gini impurity. CART uses Gini impurity and C4.5 uses entropy.</p>
<p>When the predictor is continuous, the splitting process is straightforward. When the predictor is categorical, the process can take different approaches:</p>
<ol style="list-style-type: decimal">
<li>Keep the variable as categorical and group some categories on either side of the split. In this way, the model can make more dynamic splits but must treat the categorical predictor as an ordered set of bits.</li>
<li>Use one-hot encoding (figure <a href="regression-and-decision-tree-basic.html#fig:onehotencoding">11.3</a>). Encode the categorical variable as a set of dummy (binary) variables. The model considers these dummy variables separately and evaluates each of these on one split point (because there are only two possible values: 0/1). This way, the information in the categorical variable is decomposed into independent bits of information.</li>
</ol>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:onehotencoding"></span>
<img src="images/one_hot_encoding.png" alt="One-hot encoding" width="100%" />
<p class="caption">
FIGURE 11.3: One-hot encoding
</p>
</div>
<p>When fitting tree models, people need to choose the way to treat categorical predictors. If you know some of the categories have higher predictability, then the first approach may be better. In the rest of this section, we will build tree models using the above two approaches and compare them.</p>
<p>Let’s build a classification model to identify the gender of the customer:</p>
<div class="sourceCode" id="cb301"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb301-1"><a href="regression-and-decision-tree-basic.html#cb301-1" aria-hidden="true" tabindex="-1"></a>dat <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;http://bit.ly/2P5gTw4&quot;</span>)</span>
<span id="cb301-2"><a href="regression-and-decision-tree-basic.html#cb301-2" aria-hidden="true" tabindex="-1"></a><span class="co"># use the 10 survey questions as predictors</span></span>
<span id="cb301-3"><a href="regression-and-decision-tree-basic.html#cb301-3" aria-hidden="true" tabindex="-1"></a>trainx1 <span class="ot">&lt;-</span> dat[, <span class="fu">grep</span>(<span class="st">&quot;Q&quot;</span>, <span class="fu">names</span>(dat))]</span>
<span id="cb301-4"><a href="regression-and-decision-tree-basic.html#cb301-4" aria-hidden="true" tabindex="-1"></a><span class="co"># add a categorical predictor</span></span>
<span id="cb301-5"><a href="regression-and-decision-tree-basic.html#cb301-5" aria-hidden="true" tabindex="-1"></a><span class="co"># use two ways to treat categorical predictor</span></span>
<span id="cb301-6"><a href="regression-and-decision-tree-basic.html#cb301-6" aria-hidden="true" tabindex="-1"></a><span class="co"># trainx1: use approach 1, without encoding</span></span>
<span id="cb301-7"><a href="regression-and-decision-tree-basic.html#cb301-7" aria-hidden="true" tabindex="-1"></a>trainx1<span class="sc">$</span>segment <span class="ot">&lt;-</span> dat<span class="sc">$</span>segment</span>
<span id="cb301-8"><a href="regression-and-decision-tree-basic.html#cb301-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb301-9"><a href="regression-and-decision-tree-basic.html#cb301-9" aria-hidden="true" tabindex="-1"></a><span class="co"># trainx2: use approach 2, encode it to a set of dummy variables</span></span>
<span id="cb301-10"><a href="regression-and-decision-tree-basic.html#cb301-10" aria-hidden="true" tabindex="-1"></a>dumMod <span class="ot">&lt;-</span> <span class="fu">dummyVars</span>(</span>
<span id="cb301-11"><a href="regression-and-decision-tree-basic.html#cb301-11" aria-hidden="true" tabindex="-1"></a>  <span class="sc">~</span>.,</span>
<span id="cb301-12"><a href="regression-and-decision-tree-basic.html#cb301-12" aria-hidden="true" tabindex="-1"></a>  <span class="at">data =</span> trainx1,</span>
<span id="cb301-13"><a href="regression-and-decision-tree-basic.html#cb301-13" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Combine the previous variable and the level name</span></span>
<span id="cb301-14"><a href="regression-and-decision-tree-basic.html#cb301-14" aria-hidden="true" tabindex="-1"></a>  <span class="co"># as the new dummy variable name</span></span>
<span id="cb301-15"><a href="regression-and-decision-tree-basic.html#cb301-15" aria-hidden="true" tabindex="-1"></a>  <span class="at">levelsOnly =</span> F</span>
<span id="cb301-16"><a href="regression-and-decision-tree-basic.html#cb301-16" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb301-17"><a href="regression-and-decision-tree-basic.html#cb301-17" aria-hidden="true" tabindex="-1"></a>trainx2 <span class="ot">&lt;-</span> <span class="fu">predict</span>(dumMod, trainx1)</span>
<span id="cb301-18"><a href="regression-and-decision-tree-basic.html#cb301-18" aria-hidden="true" tabindex="-1"></a><span class="co"># the response variable is gender</span></span>
<span id="cb301-19"><a href="regression-and-decision-tree-basic.html#cb301-19" aria-hidden="true" tabindex="-1"></a>trainy <span class="ot">&lt;-</span> dat<span class="sc">$</span>gender</span>
<span id="cb301-20"><a href="regression-and-decision-tree-basic.html#cb301-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb301-21"><a href="regression-and-decision-tree-basic.html#cb301-21" aria-hidden="true" tabindex="-1"></a><span class="co"># check outcome balance</span></span>
<span id="cb301-22"><a href="regression-and-decision-tree-basic.html#cb301-22" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(dat<span class="sc">$</span>gender) <span class="sc">%&gt;%</span> <span class="fu">prop.table</span>()</span></code></pre></div>
<pre><code>## 
## Female   Male 
##  0.554  0.446</code></pre>
<p>The outcome is pretty balanced, with 55% female and 45% male. We use <code>train()</code> function in <code>caret</code> package to call <code>rpart</code> to build the model. We can compare the model results from the two approaches:</p>
<pre class="pre"><code>CART 

1000 samples
  11 predictor
   2 classes: &#39;Female&#39;, &#39;Male&#39; 

No pre-processing
Resampling: Cross-Validated (10 fold) 
Summary of sample sizes: 901, 899, 900, 900, 901, 900, ... 
Resampling results across tuning parameters:

  cp       ROC     Sens    Spec  
  0.00000  0.6937  0.6517  0.6884
  0.00835  0.7026  0.6119  0.7355
  0.01670  0.6852  0.5324  0.8205
  0.02505  0.6803  0.5107  0.8498
  0.03340  0.6803  0.5107  0.8498
  
  ......

  0.23380  0.6341  0.5936  0.6745
  0.24215  0.5556  0.7873  0.3240

ROC was used to select the optimal model using the largest value.
The final value used for the model was cp = 0.00835.</code></pre>
<p>The above keeps the variable as categorical without encoding. Here <code>cp</code> is the complexity parameter. It is used to decide when to stop growing the tree. <code>cp = 0.01</code> means the algorithm only keeps the split that improves the corresponding metric by more than 0.01. Next, let’s encode the categorical variable to be a set of dummy variables and fit the model again:</p>
<div class="sourceCode" id="cb304"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb304-1"><a href="regression-and-decision-tree-basic.html#cb304-1" aria-hidden="true" tabindex="-1"></a>rpartTune2 <span class="ot">&lt;-</span> caret<span class="sc">::</span><span class="fu">train</span>(</span>
<span id="cb304-2"><a href="regression-and-decision-tree-basic.html#cb304-2" aria-hidden="true" tabindex="-1"></a>  trainx2, trainy, <span class="at">method =</span> <span class="st">&quot;rpart&quot;</span>,</span>
<span id="cb304-3"><a href="regression-and-decision-tree-basic.html#cb304-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">tuneLength =</span> <span class="dv">30</span>,</span>
<span id="cb304-4"><a href="regression-and-decision-tree-basic.html#cb304-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">metric =</span> <span class="st">&quot;ROC&quot;</span>, </span>
<span id="cb304-5"><a href="regression-and-decision-tree-basic.html#cb304-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">trControl =</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">&quot;cv&quot;</span>,</span>
<span id="cb304-6"><a href="regression-and-decision-tree-basic.html#cb304-6" aria-hidden="true" tabindex="-1"></a>                           <span class="at">summaryFunction =</span> twoClassSummary,</span>
<span id="cb304-7"><a href="regression-and-decision-tree-basic.html#cb304-7" aria-hidden="true" tabindex="-1"></a>                           <span class="at">classProbs =</span> <span class="cn">TRUE</span>,</span>
<span id="cb304-8"><a href="regression-and-decision-tree-basic.html#cb304-8" aria-hidden="true" tabindex="-1"></a>                           <span class="at">savePredictions =</span> <span class="cn">TRUE</span>)</span>
<span id="cb304-9"><a href="regression-and-decision-tree-basic.html#cb304-9" aria-hidden="true" tabindex="-1"></a>  )</span></code></pre></div>
<p>Compare the results of the two approaches.</p>
<div class="sourceCode" id="cb305"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb305-1"><a href="regression-and-decision-tree-basic.html#cb305-1" aria-hidden="true" tabindex="-1"></a>rpartRoc <span class="ot">&lt;-</span> pROC<span class="sc">::</span><span class="fu">roc</span>(<span class="at">response =</span> rpartTune1<span class="sc">$</span>pred<span class="sc">$</span>obs,</span>
<span id="cb305-2"><a href="regression-and-decision-tree-basic.html#cb305-2" aria-hidden="true" tabindex="-1"></a>                <span class="at">predictor =</span> rpartTune1<span class="sc">$</span>pred<span class="sc">$</span>Female,</span>
<span id="cb305-3"><a href="regression-and-decision-tree-basic.html#cb305-3" aria-hidden="true" tabindex="-1"></a>                <span class="at">levels =</span> <span class="fu">rev</span>(<span class="fu">levels</span>(rpartTune1<span class="sc">$</span>pred<span class="sc">$</span>obs)))</span>
<span id="cb305-4"><a href="regression-and-decision-tree-basic.html#cb305-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb305-5"><a href="regression-and-decision-tree-basic.html#cb305-5" aria-hidden="true" tabindex="-1"></a>rpartFactorRoc <span class="ot">&lt;-</span> pROC<span class="sc">::</span><span class="fu">roc</span>(<span class="at">response =</span> rpartTune2<span class="sc">$</span>pred<span class="sc">$</span>obs,</span>
<span id="cb305-6"><a href="regression-and-decision-tree-basic.html#cb305-6" aria-hidden="true" tabindex="-1"></a>                      <span class="at">predictor =</span> rpartTune2<span class="sc">$</span>pred<span class="sc">$</span>Female,</span>
<span id="cb305-7"><a href="regression-and-decision-tree-basic.html#cb305-7" aria-hidden="true" tabindex="-1"></a>                      <span class="at">levels =</span> <span class="fu">rev</span>(<span class="fu">levels</span>(rpartTune1<span class="sc">$</span>pred<span class="sc">$</span>obs)))</span>
<span id="cb305-8"><a href="regression-and-decision-tree-basic.html#cb305-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb305-9"><a href="regression-and-decision-tree-basic.html#cb305-9" aria-hidden="true" tabindex="-1"></a><span class="fu">plot.roc</span>(rpartRoc, </span>
<span id="cb305-10"><a href="regression-and-decision-tree-basic.html#cb305-10" aria-hidden="true" tabindex="-1"></a>     <span class="at">type =</span> <span class="st">&quot;s&quot;</span>, </span>
<span id="cb305-11"><a href="regression-and-decision-tree-basic.html#cb305-11" aria-hidden="true" tabindex="-1"></a>     <span class="at">print.thres =</span> <span class="fu">c</span>(.<span class="dv">5</span>),</span>
<span id="cb305-12"><a href="regression-and-decision-tree-basic.html#cb305-12" aria-hidden="true" tabindex="-1"></a>     <span class="at">print.thres.pch =</span> <span class="dv">3</span>,</span>
<span id="cb305-13"><a href="regression-and-decision-tree-basic.html#cb305-13" aria-hidden="true" tabindex="-1"></a>     <span class="at">print.thres.pattern =</span> <span class="st">&quot;&quot;</span>,</span>
<span id="cb305-14"><a href="regression-and-decision-tree-basic.html#cb305-14" aria-hidden="true" tabindex="-1"></a>     <span class="at">print.thres.cex =</span> <span class="fl">1.2</span>,</span>
<span id="cb305-15"><a href="regression-and-decision-tree-basic.html#cb305-15" aria-hidden="true" tabindex="-1"></a>     <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">legacy.axes =</span> <span class="cn">TRUE</span>,</span>
<span id="cb305-16"><a href="regression-and-decision-tree-basic.html#cb305-16" aria-hidden="true" tabindex="-1"></a>     <span class="at">print.thres.col =</span> <span class="st">&quot;red&quot;</span>)</span>
<span id="cb305-17"><a href="regression-and-decision-tree-basic.html#cb305-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb305-18"><a href="regression-and-decision-tree-basic.html#cb305-18" aria-hidden="true" tabindex="-1"></a><span class="fu">plot.roc</span>(rpartFactorRoc,</span>
<span id="cb305-19"><a href="regression-and-decision-tree-basic.html#cb305-19" aria-hidden="true" tabindex="-1"></a>     <span class="at">type =</span> <span class="st">&quot;s&quot;</span>,</span>
<span id="cb305-20"><a href="regression-and-decision-tree-basic.html#cb305-20" aria-hidden="true" tabindex="-1"></a>     <span class="at">add =</span> <span class="cn">TRUE</span>,</span>
<span id="cb305-21"><a href="regression-and-decision-tree-basic.html#cb305-21" aria-hidden="true" tabindex="-1"></a>     <span class="at">print.thres =</span> <span class="fu">c</span>(.<span class="dv">5</span>),</span>
<span id="cb305-22"><a href="regression-and-decision-tree-basic.html#cb305-22" aria-hidden="true" tabindex="-1"></a>     <span class="at">print.thres.pch =</span> <span class="dv">16</span>, <span class="at">legacy.axes =</span> <span class="cn">TRUE</span>,</span>
<span id="cb305-23"><a href="regression-and-decision-tree-basic.html#cb305-23" aria-hidden="true" tabindex="-1"></a>     <span class="at">print.thres.pattern =</span> <span class="st">&quot;&quot;</span>,</span>
<span id="cb305-24"><a href="regression-and-decision-tree-basic.html#cb305-24" aria-hidden="true" tabindex="-1"></a>     <span class="at">print.thres.cex =</span> <span class="fl">1.2</span>)</span>
<span id="cb305-25"><a href="regression-and-decision-tree-basic.html#cb305-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb305-26"><a href="regression-and-decision-tree-basic.html#cb305-26" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(.<span class="dv">75</span>, .<span class="dv">2</span>,</span>
<span id="cb305-27"><a href="regression-and-decision-tree-basic.html#cb305-27" aria-hidden="true" tabindex="-1"></a>       <span class="fu">c</span>(<span class="st">&quot;Grouped Categories&quot;</span>, <span class="st">&quot;Independent Categories&quot;</span>),</span>
<span id="cb305-28"><a href="regression-and-decision-tree-basic.html#cb305-28" aria-hidden="true" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">1</span>),</span>
<span id="cb305-29"><a href="regression-and-decision-tree-basic.html#cb305-29" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;black&quot;</span>, <span class="st">&quot;red&quot;</span>),</span>
<span id="cb305-30"><a href="regression-and-decision-tree-basic.html#cb305-30" aria-hidden="true" tabindex="-1"></a>       <span class="at">pch =</span> <span class="fu">c</span>(<span class="dv">16</span>, <span class="dv">3</span>))</span></code></pre></div>
<p><img src="IDS_files/figure-html/unnamed-chunk-146-1.svg" width="672" /></p>
<p>In this case, the two approaches lead to similar model performance.</p>
<p>Single tree is straightforward and easy to interpret but it has problems:</p>
<ol style="list-style-type: decimal">
<li>Low accuracy</li>
<li>Unstable: little change in the training data leads to very different trees.</li>
</ol>
<p>One way to overcome those is to use an ensemble of trees. In the rest of this chapter, we will introduce three ensemble methods (combine many models’ predictions): bagging tree, random forest, and gradient boosted machine. Those ensemble approaches have significant higher accuracy and stability. However, it comes with the cost of interpretability.</p>
</div>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-ISLR15" class="csl-entry">
Gareth James, Trevor Hastie, Daniela Witten, and Robert Tibshirani. 2015. <em>An Introduction to Statistical Learning</em>. 6th ed. Springer.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="tree-pruning.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="bagging-tree-1.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": true,
"twitter": true,
"linkedin": true,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/happyrabbit/IntroDataScience/11-tree.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["IDS.pdf", "IDS.epub", "IDS.mobi"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section",
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
