<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>9.2 Principal Component Regression and Partial Least Square | Practitioner’s Guide to Data Science</title>
  <meta name="description" content="Introduction to Data Science" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="9.2 Principal Component Regression and Partial Least Square | Practitioner’s Guide to Data Science" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="http://scientistcafe.com/IDS/" />
  
  <meta property="og:description" content="Introduction to Data Science" />
  <meta name="github-repo" content="happyrabbit/IntroDataScience" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="9.2 Principal Component Regression and Partial Least Square | Practitioner’s Guide to Data Science" />
  
  <meta name="twitter:description" content="Introduction to Data Science" />
  

<meta name="author" content="Hui Lin and Ming Li" />


<meta name="date" content="2023-02-27" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="ordinary-least-square.html"/>
<link rel="next" href="regularization-methods.html"/>
<script src="libs/header-attrs/header-attrs.js"></script>
<script src="libs/jquery/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="http://scientistcafe.com">Homepage</a></li>

<li class="divider"></li>
<li><a href="index.html#preface" id="toc-preface">Preface</a>
<ul>
<li><a href="goal-of-the-book.html#goal-of-the-book" id="toc-goal-of-the-book">Goal of the Book</a></li>
<li><a href="what-this-book-covers.html#what-this-book-covers" id="toc-what-this-book-covers">What This Book Covers</a></li>
<li><a href="who-this-book-is-for.html#who-this-book-is-for" id="toc-who-this-book-is-for">Who This Book Is For</a></li>
<li><a href="how-to-use-this-book.html#how-to-use-this-book" id="toc-how-to-use-this-book">How to Use This Book</a>
<ul>
<li><a href="how-to-use-this-book.html#what-the-book-assumes" id="toc-what-the-book-assumes">What the Book Assumes</a></li>
<li><a href="how-to-use-this-book.html#how-to-run-r-and-python-code" id="toc-how-to-run-r-and-python-code">How to Run R and Python Code</a></li>
</ul></li>
<li><a href="complementary-reading.html#complementary-reading" id="toc-complementary-reading">Complementary Reading</a></li>
</ul></li>
<li><a href="about-the-authors.html#about-the-authors" id="toc-about-the-authors">About the Authors</a></li>
<li><a href="introduction.html#introduction" id="toc-introduction"><span class="toc-section-number">1</span> Introduction</a>
<ul>
<li><a href="a-brief-history-of-data-science.html#a-brief-history-of-data-science" id="toc-a-brief-history-of-data-science"><span class="toc-section-number">1.1</span> A Brief History of Data Science</a></li>
<li><a href="data-science-role-and-skill-tracks.html#data-science-role-and-skill-tracks" id="toc-data-science-role-and-skill-tracks"><span class="toc-section-number">1.2</span> Data Science Role and Skill Tracks</a>
<ul>
<li><a href="data-science-role-and-skill-tracks.html#engineering" id="toc-engineering"><span class="toc-section-number">1.2.1</span> Engineering</a></li>
<li><a href="data-science-role-and-skill-tracks.html#analysis" id="toc-analysis"><span class="toc-section-number">1.2.2</span> Analysis</a></li>
<li><a href="data-science-role-and-skill-tracks.html#modelinginference" id="toc-modelinginference"><span class="toc-section-number">1.2.3</span> Modeling/Inference</a></li>
</ul></li>
<li><a href="what-kind-of-questions-can-data-science-solve.html#what-kind-of-questions-can-data-science-solve" id="toc-what-kind-of-questions-can-data-science-solve"><span class="toc-section-number">1.3</span> What Kind of Questions Can Data Science Solve?</a>
<ul>
<li><a href="what-kind-of-questions-can-data-science-solve.html#prerequisites" id="toc-prerequisites"><span class="toc-section-number">1.3.1</span> Prerequisites</a></li>
<li><a href="what-kind-of-questions-can-data-science-solve.html#problem-type" id="toc-problem-type"><span class="toc-section-number">1.3.2</span> Problem Type</a></li>
</ul></li>
<li><a href="structure-of-data-science-team.html#structure-of-data-science-team" id="toc-structure-of-data-science-team"><span class="toc-section-number">1.4</span> Structure of Data Science Team</a></li>
<li><a href="data-science-roles.html#data-science-roles" id="toc-data-science-roles"><span class="toc-section-number">1.5</span> Data Science Roles</a></li>
</ul></li>
<li><a href="SoftSkillsforDataScientists.html#SoftSkillsforDataScientists" id="toc-SoftSkillsforDataScientists"><span class="toc-section-number">2</span> Soft Skills for Data Scientists</a>
<ul>
<li><a href="comparison-between-statistician-and-data-scientist.html#comparison-between-statistician-and-data-scientist" id="toc-comparison-between-statistician-and-data-scientist"><span class="toc-section-number">2.1</span> Comparison between Statistician and Data Scientist</a></li>
<li><a href="beyond-data-and-analytics.html#beyond-data-and-analytics" id="toc-beyond-data-and-analytics"><span class="toc-section-number">2.2</span> Beyond Data and Analytics</a></li>
<li><a href="three-pillars-of-knowledge.html#three-pillars-of-knowledge" id="toc-three-pillars-of-knowledge"><span class="toc-section-number">2.3</span> Three Pillars of Knowledge</a></li>
<li><a href="data-science-project-cycle.html#data-science-project-cycle" id="toc-data-science-project-cycle"><span class="toc-section-number">2.4</span> Data Science Project Cycle</a>
<ul>
<li><a href="data-science-project-cycle.html#types-of-data-science-projects" id="toc-types-of-data-science-projects"><span class="toc-section-number">2.4.1</span> Types of Data Science Projects</a></li>
<li><a href="data-science-project-cycle.html#ProblemFormulationandProjectPlanningStage" id="toc-ProblemFormulationandProjectPlanningStage"><span class="toc-section-number">2.4.2</span> Problem Formulation and Project Planning Stage</a></li>
<li><a href="data-science-project-cycle.html#project-modeling-stage" id="toc-project-modeling-stage"><span class="toc-section-number">2.4.3</span> Project Modeling Stage</a></li>
<li><a href="data-science-project-cycle.html#ModelImplementationandPostProductionStage" id="toc-ModelImplementationandPostProductionStage"><span class="toc-section-number">2.4.4</span> Model Implementation and Post Production Stage</a></li>
<li><a href="data-science-project-cycle.html#ProjectCycleSummary" id="toc-ProjectCycleSummary"><span class="toc-section-number">2.4.5</span> Project Cycle Summary</a></li>
</ul></li>
<li><a href="common-mistakes-in-data-science.html#common-mistakes-in-data-science" id="toc-common-mistakes-in-data-science"><span class="toc-section-number">2.5</span> Common Mistakes in Data Science</a>
<ul>
<li><a href="common-mistakes-in-data-science.html#problem-formulation-stage" id="toc-problem-formulation-stage"><span class="toc-section-number">2.5.1</span> Problem Formulation Stage</a></li>
<li><a href="common-mistakes-in-data-science.html#project-planning-stage" id="toc-project-planning-stage"><span class="toc-section-number">2.5.2</span> Project Planning Stage</a></li>
<li><a href="common-mistakes-in-data-science.html#project-modeling-stage-1" id="toc-project-modeling-stage-1"><span class="toc-section-number">2.5.3</span> Project Modeling Stage</a></li>
<li><a href="common-mistakes-in-data-science.html#model-implementation-and-post-production-stage" id="toc-model-implementation-and-post-production-stage"><span class="toc-section-number">2.5.4</span> Model Implementation and Post Production Stage</a></li>
<li><a href="common-mistakes-in-data-science.html#summary-of-common-mistakes" id="toc-summary-of-common-mistakes"><span class="toc-section-number">2.5.5</span> Summary of Common Mistakes</a></li>
</ul></li>
</ul></li>
<li><a href="introduction-to-the-data.html#introduction-to-the-data" id="toc-introduction-to-the-data"><span class="toc-section-number">3</span> Introduction to The Data</a>
<ul>
<li><a href="customer-data-for-a-clothing-company.html#customer-data-for-a-clothing-company" id="toc-customer-data-for-a-clothing-company"><span class="toc-section-number">3.1</span> Customer Data for A Clothing Company</a></li>
<li><a href="swinediseasedata.html#swinediseasedata" id="toc-swinediseasedata"><span class="toc-section-number">3.2</span> Swine Disease Breakout Data</a></li>
<li><a href="mnist-dataset.html#mnist-dataset" id="toc-mnist-dataset"><span class="toc-section-number">3.3</span> MNIST Dataset</a></li>
<li><a href="imdb-dataset.html#imdb-dataset" id="toc-imdb-dataset"><span class="toc-section-number">3.4</span> IMDB Dataset</a></li>
</ul></li>
<li><a href="bigdatacloudplatform.html#bigdatacloudplatform" id="toc-bigdatacloudplatform"><span class="toc-section-number">4</span> Big Data Cloud Platform</a>
<ul>
<li><a href="power-of-cluster-of-computers.html#power-of-cluster-of-computers" id="toc-power-of-cluster-of-computers"><span class="toc-section-number">4.1</span> Power of Cluster of Computers</a></li>
<li><a href="evolution-of-cluster-computing.html#evolution-of-cluster-computing" id="toc-evolution-of-cluster-computing"><span class="toc-section-number">4.2</span> Evolution of Cluster Computing</a>
<ul>
<li><a href="evolution-of-cluster-computing.html#hadoop" id="toc-hadoop"><span class="toc-section-number">4.2.1</span> Hadoop</a></li>
<li><a href="evolution-of-cluster-computing.html#spark" id="toc-spark"><span class="toc-section-number">4.2.2</span> Spark</a></li>
</ul></li>
<li><a href="CloudEnvironment.html#CloudEnvironment" id="toc-CloudEnvironment"><span class="toc-section-number">4.3</span> Introduction of Cloud Environment</a>
<ul>
<li><a href="CloudEnvironment.html#open-account-and-create-a-cluster" id="toc-open-account-and-create-a-cluster"><span class="toc-section-number">4.3.1</span> Open Account and Create a Cluster</a></li>
<li><a href="CloudEnvironment.html#r-notebook" id="toc-r-notebook"><span class="toc-section-number">4.3.2</span> R Notebook</a></li>
<li><a href="CloudEnvironment.html#markdown-cells" id="toc-markdown-cells"><span class="toc-section-number">4.3.3</span> Markdown Cells</a></li>
</ul></li>
<li><a href="leveragesparkr.html#leveragesparkr" id="toc-leveragesparkr"><span class="toc-section-number">4.4</span> Leverage Spark Using R Notebook</a></li>
<li><a href="databases-and-sql.html#databases-and-sql" id="toc-databases-and-sql"><span class="toc-section-number">4.5</span> Databases and SQL</a>
<ul>
<li><a href="databases-and-sql.html#history" id="toc-history"><span class="toc-section-number">4.5.1</span> History</a></li>
<li><a href="databases-and-sql.html#database-table-and-view" id="toc-database-table-and-view"><span class="toc-section-number">4.5.2</span> Database, Table and View</a></li>
<li><a href="databases-and-sql.html#basic-sql-statement" id="toc-basic-sql-statement"><span class="toc-section-number">4.5.3</span> Basic SQL Statement</a></li>
<li><a href="databases-and-sql.html#advanced-topics-in-database" id="toc-advanced-topics-in-database"><span class="toc-section-number">4.5.4</span> Advanced Topics in Database</a></li>
</ul></li>
</ul></li>
<li><a href="datapreprocessing.html#datapreprocessing" id="toc-datapreprocessing"><span class="toc-section-number">5</span> Data Pre-processing</a>
<ul>
<li><a href="data-cleaning.html#data-cleaning" id="toc-data-cleaning"><span class="toc-section-number">5.1</span> Data Cleaning</a></li>
<li><a href="missing-values.html#missing-values" id="toc-missing-values"><span class="toc-section-number">5.2</span> Missing Values</a>
<ul>
<li><a href="missing-values.html#impute-missing-values-with-medianmode" id="toc-impute-missing-values-with-medianmode"><span class="toc-section-number">5.2.1</span> Impute missing values with median/mode</a></li>
<li><a href="missing-values.html#k-nearest-neighbors" id="toc-k-nearest-neighbors"><span class="toc-section-number">5.2.2</span> K-nearest neighbors</a></li>
<li><a href="missing-values.html#bagging-tree" id="toc-bagging-tree"><span class="toc-section-number">5.2.3</span> Bagging Tree</a></li>
</ul></li>
<li><a href="centering-and-scaling.html#centering-and-scaling" id="toc-centering-and-scaling"><span class="toc-section-number">5.3</span> Centering and Scaling</a></li>
<li><a href="resolve-skewness.html#resolve-skewness" id="toc-resolve-skewness"><span class="toc-section-number">5.4</span> Resolve Skewness</a></li>
<li><a href="outliers.html#outliers" id="toc-outliers"><span class="toc-section-number">5.5</span> Resolve Outliers</a></li>
<li><a href="collinearity.html#collinearity" id="toc-collinearity"><span class="toc-section-number">5.6</span> Collinearity</a></li>
<li><a href="sparse-variables.html#sparse-variables" id="toc-sparse-variables"><span class="toc-section-number">5.7</span> Sparse Variables</a></li>
<li><a href="re-encode-dummy-variables.html#re-encode-dummy-variables" id="toc-re-encode-dummy-variables"><span class="toc-section-number">5.8</span> Re-encode Dummy Variables</a></li>
</ul></li>
<li><a href="datawrangline.html#datawrangline" id="toc-datawrangline"><span class="toc-section-number">6</span> Data Wrangling</a>
<ul>
<li><a href="summarize-data.html#summarize-data" id="toc-summarize-data"><span class="toc-section-number">6.1</span> Summarize Data</a>
<ul>
<li><a href="summarize-data.html#dplyr-package" id="toc-dplyr-package"><span class="toc-section-number">6.1.1</span> <code>dplyr</code> package</a></li>
<li><a href="summarize-data.html#applyfamilyinbaser" id="toc-applyfamilyinbaser"><span class="toc-section-number">6.1.2</span> <code>apply()</code>, <code>lapply()</code> and <code>sapply()</code> in base R</a></li>
</ul></li>
<li><a href="tidy-and-reshape-data.html#tidy-and-reshape-data" id="toc-tidy-and-reshape-data"><span class="toc-section-number">6.2</span> Tidy and Reshape Data</a></li>
</ul></li>
<li><a href="modeltuningstrategy.html#modeltuningstrategy" id="toc-modeltuningstrategy"><span class="toc-section-number">7</span> Model Tuning Strategy</a>
<ul>
<li><a href="vbtradeoff.html#vbtradeoff" id="toc-vbtradeoff"><span class="toc-section-number">7.1</span> Variance-Bias Trade-Off</a></li>
<li><a href="datasplittingresampling.html#datasplittingresampling" id="toc-datasplittingresampling"><span class="toc-section-number">7.2</span> Data Splitting and Resampling</a>
<ul>
<li><a href="datasplittingresampling.html#datasplitting" id="toc-datasplitting"><span class="toc-section-number">7.2.1</span> Data Splitting</a></li>
<li><a href="datasplittingresampling.html#resampling" id="toc-resampling"><span class="toc-section-number">7.2.2</span> Resampling</a></li>
</ul></li>
</ul></li>
<li><a href="measuring-performance.html#measuring-performance" id="toc-measuring-performance"><span class="toc-section-number">8</span> Measuring Performance</a>
<ul>
<li><a href="regression-model-performance.html#regression-model-performance" id="toc-regression-model-performance"><span class="toc-section-number">8.1</span> Regression Model Performance</a></li>
<li><a href="classification-model-performance.html#classification-model-performance" id="toc-classification-model-performance"><span class="toc-section-number">8.2</span> Classification Model Performance</a>
<ul>
<li><a href="classification-model-performance.html#confusion-matrix" id="toc-confusion-matrix"><span class="toc-section-number">8.2.1</span> Confusion Matrix</a></li>
<li><a href="classification-model-performance.html#kappa-statistic" id="toc-kappa-statistic"><span class="toc-section-number">8.2.2</span> Kappa Statistic</a></li>
<li><a href="classification-model-performance.html#roc" id="toc-roc"><span class="toc-section-number">8.2.3</span> ROC</a></li>
<li><a href="classification-model-performance.html#gain-and-lift-charts" id="toc-gain-and-lift-charts"><span class="toc-section-number">8.2.4</span> Gain and Lift Charts</a></li>
</ul></li>
</ul></li>
<li><a href="regression-models.html#regression-models" id="toc-regression-models"><span class="toc-section-number">9</span> Regression Models</a>
<ul>
<li><a href="ordinary-least-square.html#ordinary-least-square" id="toc-ordinary-least-square"><span class="toc-section-number">9.1</span> Ordinary Least Square</a>
<ul>
<li><a href="ordinary-least-square.html#the-magic-p-value" id="toc-the-magic-p-value"><span class="toc-section-number">9.1.1</span> The Magic P-value</a></li>
<li><a href="ordinary-least-square.html#diagnostics-for-linear-regression" id="toc-diagnostics-for-linear-regression"><span class="toc-section-number">9.1.2</span> Diagnostics for Linear Regression</a></li>
</ul></li>
<li><a href="principal-component-regression-and-partial-least-square.html#principal-component-regression-and-partial-least-square" id="toc-principal-component-regression-and-partial-least-square"><span class="toc-section-number">9.2</span> Principal Component Regression and Partial Least Square</a></li>
</ul></li>
<li><a href="regularization-methods.html#regularization-methods" id="toc-regularization-methods"><span class="toc-section-number">10</span> Regularization Methods</a>
<ul>
<li><a href="ridge-regression.html#ridge-regression" id="toc-ridge-regression"><span class="toc-section-number">10.1</span> Ridge Regression</a></li>
<li><a href="lasso.html#lasso" id="toc-lasso"><span class="toc-section-number">10.2</span> LASSO</a></li>
<li><a href="elastic-net.html#elastic-net" id="toc-elastic-net"><span class="toc-section-number">10.3</span> Elastic Net</a></li>
<li><a href="penalized-generalized-linear-model.html#penalized-generalized-linear-model" id="toc-penalized-generalized-linear-model"><span class="toc-section-number">10.4</span> Penalized Generalized Linear Model</a>
<ul>
<li><a href="penalized-generalized-linear-model.html#introduction-to-glmnet-package" id="toc-introduction-to-glmnet-package"><span class="toc-section-number">10.4.1</span> Introduction to <code>glmnet</code> package</a></li>
<li><a href="penalized-generalized-linear-model.html#penalized-logistic-regression" id="toc-penalized-logistic-regression"><span class="toc-section-number">10.4.2</span> Penalized logistic regression</a></li>
</ul></li>
</ul></li>
<li><a href="treemodel.html#treemodel" id="toc-treemodel"><span class="toc-section-number">11</span> Tree-Based Methods</a>
<ul>
<li><a href="tree-basics.html#tree-basics" id="toc-tree-basics"><span class="toc-section-number">11.1</span> Tree Basics</a></li>
<li><a href="splitting-criteria.html#splitting-criteria" id="toc-splitting-criteria"><span class="toc-section-number">11.2</span> Splitting Criteria</a>
<ul>
<li><a href="splitting-criteria.html#gini-impurity" id="toc-gini-impurity"><span class="toc-section-number">11.2.1</span> Gini impurity</a></li>
<li><a href="splitting-criteria.html#information-gain-ig" id="toc-information-gain-ig"><span class="toc-section-number">11.2.2</span> Information Gain (IG)</a></li>
<li><a href="splitting-criteria.html#information-gain-ratio-igr" id="toc-information-gain-ratio-igr"><span class="toc-section-number">11.2.3</span> Information Gain Ratio (IGR)</a></li>
<li><a href="splitting-criteria.html#sum-of-squared-error-sse" id="toc-sum-of-squared-error-sse"><span class="toc-section-number">11.2.4</span> Sum of Squared Error (SSE)</a></li>
</ul></li>
<li><a href="tree-pruning.html#tree-pruning" id="toc-tree-pruning"><span class="toc-section-number">11.3</span> Tree Pruning</a></li>
<li><a href="regression-and-decision-tree-basic.html#regression-and-decision-tree-basic" id="toc-regression-and-decision-tree-basic"><span class="toc-section-number">11.4</span> Regression and Decision Tree Basic</a>
<ul>
<li><a href="regression-and-decision-tree-basic.html#regression-tree" id="toc-regression-tree"><span class="toc-section-number">11.4.1</span> Regression Tree</a></li>
<li><a href="regression-and-decision-tree-basic.html#decision-tree" id="toc-decision-tree"><span class="toc-section-number">11.4.2</span> Decision Tree</a></li>
</ul></li>
<li><a href="bagging-tree-1.html#bagging-tree-1" id="toc-bagging-tree-1"><span class="toc-section-number">11.5</span> Bagging Tree</a></li>
<li><a href="random-forest.html#random-forest" id="toc-random-forest"><span class="toc-section-number">11.6</span> Random Forest</a></li>
<li><a href="gradient-boosted-machine.html#gradient-boosted-machine" id="toc-gradient-boosted-machine"><span class="toc-section-number">11.7</span> Gradient Boosted Machine</a>
<ul>
<li><a href="gradient-boosted-machine.html#adaptive-boosting" id="toc-adaptive-boosting"><span class="toc-section-number">11.7.1</span> Adaptive Boosting</a></li>
<li><a href="gradient-boosted-machine.html#stochastic-gradient-boosting" id="toc-stochastic-gradient-boosting"><span class="toc-section-number">11.7.2</span> Stochastic Gradient Boosting</a></li>
</ul></li>
</ul></li>
<li><a href="deeplearning.html#deeplearning" id="toc-deeplearning"><span class="toc-section-number">12</span> Deep Learning</a>
<ul>
<li><a href="feedforward-neural-network.html#feedforward-neural-network" id="toc-feedforward-neural-network"><span class="toc-section-number">12.1</span> Feedforward Neural Network</a>
<ul>
<li><a href="feedforward-neural-network.html#logisticregasneuralnetwork" id="toc-logisticregasneuralnetwork"><span class="toc-section-number">12.1.1</span> Logistic Regression as Neural Network</a></li>
<li><a href="feedforward-neural-network.html#stochastic-gradient-descent" id="toc-stochastic-gradient-descent"><span class="toc-section-number">12.1.2</span> Stochastic Gradient Descent</a></li>
<li><a href="feedforward-neural-network.html#deepneuralnetwork" id="toc-deepneuralnetwork"><span class="toc-section-number">12.1.3</span> Deep Neural Network</a></li>
<li><a href="feedforward-neural-network.html#activationfunction" id="toc-activationfunction"><span class="toc-section-number">12.1.4</span> Activation Function</a></li>
<li><a href="feedforward-neural-network.html#optimization" id="toc-optimization"><span class="toc-section-number">12.1.5</span> Optimization</a></li>
<li><a href="feedforward-neural-network.html#deal-with-overfitting" id="toc-deal-with-overfitting"><span class="toc-section-number">12.1.6</span> Deal with Overfitting</a></li>
<li><a href="feedforward-neural-network.html#ffnnexample" id="toc-ffnnexample"><span class="toc-section-number">12.1.7</span> Image Recognition Using FFNN</a></li>
</ul></li>
<li><a href="convolutional-neural-network.html#convolutional-neural-network" id="toc-convolutional-neural-network"><span class="toc-section-number">12.2</span> Convolutional Neural Network</a>
<ul>
<li><a href="convolutional-neural-network.html#convolution-layer" id="toc-convolution-layer"><span class="toc-section-number">12.2.1</span> Convolution Layer</a></li>
<li><a href="convolutional-neural-network.html#padding-layer" id="toc-padding-layer"><span class="toc-section-number">12.2.2</span> Padding Layer</a></li>
<li><a href="convolutional-neural-network.html#pooling-layer" id="toc-pooling-layer"><span class="toc-section-number">12.2.3</span> Pooling Layer</a></li>
<li><a href="convolutional-neural-network.html#convolution-over-volume" id="toc-convolution-over-volume"><span class="toc-section-number">12.2.4</span> Convolution Over Volume</a></li>
<li><a href="convolutional-neural-network.html#cnnexample" id="toc-cnnexample"><span class="toc-section-number">12.2.5</span> Image Recognition Using CNN</a></li>
</ul></li>
<li><a href="recurrent-neural-network.html#recurrent-neural-network" id="toc-recurrent-neural-network"><span class="toc-section-number">12.3</span> Recurrent Neural Network</a>
<ul>
<li><a href="recurrent-neural-network.html#rnn-model" id="toc-rnn-model"><span class="toc-section-number">12.3.1</span> RNN Model</a></li>
<li><a href="recurrent-neural-network.html#lstm" id="toc-lstm"><span class="toc-section-number">12.3.2</span> Long Short Term Memory</a></li>
<li><a href="recurrent-neural-network.html#embedding" id="toc-embedding"><span class="toc-section-number">12.3.3</span> Word Embedding</a></li>
<li><a href="recurrent-neural-network.html#rnnexample" id="toc-rnnexample"><span class="toc-section-number">12.3.4</span> Sentiment Analysis Using RNN</a></li>
</ul></li>
</ul></li>
<li><a href="appendix.html#appendix" id="toc-appendix">Appendix</a></li>
<li><a href="largelocaldata.html#largelocaldata" id="toc-largelocaldata"><span class="toc-section-number">13</span> Handling Large Local Data</a>
<ul>
<li><a href="readr.html#readr" id="toc-readr"><span class="toc-section-number">13.1</span> <code>readr</code></a></li>
<li><a href="data.table-enhanced-data.html#data.table-enhanced-data.frame" id="toc-data.table-enhanced-data.frame"><span class="toc-section-number">13.2</span> <code>data.table</code>— enhanced <code>data.frame</code></a></li>
</ul></li>
<li><a href="r-code-for-data-simulation.html#r-code-for-data-simulation" id="toc-r-code-for-data-simulation"><span class="toc-section-number">14</span> R code for data simulation</a>
<ul>
<li><a href="appendixdata1.html#appendixdata1" id="toc-appendixdata1"><span class="toc-section-number">14.1</span> Customer Data for Clothing Company</a></li>
<li><a href="appendixdata3.html#appendixdata3" id="toc-appendixdata3"><span class="toc-section-number">14.2</span> Swine Disease Breakout Data</a></li>
</ul></li>
<li><a href="references.html#references" id="toc-references">References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Practitioner’s Guide to Data Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="principal-component-regression-and-partial-least-square" class="section level2 hasAnchor" number="9.2">
<h2><span class="header-section-number">9.2</span> Principal Component Regression and Partial Least Square<a href="principal-component-regression-and-partial-least-square.html#principal-component-regression-and-partial-least-square" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In real-life applications, explanatory variables are usually related to each other, containing similar information. For example, in the previous chapter, we used expenditure variables to predict consumer income. In that model, store expenditure (<code>store_exp</code>), online expenditure (<code>online_exp</code>), number of store transactions (<code>store_trans</code>), and number of online transactions (<code>online_trans</code>) are correlated to a certain extent, especially the number of transactions and expenditure. If there is a strong correlation among explanatory variables, then the least square-based linear regression model may not be robust. If the number of observations is less than the number of explanatory variables, the standard least square method cannot provide a unique set of coefficient estimates. We can perform data preprocessing, such as remove highly correlated variables with a preset threshold for such cases. However, this approach cannot guarantee a low correlation of the linear combination of the variables with other variables. In that case, the standard least square method will still not be robust. We need to be aware that removing highly correlated variables is not always guarantee a robust solution. We can also apply feature engineering procedures to explanatory variables such as principal component analysis (PCA). By using principal components, we can ensure they are uncorrelated with each other. However, the drawback of using PCA is that these components are linear combinations of original variables, and it is difficult to explain the fitted model. Principal component regression (PCR) is described in more detail in <span class="citation">(<a href="#ref-Massy1965" role="doc-biblioref">W 1965</a>)</span>. It can be used when there are strong correlations among variables or when the number of observations is less than the number of variables.</p>
<p>In theory, we can use PCR to reduce the number of variables used in a linear model, but the results are not good. Because the first a few principal components may not be good candidates to model the response variable. PCA is unsupervised learning such that the entire process does not consider the response variable. In PCA, it only focuses on the variability of explanatory variables. When the independent variables and response variables are related, PCR can well identify the systematic relationship between them. However, when there exist independent variables not associated with response variable, it will undermine PCR’s performance. We need to be aware that PCR does not make feature selections, and each principal component is a combination of original variables.</p>
<p>Partial least square regression (PLS) is the supervised version of PCR. Similar to PCR, PLS can also reduce the number of variables in the regression model. As PLS is also related to the variables’ variance, we usually standardize or normalize variables before PLS. Suppose we have a list of explanatory variables <span class="math inline">\(\mathbf{X}=[X_{1},X_{2},...,X_{p}]^{T}\)</span>, and their variance-covariance matrix is <span class="math inline">\(\Sigma\)</span>. PLS also transforms the original variables using linear combination to new uncorrelated variables <span class="math inline">\((Z_{1} , Z_{2} , \ldots , Z_{m})\)</span>. When <span class="math inline">\(m=p\)</span>, the result of PLS is the same as OLS. The main difference between PCR and PLS is the process of creating new variables. PLS considers the response variable.</p>
<p>PLS is from Herman Wold’s Nonlinear Iterative Partial Least Squares (NIPALS) algorithm <span class="citation">(<a href="#ref-wold1973" role="doc-biblioref">Wold 1973</a>; <a href="#ref-wold1982" role="doc-biblioref">Wold and Jöreskog 1982</a>)</span> . Later NIPALS was applied to regression problems, which was then called PLS . PLS is a method of linearizing nonlinear relationships through hidden layers. It is similar to the PCR, except that PCR does not take into account the information of the dependent variable when selecting the components. PCR’s purpose is to find the linear combinations (i.e., unsupervised) that capture the most variance of the independent variables, while PLS maximizes the linear combination of dependencies with the response variable. In the current case, the more complicated PLS does not perform better than simple linear regression. We will not discuss the PLS algorithm’s detail, and the reference mentioned above provides a more detailed description of the algorithm.</p>
<p>We focus on using R library <code>caret</code> to fit PCR and PLS models. Let us use the 10 survey questions (<code>Q1</code>-<code>Q10</code>) as the explanatory variables and income as the response variable. First load the data and preprocessing the data:</p>
<div class="sourceCode" id="cb216"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb216-1"><a href="principal-component-regression-and-partial-least-square.html#cb216-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(lattice)</span>
<span id="cb216-2"><a href="principal-component-regression-and-partial-least-square.html#cb216-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(caret)</span>
<span id="cb216-3"><a href="principal-component-regression-and-partial-least-square.html#cb216-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb216-4"><a href="principal-component-regression-and-partial-least-square.html#cb216-4" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(elasticnet)</span>
<span id="cb216-5"><a href="principal-component-regression-and-partial-least-square.html#cb216-5" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(lars)</span>
<span id="cb216-6"><a href="principal-component-regression-and-partial-least-square.html#cb216-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb216-7"><a href="principal-component-regression-and-partial-least-square.html#cb216-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Load Data</span></span>
<span id="cb216-8"><a href="principal-component-regression-and-partial-least-square.html#cb216-8" aria-hidden="true" tabindex="-1"></a>sim.dat <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;http://bit.ly/2P5gTw4&quot;</span>)</span>
<span id="cb216-9"><a href="principal-component-regression-and-partial-least-square.html#cb216-9" aria-hidden="true" tabindex="-1"></a>ymad <span class="ot">&lt;-</span> <span class="fu">mad</span>(<span class="fu">na.omit</span>(sim.dat<span class="sc">$</span>income))</span>
<span id="cb216-10"><a href="principal-component-regression-and-partial-least-square.html#cb216-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb216-11"><a href="principal-component-regression-and-partial-least-square.html#cb216-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate Z values</span></span>
<span id="cb216-12"><a href="principal-component-regression-and-partial-least-square.html#cb216-12" aria-hidden="true" tabindex="-1"></a>zs <span class="ot">&lt;-</span> (sim.dat<span class="sc">$</span>income <span class="sc">-</span> <span class="fu">mean</span>(<span class="fu">na.omit</span>(sim.dat<span class="sc">$</span>income)))<span class="sc">/</span>ymad</span>
<span id="cb216-13"><a href="principal-component-regression-and-partial-least-square.html#cb216-13" aria-hidden="true" tabindex="-1"></a><span class="co"># which(na.omit(zs&gt;3.5)) find outlier </span></span>
<span id="cb216-14"><a href="principal-component-regression-and-partial-least-square.html#cb216-14" aria-hidden="true" tabindex="-1"></a><span class="co"># which(is.na(zs)) find missing values</span></span>
<span id="cb216-15"><a href="principal-component-regression-and-partial-least-square.html#cb216-15" aria-hidden="true" tabindex="-1"></a>idex <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">which</span>(<span class="fu">na.omit</span>(zs <span class="sc">&gt;</span> <span class="fl">3.5</span>)), <span class="fu">which</span>(<span class="fu">is.na</span>(zs)))</span>
<span id="cb216-16"><a href="principal-component-regression-and-partial-least-square.html#cb216-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Remove rows with outlier and missing values</span></span>
<span id="cb216-17"><a href="principal-component-regression-and-partial-least-square.html#cb216-17" aria-hidden="true" tabindex="-1"></a>sim.dat <span class="ot">&lt;-</span> sim.dat[<span class="sc">-</span>idex, ]</span></code></pre></div>
<p>Now let’s define explanatory variable matrix (<code>xtrain</code>) by selecting these 10 survey questions columns, and define response variable (<code>ytrain</code>):</p>
<div class="sourceCode" id="cb217"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb217-1"><a href="principal-component-regression-and-partial-least-square.html#cb217-1" aria-hidden="true" tabindex="-1"></a>xtrain <span class="ot">=</span> dplyr<span class="sc">::</span><span class="fu">select</span>(sim.dat, Q1<span class="sc">:</span>Q10)</span>
<span id="cb217-2"><a href="principal-component-regression-and-partial-least-square.html#cb217-2" aria-hidden="true" tabindex="-1"></a>ytrain <span class="ot">=</span> sim.dat<span class="sc">$</span>income</span></code></pre></div>
<p>We also set up random seed and 10-folder cross-validation:</p>
<div class="sourceCode" id="cb218"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb218-1"><a href="principal-component-regression-and-partial-least-square.html#cb218-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">100</span>)</span>
<span id="cb218-2"><a href="principal-component-regression-and-partial-least-square.html#cb218-2" aria-hidden="true" tabindex="-1"></a>ctrl <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="at">number =</span> <span class="dv">10</span>)</span></code></pre></div>
<p>Fit PLS model using number of explanatory variables as the hyper-parameter to tune. As there are at most 10 explanatory variables in the model, we set up the hyper-parameter tuning range to be 1 to 10:</p>
<div class="sourceCode" id="cb219"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb219-1"><a href="principal-component-regression-and-partial-least-square.html#cb219-1" aria-hidden="true" tabindex="-1"></a>plsTune <span class="ot">&lt;-</span> <span class="fu">train</span>(xtrain, ytrain, </span>
<span id="cb219-2"><a href="principal-component-regression-and-partial-least-square.html#cb219-2" aria-hidden="true" tabindex="-1"></a>                 <span class="at">method =</span> <span class="st">&quot;pls&quot;</span>, </span>
<span id="cb219-3"><a href="principal-component-regression-and-partial-least-square.html#cb219-3" aria-hidden="true" tabindex="-1"></a>                 <span class="co"># set hyper-parameter tuning range</span></span>
<span id="cb219-4"><a href="principal-component-regression-and-partial-least-square.html#cb219-4" aria-hidden="true" tabindex="-1"></a>                 <span class="at">tuneGrid =</span> <span class="fu">expand.grid</span>(<span class="at">.ncomp =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>),</span>
<span id="cb219-5"><a href="principal-component-regression-and-partial-least-square.html#cb219-5" aria-hidden="true" tabindex="-1"></a>                 <span class="at">trControl =</span> ctrl)</span>
<span id="cb219-6"><a href="principal-component-regression-and-partial-least-square.html#cb219-6" aria-hidden="true" tabindex="-1"></a>plsTune</span></code></pre></div>
<pre><code>## Partial Least Squares 
## 
## 772 samples
##  10 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 696, 693, 694, 694, 696, 695, ... 
## Resampling results across tuning parameters:
## 
##   ncomp  RMSE   Rsquared  MAE  
##    1     27777  0.6534    19845
##    2     24420  0.7320    15976
##    3     23175  0.7590    14384
##    4     23011  0.7625    13808
##    5     22977  0.7631    13737
##    6     22978  0.7631    13729
##    7     22976  0.7631    13726
##    8     22976  0.7631    13726
##    9     22976  0.7631    13726
##   10     22976  0.7631    13726
## 
## RMSE was used to select the optimal model using
##  the smallest value.
## The final value used for the model was ncomp = 7.</code></pre>
<p>From the result, we can see that the optimal number of variables is 7. However, if we pay attention to the RMSE improvement, we will find only minimum improvement in RMSE after three variables. In practice, we could choose to use the model with three variables if the improvement does not make a practical difference, and we would rather have a simpler model. </p>
<p>We can also find the relative importance of each variable during PLS model tuning process, as described using the following code: </p>
<div class="sourceCode" id="cb221"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb221-1"><a href="principal-component-regression-and-partial-least-square.html#cb221-1" aria-hidden="true" tabindex="-1"></a>plsImp <span class="ot">&lt;-</span> <span class="fu">varImp</span>(plsTune, <span class="at">scale =</span> <span class="cn">FALSE</span>)</span>
<span id="cb221-2"><a href="principal-component-regression-and-partial-least-square.html#cb221-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(plsImp, <span class="at">top =</span> <span class="dv">10</span>, <span class="at">scales =</span> <span class="fu">list</span>(<span class="at">y =</span> <span class="fu">list</span>(<span class="at">cex =</span> <span class="fl">0.95</span>)))</span></code></pre></div>
<p><img src="IDS_files/figure-html/unnamed-chunk-95-1.svg" width="672" /></p>
<p>The above plot shows that Q1, Q2, Q3, and Q6, are more important than other variables. Now let’s fit a PCR model with number of principal components as the hyper-parameter: </p>
<div class="sourceCode" id="cb222"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb222-1"><a href="principal-component-regression-and-partial-least-square.html#cb222-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Set random seed</span></span>
<span id="cb222-2"><a href="principal-component-regression-and-partial-least-square.html#cb222-2" aria-hidden="true" tabindex="-1"></a> <span class="fu">set.seed</span>(<span class="dv">100</span>)</span>
<span id="cb222-3"><a href="principal-component-regression-and-partial-least-square.html#cb222-3" aria-hidden="true" tabindex="-1"></a> pcrTune <span class="ot">&lt;-</span> <span class="fu">train</span>(<span class="at">x =</span> xtrain, <span class="at">y =</span> ytrain,</span>
<span id="cb222-4"><a href="principal-component-regression-and-partial-least-square.html#cb222-4" aria-hidden="true" tabindex="-1"></a>          <span class="at">method =</span> <span class="st">&quot;pcr&quot;</span>,</span>
<span id="cb222-5"><a href="principal-component-regression-and-partial-least-square.html#cb222-5" aria-hidden="true" tabindex="-1"></a>          <span class="co"># set hyper-parameter tuning range</span></span>
<span id="cb222-6"><a href="principal-component-regression-and-partial-least-square.html#cb222-6" aria-hidden="true" tabindex="-1"></a>          <span class="at">tuneGrid =</span> <span class="fu">expand.grid</span>(<span class="at">.ncomp =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>),</span>
<span id="cb222-7"><a href="principal-component-regression-and-partial-least-square.html#cb222-7" aria-hidden="true" tabindex="-1"></a>          <span class="at">trControl =</span> ctrl)</span>
<span id="cb222-8"><a href="principal-component-regression-and-partial-least-square.html#cb222-8" aria-hidden="true" tabindex="-1"></a> pcrTune</span></code></pre></div>
<pre><code>## Principal Component Analysis 
## 
## 772 samples
##  10 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 696, 693, 694, 694, 696, 695, ... 
## Resampling results across tuning parameters:
## 
##   ncomp  RMSE   Rsquared  MAE  
##    1     45958  0.03243   36599
##    2     32460  0.52200   24041
##    3     23235  0.75774   14516
##    4     23262  0.75735   14545
##    5     23152  0.75957   14232
##    6     23133  0.76004   14130
##    7     23114  0.76049   14129
##    8     23115  0.76045   14130
##    9     22991  0.76283   13801
##   10     22976  0.76308   13726
## 
## RMSE was used to select the optimal model using
##  the smallest value.
## The final value used for the model was ncomp = 10.</code></pre>
<p>From the output, the default recommendation is ten components. However, if we pay attention to RMSE improvement with more components, we will find little difference after the model with three components. Again, in practice, we can keep models with three components.</p>
<p>Now let’s compare the hyper-parameter tuning process for PLS and PCR:</p>
<div class="sourceCode" id="cb224"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb224-1"><a href="principal-component-regression-and-partial-least-square.html#cb224-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Save PLS model tuning information to plsResamples</span></span>
<span id="cb224-2"><a href="principal-component-regression-and-partial-least-square.html#cb224-2" aria-hidden="true" tabindex="-1"></a>plsResamples <span class="ot">&lt;-</span> plsTune<span class="sc">$</span>results</span>
<span id="cb224-3"><a href="principal-component-regression-and-partial-least-square.html#cb224-3" aria-hidden="true" tabindex="-1"></a>plsResamples<span class="sc">$</span>Model <span class="ot">&lt;-</span> <span class="st">&quot;PLS&quot;</span></span>
<span id="cb224-4"><a href="principal-component-regression-and-partial-least-square.html#cb224-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Save PCR model tuning information to plsResamples</span></span>
<span id="cb224-5"><a href="principal-component-regression-and-partial-least-square.html#cb224-5" aria-hidden="true" tabindex="-1"></a>pcrResamples <span class="ot">&lt;-</span> pcrTune<span class="sc">$</span>results</span>
<span id="cb224-6"><a href="principal-component-regression-and-partial-least-square.html#cb224-6" aria-hidden="true" tabindex="-1"></a>pcrResamples<span class="sc">$</span>Model <span class="ot">&lt;-</span> <span class="st">&quot;PCR&quot;</span></span>
<span id="cb224-7"><a href="principal-component-regression-and-partial-least-square.html#cb224-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Combine both output for plotting</span></span>
<span id="cb224-8"><a href="principal-component-regression-and-partial-least-square.html#cb224-8" aria-hidden="true" tabindex="-1"></a>plsPlotData <span class="ot">&lt;-</span> <span class="fu">rbind</span>(plsResamples, pcrResamples)</span>
<span id="cb224-9"><a href="principal-component-regression-and-partial-least-square.html#cb224-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Leverage xyplot() function from lattice library</span></span>
<span id="cb224-10"><a href="principal-component-regression-and-partial-least-square.html#cb224-10" aria-hidden="true" tabindex="-1"></a><span class="fu">xyplot</span>(RMSE <span class="sc">~</span> ncomp, </span>
<span id="cb224-11"><a href="principal-component-regression-and-partial-least-square.html#cb224-11" aria-hidden="true" tabindex="-1"></a>       <span class="at">data =</span> plsPlotData, </span>
<span id="cb224-12"><a href="principal-component-regression-and-partial-least-square.html#cb224-12" aria-hidden="true" tabindex="-1"></a>       <span class="at">xlab =</span> <span class="st">&quot;# Components&quot;</span>, </span>
<span id="cb224-13"><a href="principal-component-regression-and-partial-least-square.html#cb224-13" aria-hidden="true" tabindex="-1"></a>       <span class="at">ylab =</span> <span class="st">&quot;RMSE (Cross-Validation)&quot;</span>, </span>
<span id="cb224-14"><a href="principal-component-regression-and-partial-least-square.html#cb224-14" aria-hidden="true" tabindex="-1"></a>       <span class="at">auto.key =</span> <span class="fu">list</span>(<span class="at">columns =</span> <span class="dv">2</span>), </span>
<span id="cb224-15"><a href="principal-component-regression-and-partial-least-square.html#cb224-15" aria-hidden="true" tabindex="-1"></a>       <span class="at">groups =</span> Model, </span>
<span id="cb224-16"><a href="principal-component-regression-and-partial-least-square.html#cb224-16" aria-hidden="true" tabindex="-1"></a>       <span class="at">type =</span> <span class="fu">c</span>(<span class="st">&quot;o&quot;</span>, <span class="st">&quot;g&quot;</span>))</span></code></pre></div>
<p><img src="IDS_files/figure-html/unnamed-chunk-97-1.svg" width="672" /></p>
<p>The plot confirms our choice of using a model with three components for both PLS and PCR.</p>

</div>
<!-- </div> -->
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-Massy1965" class="csl-entry">
W, Massy. 1965. <span>“Principal Components Regression in Exploratory Statistical Research.”</span> <em>Journal of the American Statistical Association</em> 60: 234–46.
</div>
<div id="ref-wold1973" class="csl-entry">
Wold, Herman. 1973. <span>“Nonlinear Iterative Partial Least Squares (NIPALS) Modelling: Some Current Developments.”</span> <em>Academic Press</em>, 383–407.
</div>
<div id="ref-wold1982" class="csl-entry">
Wold, Herman, and K. G. Jöreskog. 1982. <em>Systems Under Indirect Observation: Causality, Structure, Prediction</em>. North Holland, Amsterdam.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="ordinary-least-square.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="regularization-methods.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": true,
"twitter": true,
"linkedin": true,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/happyrabbit/IntroDataScience/09-Regression.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["IDS.pdf", "IDS.epub", "IDS.mobi"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section",
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
