<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>9.2 Principal Component Regression and Partial Least Square | Introduction to Data Science</title>
  <meta name="description" content="9.2 Principal Component Regression and Partial Least Square | Introduction to Data Science" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="9.2 Principal Component Regression and Partial Least Square | Introduction to Data Science" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="http://scientistcafe.com/IDS/" />
  
  <meta property="og:description" content="9.2 Principal Component Regression and Partial Least Square | Introduction to Data Science" />
  <meta name="github-repo" content="happyrabbit/IntroDataScience" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="9.2 Principal Component Regression and Partial Least Square | Introduction to Data Science" />
  
  <meta name="twitter:description" content="9.2 Principal Component Regression and Partial Least Square | Introduction to Data Science" />
  

<meta name="author" content="Hui Lin and Ming Li" />


<meta name="date" content="2022-10-21" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="ordinary-least-square.html"/>
<link rel="next" href="regularization-methods.html"/>
<script src="libs/header-attrs/header-attrs.js"></script>
<script src="libs/jquery/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="http://scientistcafe.com">Homepage</a></li>

<li class="divider"></li>
<li><a href="index.html#preface">Preface<span></span></a>
<ul>
<li><a href="goal-of-the-book.html#goal-of-the-book">Goal of the Book<span></span></a></li>
<li><a href="what-this-book-covers.html#what-this-book-covers">What This Book Covers<span></span></a></li>
<li><a href="who-this-book-is-for.html#who-this-book-is-for">Who This Book Is For<span></span></a></li>
<li><a href="how-to-use-this-book.html#how-to-use-this-book">How to Use This Book<span></span></a>
<ul>
<li><a href="how-to-use-this-book.html#what-the-book-assumes">What the book assumes<span></span></a></li>
<li><a href="how-to-use-this-book.html#how-to-run-r-and-python-code">How to run R and Python code<span></span></a></li>
</ul></li>
<li><a href="complementary-reading.html#complementary-reading">Complementary Reading<span></span></a></li>
</ul></li>
<li><a href="about-the-authors.html#about-the-authors">About the Authors<span></span></a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction<span></span></a>
<ul>
<li class="chapter" data-level="1.1" data-path="a-brief-history-of-data-science.html"><a href="a-brief-history-of-data-science.html"><i class="fa fa-check"></i><b>1.1</b> A brief history of data science<span></span></a></li>
<li class="chapter" data-level="1.2" data-path="data-science-role-and-skill-tracks.html"><a href="data-science-role-and-skill-tracks.html"><i class="fa fa-check"></i><b>1.2</b> Data science role and skill tracks<span></span></a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="data-science-role-and-skill-tracks.html"><a href="data-science-role-and-skill-tracks.html#engineering"><i class="fa fa-check"></i><b>1.2.1</b> Engineering<span></span></a></li>
<li class="chapter" data-level="1.2.2" data-path="data-science-role-and-skill-tracks.html"><a href="data-science-role-and-skill-tracks.html#analysis"><i class="fa fa-check"></i><b>1.2.2</b> Analysis<span></span></a></li>
<li class="chapter" data-level="1.2.3" data-path="data-science-role-and-skill-tracks.html"><a href="data-science-role-and-skill-tracks.html#modelinginference"><i class="fa fa-check"></i><b>1.2.3</b> Modeling/inference<span></span></a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="what-kind-of-questions-can-data-science-solve.html"><a href="what-kind-of-questions-can-data-science-solve.html"><i class="fa fa-check"></i><b>1.3</b> What kind of questions can data science solve?<span></span></a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="what-kind-of-questions-can-data-science-solve.html"><a href="what-kind-of-questions-can-data-science-solve.html#prerequisites"><i class="fa fa-check"></i><b>1.3.1</b> Prerequisites<span></span></a></li>
<li class="chapter" data-level="1.3.2" data-path="what-kind-of-questions-can-data-science-solve.html"><a href="what-kind-of-questions-can-data-science-solve.html#problem-type"><i class="fa fa-check"></i><b>1.3.2</b> Problem type<span></span></a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="structure-of-data-science-team.html"><a href="structure-of-data-science-team.html"><i class="fa fa-check"></i><b>1.4</b> Structure of data science team<span></span></a></li>
<li class="chapter" data-level="1.5" data-path="data-science-roles.html"><a href="data-science-roles.html"><i class="fa fa-check"></i><b>1.5</b> Data science roles<span></span></a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="SoftSkillsforDataScientists.html"><a href="SoftSkillsforDataScientists.html"><i class="fa fa-check"></i><b>2</b> Soft Skills for Data Scientists<span></span></a>
<ul>
<li class="chapter" data-level="2.1" data-path="comparison-between-statistician-and-data-scientist.html"><a href="comparison-between-statistician-and-data-scientist.html"><i class="fa fa-check"></i><b>2.1</b> Comparison between Statistician and Data Scientist<span></span></a></li>
<li class="chapter" data-level="2.2" data-path="beyond-data-and-analytics.html"><a href="beyond-data-and-analytics.html"><i class="fa fa-check"></i><b>2.2</b> Beyond Data and Analytics<span></span></a></li>
<li class="chapter" data-level="2.3" data-path="three-pillars-of-knowledge.html"><a href="three-pillars-of-knowledge.html"><i class="fa fa-check"></i><b>2.3</b> Three Pillars of Knowledge<span></span></a></li>
<li class="chapter" data-level="2.4" data-path="data-science-project-cycle.html"><a href="data-science-project-cycle.html"><i class="fa fa-check"></i><b>2.4</b> Data Science Project Cycle<span></span></a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="data-science-project-cycle.html"><a href="data-science-project-cycle.html#types-of-data-science-projects"><i class="fa fa-check"></i><b>2.4.1</b> Types of Data Science Projects<span></span></a></li>
<li class="chapter" data-level="2.4.2" data-path="data-science-project-cycle.html"><a href="data-science-project-cycle.html#ProblemFormulationandProjectPlanningStage"><i class="fa fa-check"></i><b>2.4.2</b> Problem Formulation and Project Planning Stage<span></span></a></li>
<li class="chapter" data-level="2.4.3" data-path="data-science-project-cycle.html"><a href="data-science-project-cycle.html#project-modeling-stage"><i class="fa fa-check"></i><b>2.4.3</b> Project Modeling Stage<span></span></a></li>
<li class="chapter" data-level="2.4.4" data-path="data-science-project-cycle.html"><a href="data-science-project-cycle.html#ModelImplementationandPostProductionStage"><i class="fa fa-check"></i><b>2.4.4</b> Model Implementation and Post Production Stage<span></span></a></li>
<li class="chapter" data-level="2.4.5" data-path="data-science-project-cycle.html"><a href="data-science-project-cycle.html#ProjectCycleSummary"><i class="fa fa-check"></i><b>2.4.5</b> Project Cycle Summary<span></span></a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="common-mistakes-in-data-science.html"><a href="common-mistakes-in-data-science.html"><i class="fa fa-check"></i><b>2.5</b> Common Mistakes in Data Science<span></span></a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="common-mistakes-in-data-science.html"><a href="common-mistakes-in-data-science.html#problem-formulation-stage"><i class="fa fa-check"></i><b>2.5.1</b> Problem Formulation Stage<span></span></a></li>
<li class="chapter" data-level="2.5.2" data-path="common-mistakes-in-data-science.html"><a href="common-mistakes-in-data-science.html#project-planning-stage"><i class="fa fa-check"></i><b>2.5.2</b> Project Planning Stage<span></span></a></li>
<li class="chapter" data-level="2.5.3" data-path="common-mistakes-in-data-science.html"><a href="common-mistakes-in-data-science.html#project-modeling-stage-1"><i class="fa fa-check"></i><b>2.5.3</b> Project Modeling Stage<span></span></a></li>
<li class="chapter" data-level="2.5.4" data-path="common-mistakes-in-data-science.html"><a href="common-mistakes-in-data-science.html#model-implementation-and-post-production-stage"><i class="fa fa-check"></i><b>2.5.4</b> Model Implementation and Post Production Stage<span></span></a></li>
<li class="chapter" data-level="2.5.5" data-path="common-mistakes-in-data-science.html"><a href="common-mistakes-in-data-science.html#summary-of-common-mistakes"><i class="fa fa-check"></i><b>2.5.5</b> Summary of Common Mistakes<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="introduction-to-the-data.html"><a href="introduction-to-the-data.html"><i class="fa fa-check"></i><b>3</b> Introduction to The Data<span></span></a>
<ul>
<li class="chapter" data-level="3.1" data-path="customer-data-for-a-clothing-company.html"><a href="customer-data-for-a-clothing-company.html"><i class="fa fa-check"></i><b>3.1</b> Customer Data for A Clothing Company<span></span></a></li>
<li class="chapter" data-level="3.2" data-path="swinediseasedata.html"><a href="swinediseasedata.html"><i class="fa fa-check"></i><b>3.2</b> Swine Disease Breakout Data<span></span></a></li>
<li class="chapter" data-level="3.3" data-path="mnist-dataset.html"><a href="mnist-dataset.html"><i class="fa fa-check"></i><b>3.3</b> MNIST Dataset<span></span></a></li>
<li class="chapter" data-level="3.4" data-path="imdb-dataset.html"><a href="imdb-dataset.html"><i class="fa fa-check"></i><b>3.4</b> IMDB Dataset<span></span></a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="bigdatacloudplatform.html"><a href="bigdatacloudplatform.html"><i class="fa fa-check"></i><b>4</b> Big Data Cloud Platform<span></span></a>
<ul>
<li class="chapter" data-level="4.1" data-path="power-of-cluster-of-computers.html"><a href="power-of-cluster-of-computers.html"><i class="fa fa-check"></i><b>4.1</b> Power of Cluster of Computers<span></span></a></li>
<li class="chapter" data-level="4.2" data-path="evolution-of-cluster-computing.html"><a href="evolution-of-cluster-computing.html"><i class="fa fa-check"></i><b>4.2</b> Evolution of Cluster Computing<span></span></a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="evolution-of-cluster-computing.html"><a href="evolution-of-cluster-computing.html#hadoop"><i class="fa fa-check"></i><b>4.2.1</b> Hadoop<span></span></a></li>
<li class="chapter" data-level="4.2.2" data-path="evolution-of-cluster-computing.html"><a href="evolution-of-cluster-computing.html#spark"><i class="fa fa-check"></i><b>4.2.2</b> Spark<span></span></a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="CloudEnvironment.html"><a href="CloudEnvironment.html"><i class="fa fa-check"></i><b>4.3</b> Introduction of Cloud Environment<span></span></a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="CloudEnvironment.html"><a href="CloudEnvironment.html#open-account-and-create-a-cluster"><i class="fa fa-check"></i><b>4.3.1</b> Open Account and Create a Cluster<span></span></a></li>
<li class="chapter" data-level="4.3.2" data-path="CloudEnvironment.html"><a href="CloudEnvironment.html#r-notebook"><i class="fa fa-check"></i><b>4.3.2</b> R Notebook<span></span></a></li>
<li class="chapter" data-level="4.3.3" data-path="CloudEnvironment.html"><a href="CloudEnvironment.html#markdown-cells"><i class="fa fa-check"></i><b>4.3.3</b> Markdown Cells<span></span></a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="leveragesparkr.html"><a href="leveragesparkr.html"><i class="fa fa-check"></i><b>4.4</b> Leverage Spark Using R Notebook<span></span></a></li>
<li class="chapter" data-level="4.5" data-path="databases-and-sql.html"><a href="databases-and-sql.html"><i class="fa fa-check"></i><b>4.5</b> Databases and SQL<span></span></a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="databases-and-sql.html"><a href="databases-and-sql.html#history"><i class="fa fa-check"></i><b>4.5.1</b> History<span></span></a></li>
<li class="chapter" data-level="4.5.2" data-path="databases-and-sql.html"><a href="databases-and-sql.html#database-table-and-view"><i class="fa fa-check"></i><b>4.5.2</b> Database, Table and View<span></span></a></li>
<li class="chapter" data-level="4.5.3" data-path="databases-and-sql.html"><a href="databases-and-sql.html#basic-sql-statement"><i class="fa fa-check"></i><b>4.5.3</b> Basic SQL Statement<span></span></a></li>
<li class="chapter" data-level="4.5.4" data-path="databases-and-sql.html"><a href="databases-and-sql.html#advanced-topics-in-database"><i class="fa fa-check"></i><b>4.5.4</b> Advanced Topics in Database<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="datapreprocessing.html"><a href="datapreprocessing.html"><i class="fa fa-check"></i><b>5</b> Data Pre-processing<span></span></a>
<ul>
<li class="chapter" data-level="5.1" data-path="data-cleaning.html"><a href="data-cleaning.html"><i class="fa fa-check"></i><b>5.1</b> Data Cleaning<span></span></a></li>
<li class="chapter" data-level="5.2" data-path="missing-values.html"><a href="missing-values.html"><i class="fa fa-check"></i><b>5.2</b> Missing Values<span></span></a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="missing-values.html"><a href="missing-values.html#impute-missing-values-with-medianmode"><i class="fa fa-check"></i><b>5.2.1</b> Impute missing values with median/mode<span></span></a></li>
<li class="chapter" data-level="5.2.2" data-path="missing-values.html"><a href="missing-values.html#k-nearest-neighbors"><i class="fa fa-check"></i><b>5.2.2</b> K-nearest neighbors<span></span></a></li>
<li class="chapter" data-level="5.2.3" data-path="missing-values.html"><a href="missing-values.html#bagging-tree"><i class="fa fa-check"></i><b>5.2.3</b> Bagging Tree<span></span></a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="centering-and-scaling.html"><a href="centering-and-scaling.html"><i class="fa fa-check"></i><b>5.3</b> Centering and Scaling<span></span></a></li>
<li class="chapter" data-level="5.4" data-path="resolve-skewness.html"><a href="resolve-skewness.html"><i class="fa fa-check"></i><b>5.4</b> Resolve Skewness<span></span></a></li>
<li class="chapter" data-level="5.5" data-path="outliers.html"><a href="outliers.html"><i class="fa fa-check"></i><b>5.5</b> Resolve Outliers<span></span></a></li>
<li class="chapter" data-level="5.6" data-path="collinearity.html"><a href="collinearity.html"><i class="fa fa-check"></i><b>5.6</b> Collinearity<span></span></a></li>
<li class="chapter" data-level="5.7" data-path="sparse-variables.html"><a href="sparse-variables.html"><i class="fa fa-check"></i><b>5.7</b> Sparse Variables<span></span></a></li>
<li class="chapter" data-level="5.8" data-path="re-encode-dummy-variables.html"><a href="re-encode-dummy-variables.html"><i class="fa fa-check"></i><b>5.8</b> Re-encode Dummy Variables<span></span></a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="datawrangline.html"><a href="datawrangline.html"><i class="fa fa-check"></i><b>6</b> Data Wrangling<span></span></a>
<ul>
<li class="chapter" data-level="6.1" data-path="summarize-data.html"><a href="summarize-data.html"><i class="fa fa-check"></i><b>6.1</b> Summarize Data<span></span></a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="summarize-data.html"><a href="summarize-data.html#dplyr-package"><i class="fa fa-check"></i><b>6.1.1</b> <code>dplyr</code> package<span></span></a></li>
<li class="chapter" data-level="6.1.2" data-path="summarize-data.html"><a href="summarize-data.html#applyfamilyinbaser"><i class="fa fa-check"></i><b>6.1.2</b> <code>apply()</code>, <code>lapply()</code> and <code>sapply()</code> in base R<span></span></a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="tidy-and-reshape-data.html"><a href="tidy-and-reshape-data.html"><i class="fa fa-check"></i><b>6.2</b> Tidy and Reshape Data<span></span></a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="modeltuningstrategy.html"><a href="modeltuningstrategy.html"><i class="fa fa-check"></i><b>7</b> Model Tuning Strategy<span></span></a>
<ul>
<li class="chapter" data-level="7.1" data-path="vbtradeoff.html"><a href="vbtradeoff.html"><i class="fa fa-check"></i><b>7.1</b> Variance-Bias Trade-Off<span></span></a></li>
<li class="chapter" data-level="7.2" data-path="datasplittingresampling.html"><a href="datasplittingresampling.html"><i class="fa fa-check"></i><b>7.2</b> Data Splitting and Resampling<span></span></a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="datasplittingresampling.html"><a href="datasplittingresampling.html#datasplitting"><i class="fa fa-check"></i><b>7.2.1</b> Data Splitting<span></span></a></li>
<li class="chapter" data-level="7.2.2" data-path="datasplittingresampling.html"><a href="datasplittingresampling.html#resampling"><i class="fa fa-check"></i><b>7.2.2</b> Resampling<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="measuring-performance.html"><a href="measuring-performance.html"><i class="fa fa-check"></i><b>8</b> Measuring Performance<span></span></a>
<ul>
<li class="chapter" data-level="8.1" data-path="regression-model-performance.html"><a href="regression-model-performance.html"><i class="fa fa-check"></i><b>8.1</b> Regression Model Performance<span></span></a></li>
<li class="chapter" data-level="8.2" data-path="classification-model-performance.html"><a href="classification-model-performance.html"><i class="fa fa-check"></i><b>8.2</b> Classification Model Performance<span></span></a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="classification-model-performance.html"><a href="classification-model-performance.html#confusion-matrix"><i class="fa fa-check"></i><b>8.2.1</b> Confusion Matrix<span></span></a></li>
<li class="chapter" data-level="8.2.2" data-path="classification-model-performance.html"><a href="classification-model-performance.html#kappa-statistic"><i class="fa fa-check"></i><b>8.2.2</b> Kappa Statistic<span></span></a></li>
<li class="chapter" data-level="8.2.3" data-path="classification-model-performance.html"><a href="classification-model-performance.html#roc"><i class="fa fa-check"></i><b>8.2.3</b> ROC<span></span></a></li>
<li class="chapter" data-level="8.2.4" data-path="classification-model-performance.html"><a href="classification-model-performance.html#gain-and-lift-charts"><i class="fa fa-check"></i><b>8.2.4</b> Gain and Lift Charts<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="regression-models.html"><a href="regression-models.html"><i class="fa fa-check"></i><b>9</b> Regression Models<span></span></a>
<ul>
<li class="chapter" data-level="9.1" data-path="ordinary-least-square.html"><a href="ordinary-least-square.html"><i class="fa fa-check"></i><b>9.1</b> Ordinary Least Square<span></span></a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="ordinary-least-square.html"><a href="ordinary-least-square.html#the-magic-p-value"><i class="fa fa-check"></i><b>9.1.1</b> The Magic P-value<span></span></a></li>
<li class="chapter" data-level="9.1.2" data-path="ordinary-least-square.html"><a href="ordinary-least-square.html#diagnostics-for-linear-regression"><i class="fa fa-check"></i><b>9.1.2</b> Diagnostics for Linear Regression<span></span></a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="principal-component-regression-and-partial-least-square.html"><a href="principal-component-regression-and-partial-least-square.html"><i class="fa fa-check"></i><b>9.2</b> Principal Component Regression and Partial Least Square<span></span></a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="regularization-methods.html"><a href="regularization-methods.html"><i class="fa fa-check"></i><b>10</b> Regularization Methods<span></span></a>
<ul>
<li class="chapter" data-level="10.1" data-path="ridge-regression.html"><a href="ridge-regression.html"><i class="fa fa-check"></i><b>10.1</b> Ridge Regression<span></span></a></li>
<li class="chapter" data-level="10.2" data-path="lasso.html"><a href="lasso.html"><i class="fa fa-check"></i><b>10.2</b> LASSO<span></span></a></li>
<li class="chapter" data-level="10.3" data-path="elastic-net.html"><a href="elastic-net.html"><i class="fa fa-check"></i><b>10.3</b> Elastic Net<span></span></a></li>
<li class="chapter" data-level="10.4" data-path="penalized-generalized-linear-model.html"><a href="penalized-generalized-linear-model.html"><i class="fa fa-check"></i><b>10.4</b> Penalized Generalized Linear Model<span></span></a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="penalized-generalized-linear-model.html"><a href="penalized-generalized-linear-model.html#introduction-to-glmnet-package"><i class="fa fa-check"></i><b>10.4.1</b> Introduction to <code>glmnet</code> package<span></span></a></li>
<li class="chapter" data-level="10.4.2" data-path="penalized-generalized-linear-model.html"><a href="penalized-generalized-linear-model.html#penalized-logistic-regression"><i class="fa fa-check"></i><b>10.4.2</b> Penalized logistic regression<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="treemodel.html"><a href="treemodel.html"><i class="fa fa-check"></i><b>11</b> Tree-Based Methods<span></span></a>
<ul>
<li class="chapter" data-level="11.1" data-path="tree-basics.html"><a href="tree-basics.html"><i class="fa fa-check"></i><b>11.1</b> Tree Basics<span></span></a></li>
<li class="chapter" data-level="11.2" data-path="splitting-criteria.html"><a href="splitting-criteria.html"><i class="fa fa-check"></i><b>11.2</b> Splitting Criteria<span></span></a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="splitting-criteria.html"><a href="splitting-criteria.html#gini-impurity"><i class="fa fa-check"></i><b>11.2.1</b> Gini impurity<span></span></a></li>
<li class="chapter" data-level="11.2.2" data-path="splitting-criteria.html"><a href="splitting-criteria.html#information-gain-ig"><i class="fa fa-check"></i><b>11.2.2</b> Information Gain (IG)<span></span></a></li>
<li class="chapter" data-level="11.2.3" data-path="splitting-criteria.html"><a href="splitting-criteria.html#information-gain-ratio-igr"><i class="fa fa-check"></i><b>11.2.3</b> Information Gain Ratio (IGR)<span></span></a></li>
<li class="chapter" data-level="11.2.4" data-path="splitting-criteria.html"><a href="splitting-criteria.html#sum-of-squared-error-sse"><i class="fa fa-check"></i><b>11.2.4</b> Sum of Squared Error (SSE)<span></span></a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="tree-pruning.html"><a href="tree-pruning.html"><i class="fa fa-check"></i><b>11.3</b> Tree Pruning<span></span></a></li>
<li class="chapter" data-level="11.4" data-path="regression-and-decision-tree-basic.html"><a href="regression-and-decision-tree-basic.html"><i class="fa fa-check"></i><b>11.4</b> Regression and Decision Tree Basic<span></span></a>
<ul>
<li class="chapter" data-level="11.4.1" data-path="regression-and-decision-tree-basic.html"><a href="regression-and-decision-tree-basic.html#regression-tree"><i class="fa fa-check"></i><b>11.4.1</b> Regression Tree<span></span></a></li>
<li class="chapter" data-level="11.4.2" data-path="regression-and-decision-tree-basic.html"><a href="regression-and-decision-tree-basic.html#decision-tree"><i class="fa fa-check"></i><b>11.4.2</b> Decision Tree<span></span></a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="bagging-tree-1.html"><a href="bagging-tree-1.html"><i class="fa fa-check"></i><b>11.5</b> Bagging Tree<span></span></a></li>
<li class="chapter" data-level="11.6" data-path="random-forest.html"><a href="random-forest.html"><i class="fa fa-check"></i><b>11.6</b> Random Forest<span></span></a></li>
<li class="chapter" data-level="11.7" data-path="gradient-boosted-machine.html"><a href="gradient-boosted-machine.html"><i class="fa fa-check"></i><b>11.7</b> Gradient Boosted Machine<span></span></a>
<ul>
<li class="chapter" data-level="11.7.1" data-path="gradient-boosted-machine.html"><a href="gradient-boosted-machine.html#adaptive-boosting"><i class="fa fa-check"></i><b>11.7.1</b> Adaptive Boosting<span></span></a></li>
<li class="chapter" data-level="11.7.2" data-path="gradient-boosted-machine.html"><a href="gradient-boosted-machine.html#stochastic-gradient-boosting"><i class="fa fa-check"></i><b>11.7.2</b> Stochastic Gradient Boosting<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="deeplearning.html"><a href="deeplearning.html"><i class="fa fa-check"></i><b>12</b> Deep Learning<span></span></a>
<ul>
<li class="chapter" data-level="12.1" data-path="feedforward-neural-network.html"><a href="feedforward-neural-network.html"><i class="fa fa-check"></i><b>12.1</b> Feedforward Neural Network<span></span></a>
<ul>
<li class="chapter" data-level="12.1.1" data-path="feedforward-neural-network.html"><a href="feedforward-neural-network.html#logisticregasneuralnetwork"><i class="fa fa-check"></i><b>12.1.1</b> Logistic Regression as Neural Network<span></span></a></li>
<li class="chapter" data-level="12.1.2" data-path="feedforward-neural-network.html"><a href="feedforward-neural-network.html#stochastic-gradient-descent"><i class="fa fa-check"></i><b>12.1.2</b> Stochastic Gradient Descent<span></span></a></li>
<li class="chapter" data-level="12.1.3" data-path="feedforward-neural-network.html"><a href="feedforward-neural-network.html#deepneuralnetwork"><i class="fa fa-check"></i><b>12.1.3</b> Deep Neural Network<span></span></a></li>
<li class="chapter" data-level="12.1.4" data-path="feedforward-neural-network.html"><a href="feedforward-neural-network.html#activationfunction"><i class="fa fa-check"></i><b>12.1.4</b> Activation Function<span></span></a></li>
<li class="chapter" data-level="12.1.5" data-path="feedforward-neural-network.html"><a href="feedforward-neural-network.html#optimization"><i class="fa fa-check"></i><b>12.1.5</b> Optimization<span></span></a></li>
<li class="chapter" data-level="12.1.6" data-path="feedforward-neural-network.html"><a href="feedforward-neural-network.html#deal-with-overfitting"><i class="fa fa-check"></i><b>12.1.6</b> Deal with Overfitting<span></span></a></li>
<li class="chapter" data-level="12.1.7" data-path="feedforward-neural-network.html"><a href="feedforward-neural-network.html#ffnnexample"><i class="fa fa-check"></i><b>12.1.7</b> Image Recognition Using FFNN<span></span></a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="convolutional-neural-network.html"><a href="convolutional-neural-network.html"><i class="fa fa-check"></i><b>12.2</b> Convolutional Neural Network<span></span></a>
<ul>
<li class="chapter" data-level="12.2.1" data-path="convolutional-neural-network.html"><a href="convolutional-neural-network.html#convolution-layer"><i class="fa fa-check"></i><b>12.2.1</b> Convolution Layer<span></span></a></li>
<li class="chapter" data-level="12.2.2" data-path="convolutional-neural-network.html"><a href="convolutional-neural-network.html#padding-layer"><i class="fa fa-check"></i><b>12.2.2</b> Padding Layer<span></span></a></li>
<li class="chapter" data-level="12.2.3" data-path="convolutional-neural-network.html"><a href="convolutional-neural-network.html#pooling-layer"><i class="fa fa-check"></i><b>12.2.3</b> Pooling Layer<span></span></a></li>
<li class="chapter" data-level="12.2.4" data-path="convolutional-neural-network.html"><a href="convolutional-neural-network.html#convolution-over-volume"><i class="fa fa-check"></i><b>12.2.4</b> Convolution Over Volume<span></span></a></li>
<li class="chapter" data-level="12.2.5" data-path="convolutional-neural-network.html"><a href="convolutional-neural-network.html#cnnexample"><i class="fa fa-check"></i><b>12.2.5</b> Image Recognition Using CNN<span></span></a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="recurrent-neural-network.html"><a href="recurrent-neural-network.html"><i class="fa fa-check"></i><b>12.3</b> Recurrent Neural Network<span></span></a>
<ul>
<li class="chapter" data-level="12.3.1" data-path="recurrent-neural-network.html"><a href="recurrent-neural-network.html#rnn-model"><i class="fa fa-check"></i><b>12.3.1</b> RNN Model<span></span></a></li>
<li class="chapter" data-level="12.3.2" data-path="recurrent-neural-network.html"><a href="recurrent-neural-network.html#lstm"><i class="fa fa-check"></i><b>12.3.2</b> Long Short Term Memory<span></span></a></li>
<li class="chapter" data-level="12.3.3" data-path="recurrent-neural-network.html"><a href="recurrent-neural-network.html#embedding"><i class="fa fa-check"></i><b>12.3.3</b> Word Embedding<span></span></a></li>
<li class="chapter" data-level="12.3.4" data-path="recurrent-neural-network.html"><a href="recurrent-neural-network.html#rnnexample"><i class="fa fa-check"></i><b>12.3.4</b> Sentiment Analysis Using RNN<span></span></a></li>
</ul></li>
</ul></li>
<li class="appendix"><span><b>Appendix<span></span></b></span></li>
<li class="chapter" data-level="A" data-path="largelocaldata.html"><a href="largelocaldata.html"><i class="fa fa-check"></i><b>A</b> Handling Large Local Data<span></span></a>
<ul>
<li class="chapter" data-level="A.1" data-path="readr.html"><a href="readr.html"><i class="fa fa-check"></i><b>A.1</b> <code>readr</code><span></span></a></li>
<li class="chapter" data-level="A.2" data-path="data.table-enhanced-data.html"><a href="data.table-enhanced-data.html"><i class="fa fa-check"></i><b>A.2</b> <code>data.table</code>— enhanced <code>data.frame</code><span></span></a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="r-code-for-data-simulation.html"><a href="r-code-for-data-simulation.html"><i class="fa fa-check"></i><b>B</b> R code for data simulation<span></span></a>
<ul>
<li class="chapter" data-level="B.1" data-path="appendixdata1.html"><a href="appendixdata1.html"><i class="fa fa-check"></i><b>B.1</b> Customer Data for Clothing Company<span></span></a></li>
<li class="chapter" data-level="B.2" data-path="appendixdata3.html"><a href="appendixdata3.html"><i class="fa fa-check"></i><b>B.2</b> Swine Disease Breakout Data<span></span></a></li>
</ul></li>
<li><a href="references.html#references">References<span></span></a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Data Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="principal-component-regression-and-partial-least-square" class="section level2 hasAnchor" number="9.2">
<h2><span class="header-section-number">9.2</span> Principal Component Regression and Partial Least Square<a href="principal-component-regression-and-partial-least-square.html#principal-component-regression-and-partial-least-square" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In real-life applications, explanatory variables are usually related to each other, containing similar information. For example, in the previous chapter, we used expenditure variables to predict consumer income. In that model, store expenditure (<code>store_exp</code>), online expenditure (<code>online_exp</code>), number of store transactions (<code>store_trans</code>), and number of online transactions (<code>online_trans</code>) are correlated to a certain extent, especially the number of transactions and expenditure. If there is a strong correlation among explanatory variables, then the least square-based linear regression model may not be robust. If the number of observations is less than the number of explanatory variables, the standard least square method cannot provide a unique set of coefficient estimates. We can perform data preprocessing, such as remove highly correlated variables with a preset threshold for such cases. However, this approach cannot guarantee a low correlation of the linear combination of the variables with other variables. In that case, the standard least square method will still not be robust. We need to be aware that removing highly correlated variables is not always guarantee a robust solution. We can also apply feature engineering procedures to explanatory variables such as principal component analysis (PCA). By using principal components, we can ensure they are uncorrelated with each other. However, the drawback of using PCA is that these components are linear combinations of original variables, and it is difficult to explain the fitted model. Principal component regression (PCR) is described in more detail in <span class="citation">(<a href="#ref-Massy1965" role="doc-biblioref">W 1965</a>)</span>. It can be used when there are strong correlations among variables or when the number of observations is less than the number of variables.</p>
<p>In theory, we can use PCR to reduce the number of variables used in a linear model, but the results are not good. Because the first a few principal components may not be good candidates to model the response variable. PCA is unsupervised learning such that the entire process does not consider the response variable. In PCA, it only focuses on the variability of explanatory variables. When the independent variables and response variables are related, PCR can well identify the systematic relationship between them. However, when there exist independent variables not associated with response variable, it will undermine PCR’s performance. We need to be aware that PCR does not make feature selections, and each principal component is a combination of original variables.</p>
<p>Partial least square regression (PLS) is the supervised version of PCR. Similar to PCR, PLS can also reduce the number of variables in the regression model. As PLS is also related to the variables’ variance, we usually standardize or normalize variables before PLS. Suppose we have a list of explanatory variables <span class="math inline">\(\mathbf{X}=[X_{1},X_{2},...,X_{p}]^{T}\)</span>, and their variance-covariance matrix is <span class="math inline">\(\Sigma\)</span>. PLS also transforms the original variables using linear combination to new uncorrelated variables <span class="math inline">\((Z_{1} , Z_{2} , \ldots , Z_{m})\)</span>. When <span class="math inline">\(m=p\)</span>, the result of PLS is the same as OLS. The main difference between PCR and PLS is the process of creating new variables. PLS considers the response variable.</p>
<p>PLS is from Herman Wold’s Nonlinear Iterative Partial Least Squares (NIPALS) algorithm <span class="citation">(<a href="#ref-wold1973" role="doc-biblioref">Wold 1973</a>; <a href="#ref-wold1982" role="doc-biblioref">Wold and Jöreskog 1982</a>)</span> . Later NIPALS was applied to regression problems, which was then called PLS . PLS is a method of linearizing nonlinear relationships through hidden layers. It is similar to the PCR, except that PCR does not take into account the information of the dependent variable when selecting the components. PCR’s purpose is to find the linear combinations (i.e., unsupervised) that capture the most variance of the independent variables, while PLS maximizes the linear combination of dependencies with the response variable. In the current case, the more complicated PLS does not perform better than simple linear regression. We will not discuss the PLS algorithm’s detail, and the reference mentioned above provides a more detailed description of the algorithm.</p>
<p>We focus on using R library <code>caret</code> to fit PCR and PLS models. Let us use the 10 survey questions (<code>Q1</code>-<code>Q10</code>) as the explanatory variables and income as the response variable. First load the data and preprocessing the data:</p>
<div class="sourceCode" id="cb216"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb216-1"><a href="principal-component-regression-and-partial-least-square.html#cb216-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(lattice)</span>
<span id="cb216-2"><a href="principal-component-regression-and-partial-least-square.html#cb216-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(caret)</span>
<span id="cb216-3"><a href="principal-component-regression-and-partial-least-square.html#cb216-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb216-4"><a href="principal-component-regression-and-partial-least-square.html#cb216-4" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(elasticnet)</span>
<span id="cb216-5"><a href="principal-component-regression-and-partial-least-square.html#cb216-5" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(lars)</span>
<span id="cb216-6"><a href="principal-component-regression-and-partial-least-square.html#cb216-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb216-7"><a href="principal-component-regression-and-partial-least-square.html#cb216-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Load Data</span></span>
<span id="cb216-8"><a href="principal-component-regression-and-partial-least-square.html#cb216-8" aria-hidden="true" tabindex="-1"></a>sim.dat <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;http://bit.ly/2P5gTw4&quot;</span>)</span>
<span id="cb216-9"><a href="principal-component-regression-and-partial-least-square.html#cb216-9" aria-hidden="true" tabindex="-1"></a>ymad <span class="ot">&lt;-</span> <span class="fu">mad</span>(<span class="fu">na.omit</span>(sim.dat<span class="sc">$</span>income))</span>
<span id="cb216-10"><a href="principal-component-regression-and-partial-least-square.html#cb216-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb216-11"><a href="principal-component-regression-and-partial-least-square.html#cb216-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate Z values</span></span>
<span id="cb216-12"><a href="principal-component-regression-and-partial-least-square.html#cb216-12" aria-hidden="true" tabindex="-1"></a>zs <span class="ot">&lt;-</span> (sim.dat<span class="sc">$</span>income <span class="sc">-</span> <span class="fu">mean</span>(<span class="fu">na.omit</span>(sim.dat<span class="sc">$</span>income)))<span class="sc">/</span>ymad</span>
<span id="cb216-13"><a href="principal-component-regression-and-partial-least-square.html#cb216-13" aria-hidden="true" tabindex="-1"></a><span class="co"># which(na.omit(zs&gt;3.5)) find outlier </span></span>
<span id="cb216-14"><a href="principal-component-regression-and-partial-least-square.html#cb216-14" aria-hidden="true" tabindex="-1"></a><span class="co"># which(is.na(zs)) find missing values</span></span>
<span id="cb216-15"><a href="principal-component-regression-and-partial-least-square.html#cb216-15" aria-hidden="true" tabindex="-1"></a>idex <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">which</span>(<span class="fu">na.omit</span>(zs <span class="sc">&gt;</span> <span class="fl">3.5</span>)), <span class="fu">which</span>(<span class="fu">is.na</span>(zs)))</span>
<span id="cb216-16"><a href="principal-component-regression-and-partial-least-square.html#cb216-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Remove rows with outlier and missing values</span></span>
<span id="cb216-17"><a href="principal-component-regression-and-partial-least-square.html#cb216-17" aria-hidden="true" tabindex="-1"></a>sim.dat <span class="ot">&lt;-</span> sim.dat[<span class="sc">-</span>idex, ]</span></code></pre></div>
<p>Now let’s define explanatory variable matrix (<code>xtrain</code>) by selecting these 10 survey questions columns, and define response variable (<code>ytrain</code>):</p>
<div class="sourceCode" id="cb217"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb217-1"><a href="principal-component-regression-and-partial-least-square.html#cb217-1" aria-hidden="true" tabindex="-1"></a>xtrain <span class="ot">=</span> dplyr<span class="sc">::</span><span class="fu">select</span>(sim.dat, Q1<span class="sc">:</span>Q10)</span>
<span id="cb217-2"><a href="principal-component-regression-and-partial-least-square.html#cb217-2" aria-hidden="true" tabindex="-1"></a>ytrain <span class="ot">=</span> sim.dat<span class="sc">$</span>income</span></code></pre></div>
<p>We also set up random seed and 10-folder cross-validation:</p>
<div class="sourceCode" id="cb218"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb218-1"><a href="principal-component-regression-and-partial-least-square.html#cb218-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">100</span>)</span>
<span id="cb218-2"><a href="principal-component-regression-and-partial-least-square.html#cb218-2" aria-hidden="true" tabindex="-1"></a>ctrl <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="at">number =</span> <span class="dv">10</span>)</span></code></pre></div>
<p>Fit PLS model using number of explanatory variables as the hyper-parameter to tune. As there are at most 10 explanatory variables in the model, we set up the hyper-parameter tuning range to be 1 to 10:</p>
<div class="sourceCode" id="cb219"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb219-1"><a href="principal-component-regression-and-partial-least-square.html#cb219-1" aria-hidden="true" tabindex="-1"></a>plsTune <span class="ot">&lt;-</span> <span class="fu">train</span>(xtrain, ytrain, </span>
<span id="cb219-2"><a href="principal-component-regression-and-partial-least-square.html#cb219-2" aria-hidden="true" tabindex="-1"></a>                 <span class="at">method =</span> <span class="st">&quot;pls&quot;</span>, </span>
<span id="cb219-3"><a href="principal-component-regression-and-partial-least-square.html#cb219-3" aria-hidden="true" tabindex="-1"></a>                 <span class="co"># set hyper-parameter tuning range</span></span>
<span id="cb219-4"><a href="principal-component-regression-and-partial-least-square.html#cb219-4" aria-hidden="true" tabindex="-1"></a>                 <span class="at">tuneGrid =</span> <span class="fu">expand.grid</span>(<span class="at">.ncomp =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>),</span>
<span id="cb219-5"><a href="principal-component-regression-and-partial-least-square.html#cb219-5" aria-hidden="true" tabindex="-1"></a>                 <span class="at">trControl =</span> ctrl)</span>
<span id="cb219-6"><a href="principal-component-regression-and-partial-least-square.html#cb219-6" aria-hidden="true" tabindex="-1"></a>plsTune</span></code></pre></div>
<pre><code>## Partial Least Squares 
## 
## 772 samples
##  10 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 696, 693, 694, 694, 696, 695, ... 
## Resampling results across tuning parameters:
## 
##   ncomp  RMSE   Rsquared  MAE  
##    1     27777  0.6534    19845
##    2     24420  0.7320    15976
##    3     23175  0.7590    14384
##    4     23011  0.7625    13808
##    5     22977  0.7631    13737
##    6     22978  0.7631    13729
##    7     22976  0.7631    13726
##    8     22976  0.7631    13726
##    9     22976  0.7631    13726
##   10     22976  0.7631    13726
## 
## RMSE was used to select the optimal model using
##  the smallest value.
## The final value used for the model was ncomp = 7.</code></pre>
<p>From the result, we can see that the optimal number of variables is 7. However, if we pay attention to the RMSE improvement, we will find only minimum improvement in RMSE after three variables. In practice, we could choose to use the model with three variables if the improvement does not make a practical difference, and we would rather have a simpler model. </p>
<p>We can also find the relative importance of each variable during PLS model tuning process, as described using the following code: </p>
<div class="sourceCode" id="cb221"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb221-1"><a href="principal-component-regression-and-partial-least-square.html#cb221-1" aria-hidden="true" tabindex="-1"></a>plsImp <span class="ot">&lt;-</span> <span class="fu">varImp</span>(plsTune, <span class="at">scale =</span> <span class="cn">FALSE</span>)</span>
<span id="cb221-2"><a href="principal-component-regression-and-partial-least-square.html#cb221-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(plsImp, <span class="at">top =</span> <span class="dv">10</span>, <span class="at">scales =</span> <span class="fu">list</span>(<span class="at">y =</span> <span class="fu">list</span>(<span class="at">cex =</span> <span class="fl">0.95</span>)))</span></code></pre></div>
<p><img src="IDS_files/figure-html/unnamed-chunk-95-1.svg" width="672" /></p>
<p>The above plot shows that Q1, Q2, Q3, and Q6, are more important than other variables. Now let’s fit a PCR model with number of principal components as the hyper-parameter: </p>
<div class="sourceCode" id="cb222"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb222-1"><a href="principal-component-regression-and-partial-least-square.html#cb222-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Set random seed</span></span>
<span id="cb222-2"><a href="principal-component-regression-and-partial-least-square.html#cb222-2" aria-hidden="true" tabindex="-1"></a> <span class="fu">set.seed</span>(<span class="dv">100</span>)</span>
<span id="cb222-3"><a href="principal-component-regression-and-partial-least-square.html#cb222-3" aria-hidden="true" tabindex="-1"></a> pcrTune <span class="ot">&lt;-</span> <span class="fu">train</span>(<span class="at">x =</span> xtrain, <span class="at">y =</span> ytrain,</span>
<span id="cb222-4"><a href="principal-component-regression-and-partial-least-square.html#cb222-4" aria-hidden="true" tabindex="-1"></a>          <span class="at">method =</span> <span class="st">&quot;pcr&quot;</span>,</span>
<span id="cb222-5"><a href="principal-component-regression-and-partial-least-square.html#cb222-5" aria-hidden="true" tabindex="-1"></a>          <span class="co"># set hyper-parameter tuning range</span></span>
<span id="cb222-6"><a href="principal-component-regression-and-partial-least-square.html#cb222-6" aria-hidden="true" tabindex="-1"></a>          <span class="at">tuneGrid =</span> <span class="fu">expand.grid</span>(<span class="at">.ncomp =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>),</span>
<span id="cb222-7"><a href="principal-component-regression-and-partial-least-square.html#cb222-7" aria-hidden="true" tabindex="-1"></a>          <span class="at">trControl =</span> ctrl)</span>
<span id="cb222-8"><a href="principal-component-regression-and-partial-least-square.html#cb222-8" aria-hidden="true" tabindex="-1"></a> pcrTune</span></code></pre></div>
<pre><code>## Principal Component Analysis 
## 
## 772 samples
##  10 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 696, 693, 694, 694, 696, 695, ... 
## Resampling results across tuning parameters:
## 
##   ncomp  RMSE   Rsquared  MAE  
##    1     45958  0.03243   36599
##    2     32460  0.52200   24041
##    3     23235  0.75774   14516
##    4     23262  0.75735   14545
##    5     23152  0.75957   14232
##    6     23133  0.76004   14130
##    7     23114  0.76049   14129
##    8     23115  0.76045   14130
##    9     22991  0.76283   13801
##   10     22976  0.76308   13726
## 
## RMSE was used to select the optimal model using
##  the smallest value.
## The final value used for the model was ncomp = 10.</code></pre>
<p>From the output, the default recommendation is ten components. However, if we pay attention to RMSE improvement with more components, we will find little difference after the model with three components. Again, in practice, we can keep models with three components.</p>
<p>Now let’s compare the hyper-parameter tuning process for PLS and PCR:</p>
<div class="sourceCode" id="cb224"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb224-1"><a href="principal-component-regression-and-partial-least-square.html#cb224-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Save PLS model tuning information to plsResamples</span></span>
<span id="cb224-2"><a href="principal-component-regression-and-partial-least-square.html#cb224-2" aria-hidden="true" tabindex="-1"></a>plsResamples <span class="ot">&lt;-</span> plsTune<span class="sc">$</span>results</span>
<span id="cb224-3"><a href="principal-component-regression-and-partial-least-square.html#cb224-3" aria-hidden="true" tabindex="-1"></a>plsResamples<span class="sc">$</span>Model <span class="ot">&lt;-</span> <span class="st">&quot;PLS&quot;</span></span>
<span id="cb224-4"><a href="principal-component-regression-and-partial-least-square.html#cb224-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Save PCR model tuning information to plsResamples</span></span>
<span id="cb224-5"><a href="principal-component-regression-and-partial-least-square.html#cb224-5" aria-hidden="true" tabindex="-1"></a>pcrResamples <span class="ot">&lt;-</span> pcrTune<span class="sc">$</span>results</span>
<span id="cb224-6"><a href="principal-component-regression-and-partial-least-square.html#cb224-6" aria-hidden="true" tabindex="-1"></a>pcrResamples<span class="sc">$</span>Model <span class="ot">&lt;-</span> <span class="st">&quot;PCR&quot;</span></span>
<span id="cb224-7"><a href="principal-component-regression-and-partial-least-square.html#cb224-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Combine both output for plotting</span></span>
<span id="cb224-8"><a href="principal-component-regression-and-partial-least-square.html#cb224-8" aria-hidden="true" tabindex="-1"></a>plsPlotData <span class="ot">&lt;-</span> <span class="fu">rbind</span>(plsResamples, pcrResamples)</span>
<span id="cb224-9"><a href="principal-component-regression-and-partial-least-square.html#cb224-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Leverage xyplot() function from lattice library</span></span>
<span id="cb224-10"><a href="principal-component-regression-and-partial-least-square.html#cb224-10" aria-hidden="true" tabindex="-1"></a><span class="fu">xyplot</span>(RMSE <span class="sc">~</span> ncomp, </span>
<span id="cb224-11"><a href="principal-component-regression-and-partial-least-square.html#cb224-11" aria-hidden="true" tabindex="-1"></a>       <span class="at">data =</span> plsPlotData, </span>
<span id="cb224-12"><a href="principal-component-regression-and-partial-least-square.html#cb224-12" aria-hidden="true" tabindex="-1"></a>       <span class="at">xlab =</span> <span class="st">&quot;# Components&quot;</span>, </span>
<span id="cb224-13"><a href="principal-component-regression-and-partial-least-square.html#cb224-13" aria-hidden="true" tabindex="-1"></a>       <span class="at">ylab =</span> <span class="st">&quot;RMSE (Cross-Validation)&quot;</span>, </span>
<span id="cb224-14"><a href="principal-component-regression-and-partial-least-square.html#cb224-14" aria-hidden="true" tabindex="-1"></a>       <span class="at">auto.key =</span> <span class="fu">list</span>(<span class="at">columns =</span> <span class="dv">2</span>), </span>
<span id="cb224-15"><a href="principal-component-regression-and-partial-least-square.html#cb224-15" aria-hidden="true" tabindex="-1"></a>       <span class="at">groups =</span> Model, </span>
<span id="cb224-16"><a href="principal-component-regression-and-partial-least-square.html#cb224-16" aria-hidden="true" tabindex="-1"></a>       <span class="at">type =</span> <span class="fu">c</span>(<span class="st">&quot;o&quot;</span>, <span class="st">&quot;g&quot;</span>))</span></code></pre></div>
<p><img src="IDS_files/figure-html/unnamed-chunk-97-1.svg" width="672" /></p>
<p>The plot confirms our choice of using a model with three components for both PLS and PCR.</p>

</div>
<!-- </div> -->
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-Massy1965" class="csl-entry">
W, Massy. 1965. <span>“Principal Components Regression in Exploratory Statistical Research.”</span> <em>Journal of the American Statistical Association</em> 60: 234–46.
</div>
<div id="ref-wold1973" class="csl-entry">
Wold, Herman. 1973. <span>“Nonlinear Iterative Partial Least Squares (NIPALS) Modelling: Some Current Developments.”</span> <em>Academic Press</em>, 383–407.
</div>
<div id="ref-wold1982" class="csl-entry">
Wold, Herman, and K. G. Jöreskog. 1982. <em>Systems Under Indirect Observation: Causality, Structure, Prediction</em>. North Holland, Amsterdam.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="ordinary-least-square.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="regularization-methods.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": true,
"twitter": true,
"linkedin": true,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/happyrabbit/IntroDataScience/09-Regression.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["IDS.pdf", "IDS.epub", "IDS.mobi"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section",
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
