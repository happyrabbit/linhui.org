<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>7.2 Data Splitting and Resampling | Practitioner’s Guide to Data Science</title>
  <meta name="description" content="Introduction to Data Science" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="7.2 Data Splitting and Resampling | Practitioner’s Guide to Data Science" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="http://scientistcafe.com/IDS/" />
  
  <meta property="og:description" content="Introduction to Data Science" />
  <meta name="github-repo" content="happyrabbit/IntroDataScience" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="7.2 Data Splitting and Resampling | Practitioner’s Guide to Data Science" />
  
  <meta name="twitter:description" content="Introduction to Data Science" />
  

<meta name="author" content="Hui Lin and Ming Li" />


<meta name="date" content="2023-02-19" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="vbtradeoff.html"/>
<link rel="next" href="measuring-performance.html"/>
<script src="libs/header-attrs/header-attrs.js"></script>
<script src="libs/jquery/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="http://scientistcafe.com">Homepage</a></li>

<li class="divider"></li>
<li><a href="index.html#preface" id="toc-preface">Preface</a>
<ul>
<li><a href="goal-of-the-book.html#goal-of-the-book" id="toc-goal-of-the-book">Goal of the Book</a></li>
<li><a href="what-this-book-covers.html#what-this-book-covers" id="toc-what-this-book-covers">What This Book Covers</a></li>
<li><a href="who-this-book-is-for.html#who-this-book-is-for" id="toc-who-this-book-is-for">Who This Book Is For</a></li>
<li><a href="how-to-use-this-book.html#how-to-use-this-book" id="toc-how-to-use-this-book">How to Use This Book</a>
<ul>
<li><a href="how-to-use-this-book.html#what-the-book-assumes" id="toc-what-the-book-assumes">What the Book Assumes</a></li>
<li><a href="how-to-use-this-book.html#how-to-run-r-and-python-code" id="toc-how-to-run-r-and-python-code">How to Run R and Python Code</a></li>
</ul></li>
<li><a href="complementary-reading.html#complementary-reading" id="toc-complementary-reading">Complementary Reading</a></li>
</ul></li>
<li><a href="about-the-authors.html#about-the-authors" id="toc-about-the-authors">About the Authors</a></li>
<li><a href="introduction.html#introduction" id="toc-introduction"><span class="toc-section-number">1</span> Introduction</a>
<ul>
<li><a href="a-brief-history-of-data-science.html#a-brief-history-of-data-science" id="toc-a-brief-history-of-data-science"><span class="toc-section-number">1.1</span> A brief history of data science</a></li>
<li><a href="data-science-role-and-skill-tracks.html#data-science-role-and-skill-tracks" id="toc-data-science-role-and-skill-tracks"><span class="toc-section-number">1.2</span> Data science role and skill tracks</a>
<ul>
<li><a href="data-science-role-and-skill-tracks.html#engineering" id="toc-engineering"><span class="toc-section-number">1.2.1</span> Engineering</a></li>
<li><a href="data-science-role-and-skill-tracks.html#analysis" id="toc-analysis"><span class="toc-section-number">1.2.2</span> Analysis</a></li>
<li><a href="data-science-role-and-skill-tracks.html#modelinginference" id="toc-modelinginference"><span class="toc-section-number">1.2.3</span> Modeling/inference</a></li>
</ul></li>
<li><a href="what-kind-of-questions-can-data-science-solve.html#what-kind-of-questions-can-data-science-solve" id="toc-what-kind-of-questions-can-data-science-solve"><span class="toc-section-number">1.3</span> What kind of questions can data science solve?</a>
<ul>
<li><a href="what-kind-of-questions-can-data-science-solve.html#prerequisites" id="toc-prerequisites"><span class="toc-section-number">1.3.1</span> Prerequisites</a></li>
<li><a href="what-kind-of-questions-can-data-science-solve.html#problem-type" id="toc-problem-type"><span class="toc-section-number">1.3.2</span> Problem type</a></li>
</ul></li>
<li><a href="structure-of-data-science-team.html#structure-of-data-science-team" id="toc-structure-of-data-science-team"><span class="toc-section-number">1.4</span> Structure of data science team</a></li>
<li><a href="data-science-roles.html#data-science-roles" id="toc-data-science-roles"><span class="toc-section-number">1.5</span> Data science roles</a></li>
</ul></li>
<li><a href="SoftSkillsforDataScientists.html#SoftSkillsforDataScientists" id="toc-SoftSkillsforDataScientists"><span class="toc-section-number">2</span> Soft Skills for Data Scientists</a>
<ul>
<li><a href="comparison-between-statistician-and-data-scientist.html#comparison-between-statistician-and-data-scientist" id="toc-comparison-between-statistician-and-data-scientist"><span class="toc-section-number">2.1</span> Comparison between Statistician and Data Scientist</a></li>
<li><a href="beyond-data-and-analytics.html#beyond-data-and-analytics" id="toc-beyond-data-and-analytics"><span class="toc-section-number">2.2</span> Beyond Data and Analytics</a></li>
<li><a href="three-pillars-of-knowledge.html#three-pillars-of-knowledge" id="toc-three-pillars-of-knowledge"><span class="toc-section-number">2.3</span> Three Pillars of Knowledge</a></li>
<li><a href="data-science-project-cycle.html#data-science-project-cycle" id="toc-data-science-project-cycle"><span class="toc-section-number">2.4</span> Data Science Project Cycle</a>
<ul>
<li><a href="data-science-project-cycle.html#types-of-data-science-projects" id="toc-types-of-data-science-projects"><span class="toc-section-number">2.4.1</span> Types of Data Science Projects</a></li>
<li><a href="data-science-project-cycle.html#ProblemFormulationandProjectPlanningStage" id="toc-ProblemFormulationandProjectPlanningStage"><span class="toc-section-number">2.4.2</span> Problem Formulation and Project Planning Stage</a></li>
<li><a href="data-science-project-cycle.html#project-modeling-stage" id="toc-project-modeling-stage"><span class="toc-section-number">2.4.3</span> Project Modeling Stage</a></li>
<li><a href="data-science-project-cycle.html#ModelImplementationandPostProductionStage" id="toc-ModelImplementationandPostProductionStage"><span class="toc-section-number">2.4.4</span> Model Implementation and Post Production Stage</a></li>
<li><a href="data-science-project-cycle.html#ProjectCycleSummary" id="toc-ProjectCycleSummary"><span class="toc-section-number">2.4.5</span> Project Cycle Summary</a></li>
</ul></li>
<li><a href="common-mistakes-in-data-science.html#common-mistakes-in-data-science" id="toc-common-mistakes-in-data-science"><span class="toc-section-number">2.5</span> Common Mistakes in Data Science</a>
<ul>
<li><a href="common-mistakes-in-data-science.html#problem-formulation-stage" id="toc-problem-formulation-stage"><span class="toc-section-number">2.5.1</span> Problem Formulation Stage</a></li>
<li><a href="common-mistakes-in-data-science.html#project-planning-stage" id="toc-project-planning-stage"><span class="toc-section-number">2.5.2</span> Project Planning Stage</a></li>
<li><a href="common-mistakes-in-data-science.html#project-modeling-stage-1" id="toc-project-modeling-stage-1"><span class="toc-section-number">2.5.3</span> Project Modeling Stage</a></li>
<li><a href="common-mistakes-in-data-science.html#model-implementation-and-post-production-stage" id="toc-model-implementation-and-post-production-stage"><span class="toc-section-number">2.5.4</span> Model Implementation and Post Production Stage</a></li>
<li><a href="common-mistakes-in-data-science.html#summary-of-common-mistakes" id="toc-summary-of-common-mistakes"><span class="toc-section-number">2.5.5</span> Summary of Common Mistakes</a></li>
</ul></li>
</ul></li>
<li><a href="introduction-to-the-data.html#introduction-to-the-data" id="toc-introduction-to-the-data"><span class="toc-section-number">3</span> Introduction to The Data</a>
<ul>
<li><a href="customer-data-for-a-clothing-company.html#customer-data-for-a-clothing-company" id="toc-customer-data-for-a-clothing-company"><span class="toc-section-number">3.1</span> Customer Data for A Clothing Company</a></li>
<li><a href="swinediseasedata.html#swinediseasedata" id="toc-swinediseasedata"><span class="toc-section-number">3.2</span> Swine Disease Breakout Data</a></li>
<li><a href="mnist-dataset.html#mnist-dataset" id="toc-mnist-dataset"><span class="toc-section-number">3.3</span> MNIST Dataset</a></li>
<li><a href="imdb-dataset.html#imdb-dataset" id="toc-imdb-dataset"><span class="toc-section-number">3.4</span> IMDB Dataset</a></li>
</ul></li>
<li><a href="bigdatacloudplatform.html#bigdatacloudplatform" id="toc-bigdatacloudplatform"><span class="toc-section-number">4</span> Big Data Cloud Platform</a>
<ul>
<li><a href="power-of-cluster-of-computers.html#power-of-cluster-of-computers" id="toc-power-of-cluster-of-computers"><span class="toc-section-number">4.1</span> Power of Cluster of Computers</a></li>
<li><a href="evolution-of-cluster-computing.html#evolution-of-cluster-computing" id="toc-evolution-of-cluster-computing"><span class="toc-section-number">4.2</span> Evolution of Cluster Computing</a>
<ul>
<li><a href="evolution-of-cluster-computing.html#hadoop" id="toc-hadoop"><span class="toc-section-number">4.2.1</span> Hadoop</a></li>
<li><a href="evolution-of-cluster-computing.html#spark" id="toc-spark"><span class="toc-section-number">4.2.2</span> Spark</a></li>
</ul></li>
<li><a href="CloudEnvironment.html#CloudEnvironment" id="toc-CloudEnvironment"><span class="toc-section-number">4.3</span> Introduction of Cloud Environment</a>
<ul>
<li><a href="CloudEnvironment.html#open-account-and-create-a-cluster" id="toc-open-account-and-create-a-cluster"><span class="toc-section-number">4.3.1</span> Open Account and Create a Cluster</a></li>
<li><a href="CloudEnvironment.html#r-notebook" id="toc-r-notebook"><span class="toc-section-number">4.3.2</span> R Notebook</a></li>
<li><a href="CloudEnvironment.html#markdown-cells" id="toc-markdown-cells"><span class="toc-section-number">4.3.3</span> Markdown Cells</a></li>
</ul></li>
<li><a href="leveragesparkr.html#leveragesparkr" id="toc-leveragesparkr"><span class="toc-section-number">4.4</span> Leverage Spark Using R Notebook</a></li>
<li><a href="databases-and-sql.html#databases-and-sql" id="toc-databases-and-sql"><span class="toc-section-number">4.5</span> Databases and SQL</a>
<ul>
<li><a href="databases-and-sql.html#history" id="toc-history"><span class="toc-section-number">4.5.1</span> History</a></li>
<li><a href="databases-and-sql.html#database-table-and-view" id="toc-database-table-and-view"><span class="toc-section-number">4.5.2</span> Database, Table and View</a></li>
<li><a href="databases-and-sql.html#basic-sql-statement" id="toc-basic-sql-statement"><span class="toc-section-number">4.5.3</span> Basic SQL Statement</a></li>
<li><a href="databases-and-sql.html#advanced-topics-in-database" id="toc-advanced-topics-in-database"><span class="toc-section-number">4.5.4</span> Advanced Topics in Database</a></li>
</ul></li>
</ul></li>
<li><a href="datapreprocessing.html#datapreprocessing" id="toc-datapreprocessing"><span class="toc-section-number">5</span> Data Pre-processing</a>
<ul>
<li><a href="data-cleaning.html#data-cleaning" id="toc-data-cleaning"><span class="toc-section-number">5.1</span> Data Cleaning</a></li>
<li><a href="missing-values.html#missing-values" id="toc-missing-values"><span class="toc-section-number">5.2</span> Missing Values</a>
<ul>
<li><a href="missing-values.html#impute-missing-values-with-medianmode" id="toc-impute-missing-values-with-medianmode"><span class="toc-section-number">5.2.1</span> Impute missing values with median/mode</a></li>
<li><a href="missing-values.html#k-nearest-neighbors" id="toc-k-nearest-neighbors"><span class="toc-section-number">5.2.2</span> K-nearest neighbors</a></li>
<li><a href="missing-values.html#bagging-tree" id="toc-bagging-tree"><span class="toc-section-number">5.2.3</span> Bagging Tree</a></li>
</ul></li>
<li><a href="centering-and-scaling.html#centering-and-scaling" id="toc-centering-and-scaling"><span class="toc-section-number">5.3</span> Centering and Scaling</a></li>
<li><a href="resolve-skewness.html#resolve-skewness" id="toc-resolve-skewness"><span class="toc-section-number">5.4</span> Resolve Skewness</a></li>
<li><a href="outliers.html#outliers" id="toc-outliers"><span class="toc-section-number">5.5</span> Resolve Outliers</a></li>
<li><a href="collinearity.html#collinearity" id="toc-collinearity"><span class="toc-section-number">5.6</span> Collinearity</a></li>
<li><a href="sparse-variables.html#sparse-variables" id="toc-sparse-variables"><span class="toc-section-number">5.7</span> Sparse Variables</a></li>
<li><a href="re-encode-dummy-variables.html#re-encode-dummy-variables" id="toc-re-encode-dummy-variables"><span class="toc-section-number">5.8</span> Re-encode Dummy Variables</a></li>
</ul></li>
<li><a href="datawrangline.html#datawrangline" id="toc-datawrangline"><span class="toc-section-number">6</span> Data Wrangling</a>
<ul>
<li><a href="summarize-data.html#summarize-data" id="toc-summarize-data"><span class="toc-section-number">6.1</span> Summarize Data</a>
<ul>
<li><a href="summarize-data.html#dplyr-package" id="toc-dplyr-package"><span class="toc-section-number">6.1.1</span> <code>dplyr</code> package</a></li>
<li><a href="summarize-data.html#applyfamilyinbaser" id="toc-applyfamilyinbaser"><span class="toc-section-number">6.1.2</span> <code>apply()</code>, <code>lapply()</code> and <code>sapply()</code> in base R</a></li>
</ul></li>
<li><a href="tidy-and-reshape-data.html#tidy-and-reshape-data" id="toc-tidy-and-reshape-data"><span class="toc-section-number">6.2</span> Tidy and Reshape Data</a></li>
</ul></li>
<li><a href="modeltuningstrategy.html#modeltuningstrategy" id="toc-modeltuningstrategy"><span class="toc-section-number">7</span> Model Tuning Strategy</a>
<ul>
<li><a href="vbtradeoff.html#vbtradeoff" id="toc-vbtradeoff"><span class="toc-section-number">7.1</span> Variance-Bias Trade-Off</a></li>
<li><a href="datasplittingresampling.html#datasplittingresampling" id="toc-datasplittingresampling"><span class="toc-section-number">7.2</span> Data Splitting and Resampling</a>
<ul>
<li><a href="datasplittingresampling.html#datasplitting" id="toc-datasplitting"><span class="toc-section-number">7.2.1</span> Data Splitting</a></li>
<li><a href="datasplittingresampling.html#resampling" id="toc-resampling"><span class="toc-section-number">7.2.2</span> Resampling</a></li>
</ul></li>
</ul></li>
<li><a href="measuring-performance.html#measuring-performance" id="toc-measuring-performance"><span class="toc-section-number">8</span> Measuring Performance</a>
<ul>
<li><a href="regression-model-performance.html#regression-model-performance" id="toc-regression-model-performance"><span class="toc-section-number">8.1</span> Regression Model Performance</a></li>
<li><a href="classification-model-performance.html#classification-model-performance" id="toc-classification-model-performance"><span class="toc-section-number">8.2</span> Classification Model Performance</a>
<ul>
<li><a href="classification-model-performance.html#confusion-matrix" id="toc-confusion-matrix"><span class="toc-section-number">8.2.1</span> Confusion Matrix</a></li>
<li><a href="classification-model-performance.html#kappa-statistic" id="toc-kappa-statistic"><span class="toc-section-number">8.2.2</span> Kappa Statistic</a></li>
<li><a href="classification-model-performance.html#roc" id="toc-roc"><span class="toc-section-number">8.2.3</span> ROC</a></li>
<li><a href="classification-model-performance.html#gain-and-lift-charts" id="toc-gain-and-lift-charts"><span class="toc-section-number">8.2.4</span> Gain and Lift Charts</a></li>
</ul></li>
</ul></li>
<li><a href="regression-models.html#regression-models" id="toc-regression-models"><span class="toc-section-number">9</span> Regression Models</a>
<ul>
<li><a href="ordinary-least-square.html#ordinary-least-square" id="toc-ordinary-least-square"><span class="toc-section-number">9.1</span> Ordinary Least Square</a>
<ul>
<li><a href="ordinary-least-square.html#the-magic-p-value" id="toc-the-magic-p-value"><span class="toc-section-number">9.1.1</span> The Magic P-value</a></li>
<li><a href="ordinary-least-square.html#diagnostics-for-linear-regression" id="toc-diagnostics-for-linear-regression"><span class="toc-section-number">9.1.2</span> Diagnostics for Linear Regression</a></li>
</ul></li>
<li><a href="principal-component-regression-and-partial-least-square.html#principal-component-regression-and-partial-least-square" id="toc-principal-component-regression-and-partial-least-square"><span class="toc-section-number">9.2</span> Principal Component Regression and Partial Least Square</a></li>
</ul></li>
<li><a href="regularization-methods.html#regularization-methods" id="toc-regularization-methods"><span class="toc-section-number">10</span> Regularization Methods</a>
<ul>
<li><a href="ridge-regression.html#ridge-regression" id="toc-ridge-regression"><span class="toc-section-number">10.1</span> Ridge Regression</a></li>
<li><a href="lasso.html#lasso" id="toc-lasso"><span class="toc-section-number">10.2</span> LASSO</a></li>
<li><a href="elastic-net.html#elastic-net" id="toc-elastic-net"><span class="toc-section-number">10.3</span> Elastic Net</a></li>
<li><a href="penalized-generalized-linear-model.html#penalized-generalized-linear-model" id="toc-penalized-generalized-linear-model"><span class="toc-section-number">10.4</span> Penalized Generalized Linear Model</a>
<ul>
<li><a href="penalized-generalized-linear-model.html#introduction-to-glmnet-package" id="toc-introduction-to-glmnet-package"><span class="toc-section-number">10.4.1</span> Introduction to <code>glmnet</code> package</a></li>
<li><a href="penalized-generalized-linear-model.html#penalized-logistic-regression" id="toc-penalized-logistic-regression"><span class="toc-section-number">10.4.2</span> Penalized logistic regression</a></li>
</ul></li>
</ul></li>
<li><a href="treemodel.html#treemodel" id="toc-treemodel"><span class="toc-section-number">11</span> Tree-Based Methods</a>
<ul>
<li><a href="tree-basics.html#tree-basics" id="toc-tree-basics"><span class="toc-section-number">11.1</span> Tree Basics</a></li>
<li><a href="splitting-criteria.html#splitting-criteria" id="toc-splitting-criteria"><span class="toc-section-number">11.2</span> Splitting Criteria</a>
<ul>
<li><a href="splitting-criteria.html#gini-impurity" id="toc-gini-impurity"><span class="toc-section-number">11.2.1</span> Gini impurity</a></li>
<li><a href="splitting-criteria.html#information-gain-ig" id="toc-information-gain-ig"><span class="toc-section-number">11.2.2</span> Information Gain (IG)</a></li>
<li><a href="splitting-criteria.html#information-gain-ratio-igr" id="toc-information-gain-ratio-igr"><span class="toc-section-number">11.2.3</span> Information Gain Ratio (IGR)</a></li>
<li><a href="splitting-criteria.html#sum-of-squared-error-sse" id="toc-sum-of-squared-error-sse"><span class="toc-section-number">11.2.4</span> Sum of Squared Error (SSE)</a></li>
</ul></li>
<li><a href="tree-pruning.html#tree-pruning" id="toc-tree-pruning"><span class="toc-section-number">11.3</span> Tree Pruning</a></li>
<li><a href="regression-and-decision-tree-basic.html#regression-and-decision-tree-basic" id="toc-regression-and-decision-tree-basic"><span class="toc-section-number">11.4</span> Regression and Decision Tree Basic</a>
<ul>
<li><a href="regression-and-decision-tree-basic.html#regression-tree" id="toc-regression-tree"><span class="toc-section-number">11.4.1</span> Regression Tree</a></li>
<li><a href="regression-and-decision-tree-basic.html#decision-tree" id="toc-decision-tree"><span class="toc-section-number">11.4.2</span> Decision Tree</a></li>
</ul></li>
<li><a href="bagging-tree-1.html#bagging-tree-1" id="toc-bagging-tree-1"><span class="toc-section-number">11.5</span> Bagging Tree</a></li>
<li><a href="random-forest.html#random-forest" id="toc-random-forest"><span class="toc-section-number">11.6</span> Random Forest</a></li>
<li><a href="gradient-boosted-machine.html#gradient-boosted-machine" id="toc-gradient-boosted-machine"><span class="toc-section-number">11.7</span> Gradient Boosted Machine</a>
<ul>
<li><a href="gradient-boosted-machine.html#adaptive-boosting" id="toc-adaptive-boosting"><span class="toc-section-number">11.7.1</span> Adaptive Boosting</a></li>
<li><a href="gradient-boosted-machine.html#stochastic-gradient-boosting" id="toc-stochastic-gradient-boosting"><span class="toc-section-number">11.7.2</span> Stochastic Gradient Boosting</a></li>
</ul></li>
</ul></li>
<li><a href="deeplearning.html#deeplearning" id="toc-deeplearning"><span class="toc-section-number">12</span> Deep Learning</a>
<ul>
<li><a href="feedforward-neural-network.html#feedforward-neural-network" id="toc-feedforward-neural-network"><span class="toc-section-number">12.1</span> Feedforward Neural Network</a>
<ul>
<li><a href="feedforward-neural-network.html#logisticregasneuralnetwork" id="toc-logisticregasneuralnetwork"><span class="toc-section-number">12.1.1</span> Logistic Regression as Neural Network</a></li>
<li><a href="feedforward-neural-network.html#stochastic-gradient-descent" id="toc-stochastic-gradient-descent"><span class="toc-section-number">12.1.2</span> Stochastic Gradient Descent</a></li>
<li><a href="feedforward-neural-network.html#deepneuralnetwork" id="toc-deepneuralnetwork"><span class="toc-section-number">12.1.3</span> Deep Neural Network</a></li>
<li><a href="feedforward-neural-network.html#activationfunction" id="toc-activationfunction"><span class="toc-section-number">12.1.4</span> Activation Function</a></li>
<li><a href="feedforward-neural-network.html#optimization" id="toc-optimization"><span class="toc-section-number">12.1.5</span> Optimization</a></li>
<li><a href="feedforward-neural-network.html#deal-with-overfitting" id="toc-deal-with-overfitting"><span class="toc-section-number">12.1.6</span> Deal with Overfitting</a></li>
<li><a href="feedforward-neural-network.html#ffnnexample" id="toc-ffnnexample"><span class="toc-section-number">12.1.7</span> Image Recognition Using FFNN</a></li>
</ul></li>
<li><a href="convolutional-neural-network.html#convolutional-neural-network" id="toc-convolutional-neural-network"><span class="toc-section-number">12.2</span> Convolutional Neural Network</a>
<ul>
<li><a href="convolutional-neural-network.html#convolution-layer" id="toc-convolution-layer"><span class="toc-section-number">12.2.1</span> Convolution Layer</a></li>
<li><a href="convolutional-neural-network.html#padding-layer" id="toc-padding-layer"><span class="toc-section-number">12.2.2</span> Padding Layer</a></li>
<li><a href="convolutional-neural-network.html#pooling-layer" id="toc-pooling-layer"><span class="toc-section-number">12.2.3</span> Pooling Layer</a></li>
<li><a href="convolutional-neural-network.html#convolution-over-volume" id="toc-convolution-over-volume"><span class="toc-section-number">12.2.4</span> Convolution Over Volume</a></li>
<li><a href="convolutional-neural-network.html#cnnexample" id="toc-cnnexample"><span class="toc-section-number">12.2.5</span> Image Recognition Using CNN</a></li>
</ul></li>
<li><a href="recurrent-neural-network.html#recurrent-neural-network" id="toc-recurrent-neural-network"><span class="toc-section-number">12.3</span> Recurrent Neural Network</a>
<ul>
<li><a href="recurrent-neural-network.html#rnn-model" id="toc-rnn-model"><span class="toc-section-number">12.3.1</span> RNN Model</a></li>
<li><a href="recurrent-neural-network.html#lstm" id="toc-lstm"><span class="toc-section-number">12.3.2</span> Long Short Term Memory</a></li>
<li><a href="recurrent-neural-network.html#embedding" id="toc-embedding"><span class="toc-section-number">12.3.3</span> Word Embedding</a></li>
<li><a href="recurrent-neural-network.html#rnnexample" id="toc-rnnexample"><span class="toc-section-number">12.3.4</span> Sentiment Analysis Using RNN</a></li>
</ul></li>
</ul></li>
<li><a href="#appendix-appendix" id="toc-appendix-appendix">(APPENDIX) Appendix</a></li>
<li><a href="largelocaldata.html#largelocaldata" id="toc-largelocaldata"><span class="toc-section-number">13</span> Handling Large Local Data</a>
<ul>
<li><a href="readr.html#readr" id="toc-readr"><span class="toc-section-number">13.1</span> <code>readr</code></a></li>
<li><a href="data.table-enhanced-data.html#data.table-enhanced-data.frame" id="toc-data.table-enhanced-data.frame"><span class="toc-section-number">13.2</span> <code>data.table</code>— enhanced <code>data.frame</code></a></li>
</ul></li>
<li><a href="r-code-for-data-simulation.html#r-code-for-data-simulation" id="toc-r-code-for-data-simulation"><span class="toc-section-number">14</span> R code for data simulation</a>
<ul>
<li><a href="appendixdata1.html#appendixdata1" id="toc-appendixdata1"><span class="toc-section-number">14.1</span> Customer Data for Clothing Company</a></li>
<li><a href="appendixdata3.html#appendixdata3" id="toc-appendixdata3"><span class="toc-section-number">14.2</span> Swine Disease Breakout Data</a></li>
</ul></li>
<li><a href="references.html#references" id="toc-references">References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Practitioner’s Guide to Data Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="datasplittingresampling" class="section level2 hasAnchor" number="7.2">
<h2><span class="header-section-number">7.2</span> Data Splitting and Resampling<a href="datasplittingresampling.html#datasplittingresampling" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Highly adaptable models can model complex relationships. However, they tend to overfit, which leads to a poor prediction by learning too much from the current sample set. Those models are susceptible to the specific sample set used to fit them. The model prediction may be off when future data is unlike past data. Conversely, a simple model, such as ordinary linear regression, tends to underfit, leading to a poor prediction by learning too little from the data. It systematically over-predicts or under-predicts the data regardless of how well future data resemble past data.</p>
<p>Model evaluation is essential to assess the efficacy of a model. A modeler needs to understand how a model fits the existing data and how it would work on future data. Also, trying multiple models and comparing them is always a good practice. All these need data splitting and resampling.</p>
<div id="datasplitting" class="section level3 hasAnchor" number="7.2.1">
<h3><span class="header-section-number">7.2.1</span> Data Splitting<a href="datasplittingresampling.html#datasplitting" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><em>Data splitting</em> is to put part of the data aside as an evaluation set (or hold-outs, out-of-bag samples) and use the rest for model tuning. Training samples are also called in-sample. Model performance metrics evaluated using in-sample are retrodictive, not predictive.</p>
<p>Traditional business intelligence usually handles data description. Answer simple questions by querying and summarizing the data, such as:</p>
<ul>
<li>What are the monthly sales of a product in 2020?</li>
<li>What is the number of site visits in the past month?</li>
<li>What is the sales difference in 2021 for two different product designs?</li>
</ul>
<p>There is no need to go through the tedious process of splitting the data, tuning, and evaluating a model to answer questions of this kind.</p>
<p>Some models have hyperparameters (aka. tuning parameters) not derived by training the model, such as the penalty parameter in lasso, the number of trees in a random forest, and the learning rate in deep learning. They often control the model’s process, and no analytical formula is available to calculate the optimized value. A poor choice can result in over-fitting, under-fitting, or optimization failure. A standard approach to searching for the best tuning parameters is through cross-validation, which is a data resampling approach.</p>
<p>To get a reasonable performance precision based on a single test set, the size of the test set may need to be large. So a conventional approach is to use a subset of samples to fit the model and use the rest to evaluate model performance. This process will repeat multiple times to get a performance profile. In that sense, resampling is based on splitting. The general steps are:</p>
<div class="figure">
<img src="images/ParameterTuningProcess.png" style="width:80.0%" alt="" />
<p class="caption">Parameter Tuning Process</p>
</div>
<p>The above is an outline of the general procedure to tune parameters. Now let’s focus on the critical part of the process: data splitting. Ideally, we should evaluate the model using samples not used to build or fine-tune the model. So it provides an unbiased sense of model effectiveness. When the sample size is large, it is a good practice to set aside part of the samples to evaluate the final model. People use <strong>training data</strong> to indicate the sample set used to fit the model. Use <strong>testing data</strong> to tune hyperparameters and <strong>validation data</strong> to evaluate performance and compare different models.</p>
<p>Let’s focus on data splitting in the model tuning process, where we split data into training and testing sets.</p>
<p>The first decision is the proportion of data in the test set. There are two factors to consider here: (1) sample size; (2) computation intensity. Suppose the sample size is large enough, which is the most common situation according to my experience. In that case, you can try using 20%, 30%, and 40% of the data as the test set and see which works best. If the model is computationally intense, you may consider starting from a smaller subset to train the model and hence have a higher portion of data in the test set. You may need to increase the training set depending on how it performs. If the sample size is small, you can use cross-validation or bootstrap, which is the topic of the next section.</p>
<p>The next is to decide which samples are in the test set. There is a desire to make the training and test sets as similar as possible. A simple way is to split data randomly, which does not control for any data attributes. However, sometimes we may want to ensure that training and testing data have a similar outcome distribution. For example, suppose you want to predict the likelihood of customer retention. In that case, you want two data sets with a similar percentage of retained customers.</p>
<p>There are three main ways to split the data that account for the similarity of resulted data sets. We will describe the three approaches using the clothing company’s customer data as examples.</p>
<ol style="list-style-type: decimal">
<li>Split data according to the outcome variable</li>
</ol>
<p>Assume the outcome variable is customer segment (column <code>segment</code>), and we decide to use 80% as training and 20% as testing. The goal is to make the proportions of the categories in the two sets as similar as possible. The <code>createDataPartition()</code> function in <code>caret</code> will return a balanced splitting based on assigned variable.</p>
<div class="sourceCode" id="cb149"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb149-1"><a href="datasplittingresampling.html#cb149-1" aria-hidden="true" tabindex="-1"></a><span class="co"># load data</span></span>
<span id="cb149-2"><a href="datasplittingresampling.html#cb149-2" aria-hidden="true" tabindex="-1"></a>sim.dat <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;http://bit.ly/2P5gTw4&quot;</span>)</span>
<span id="cb149-3"><a href="datasplittingresampling.html#cb149-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(caret)</span>
<span id="cb149-4"><a href="datasplittingresampling.html#cb149-4" aria-hidden="true" tabindex="-1"></a><span class="co"># set random seed to make sure reproducibility</span></span>
<span id="cb149-5"><a href="datasplittingresampling.html#cb149-5" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">3456</span>)</span>
<span id="cb149-6"><a href="datasplittingresampling.html#cb149-6" aria-hidden="true" tabindex="-1"></a>trainIndex <span class="ot">&lt;-</span> <span class="fu">createDataPartition</span>(sim.dat<span class="sc">$</span>segment, </span>
<span id="cb149-7"><a href="datasplittingresampling.html#cb149-7" aria-hidden="true" tabindex="-1"></a>                                  <span class="at">p =</span> <span class="fl">0.8</span>, </span>
<span id="cb149-8"><a href="datasplittingresampling.html#cb149-8" aria-hidden="true" tabindex="-1"></a>                                  <span class="at">list =</span> <span class="cn">FALSE</span>,</span>
<span id="cb149-9"><a href="datasplittingresampling.html#cb149-9" aria-hidden="true" tabindex="-1"></a>                                  <span class="at">times =</span> <span class="dv">1</span>)</span>
<span id="cb149-10"><a href="datasplittingresampling.html#cb149-10" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(trainIndex)</span></code></pre></div>
<pre><code>##      Resample1
## [1,]         1
## [2,]         2
## [3,]         3
## [4,]         4
## [5,]         6
## [6,]         7</code></pre>
<p>The <code>list = FALSE</code> in the call to <code>createDataPartition</code> is to return a data frame. The <code>times = 1</code> tells R how many times you want to split the data. Here we only do it once, but you can repeat the splitting multiple times. In that case, the function will return multiple vectors indicating the rows to training/test. You can set <code>times = 2</code> and rerun the above code to see the result. Then we can use the returned indicator vector <code>trainIndex</code> to get training and test sets:</p>
<div class="sourceCode" id="cb151"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb151-1"><a href="datasplittingresampling.html#cb151-1" aria-hidden="true" tabindex="-1"></a><span class="co"># get training set</span></span>
<span id="cb151-2"><a href="datasplittingresampling.html#cb151-2" aria-hidden="true" tabindex="-1"></a>datTrain <span class="ot">&lt;-</span> sim.dat[trainIndex, ]</span>
<span id="cb151-3"><a href="datasplittingresampling.html#cb151-3" aria-hidden="true" tabindex="-1"></a><span class="co"># get test set</span></span>
<span id="cb151-4"><a href="datasplittingresampling.html#cb151-4" aria-hidden="true" tabindex="-1"></a>datTest <span class="ot">&lt;-</span> sim.dat[<span class="sc">-</span>trainIndex, ]</span></code></pre></div>
<p>According to the setting, there are 800 samples in the training set and 200 in the testing set. Let’s check the distribution of the two groups:</p>
<div class="sourceCode" id="cb152"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb152-1"><a href="datasplittingresampling.html#cb152-1" aria-hidden="true" tabindex="-1"></a>datTrain <span class="sc">%&gt;%</span></span>
<span id="cb152-2"><a href="datasplittingresampling.html#cb152-2" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">group_by</span>(segment) <span class="sc">%&gt;%</span></span>
<span id="cb152-3"><a href="datasplittingresampling.html#cb152-3" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">summarise</span>(<span class="at">count =</span> <span class="fu">n</span>(),</span>
<span id="cb152-4"><a href="datasplittingresampling.html#cb152-4" aria-hidden="true" tabindex="-1"></a>            <span class="at">percentage =</span> <span class="fu">round</span>(<span class="fu">length</span>(segment)<span class="sc">/</span><span class="fu">nrow</span>(datTrain), <span class="dv">2</span>))</span></code></pre></div>
<pre><code>## # A tibble: 4 × 3
##   segment     count percentage
##   &lt;chr&gt;       &lt;int&gt;      &lt;dbl&gt;
## 1 Conspicuous   160       0.2 
## 2 Price         200       0.25
## 3 Quality       160       0.2 
## 4 Style         280       0.35</code></pre>
<div class="sourceCode" id="cb154"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb154-1"><a href="datasplittingresampling.html#cb154-1" aria-hidden="true" tabindex="-1"></a>datTest <span class="sc">%&gt;%</span></span>
<span id="cb154-2"><a href="datasplittingresampling.html#cb154-2" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">group_by</span>(segment) <span class="sc">%&gt;%</span></span>
<span id="cb154-3"><a href="datasplittingresampling.html#cb154-3" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">summarise</span>(<span class="at">count =</span> <span class="fu">n</span>(),</span>
<span id="cb154-4"><a href="datasplittingresampling.html#cb154-4" aria-hidden="true" tabindex="-1"></a>            <span class="at">percentage =</span> <span class="fu">round</span>(<span class="fu">length</span>(segment)<span class="sc">/</span><span class="fu">nrow</span>(datTest), <span class="dv">2</span>))</span></code></pre></div>
<pre><code>## # A tibble: 4 × 3
##   segment     count percentage
##   &lt;chr&gt;       &lt;int&gt;      &lt;dbl&gt;
## 1 Conspicuous    40       0.2 
## 2 Price          50       0.25
## 3 Quality        40       0.2 
## 4 Style          70       0.35</code></pre>
<p>The percentages are the same for these two sets. In practice, it is possible that the distributions are not identical but should be close.</p>
<ol start="2" style="list-style-type: decimal">
<li>Divide data according to predictors</li>
</ol>
<p>An alternative way is to split data based on the predictors. The goal is to get a diverse subset from a dataset to represent the sample. In other words, we need an algorithm to identify the <span class="math inline">\(n\)</span> most diverse samples from a dataset with size <span class="math inline">\(N\)</span>. However, the task is generally infeasible for non-trivial values of <span class="math inline">\(n\)</span> and <span class="math inline">\(N\)</span> <span class="citation">(<a href="#ref-willett" role="doc-biblioref">Willett 2004</a>)</span>. And hence practicable approaches to dissimilarity-based selection involve approximate methods that are sub-optimal. A major class of algorithms split the data on <em>maximum dissimilarity sampling</em>. The process starts from:</p>
<ul>
<li>Initialize a single sample as starting test set</li>
<li>Calculate the dissimilarity between this initial sample and each remaining samples in the dataset</li>
<li>Add the most dissimilar unallocated sample to the test set</li>
</ul>
<p>To move forward, we need to define the dissimilarity between groups. Each definition results in a different version of the algorithm and hence a different subset. It is the same problem as in hierarchical clustering where you need to define a way to measure the distance between clusters. The possible approaches are to use minimum, maximum, sum of all distances, the average of all distances, etc. Unfortunately, there is not a single best choice, and you may have to try multiple methods and check the resulted sample sets. R users can implement the algorithm using <code>maxDissim()</code> function from <code>caret</code> package. The <code>obj</code> argument is to set the definition of dissimilarity. Refer to the help documentation for more details (<code>?maxDissim</code>).</p>
<p>Let’s use two variables (<code>age</code> and <code>income</code>) from the customer data as an example to illustrate how it works in R and compare maximum dissimilarity sampling with random sampling.</p>
<div class="sourceCode" id="cb156"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb156-1"><a href="datasplittingresampling.html#cb156-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(lattice)</span>
<span id="cb156-2"><a href="datasplittingresampling.html#cb156-2" aria-hidden="true" tabindex="-1"></a><span class="co"># select variables</span></span>
<span id="cb156-3"><a href="datasplittingresampling.html#cb156-3" aria-hidden="true" tabindex="-1"></a>testing <span class="ot">&lt;-</span> <span class="fu">subset</span>(sim.dat, <span class="at">select =</span> <span class="fu">c</span>(<span class="st">&quot;age&quot;</span>, <span class="st">&quot;income&quot;</span>))</span></code></pre></div>
<p>Random select 5 samples as initial subset (<code>start</code>) , the rest will be in <code>samplePool</code>:</p>
<div class="sourceCode" id="cb157"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb157-1"><a href="datasplittingresampling.html#cb157-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">5</span>)</span>
<span id="cb157-2"><a href="datasplittingresampling.html#cb157-2" aria-hidden="true" tabindex="-1"></a><span class="co"># select 5 random samples</span></span>
<span id="cb157-3"><a href="datasplittingresampling.html#cb157-3" aria-hidden="true" tabindex="-1"></a>startSet <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">dim</span>(testing)[<span class="dv">1</span>], <span class="dv">5</span>)</span>
<span id="cb157-4"><a href="datasplittingresampling.html#cb157-4" aria-hidden="true" tabindex="-1"></a>start <span class="ot">&lt;-</span> testing[startSet, ]</span>
<span id="cb157-5"><a href="datasplittingresampling.html#cb157-5" aria-hidden="true" tabindex="-1"></a><span class="co"># save the rest in data frame &#39;samplePool&#39;</span></span>
<span id="cb157-6"><a href="datasplittingresampling.html#cb157-6" aria-hidden="true" tabindex="-1"></a>samplePool <span class="ot">&lt;-</span> testing[<span class="sc">-</span>startSet, ]</span></code></pre></div>
<p>Use <code>maxDissim()</code> to select another 5 samples from <code>samplePool</code> that are as different as possible with the initical set <code>start</code>:</p>
<div class="sourceCode" id="cb158"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb158-1"><a href="datasplittingresampling.html#cb158-1" aria-hidden="true" tabindex="-1"></a>selectId <span class="ot">&lt;-</span> <span class="fu">maxDissim</span>(start, samplePool, <span class="at">obj =</span> minDiss, <span class="at">n =</span> <span class="dv">5</span>)</span>
<span id="cb158-2"><a href="datasplittingresampling.html#cb158-2" aria-hidden="true" tabindex="-1"></a>minDissSet <span class="ot">&lt;-</span> samplePool[selectId, ]</span></code></pre></div>
<p>The <code>obj = minDiss</code> in the above code tells R to use minimum dissimilarity to define the distance between groups. Next, random select 5 samples from <code>samplePool</code> in data frame <code>RandomSet</code>:</p>
<div class="sourceCode" id="cb159"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb159-1"><a href="datasplittingresampling.html#cb159-1" aria-hidden="true" tabindex="-1"></a>selectId <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">dim</span>(samplePool)[<span class="dv">1</span>], <span class="dv">5</span>)</span>
<span id="cb159-2"><a href="datasplittingresampling.html#cb159-2" aria-hidden="true" tabindex="-1"></a>RandomSet <span class="ot">&lt;-</span> samplePool[selectId, ]</span></code></pre></div>
<p>Plot the resulted set to compare different sampling methods:</p>
<div class="sourceCode" id="cb160"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb160-1"><a href="datasplittingresampling.html#cb160-1" aria-hidden="true" tabindex="-1"></a>start<span class="sc">$</span>group <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="st">&quot;Initial Set&quot;</span>, <span class="fu">nrow</span>(start))</span>
<span id="cb160-2"><a href="datasplittingresampling.html#cb160-2" aria-hidden="true" tabindex="-1"></a>minDissSet<span class="sc">$</span>group <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="st">&quot;Maximum Dissimilarity Sampling&quot;</span>, </span>
<span id="cb160-3"><a href="datasplittingresampling.html#cb160-3" aria-hidden="true" tabindex="-1"></a>                        <span class="fu">nrow</span>(minDissSet))</span>
<span id="cb160-4"><a href="datasplittingresampling.html#cb160-4" aria-hidden="true" tabindex="-1"></a>RandomSet<span class="sc">$</span>group <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="st">&quot;Random Sampling&quot;</span>, </span>
<span id="cb160-5"><a href="datasplittingresampling.html#cb160-5" aria-hidden="true" tabindex="-1"></a>                       <span class="fu">nrow</span>(RandomSet))</span>
<span id="cb160-6"><a href="datasplittingresampling.html#cb160-6" aria-hidden="true" tabindex="-1"></a><span class="fu">xyplot</span>(age <span class="sc">~</span> income, </span>
<span id="cb160-7"><a href="datasplittingresampling.html#cb160-7" aria-hidden="true" tabindex="-1"></a>       <span class="at">data =</span> <span class="fu">rbind</span>(start, minDissSet, RandomSet), </span>
<span id="cb160-8"><a href="datasplittingresampling.html#cb160-8" aria-hidden="true" tabindex="-1"></a>       <span class="at">grid =</span> <span class="cn">TRUE</span>, </span>
<span id="cb160-9"><a href="datasplittingresampling.html#cb160-9" aria-hidden="true" tabindex="-1"></a>       <span class="at">group =</span> group, </span>
<span id="cb160-10"><a href="datasplittingresampling.html#cb160-10" aria-hidden="true" tabindex="-1"></a>       <span class="at">auto.key =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:maxdis"></span>
<img src="IDS_files/figure-html/maxdis-1.svg" alt="Compare Maximum Dissimilarity Sampling with  Random Sampling" width="80%" />
<p class="caption">
FIGURE 7.3: Compare Maximum Dissimilarity Sampling with Random Sampling
</p>
</div>
<p>The points from maximum dissimilarity sampling are far away from the initial samples ( Fig. <a href="datasplittingresampling.html#fig:maxdis">7.3</a>, while the random samples are much closer to the initial ones. Why do we need a diverse subset? Because we hope the test set to be representative. If all test set samples are from respondents younger than 30, model performance on the test set has a high risk to fail to tell you how the model will perform on more general population.</p>
<ul>
<li>Divide data according to time</li>
</ul>
<p>For time series data, random sampling is usually not the best way. There is an approach to divide data according to time-series. Since time series is beyond the scope of this book, there is not much discussion here. For more detail of this method, see <span class="citation">(<a href="#ref-Hyndman" role="doc-biblioref">Hyndman and Athanasopoulos 2013</a>)</span>. We will use a simulated first-order autoregressive model (i.e., AR(1) model) time-series data with 100 observations to show how to implement using the function <code>createTimeSlices()</code> in the <code>caret</code> package.</p>
<div class="sourceCode" id="cb161"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb161-1"><a href="datasplittingresampling.html#cb161-1" aria-hidden="true" tabindex="-1"></a><span class="co"># simulte AR(1) time series samples</span></span>
<span id="cb161-2"><a href="datasplittingresampling.html#cb161-2" aria-hidden="true" tabindex="-1"></a>timedata <span class="ot">=</span> <span class="fu">arima.sim</span>(<span class="fu">list</span>(<span class="at">order=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>), <span class="at">ar=</span><span class="sc">-</span>.<span class="dv">9</span>), <span class="at">n=</span><span class="dv">100</span>)</span>
<span id="cb161-3"><a href="datasplittingresampling.html#cb161-3" aria-hidden="true" tabindex="-1"></a><span class="co"># plot time series</span></span>
<span id="cb161-4"><a href="datasplittingresampling.html#cb161-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(timedata, <span class="at">main=</span>(<span class="fu">expression</span>(<span class="fu">AR</span>(<span class="dv">1</span>)<span class="sc">~</span><span class="er">~~</span>phi<span class="sc">==-</span>.<span class="dv">9</span>)))     </span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:times"></span>
<img src="IDS_files/figure-html/times-1.svg" alt="Divide data according to time" width="80%" />
<p class="caption">
FIGURE 7.4: Divide data according to time
</p>
</div>
<p>Fig. <a href="datasplittingresampling.html#fig:times">7.4</a> shows 100 simulated time series observation. The goal is to make sure both training and test set to cover the whole period.</p>
<div class="sourceCode" id="cb162"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb162-1"><a href="datasplittingresampling.html#cb162-1" aria-hidden="true" tabindex="-1"></a>timeSlices <span class="ot">&lt;-</span> <span class="fu">createTimeSlices</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(timedata), </span>
<span id="cb162-2"><a href="datasplittingresampling.html#cb162-2" aria-hidden="true" tabindex="-1"></a>                   <span class="at">initialWindow =</span> <span class="dv">36</span>, </span>
<span id="cb162-3"><a href="datasplittingresampling.html#cb162-3" aria-hidden="true" tabindex="-1"></a>                   <span class="at">horizon =</span> <span class="dv">12</span>, </span>
<span id="cb162-4"><a href="datasplittingresampling.html#cb162-4" aria-hidden="true" tabindex="-1"></a>                   <span class="at">fixedWindow =</span> T)</span>
<span id="cb162-5"><a href="datasplittingresampling.html#cb162-5" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(timeSlices,<span class="at">max.level =</span> <span class="dv">1</span>)</span></code></pre></div>
<pre><code>## List of 2
##  $ train:List of 53
##  $ test :List of 53</code></pre>
<p>There are three arguments in the above <code>createTimeSlices()</code>.</p>
<ul>
<li><code>initialWindow</code>: The initial number of consecutive values in each training set sample</li>
<li><code>horizon</code>: the number of consecutive values in test set sample</li>
<li><code>fixedWindow</code>: if FALSE, all training samples start at 1</li>
</ul>
<p>The function returns two lists, one for the training set, the other for the test set. Let’s look at the first training sample:</p>
<div class="sourceCode" id="cb164"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb164-1"><a href="datasplittingresampling.html#cb164-1" aria-hidden="true" tabindex="-1"></a><span class="co"># get result for the 1st training set</span></span>
<span id="cb164-2"><a href="datasplittingresampling.html#cb164-2" aria-hidden="true" tabindex="-1"></a>trainSlices <span class="ot">&lt;-</span> timeSlices[[<span class="dv">1</span>]]</span>
<span id="cb164-3"><a href="datasplittingresampling.html#cb164-3" aria-hidden="true" tabindex="-1"></a><span class="co"># get result for the 1st test set</span></span>
<span id="cb164-4"><a href="datasplittingresampling.html#cb164-4" aria-hidden="true" tabindex="-1"></a>testSlices <span class="ot">&lt;-</span> timeSlices[[<span class="dv">2</span>]]</span>
<span id="cb164-5"><a href="datasplittingresampling.html#cb164-5" aria-hidden="true" tabindex="-1"></a><span class="co"># check the index for the 1st training and test set</span></span>
<span id="cb164-6"><a href="datasplittingresampling.html#cb164-6" aria-hidden="true" tabindex="-1"></a>trainSlices[[<span class="dv">1</span>]]</span></code></pre></div>
<pre><code>##  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17
## [18] 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34
## [35] 35 36</code></pre>
<div class="sourceCode" id="cb166"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb166-1"><a href="datasplittingresampling.html#cb166-1" aria-hidden="true" tabindex="-1"></a>testSlices[[<span class="dv">1</span>]]</span></code></pre></div>
<pre><code>##  [1] 37 38 39 40 41 42 43 44 45 46 47 48</code></pre>
<p>The first training set is consist of sample 1-36 in the dataset (<code>initialWindow = 36</code>). Then sample 37-48 are in the first test set ( <code>horizon = 12</code>). Type <code>head(trainSlices)</code> or <code>head(testSlices)</code> to check the later samples. If you are not clear about the argument <code>fixedWindow</code>, try to change the setting to be <code>F</code> and check the change in <code>trainSlices</code> and <code>testSlices</code>.</p>
<p>Understand and implement data splitting is not difficult. But there are two things to note:</p>
<ol style="list-style-type: decimal">
<li>The randomness in the splitting process will lead to uncertainty in performance measurement.</li>
<li>When the dataset is small, it can be too expensive to leave out test set. In this situation, if collecting more data is just not possible, the best shot is to use leave-one-out cross-validation which is discussed in the next section.</li>
</ol>
</div>
<div id="resampling" class="section level3 hasAnchor" number="7.2.2">
<h3><span class="header-section-number">7.2.2</span> Resampling<a href="datasplittingresampling.html#resampling" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>You can consider resampling as repeated splitting. The basic idea is: use part of the data to fit model and then use the rest of data to calculate model performance. Repeat the process multiple times and aggregate the results. The differences in resampling techniques usually center around the ways to choose subsamples. There are two main reasons that we may need resampling:</p>
<ol style="list-style-type: decimal">
<li><p>Estimate tuning parameters through resampling. Some examples of models with such parameters are Support Vector Machine (SVM), models including the penalty (LASSO) and random forest.</p></li>
<li><p>For models without tuning parameter, such as ordinary linear regression and partial least square regression, the model fitting doesn’t require resampling. But you can study the model stability through resampling.</p></li>
</ol>
<p>We will introduce three most common resampling techniques: k-fold cross-validation, repeated training/test splitting, and bootstrap.</p>
<div id="k-fold-cross-validation" class="section level4 hasAnchor" number="7.2.2.1">
<h4><span class="header-section-number">7.2.2.1</span> k-fold cross-validation<a href="datasplittingresampling.html#k-fold-cross-validation" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>k-fold cross-validation is to partition the original sample into <span class="math inline">\(k\)</span> equal size subsamples (folds). Use one of the <span class="math inline">\(k\)</span> folds to validate the model and the rest <span class="math inline">\(k-1\)</span> to train model. Then repeat the process <span class="math inline">\(k\)</span> times with each of the <span class="math inline">\(k\)</span> folds as the test set. Aggregate the results into a performance profile.</p>
<p>Denote by <span class="math inline">\(\hat{f}^{-\kappa}(X)\)</span> the fitted function, computed with the <span class="math inline">\(\kappa^{th}\)</span> fold removed and <span class="math inline">\(x_i^\kappa\)</span> the predictors for samples in left-out fold. The process of k-fold cross-validation is as follows:</p>
<p>It is a standard way to find the value of tuning parameter that gives you the best performance. It is also a way to study the variability of model performance.</p>
<p>The following figure represents a 5-fold cross-validation example.</p>
<div class="figure">
<img src="images/cv5fold.png" alt="" />
<p class="caption">5-fold cross-validation</p>
</div>
<p>A special case of k-fold cross-validation is Leave One Out Cross Validation (LOOCV) where <span class="math inline">\(k=1\)</span>. When sample size is small, it is desired to use as many data to train the model. Most of the functions have default setting <span class="math inline">\(k=10\)</span>. The choice is usually 5-10 in practice, but there is no standard rule. The more folds to use, the more samples are used to fit model, and then the performance estimate is closer to the theoretical performance. Meanwhile, the variance of the performance is larger since the samples to fit model in different iterations are more similar. However, LOOCV has high computational cost since the number of interactions is the same as the sample size and each model fit uses a subset that is nearly the same size of the training set. On the other hand, when k is small (such as 2 or 3), the computation is more efficient, but the bias will increase. When the sample size is large, the impact of <span class="math inline">\(k\)</span> becomes marginal.</p>
<p>Chapter 7 of <span class="citation">(<a href="#ref-Hastie2008" role="doc-biblioref">Hastie T 2008</a>)</span> presents a more in-depth and more detailed discussion about the bias-variance trade-off in k-fold cross-validation.</p>
<p>You can implement k-fold cross-validation using <code>createFolds()</code> in <code>caret</code>:</p>
<div class="sourceCode" id="cb168"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb168-1"><a href="datasplittingresampling.html#cb168-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(caret)</span>
<span id="cb168-2"><a href="datasplittingresampling.html#cb168-2" aria-hidden="true" tabindex="-1"></a>class <span class="ot">&lt;-</span> sim.dat<span class="sc">$</span>segment</span>
<span id="cb168-3"><a href="datasplittingresampling.html#cb168-3" aria-hidden="true" tabindex="-1"></a><span class="co"># creat k-folds</span></span>
<span id="cb168-4"><a href="datasplittingresampling.html#cb168-4" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb168-5"><a href="datasplittingresampling.html#cb168-5" aria-hidden="true" tabindex="-1"></a>cv <span class="ot">&lt;-</span> <span class="fu">createFolds</span>(class, <span class="at">k =</span> <span class="dv">10</span>, <span class="at">returnTrain =</span> T)</span>
<span id="cb168-6"><a href="datasplittingresampling.html#cb168-6" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(cv)</span></code></pre></div>
<pre><code>## List of 10
##  $ Fold01: int [1:900] 1 2 3 4 5 6 7 8 9 10 ...
##  $ Fold02: int [1:900] 1 2 3 4 5 6 7 9 10 11 ...
##  $ Fold03: int [1:900] 1 2 3 4 5 6 7 8 10 11 ...
##  $ Fold04: int [1:900] 1 2 3 4 5 6 7 8 9 11 ...
##  $ Fold05: int [1:900] 1 3 4 6 7 8 9 10 11 12 ...
##  $ Fold06: int [1:900] 1 2 3 4 5 6 7 8 9 10 ...
##  $ Fold07: int [1:900] 2 3 4 5 6 7 8 9 10 11 ...
##  $ Fold08: int [1:900] 1 2 3 4 5 8 9 10 11 12 ...
##  $ Fold09: int [1:900] 1 2 4 5 6 7 8 9 10 11 ...
##  $ Fold10: int [1:900] 1 2 3 5 6 7 8 9 10 11 ...</code></pre>
<p>The above code creates ten folds (<code>k=10</code>) according to the customer segments (we set <code>class</code> to be the categorical variable <code>segment</code>). The function returns a list of 10 with the index of rows in training set.</p>
</div>
<div id="repeated-trainingtest-splits" class="section level4 hasAnchor" number="7.2.2.2">
<h4><span class="header-section-number">7.2.2.2</span> Repeated Training/Test Splits<a href="datasplittingresampling.html#repeated-trainingtest-splits" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>In fact, this method is nothing but repeating the training/test set division on the original data. Fit the model with the training set, and evaluate the model with the test set. Unlike k-fold cross-validation, the test set generated by this procedure may have duplicate samples. A sample usually shows up in more than one test sets. There is no standard rule for split ratio and number of repetitions. The most common choice in practice is to use 75% to 80% of the total sample for training. The remaining samples are for validation. The more sample in the training set, the less biased the model performance estimate is. Increasing the repetitions can reduce the uncertainty in the performance estimates. Of course, it is at the cost of computational time when the model is complex. The number of repetitions is also related to the sample size of the test set. If the size is small, the performance estimate is more volatile. In this case, the number of repetitions needs to be higher to deal with the uncertainty of the evaluation results.</p>
<p>We can use the same function (<code>createDataPartition ()</code>) as before. If you look back, you will see <code>times = 1</code>. The only thing to change is to set it to the number of repetitions.</p>
<div class="sourceCode" id="cb170"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb170-1"><a href="datasplittingresampling.html#cb170-1" aria-hidden="true" tabindex="-1"></a>trainIndex <span class="ot">&lt;-</span> <span class="fu">createDataPartition</span>(sim.dat<span class="sc">$</span>segment, </span>
<span id="cb170-2"><a href="datasplittingresampling.html#cb170-2" aria-hidden="true" tabindex="-1"></a>                                  <span class="at">p =</span> .<span class="dv">8</span>, </span>
<span id="cb170-3"><a href="datasplittingresampling.html#cb170-3" aria-hidden="true" tabindex="-1"></a>                                  <span class="at">list =</span> <span class="cn">FALSE</span>, </span>
<span id="cb170-4"><a href="datasplittingresampling.html#cb170-4" aria-hidden="true" tabindex="-1"></a>                                  <span class="at">times =</span> <span class="dv">5</span>)</span>
<span id="cb170-5"><a href="datasplittingresampling.html#cb170-5" aria-hidden="true" tabindex="-1"></a>dplyr<span class="sc">::</span><span class="fu">glimpse</span>(trainIndex)</span></code></pre></div>
<pre><code>##  int [1:800, 1:5] 1 3 4 5 6 7 8 9 10 11 ...
##  - attr(*, &quot;dimnames&quot;)=List of 2
##   ..$ : NULL
##   ..$ : chr [1:5] &quot;Resample1&quot; &quot;Resample2&quot; &quot;Resample3&quot; &quot;Resample4&quot; ...</code></pre>
<p>Once know how to split the data, the repetition comes naturally.</p>
</div>
<div id="bootstrap-methods" class="section level4 hasAnchor" number="7.2.2.3">
<h4><span class="header-section-number">7.2.2.3</span> Bootstrap Methods<a href="datasplittingresampling.html#bootstrap-methods" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Bootstrap is a powerful statistical tool (a little magic too). It can be used to analyze the uncertainty of parameter estimates <span class="citation">(<a href="#ref-bootstrap1986" role="doc-biblioref">Efron and Tibshirani 1986</a>)</span> quantitatively. For example, estimate the standard deviation of linear regression coefficients. The power of this method is that the concept is so simple that it can be easily applied to any model as long as the computation allows. However, you can hardly obtain the standard deviation for some models by using the traditional statistical inference.</p>
<p>Since it is with replacement, a sample can be selected multiple times, and the bootstrap sample size is the same as the original data. So for every bootstrap set, there are some left-out samples, which is also called “out-of-bag samples.” The out-of-bag sample is used to evaluate the model. Efron points out that under normal circumstances <span class="citation">(<a href="#ref-efron1983" role="doc-biblioref">Efron 1983</a>)</span>, bootstrap estimates the error rate of the model with more certainty. The probability of an observation <span class="math inline">\(i\)</span> in bootstrap sample B is:</p>
<p><span class="math inline">\(\begin{array}{ccc} Pr{i\in B} &amp; = &amp; 1-\left(1-\frac{1}{N}\right)^{N}\\  &amp; \approx &amp; 1-e^{-1}\\  &amp; = &amp; 0.632 \end{array}\)</span></p>
<p>On average, 63.2% of the observations appeared at least once in a bootstrap sample, so the estimation bias is similar to 2-fold cross-validation. As mentioned earlier, the smaller the number of folds, the larger the bias. Increasing the sample size will ease the problem. In general, bootstrap has larger bias and smaller variance than cross-validation. Efron came up the following “.632 estimator” to alleviate this bias:</p>
<p><span class="math display">\[(0.632 × original\ bootstrap\ estimate) + (0.368 × apparent\ error\ rate)\]</span></p>
<p>The apparent error rate is the error rate when the data is used twice, both to fit the model and to check its accuracy and it is apparently over-optimistic. The modified bootstrap estimate reduces the bias but can be unstable with small samples size. This estimate can also be unduly optimistic when the model severely over-fits since the apparent error rate will be close to zero. Efron and Tibshirani <span class="citation">(<a href="#ref-b632plus" role="doc-biblioref">Efron and Tibshirani 1997</a>)</span> discuss another technique, called the “632+ method,” for adjusting the bootstrap estimates.</p>

</div>
</div>
</div>
<!-- </div> -->
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-efron1983" class="csl-entry">
Efron, B. 1983. <span>“Estimating the Error Rate of a Prediction Rule: Improvement on Cross-Validation.”</span> <em>Journal of the American Statistical Association</em>, 316–31.
</div>
<div id="ref-bootstrap1986" class="csl-entry">
Efron, B, and R Tibshirani. 1986. <span>“Bootstrap Methods for Standard Errors, Confidence Intervals, and Other Measures of Statistical Accuracy.”</span> <em>Statistical Science</em>, 54–75.
</div>
<div id="ref-b632plus" class="csl-entry">
———. 1997. <span>“Improvements on Cross-Validation: The 632+ Bootstrap Method.”</span> <em>Journal of the American Statistical Association</em> 92 (438): 548–60.
</div>
<div id="ref-Hastie2008" class="csl-entry">
Hastie T, Friedman J, Tibshirani R. 2008. <em>The Elements of Statistical Learning: Data Mining, Inference and Prediction</em>. 2nd ed. Springer.
</div>
<div id="ref-Hyndman" class="csl-entry">
Hyndman, R. J., and G. Athanasopoulos. 2013. <em>Forecasting: Principles and Practice</em>. Vol. Section 2/5. OTect: Melbourne, Australia.
</div>
<div id="ref-willett" class="csl-entry">
Willett, Peter. 2004. <span>“Dissimilarity-Based Algorithms for Selecting Structurally Diverse Sets of Compounds.”</span> <em>Journal of Computational Biology</em> 6(3-4) (doi:10.1089/106652799318382): 447–57.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="vbtradeoff.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="measuring-performance.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": true,
"twitter": true,
"linkedin": true,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/happyrabbit/IntroDataScience/07-ModelTuning.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["IDS.pdf", "IDS.epub", "IDS.mobi"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section",
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
