<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>10.5 Penalized Generalized Linear Model | Introduction to Data Science</title>
  <meta name="description" content="10.5 Penalized Generalized Linear Model | Introduction to Data Science" />
  <meta name="generator" content="bookdown 0.17 and GitBook 2.6.7" />

  <meta property="og:title" content="10.5 Penalized Generalized Linear Model | Introduction to Data Science" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="http://scientistcafe.com/IDS/" />
  
  <meta property="og:description" content="10.5 Penalized Generalized Linear Model | Introduction to Data Science" />
  <meta name="github-repo" content="happyrabbit/IntroDataScience" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="10.5 Penalized Generalized Linear Model | Introduction to Data Science" />
  
  <meta name="twitter:description" content="10.5 Penalized Generalized Linear Model | Introduction to Data Science" />
  

<meta name="author" content="Hui Lin and Ming Li" />


<meta name="date" content="2020-02-12" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="elastic-net.html"/>
<link rel="next" href="treemodel.html"/>
<script src="libs/jquery/jquery.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="http://scientistcafe.com">Homepage</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="goal-of-the-book.html"><a href="goal-of-the-book.html"><i class="fa fa-check"></i>Goal of the Book</a></li>
<li class="chapter" data-level="" data-path="who-this-book-is-for.html"><a href="who-this-book-is-for.html"><i class="fa fa-check"></i>Who This Book Is For</a></li>
<li class="chapter" data-level="" data-path="what-this-book-covers.html"><a href="what-this-book-covers.html"><i class="fa fa-check"></i>What This Book Covers</a></li>
<li class="chapter" data-level="" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="about-the-authors.html"><a href="about-the-authors.html"><i class="fa fa-check"></i>About the Authors</a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="blind-men-and-an-elephant.html"><a href="blind-men-and-an-elephant.html"><i class="fa fa-check"></i><b>1.1</b> Blind men and an elephant</a><ul>
<li class="chapter" data-level="1.1.1" data-path="blind-men-and-an-elephant.html"><a href="blind-men-and-an-elephant.html#data-science-roleskill-tracks"><i class="fa fa-check"></i><b>1.1.1</b> Data science role/skill tracks</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="what-should-data-science-do.html"><a href="what-should-data-science-do.html"><i class="fa fa-check"></i><b>1.2</b> What should data science do?</a><ul>
<li class="chapter" data-level="1.2.1" data-path="what-should-data-science-do.html"><a href="what-should-data-science-do.html#lets-dream-big"><i class="fa fa-check"></i><b>1.2.1</b> Let’s dream big</a></li>
<li class="chapter" data-level="1.2.2" data-path="what-should-data-science-do.html"><a href="what-should-data-science-do.html#what-kind-of-questions-can-data-science-solve"><i class="fa fa-check"></i><b>1.2.2</b> What kind of questions can data science solve?</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="structure-data-science-team.html"><a href="structure-data-science-team.html"><i class="fa fa-check"></i><b>1.3</b> Structure data science team</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="soft-skills-for-data-scientists.html"><a href="soft-skills-for-data-scientists.html"><i class="fa fa-check"></i><b>2</b> Soft Skills for Data Scientists</a><ul>
<li class="chapter" data-level="2.1" data-path="comparison-between-statistician-and-data-scientist.html"><a href="comparison-between-statistician-and-data-scientist.html"><i class="fa fa-check"></i><b>2.1</b> Comparison between Statistician and Data Scientist</a></li>
<li class="chapter" data-level="2.2" data-path="beyond-data-and-analytics.html"><a href="beyond-data-and-analytics.html"><i class="fa fa-check"></i><b>2.2</b> Beyond Data and Analytics</a></li>
<li class="chapter" data-level="2.3" data-path="three-pillars-of-knowledge.html"><a href="three-pillars-of-knowledge.html"><i class="fa fa-check"></i><b>2.3</b> Three Pillars of Knowledge</a></li>
<li class="chapter" data-level="2.4" data-path="data-science-project-cycle.html"><a href="data-science-project-cycle.html"><i class="fa fa-check"></i><b>2.4</b> Data Science Project Cycle</a><ul>
<li class="chapter" data-level="2.4.1" data-path="data-science-project-cycle.html"><a href="data-science-project-cycle.html#types-of-data-science-projects"><i class="fa fa-check"></i><b>2.4.1</b> Types of Data Science Projects</a></li>
<li class="chapter" data-level="2.4.2" data-path="data-science-project-cycle.html"><a href="data-science-project-cycle.html#at-the-planning-stage"><i class="fa fa-check"></i><b>2.4.2</b> At the Planning Stage</a></li>
<li class="chapter" data-level="2.4.3" data-path="data-science-project-cycle.html"><a href="data-science-project-cycle.html#at-the-modeling-stage"><i class="fa fa-check"></i><b>2.4.3</b> At the Modeling Stage</a></li>
<li class="chapter" data-level="2.4.4" data-path="data-science-project-cycle.html"><a href="data-science-project-cycle.html#at-the-production-stage"><i class="fa fa-check"></i><b>2.4.4</b> At the Production Stage</a></li>
<li class="chapter" data-level="2.4.5" data-path="data-science-project-cycle.html"><a href="data-science-project-cycle.html#summary"><i class="fa fa-check"></i><b>2.4.5</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="common-mistakes-in-data-science.html"><a href="common-mistakes-in-data-science.html"><i class="fa fa-check"></i><b>2.5</b> Common Mistakes in Data Science</a><ul>
<li class="chapter" data-level="2.5.1" data-path="common-mistakes-in-data-science.html"><a href="common-mistakes-in-data-science.html#problem-formulation-stage"><i class="fa fa-check"></i><b>2.5.1</b> Problem Formulation Stage</a></li>
<li class="chapter" data-level="2.5.2" data-path="common-mistakes-in-data-science.html"><a href="common-mistakes-in-data-science.html#problem-planning-stage"><i class="fa fa-check"></i><b>2.5.2</b> Problem Planning Stage</a></li>
<li class="chapter" data-level="2.5.3" data-path="common-mistakes-in-data-science.html"><a href="common-mistakes-in-data-science.html#modeling-stage"><i class="fa fa-check"></i><b>2.5.3</b> Modeling Stage</a></li>
<li class="chapter" data-level="2.5.4" data-path="common-mistakes-in-data-science.html"><a href="common-mistakes-in-data-science.html#production-stage"><i class="fa fa-check"></i><b>2.5.4</b> Production Stage</a></li>
<li class="chapter" data-level="2.5.5" data-path="common-mistakes-in-data-science.html"><a href="common-mistakes-in-data-science.html#summary-1"><i class="fa fa-check"></i><b>2.5.5</b> Summary</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="introduction-to-the-data.html"><a href="introduction-to-the-data.html"><i class="fa fa-check"></i><b>3</b> Introduction to The Data</a><ul>
<li class="chapter" data-level="3.1" data-path="customer-data-for-a-clothing-company.html"><a href="customer-data-for-a-clothing-company.html"><i class="fa fa-check"></i><b>3.1</b> Customer Data for A Clothing Company</a></li>
<li class="chapter" data-level="3.2" data-path="customer-satisfaction-survey-data-from-airline-company.html"><a href="customer-satisfaction-survey-data-from-airline-company.html"><i class="fa fa-check"></i><b>3.2</b> Customer Satisfaction Survey Data from Airline Company</a></li>
<li class="chapter" data-level="3.3" data-path="swinediseasedata.html"><a href="swinediseasedata.html"><i class="fa fa-check"></i><b>3.3</b> Swine Disease Breakout Data</a></li>
<li class="chapter" data-level="3.4" data-path="mnist-dataset.html"><a href="mnist-dataset.html"><i class="fa fa-check"></i><b>3.4</b> MNIST Dataset</a></li>
<li class="chapter" data-level="3.5" data-path="imdb-dataset.html"><a href="imdb-dataset.html"><i class="fa fa-check"></i><b>3.5</b> IMDB Dataset</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="big-data-cloud-platform.html"><a href="big-data-cloud-platform.html"><i class="fa fa-check"></i><b>4</b> Big Data Cloud Platform</a><ul>
<li class="chapter" data-level="4.1" data-path="power-of-cluster-of-computers.html"><a href="power-of-cluster-of-computers.html"><i class="fa fa-check"></i><b>4.1</b> Power of Cluster of Computers</a></li>
<li class="chapter" data-level="4.2" data-path="evolution-of-cluster-computing.html"><a href="evolution-of-cluster-computing.html"><i class="fa fa-check"></i><b>4.2</b> Evolution of Cluster Computing</a><ul>
<li class="chapter" data-level="4.2.1" data-path="evolution-of-cluster-computing.html"><a href="evolution-of-cluster-computing.html#hadoop"><i class="fa fa-check"></i><b>4.2.1</b> Hadoop</a></li>
<li class="chapter" data-level="4.2.2" data-path="evolution-of-cluster-computing.html"><a href="evolution-of-cluster-computing.html#spark"><i class="fa fa-check"></i><b>4.2.2</b> Spark</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="CloudEnvironment.html"><a href="CloudEnvironment.html"><i class="fa fa-check"></i><b>4.3</b> Introduction of Cloud Environment</a><ul>
<li class="chapter" data-level="4.3.1" data-path="CloudEnvironment.html"><a href="CloudEnvironment.html#open-account-and-create-a-cluster"><i class="fa fa-check"></i><b>4.3.1</b> Open Account and Create a Cluster</a></li>
<li class="chapter" data-level="4.3.2" data-path="CloudEnvironment.html"><a href="CloudEnvironment.html#r-notebook"><i class="fa fa-check"></i><b>4.3.2</b> R Notebook</a></li>
<li class="chapter" data-level="4.3.3" data-path="CloudEnvironment.html"><a href="CloudEnvironment.html#markdown-cells"><i class="fa fa-check"></i><b>4.3.3</b> Markdown Cells</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="leverage-spark-using-r-notebook.html"><a href="leverage-spark-using-r-notebook.html"><i class="fa fa-check"></i><b>4.4</b> Leverage Spark Using R Notebook</a></li>
<li class="chapter" data-level="4.5" data-path="databases-and-sql.html"><a href="databases-and-sql.html"><i class="fa fa-check"></i><b>4.5</b> Databases and SQL</a><ul>
<li class="chapter" data-level="4.5.1" data-path="databases-and-sql.html"><a href="databases-and-sql.html#history"><i class="fa fa-check"></i><b>4.5.1</b> History</a></li>
<li class="chapter" data-level="4.5.2" data-path="databases-and-sql.html"><a href="databases-and-sql.html#database-table-and-view"><i class="fa fa-check"></i><b>4.5.2</b> Database, Table and View</a></li>
<li class="chapter" data-level="4.5.3" data-path="databases-and-sql.html"><a href="databases-and-sql.html#basic-sql-statement"><i class="fa fa-check"></i><b>4.5.3</b> Basic SQL Statement</a></li>
<li class="chapter" data-level="4.5.4" data-path="databases-and-sql.html"><a href="databases-and-sql.html#advanced-topics-in-database"><i class="fa fa-check"></i><b>4.5.4</b> Advanced Topics in Database</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="data-pre-processing.html"><a href="data-pre-processing.html"><i class="fa fa-check"></i><b>5</b> Data Pre-processing</a><ul>
<li class="chapter" data-level="5.1" data-path="data-cleaning.html"><a href="data-cleaning.html"><i class="fa fa-check"></i><b>5.1</b> Data Cleaning</a></li>
<li class="chapter" data-level="5.2" data-path="missing-values.html"><a href="missing-values.html"><i class="fa fa-check"></i><b>5.2</b> Missing Values</a><ul>
<li class="chapter" data-level="5.2.1" data-path="missing-values.html"><a href="missing-values.html#impute-missing-values-with-medianmode"><i class="fa fa-check"></i><b>5.2.1</b> Impute missing values with median/mode</a></li>
<li class="chapter" data-level="5.2.2" data-path="missing-values.html"><a href="missing-values.html#k-nearest-neighbors"><i class="fa fa-check"></i><b>5.2.2</b> K-nearest neighbors</a></li>
<li class="chapter" data-level="5.2.3" data-path="missing-values.html"><a href="missing-values.html#bagging-tree"><i class="fa fa-check"></i><b>5.2.3</b> Bagging Tree</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="centering-and-scaling.html"><a href="centering-and-scaling.html"><i class="fa fa-check"></i><b>5.3</b> Centering and Scaling</a></li>
<li class="chapter" data-level="5.4" data-path="resolve-skewness.html"><a href="resolve-skewness.html"><i class="fa fa-check"></i><b>5.4</b> Resolve Skewness</a></li>
<li class="chapter" data-level="5.5" data-path="outliers.html"><a href="outliers.html"><i class="fa fa-check"></i><b>5.5</b> Resolve Outliers</a></li>
<li class="chapter" data-level="5.6" data-path="collinearity.html"><a href="collinearity.html"><i class="fa fa-check"></i><b>5.6</b> Collinearity</a></li>
<li class="chapter" data-level="5.7" data-path="sparse-variables.html"><a href="sparse-variables.html"><i class="fa fa-check"></i><b>5.7</b> Sparse Variables</a></li>
<li class="chapter" data-level="5.8" data-path="re-encode-dummy-variables.html"><a href="re-encode-dummy-variables.html"><i class="fa fa-check"></i><b>5.8</b> Re-encode Dummy Variables</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="data-wrangling.html"><a href="data-wrangling.html"><i class="fa fa-check"></i><b>6</b> Data Wrangling</a><ul>
<li class="chapter" data-level="6.1" data-path="read-and-write-data.html"><a href="read-and-write-data.html"><i class="fa fa-check"></i><b>6.1</b> Read and write data</a><ul>
<li class="chapter" data-level="6.1.1" data-path="read-and-write-data.html"><a href="read-and-write-data.html#readr"><i class="fa fa-check"></i><b>6.1.1</b> <code>readr</code></a></li>
<li class="chapter" data-level="6.1.2" data-path="read-and-write-data.html"><a href="read-and-write-data.html#data.table-enhanced-data.frame"><i class="fa fa-check"></i><b>6.1.2</b> <code>data.table</code>— enhanced <code>data.frame</code></a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="summarize-data.html"><a href="summarize-data.html"><i class="fa fa-check"></i><b>6.2</b> Summarize data</a><ul>
<li class="chapter" data-level="6.2.1" data-path="summarize-data.html"><a href="summarize-data.html#apply-lapply-and-sapply-in-base-r"><i class="fa fa-check"></i><b>6.2.1</b> <code>apply()</code>, <code>lapply()</code> and <code>sapply()</code> in base R</a></li>
<li class="chapter" data-level="6.2.2" data-path="summarize-data.html"><a href="summarize-data.html#dplyr-package"><i class="fa fa-check"></i><b>6.2.2</b> <code>dplyr</code> package</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="tidy-and-reshape-data.html"><a href="tidy-and-reshape-data.html"><i class="fa fa-check"></i><b>6.3</b> Tidy and Reshape Data</a><ul>
<li class="chapter" data-level="6.3.1" data-path="tidy-and-reshape-data.html"><a href="tidy-and-reshape-data.html#reshape2-package"><i class="fa fa-check"></i><b>6.3.1</b> <code>reshape2</code> package</a></li>
<li class="chapter" data-level="6.3.2" data-path="tidy-and-reshape-data.html"><a href="tidy-and-reshape-data.html#tidyr-package"><i class="fa fa-check"></i><b>6.3.2</b> <code>tidyr</code> package</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="modeltuningstrategy.html"><a href="modeltuningstrategy.html"><i class="fa fa-check"></i><b>7</b> Model Tuning Strategy</a><ul>
<li class="chapter" data-level="7.1" data-path="systematic-error-and-random-error.html"><a href="systematic-error-and-random-error.html"><i class="fa fa-check"></i><b>7.1</b> Systematic Error and Random Error</a><ul>
<li class="chapter" data-level="7.1.1" data-path="systematic-error-and-random-error.html"><a href="systematic-error-and-random-error.html#vbtradeoff"><i class="fa fa-check"></i><b>7.1.1</b> Variance-Bias Trade-Off</a></li>
<li class="chapter" data-level="7.1.2" data-path="systematic-error-and-random-error.html"><a href="systematic-error-and-random-error.html#measurement-error-in-the-response"><i class="fa fa-check"></i><b>7.1.2</b> Measurement Error in the Response</a></li>
<li class="chapter" data-level="7.1.3" data-path="systematic-error-and-random-error.html"><a href="systematic-error-and-random-error.html#measurement-error-in-the-independent-variables"><i class="fa fa-check"></i><b>7.1.3</b> Measurement Error in the Independent Variables</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="data-splitting-and-resampling.html"><a href="data-splitting-and-resampling.html"><i class="fa fa-check"></i><b>7.2</b> Data Splitting and Resampling</a><ul>
<li class="chapter" data-level="7.2.1" data-path="data-splitting-and-resampling.html"><a href="data-splitting-and-resampling.html#data-splitting"><i class="fa fa-check"></i><b>7.2.1</b> Data Splitting</a></li>
<li class="chapter" data-level="7.2.2" data-path="data-splitting-and-resampling.html"><a href="data-splitting-and-resampling.html#resampling"><i class="fa fa-check"></i><b>7.2.2</b> Resampling</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="measuring-performance.html"><a href="measuring-performance.html"><i class="fa fa-check"></i><b>8</b> Measuring Performance</a><ul>
<li class="chapter" data-level="8.1" data-path="regression-model-performance.html"><a href="regression-model-performance.html"><i class="fa fa-check"></i><b>8.1</b> Regression Model Performance</a></li>
<li class="chapter" data-level="8.2" data-path="classification-model-performance.html"><a href="classification-model-performance.html"><i class="fa fa-check"></i><b>8.2</b> Classification Model Performance</a><ul>
<li class="chapter" data-level="8.2.1" data-path="classification-model-performance.html"><a href="classification-model-performance.html#confusion-matrix"><i class="fa fa-check"></i><b>8.2.1</b> Confusion Matrix</a></li>
<li class="chapter" data-level="8.2.2" data-path="classification-model-performance.html"><a href="classification-model-performance.html#kappa-statistic"><i class="fa fa-check"></i><b>8.2.2</b> Kappa Statistic</a></li>
<li class="chapter" data-level="8.2.3" data-path="classification-model-performance.html"><a href="classification-model-performance.html#roc"><i class="fa fa-check"></i><b>8.2.3</b> ROC</a></li>
<li class="chapter" data-level="8.2.4" data-path="classification-model-performance.html"><a href="classification-model-performance.html#gain-and-lift-charts"><i class="fa fa-check"></i><b>8.2.4</b> Gain and Lift Charts</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="regression-models.html"><a href="regression-models.html"><i class="fa fa-check"></i><b>9</b> Regression Models</a><ul>
<li class="chapter" data-level="9.1" data-path="ordinary-least-square.html"><a href="ordinary-least-square.html"><i class="fa fa-check"></i><b>9.1</b> Ordinary Least Square</a></li>
<li class="chapter" data-level="9.2" data-path="multivariate-adaptive-regression-splines.html"><a href="multivariate-adaptive-regression-splines.html"><i class="fa fa-check"></i><b>9.2</b> Multivariate Adaptive Regression Splines</a></li>
<li class="chapter" data-level="9.3" data-path="generalized-linear-model.html"><a href="generalized-linear-model.html"><i class="fa fa-check"></i><b>9.3</b> Generalized Linear Model</a></li>
<li class="chapter" data-level="9.4" data-path="pcr-and-pls.html"><a href="pcr-and-pls.html"><i class="fa fa-check"></i><b>9.4</b> PCR and PLS</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="regularization-methods.html"><a href="regularization-methods.html"><i class="fa fa-check"></i><b>10</b> Regularization Methods</a><ul>
<li class="chapter" data-level="10.1" data-path="ridge-regression.html"><a href="ridge-regression.html"><i class="fa fa-check"></i><b>10.1</b> Ridge Regression</a></li>
<li class="chapter" data-level="10.2" data-path="lasso.html"><a href="lasso.html"><i class="fa fa-check"></i><b>10.2</b> LASSO</a></li>
<li class="chapter" data-level="10.3" data-path="variable-selection-property-of-the-lasso.html"><a href="variable-selection-property-of-the-lasso.html"><i class="fa fa-check"></i><b>10.3</b> Variable selection property of the lasso</a></li>
<li class="chapter" data-level="10.4" data-path="elastic-net.html"><a href="elastic-net.html"><i class="fa fa-check"></i><b>10.4</b> Elastic Net</a></li>
<li class="chapter" data-level="10.5" data-path="penalized-generalized-linear-model.html"><a href="penalized-generalized-linear-model.html"><i class="fa fa-check"></i><b>10.5</b> Penalized Generalized Linear Model</a><ul>
<li class="chapter" data-level="10.5.1" data-path="penalized-generalized-linear-model.html"><a href="penalized-generalized-linear-model.html#introduction-to-glmnet-package"><i class="fa fa-check"></i><b>10.5.1</b> Introduction to <code>glmnet</code> package</a></li>
<li class="chapter" data-level="10.5.2" data-path="penalized-generalized-linear-model.html"><a href="penalized-generalized-linear-model.html#penalized-logistic-regression"><i class="fa fa-check"></i><b>10.5.2</b> Penalized logistic regression</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="treemodel.html"><a href="treemodel.html"><i class="fa fa-check"></i><b>11</b> Tree-Based Methods</a><ul>
<li class="chapter" data-level="11.1" data-path="splitting-criteria.html"><a href="splitting-criteria.html"><i class="fa fa-check"></i><b>11.1</b> Splitting Criteria</a></li>
<li class="chapter" data-level="11.2" data-path="tree-pruning.html"><a href="tree-pruning.html"><i class="fa fa-check"></i><b>11.2</b> Tree Pruning</a></li>
<li class="chapter" data-level="11.3" data-path="regression-and-decision-tree-basic.html"><a href="regression-and-decision-tree-basic.html"><i class="fa fa-check"></i><b>11.3</b> Regression and Decision Tree Basic</a><ul>
<li class="chapter" data-level="11.3.1" data-path="regression-and-decision-tree-basic.html"><a href="regression-and-decision-tree-basic.html#regression-tree"><i class="fa fa-check"></i><b>11.3.1</b> Regression Tree</a></li>
<li class="chapter" data-level="11.3.2" data-path="regression-and-decision-tree-basic.html"><a href="regression-and-decision-tree-basic.html#decision-tree"><i class="fa fa-check"></i><b>11.3.2</b> Decision Tree</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="bagging-tree-1.html"><a href="bagging-tree-1.html"><i class="fa fa-check"></i><b>11.4</b> Bagging Tree</a></li>
<li class="chapter" data-level="11.5" data-path="random-forest.html"><a href="random-forest.html"><i class="fa fa-check"></i><b>11.5</b> Random Forest</a></li>
<li class="chapter" data-level="11.6" data-path="gradient-boosted-machine.html"><a href="gradient-boosted-machine.html"><i class="fa fa-check"></i><b>11.6</b> Gradient Boosted Machine</a><ul>
<li class="chapter" data-level="11.6.1" data-path="gradient-boosted-machine.html"><a href="gradient-boosted-machine.html#adaptive-boosting"><i class="fa fa-check"></i><b>11.6.1</b> Adaptive Boosting</a></li>
<li class="chapter" data-level="11.6.2" data-path="gradient-boosted-machine.html"><a href="gradient-boosted-machine.html#stochastic-gradient-boosting"><i class="fa fa-check"></i><b>11.6.2</b> Stochastic Gradient Boosting</a></li>
<li class="chapter" data-level="11.6.3" data-path="gradient-boosted-machine.html"><a href="gradient-boosted-machine.html#boosting-as-additive-model"><i class="fa fa-check"></i><b>11.6.3</b> Boosting as Additive Model</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="deep-learning.html"><a href="deep-learning.html"><i class="fa fa-check"></i><b>12</b> Deep Learning</a><ul>
<li class="chapter" data-level="12.1" data-path="projection-pursuit-regression.html"><a href="projection-pursuit-regression.html"><i class="fa fa-check"></i><b>12.1</b> Projection Pursuit Regression</a></li>
<li class="chapter" data-level="12.2" data-path="feedforward-neural-network.html"><a href="feedforward-neural-network.html"><i class="fa fa-check"></i><b>12.2</b> Feedforward Neural Network</a><ul>
<li class="chapter" data-level="12.2.1" data-path="feedforward-neural-network.html"><a href="feedforward-neural-network.html#logistic_reg_as_neural_network"><i class="fa fa-check"></i><b>12.2.1</b> Logistic Regression as Neural Network</a></li>
<li class="chapter" data-level="12.2.2" data-path="feedforward-neural-network.html"><a href="feedforward-neural-network.html#gradient-descent"><i class="fa fa-check"></i><b>12.2.2</b> Gradient Descent</a></li>
<li class="chapter" data-level="12.2.3" data-path="feedforward-neural-network.html"><a href="feedforward-neural-network.html#deep-neural-network"><i class="fa fa-check"></i><b>12.2.3</b> Deep Neural Network</a></li>
<li class="chapter" data-level="12.2.4" data-path="feedforward-neural-network.html"><a href="feedforward-neural-network.html#activation-function"><i class="fa fa-check"></i><b>12.2.4</b> Activation Function</a></li>
<li class="chapter" data-level="12.2.5" data-path="feedforward-neural-network.html"><a href="feedforward-neural-network.html#deal-with-overfitting"><i class="fa fa-check"></i><b>12.2.5</b> Deal with Overfitting</a></li>
<li class="chapter" data-level="12.2.6" data-path="feedforward-neural-network.html"><a href="feedforward-neural-network.html#optimization"><i class="fa fa-check"></i><b>12.2.6</b> Optimization</a></li>
<li class="chapter" data-level="12.2.7" data-path="feedforward-neural-network.html"><a href="feedforward-neural-network.html#ffnnexample"><i class="fa fa-check"></i><b>12.2.7</b> Image Recognition Using FFNN</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="convolutional-neural-network.html"><a href="convolutional-neural-network.html"><i class="fa fa-check"></i><b>12.3</b> Convolutional Neural Network</a><ul>
<li class="chapter" data-level="12.3.1" data-path="convolutional-neural-network.html"><a href="convolutional-neural-network.html#convolution-layer"><i class="fa fa-check"></i><b>12.3.1</b> Convolution Layer</a></li>
<li class="chapter" data-level="12.3.2" data-path="convolutional-neural-network.html"><a href="convolutional-neural-network.html#padding-layer"><i class="fa fa-check"></i><b>12.3.2</b> Padding Layer</a></li>
<li class="chapter" data-level="12.3.3" data-path="convolutional-neural-network.html"><a href="convolutional-neural-network.html#pooling-layer"><i class="fa fa-check"></i><b>12.3.3</b> Pooling Layer</a></li>
<li class="chapter" data-level="12.3.4" data-path="convolutional-neural-network.html"><a href="convolutional-neural-network.html#convolution-over-volume"><i class="fa fa-check"></i><b>12.3.4</b> Convolution Over Volume</a></li>
<li class="chapter" data-level="12.3.5" data-path="convolutional-neural-network.html"><a href="convolutional-neural-network.html#cnnexample"><i class="fa fa-check"></i><b>12.3.5</b> Image Recognition Using CNN</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="recurrent-neural-network.html"><a href="recurrent-neural-network.html"><i class="fa fa-check"></i><b>12.4</b> Recurrent Neural Network</a><ul>
<li class="chapter" data-level="12.4.1" data-path="recurrent-neural-network.html"><a href="recurrent-neural-network.html#rnn-model"><i class="fa fa-check"></i><b>12.4.1</b> RNN Model</a></li>
<li class="chapter" data-level="12.4.2" data-path="recurrent-neural-network.html"><a href="recurrent-neural-network.html#word-embedding"><i class="fa fa-check"></i><b>12.4.2</b> Word Embedding</a></li>
<li class="chapter" data-level="12.4.3" data-path="recurrent-neural-network.html"><a href="recurrent-neural-network.html#long-short-term-memory"><i class="fa fa-check"></i><b>12.4.3</b> Long Short Term Memory</a></li>
</ul></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="r-code-for-data-simulation.html"><a href="r-code-for-data-simulation.html"><i class="fa fa-check"></i><b>A</b> R code for data simulation</a><ul>
<li class="chapter" data-level="A.1" data-path="customer-data-for-clothing-company.html"><a href="customer-data-for-clothing-company.html"><i class="fa fa-check"></i><b>A.1</b> Customer Data for Clothing Company</a></li>
<li class="chapter" data-level="A.2" data-path="customer-satisfaction-survey-data-from-airline-company-1.html"><a href="customer-satisfaction-survey-data-from-airline-company-1.html"><i class="fa fa-check"></i><b>A.2</b> Customer Satisfaction Survey Data from Airline Company</a></li>
<li class="chapter" data-level="A.3" data-path="swine-disease-breakout-data.html"><a href="swine-disease-breakout-data.html"><i class="fa fa-check"></i><b>A.3</b> Swine Disease Breakout Data</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Data Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="penalized-generalized-linear-model" class="section level2">
<h2><span class="header-section-number">10.5</span> Penalized Generalized Linear Model</h2>
<p>Adding penalties is a general technique that can be applied to many methods other than linear regression. In this section, we will introduce the penalized generalized linear model. It is to fit the generalized linear model by minimizing a penalized maximum likelihood. The penalty can be <span class="math inline">\(L_1\)</span>, <span class="math inline">\(L_2\)</span> or a combination of the two. The estimates of coefficients minimize the following:</p>
<p><span class="math display">\[\underset{\beta_{0},\mathbf{\beta}}{min}\frac{1}{N}\Sigma_{i=1}^{N}w_{i}l(y_{i},\beta_{0}+\mathbf{\beta^{T}x_{i}})+\lambda[(1-\alpha)\parallel\mathbf{\beta}\parallel_{2}^{2}/2+\alpha\parallel\mathbf{\beta}\parallel_{1}]\]</span></p>
<p>where</p>
<p><span class="math display">\[l(y_{i},\beta_{0}+\mathbf{\beta^{T}x_{i}})=-log[\mathcal{L}(y_{i},\beta_{0}+\mathbf{\beta^{T}x_{i}})]\]</span></p>
<p>It is the negative logarithm of the likelihood, <span class="math inline">\(\mathcal{L}(y_{i},\beta_{0}+\mathbf{\beta^{T}x_{i}})\)</span>. Maximize likelihood is to minimize <span class="math inline">\(l(y_{i},\beta_{0}+\mathbf{\beta^{T}x_{i}})\)</span>.</p>
<p>Parameter <span class="math inline">\(\alpha\)</span> decides the penalty, i.e, between <span class="math inline">\(L_2\)</span> (<span class="math inline">\(\alpha=0\)</span>) and <span class="math inline">\(L_1\)</span> (<span class="math inline">\(\alpha=1\)</span>). <span class="math inline">\(\lambda\)</span> controls the weight of the whole penalty item. The higher <span class="math inline">\(\lambda\)</span> is, the more weight the penalty carries comparing to likelihood. As discussed above, the ridge penalty shrinks the coefficients towards 0 but can’t be exactly 0. The lasso penalty can set 0 estimates so it has the property of feature selection. The elastic net combines both. Here we have two tuning parameters, <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\lambda\)</span>.</p>
<div id="introduction-to-glmnet-package" class="section level3">
<h3><span class="header-section-number">10.5.1</span> Introduction to <code>glmnet</code> package</h3>
<p><code>glmnet</code> is a package that fits a penalized generalized linear model using <em>cyclical coordinate descent</em>. It successively optimizes the objective function over each parameter with others fixed, and cycles repeatedly until convergence. Since the linear model is a special case of the generalized linear model, <code>glmnet</code> can also fit a penalized linear model. Other than that, it can also fit penalized logistic regression, multinomial, Poisson, and Cox regression models.</p>
<p>The default family option in the function <code>glmnet()</code> is <code>gaussian</code>. It is the linear regression we discussed so far in this chapter. But the parameterization is a little different in the generalized linear model framework (we have <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\lambda\)</span>). Let’s start from our previous example, using the same training data but <code>glmnet()</code> to fit model:</p>
<div class="sourceCode" id="cb312"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb312-1" data-line-number="1"><span class="kw">library</span>(glmnet)</a>
<a class="sourceLine" id="cb312-2" data-line-number="2">dat &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;http://bit.ly/2P5gTw4&quot;</span>)</a>
<a class="sourceLine" id="cb312-3" data-line-number="3"><span class="co"># data cleaning: delete wrong observations since expense can&#39;t be negative</span></a>
<a class="sourceLine" id="cb312-4" data-line-number="4">dat &lt;-<span class="st"> </span><span class="kw">subset</span>(dat, store_exp <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span> <span class="op">&amp;</span><span class="st"> </span>online_exp <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>)</a>
<a class="sourceLine" id="cb312-5" data-line-number="5"><span class="co"># get predictors</span></a>
<a class="sourceLine" id="cb312-6" data-line-number="6">trainx &lt;-<span class="st"> </span>dat[, <span class="kw">grep</span>(<span class="st">&quot;Q&quot;</span>, <span class="kw">names</span>(dat))]</a>
<a class="sourceLine" id="cb312-7" data-line-number="7"><span class="co"># get response</span></a>
<a class="sourceLine" id="cb312-8" data-line-number="8">trainy &lt;-<span class="st"> </span>dat<span class="op">$</span>store_exp <span class="op">+</span><span class="st"> </span>dat<span class="op">$</span>online_exp</a>
<a class="sourceLine" id="cb312-9" data-line-number="9">glmfit =<span class="st"> </span><span class="kw">glmnet</span>(<span class="kw">as.matrix</span>(trainx), trainy)</a></code></pre></div>
<p>The object <code>glmfit</code> returned by <code>glmnet()</code> has the information of the fitted model for the later operations. An easy way to extract the components is through various functions on <code>glmfit</code>, such as <code>plot()</code>, <code>print()</code>, <code>coef()</code> and <code>predict()</code> . For example, the following code visualizes the path of coefficients as penalty increases:</p>
<div class="sourceCode" id="cb313"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb313-1" data-line-number="1"><span class="kw">plot</span>(glmfit, <span class="dt">label =</span> T)</a></code></pre></div>
<p><img src="IDS_files/figure-html/unnamed-chunk-119-1.svg" width="672" /></p>
<p>Each curve in the plot represents one predictor. The default setting is <span class="math inline">\(\alpha=1\)</span> which means there is only lasso penalty. From left to right, <span class="math inline">\(L_I\)</span> norm is increasing which means <span class="math inline">\(\lambda\)</span> is decreasing. The bottom x-axis is <span class="math inline">\(L_1\)</span> norm (i.e. <span class="math inline">\(\parallel\mathbf{\beta}\parallel_{1}\)</span>). The upper x-axis is the effective degrees of freedom (df) for the lasso. You can check the detail for every step by:</p>
<div class="sourceCode" id="cb314"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb314-1" data-line-number="1"><span class="kw">print</span>(glmfit)</a></code></pre></div>
<pre class="pre"><code>Call:  glmnet(x = as.matrix(trainx), y = trainy) 

   Df  %Dev Lambda
1   0 0.000   3040
2   2 0.104   2770
3   2 0.192   2530
4   2 0.265   2300
5   3 0.326   2100
6   3 0.389   1910
7   3 0.442   1740
8   3 0.485   1590
9   3 0.521   1450
...</code></pre>
<p>The first column <code>Df</code> is the degree of freedom (i.e. the number of non-zero coefficients), <code>%Dev</code> is the percentage of deviance explained and <code>Lambda</code> is the value of tuning parameter <span class="math inline">\(\lambda\)</span>. By default, the function will try 100 different values of <span class="math inline">\(\lambda\)</span>. However, if as <span class="math inline">\(\lambda\)</span> changes, the <code>%Dev</code> doesn’t change sufficiently, the algorithm will stop before it goes through all the values of <span class="math inline">\(\lambda\)</span>. We didn’t show the full output above. But it only uses 68 different values of <span class="math inline">\(\lambda\)</span>. You can also set the value of <span class="math inline">\(\lambda\)</span> using <code>s=</code> :</p>
<div class="sourceCode" id="cb316"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb316-1" data-line-number="1"><span class="kw">coef</span>(glmfit, <span class="dt">s =</span> <span class="dv">1200</span>)</a></code></pre></div>
<pre><code>## 11 x 1 sparse Matrix of class &quot;dgCMatrix&quot;
##                  1
## (Intercept) 2255.2
## Q1          -390.9
## Q2           653.6
## Q3           624.4
## Q4             .  
## Q5             .  
## Q6             .  
## Q7             .  
## Q8             .  
## Q9             .  
## Q10            .</code></pre>
<p>When <span class="math inline">\(\lambda=1200\)</span>, there are three coefficients with non-zero estimates(<code>Q1</code>, <code>Q2</code> and <code>Q3</code>). You can apply models with different values of tuning parameter to new data using <code>predict()</code>:</p>
<div class="sourceCode" id="cb318"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb318-1" data-line-number="1">newdat =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">sample</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">9</span>, <span class="dv">30</span>, <span class="dt">replace =</span> T), <span class="dt">nrow =</span> <span class="dv">3</span>)</a>
<a class="sourceLine" id="cb318-2" data-line-number="2"><span class="kw">predict</span>(glmfit, newdat, <span class="dt">s =</span> <span class="kw">c</span>(<span class="dv">1741</span>, <span class="dv">2000</span>))</a></code></pre></div>
<pre><code>##         1    2
## [1,] 6004 5968
## [2,] 7101 6674
## [3,] 9158 8411</code></pre>
<p>Each column corresponds to a value of <span class="math inline">\(\lambda\)</span>. To tune the value of <span class="math inline">\(\lambda\)</span>, we can easily use <code>cv.glmnet()</code> function to do cross-validation.<code>cv.glmnet()</code> returns the cross-validation results as a list object. We store the object in <code>cvfit</code> and use it for further operations.</p>
<div class="sourceCode" id="cb320"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb320-1" data-line-number="1">cvfit =<span class="st"> </span><span class="kw">cv.glmnet</span>(<span class="kw">as.matrix</span>(trainx), trainy)</a></code></pre></div>
<p>We can plot the object using <code>plot()</code>. The red dotted line is the cross-validation curve. Each red point is the cross-validation mean squared error for a value of <span class="math inline">\(\lambda\)</span>. The grey bars around the red points indicate the upper and lower standard deviation. The two gray dotted vertical lines represent the two selected values of <span class="math inline">\(\lambda\)</span>, one gives the minimum mean cross-validated error (<code>lambda.min</code>), the other gives the error that is within one standard error of the minimum (<code>lambda.1se</code>).</p>
<div class="sourceCode" id="cb321"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb321-1" data-line-number="1"><span class="kw">plot</span>(cvfit)</a></code></pre></div>
<p><img src="IDS_files/figure-html/unnamed-chunk-123-1.svg" width="672" /></p>
<p>You can check the two selected <span class="math inline">\(\lambda\)</span> values by:</p>
<div class="sourceCode" id="cb322"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb322-1" data-line-number="1"><span class="co"># lambda with minimum mean cross-validated error</span></a>
<a class="sourceLine" id="cb322-2" data-line-number="2">cvfit<span class="op">$</span>lambda.min</a></code></pre></div>
<pre><code>## [1] 12.57</code></pre>
<div class="sourceCode" id="cb324"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb324-1" data-line-number="1"><span class="co"># lambda with one standard error of the minimum</span></a>
<a class="sourceLine" id="cb324-2" data-line-number="2">cvfit<span class="op">$</span>lambda<span class="fl">.1</span>se</a></code></pre></div>
<pre><code>## [1] 1200</code></pre>
<p>You can look at the coefficient estimates for different <span class="math inline">\(\lambda\)</span> by:</p>
<div class="sourceCode" id="cb326"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb326-1" data-line-number="1"><span class="co"># coefficient estimates for model with the error </span></a>
<a class="sourceLine" id="cb326-2" data-line-number="2"><span class="co"># that is within one standard error of the minimum</span></a>
<a class="sourceLine" id="cb326-3" data-line-number="3"><span class="kw">coef</span>(cvfit, <span class="dt">s =</span> <span class="st">&quot;lambda.1se&quot;</span>)</a></code></pre></div>
<pre><code>## 11 x 1 sparse Matrix of class &quot;dgCMatrix&quot;
##                  1
## (Intercept) 2255.3
## Q1          -391.1
## Q2           653.7
## Q3           624.5
## Q4             .  
## Q5             .  
## Q6             .  
## Q7             .  
## Q8             .  
## Q9             .  
## Q10            .</code></pre>
</div>
<div id="penalized-logistic-regression" class="section level3">
<h3><span class="header-section-number">10.5.2</span> Penalized logistic regression</h3>
<div id="multivariate-logistic-regression-model" class="section level4">
<h4><span class="header-section-number">10.5.2.1</span> Multivariate logistic regression model</h4>
<p>Logistic regression is a traditional statistical method for a two-category classification problem. It is simple yet useful. Here we use the swine disease breakout data as an example to illustrate the learning method and code implementation. Refer to section <a href="swinediseasedata.html#swinediseasedata">3.3</a> for more details about the dataset. The goal is to predict if a farm will have a swine disease outbreak (i.e build a risk scoring system).</p>
<p>Consider risk scoring system construction using a sample of <span class="math inline">\(n\)</span> observations, with information collected for <span class="math inline">\(G\)</span> categorical predictors and one binary response variable for each observation. The predictors are 120 survey questions (i.e. G=120). There were three possible answers for each question (A, B and C). So each predictor is encoded to two dummy variables (we consider C as the baseline.). Let <span class="math inline">\(\mathbf{x_{i,g}}\)</span> be the vector of dummy variables associated with the <span class="math inline">\(g^{th}\)</span> categorical predictor for the <span class="math inline">\(i^{th}\)</span> observation, where <span class="math inline">\(i=1,\cdots,n\)</span>, <span class="math inline">\(g=1,\cdots,G\)</span>. For example, if the first farm chooses B for question 2, then the corresponding observation is <span class="math inline">\(\mathbf{x_{12}}=(0,1)^{T}\)</span>. Each question has a degree of freedom of 2.</p>
<p>We denote the degrees of freedom of the <span class="math inline">\(g^{th}\)</span> predictor by <span class="math inline">\(df_g\)</span>, which is also the length of vector <span class="math inline">\(\mathbf{x_{i,g}}\)</span>. Let <span class="math inline">\(y_i\)</span> (= 1, diseased; or 0, not diseased) be the binary response for the <span class="math inline">\(i\)</span>th observation. Denote the probability
of disease for <span class="math inline">\(i\)</span>th subject by <span class="math inline">\(\theta_i\)</span>, the model can be formulated as:</p>
<p><span class="math display">\[y_{i}\sim Bounoulli(\theta_{i})\]</span></p>
<p><span class="math display">\[log\left(\frac{\theta_{i}}{1-\theta_{i}}\right)=\eta_{\mathbf{\beta}}(x_{i})=\beta_{0}+\sum_{g=1}^{G}\mathbf{x_{i,g}}^{T}\mathbf{\mathbf{\beta_{g}}}\]</span></p>
<p>where <span class="math inline">\(\beta_{0}\)</span> is the intercept and <span class="math inline">\(\mathbf{\beta_{g}}\)</span> is the parameter vector corresponding to the <span class="math inline">\(g^{th}\)</span> predictor. As we mentioned, here <span class="math inline">\(\mathbf{\beta_{g}}\)</span> has length 2.</p>
<p>Traditional estimation of logistic
parameters <span class="math inline">\(\mathbf{\beta}=(\beta_{0}^{T},\mathbf{\beta_{1}}^{T},\mathbf{\beta_{2}}^{T},...,\mathbf{\beta_{G}}^{T})^{T}\)</span> is done through maximizing the log-likelihood</p>
<p><span class="math display" id="eq:logisticlikelihood">\[\begin{eqnarray*}
l(\mathbf{\beta})&amp;=&amp;log[\prod_{i=1}^{n}\theta_{i}^{y_{i}}(1-\theta_{i})^{1-y_{i}}]\\
&amp;=&amp;\sum_{i=1}^{n}\{y_{i}log(\theta_{i})+(1-y_{i})log(1-\theta_{i})\}\\
&amp;=&amp;\sum_{i=1}^{n}\{\ y_{i}\eta_{\mathbf{\beta}}(\mathbf{x_{i}})-log[1+exp(\eta_{\mathbf{\beta}}(\mathbf{x_{i}}))]\ \}
\tag{10.8}
\end{eqnarray*}\]</span></p>
<p>For logistic regression analysis with a large number of explanatory variables, complete- or quasi-complete-separation may result which makes
the maximum likelihood estimation unstable <span class="citation">(Wedderburn <a href="#ref-Wed1976">1976</a>)</span>, <span class="citation">(A and J <a href="#ref-albert1984">1984</a>)</span>. For example:</p>
<div class="sourceCode" id="cb328"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb328-1" data-line-number="1"><span class="kw">library</span>(MASS)</a>
<a class="sourceLine" id="cb328-2" data-line-number="2">dat &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;http://bit.ly/2KXb1Qi&quot;</span>)</a>
<a class="sourceLine" id="cb328-3" data-line-number="3">fit &lt;-<span class="st"> </span><span class="kw">glm</span>(y<span class="op">~</span>., dat, <span class="dt">family =</span> <span class="st">&quot;binomial&quot;</span>)</a></code></pre></div>
<pre><code>## Warning: glm.fit: algorithm did not converge</code></pre>
<pre><code>## Warning: glm.fit: fitted probabilities numerically 0 or
## 1 occurred</code></pre>
<p>There is an error saying “<code>algorithm did not converge</code>”. It is because there is complete separation. It happens when there are a large number of explanatory variables which makes the estimation of the coefficients unstable. To stabilize the estimation of parameter coefficients, one popular approach is the lasso algorithm with <span class="math inline">\(L_1\)</span> norm penalty proposed by Tibshirani<span class="citation">(R <a href="#ref-Tibshirani1996">1996</a>)</span>. Because the lasso algorithm can estimate some variable coefficients to be 0, it can also be used as a variable selection tool.</p>
</div>
<div id="penalized-logistic-regression-1" class="section level4">
<h4><span class="header-section-number">10.5.2.2</span> Penalized logistic regression</h4>
<p>Penalized logistic regression adds penalty to the likelihood function:</p>
<p><span class="math display">\[
\underset{\mathbf{\beta}\in \mathbb{R}^{p+1}}{min} -\sum_{i=1}^{n}\{\ y_{i}\eta_{\mathbf{\beta}}(\mathbf{x_{i}})-log[1+exp(\eta_{\mathbf{\beta}}(\mathbf{x_{i}}))]\ \}+\lambda [(1-\alpha) \parallel \mathbf{\beta}\parallel _{2}^{2}/2] + \alpha \parallel \mathbf{\beta}\parallel _{1} ]
\]</span></p>
<div class="sourceCode" id="cb331"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb331-1" data-line-number="1">dat &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;http://bit.ly/2KXb1Qi&quot;</span>)</a>
<a class="sourceLine" id="cb331-2" data-line-number="2">trainx =<span class="st"> </span>dplyr<span class="op">::</span><span class="kw">select</span>(dat, <span class="op">-</span>y)</a>
<a class="sourceLine" id="cb331-3" data-line-number="3">trainy =<span class="st"> </span>dat<span class="op">$</span>y</a>
<a class="sourceLine" id="cb331-4" data-line-number="4">fit &lt;-<span class="st"> </span><span class="kw">glmnet</span>(<span class="kw">as.matrix</span>(trainx), trainy, <span class="dt">family =</span> <span class="st">&quot;binomial&quot;</span>)</a></code></pre></div>
<p>The error message is gone when we use penalized regression. We can visualize the shrinking path of coefficients as penalty increases. The use of <code>predict()</code> function is a little different. For the generalized linear model, you can return different results by setting the <code>type</code> argument. The choices are:</p>
<ul>
<li><code>link</code>: return the link function value</li>
<li><code>response</code>: return the probability</li>
<li><code>class</code>: return the category (0/1)</li>
<li><code>coefficients</code>: return the coefficient estimates</li>
<li><code>nonzero</code>: return an indicator for non-zero estimates (i.e. which variables are selected)</li>
</ul>
<p>The default setting is to predict the probability of the second level of the response variable. For example, the second level of the response variable for <code>trainy</code> here is:</p>
<div class="sourceCode" id="cb332"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb332-1" data-line-number="1"><span class="kw">levels</span>(<span class="kw">as.factor</span>(trainy))</a></code></pre></div>
<pre><code>## [1] &quot;0&quot; &quot;1&quot;</code></pre>
<p>The first column of the above output is the predicted link function value when <span class="math inline">\(\lambda=0.02833\)</span>. The second column of the output is the predicted link function when <span class="math inline">\(\lambda=0.0311\)</span>. Similarly, you can change the setting for <code>type</code> to produce different outputs. You can use the <code>cv.glmnet()</code> function to tune parameters. The parameter setting is nearly the same as before, the only difference is the setting of <code>type.measure</code>. Since the response is categorical, not continuous, we have different performance measurements. The most common settings of <code>type.measure</code> for classification are:</p>
<ul>
<li><code>class</code>: error rate</li>
<li><code>auc</code>: it is the area under the ROC for the dichotomous problem
So the model is to predict the probability of outcome “1”. Take a baby example of 3 observations and 2 values of <span class="math inline">\(\lambda\)</span> to show the usage of <code>predict()</code> function:</li>
</ul>
<div class="sourceCode" id="cb334"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb334-1" data-line-number="1">newdat =<span class="st"> </span><span class="kw">as.matrix</span>(trainx[<span class="dv">1</span><span class="op">:</span><span class="dv">3</span>, ])</a>
<a class="sourceLine" id="cb334-2" data-line-number="2"><span class="kw">predict</span>(fit, newdat, <span class="dt">type =</span> <span class="st">&quot;link&quot;</span>, <span class="dt">s =</span> <span class="kw">c</span>(<span class="fl">2.833e-02</span>, <span class="fl">3.110e-02</span>))</a></code></pre></div>
<pre><code>##         1       2
## 1  0.1943  0.1443
## 2 -0.9913 -1.0077
## 3 -0.5841 -0.5496</code></pre>
<p>For example:</p>
<div class="sourceCode" id="cb336"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb336-1" data-line-number="1">cvfit =<span class="st"> </span><span class="kw">cv.glmnet</span>(<span class="kw">as.matrix</span>(trainx), trainy, <span class="dt">family =</span> <span class="st">&quot;binomial&quot;</span>, <span class="dt">type.measure =</span> <span class="st">&quot;class&quot;</span>)</a>
<a class="sourceLine" id="cb336-2" data-line-number="2"><span class="kw">plot</span>(cvfit)</a></code></pre></div>
<p><img src="IDS_files/figure-html/unnamed-chunk-130-1.svg" width="672" /></p>
<p>The above uses error rate as performance criteria and use 10-fold cross-validation. Similarly, you can get the <span class="math inline">\(\lambda\)</span> value for the minimum error rate and the error rate that is 1 standard error from the minimum:</p>
<div class="sourceCode" id="cb337"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb337-1" data-line-number="1">cvfit<span class="op">$</span>lambda.min</a></code></pre></div>
<pre><code>## [1] 2.643e-05</code></pre>
<div class="sourceCode" id="cb339"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb339-1" data-line-number="1">cvfit<span class="op">$</span>lambda<span class="fl">.1</span>se</a></code></pre></div>
<pre><code>## [1] 0.003334</code></pre>
<p>You can use the same way to get the parameter estimates and make prediction.</p>
</div>
<div id="group-lasso-logistic-regression" class="section level4">
<h4><span class="header-section-number">10.5.2.3</span> Group lasso logistic regression</h4>
<p>For models with categorical survey questions (explanatory variables), however, the original lasso algorithm only selects individual dummy variables instead of sets of the dummy variables grouped by the question in the survey. Another disadvantage of applying lasso to grouped variables is that the estimates are affected by the way dummy variables are encoded. Thus the group lasso <span class="citation">(Yuan and Lin <a href="#ref-Yuan2007">2007</a>)</span> method has been proposed to enable variable selection in linear regression models on groups of variables, instead of on single variables. For logistic regression models, the group lasso algorithm was first studied by Kim et al. <span class="citation">(Y. Kim and Kim <a href="#ref-Kim2006">2006</a>)</span>. They proposed a gradient descent algorithm to solve the corresponding constrained problem, which does, however, depend on unknown constants. Meier et al. <span class="citation">(L Meier and Buhlmann <a href="#ref-Meier2008">2008</a>)</span> proposed a new algorithm that could work directly on the penalized problem and its convergence property does not depend on unknown constants. The algorithm is especially suitable for high-dimensional problems. It can also be applied to solve the corresponding convex optimization problem in generalized linear models. The group lasso estimator proposed by Meier et al. <span class="citation">(L Meier and Buhlmann <a href="#ref-Meier2008">2008</a>)</span> for logistic regression has been shown to be statistically consistent, even with a large number of categorical predictors.</p>
<p>In this section, we illustrate how to use the logistic group lasso algorithm to construct risk scoring systems for predicting disease. Instead of maximizing the log-likelihood in the maximum likelihood method, the logistic group lasso estimates are calculated by minimizing the convex function:</p>
<p><span class="math display">\[
S_{\lambda}(\mathbf{\beta})=-l(\mathbf{\beta})+\lambda\sum_{g=1}^{G}s(df_{g})\parallel\mathbf{\beta_{g}}\parallel_{2}
\]</span></p>
<p>where <span class="math inline">\(\lambda\)</span> is a tuning parameter for the penalty and <span class="math inline">\(s(\cdot)\)</span> is a function to rescale the penalty. In lasso algorithms, the selection of <span class="math inline">\(\lambda\)</span> is usually determined by cross-validation using data. For <span class="math inline">\(s(\cdot)\)</span>, we use the square root function <span class="math inline">\(s(df_g)=df_g^{0.5}\)</span> as suggested in Meier et al.<span class="citation">(L Meier and Buhlmann <a href="#ref-Meier2008">2008</a>)</span>. It ensures the penalty is of the order of the number of parameters <span class="math inline">\(df_g\)</span> as used in <span class="citation">(Yuan and Lin <a href="#ref-Yuan2007">2007</a>)</span>.</p>
<p>Here we consider selection of the tuning parameter <span class="math inline">\(\lambda\)</span> from a multiplicative grid of 100 values <span class="math inline">\(\{0.96\lambda_{max},0.96^{2}\lambda_{max},0.96^{3}\lambda_{max},...,0.96^{100}\lambda_{max}\}\)</span>. Here <span class="math inline">\(\lambda_{max}\)</span> is defined as</p>
<p><span class="math display">\[\begin{equation}
\lambda_{max}=\underset{g\in\{1,...,G\}}{max}\left\{\frac{1}{s(df_{g})}\parallel \mathbf{x_{g}}^{T}(\mathbf{y}-\bar{\mathbf{y}} )\parallel_{2}\right\},
\end{equation}\]</span></p>
<p>such that when <span class="math inline">\(\lambda=\lambda_{max}\)</span>, only the intercept is
in the model. When <span class="math inline">\(\lambda\)</span> goes to <span class="math inline">\(0\)</span>, the model is equivalent to ordinary
logistic regression.</p>
<p>Three criteria may be used to select the optimal value of <span class="math inline">\(\lambda\)</span>. One is AUC which you should have seem many times in this book by now. The log-likelihood score used in Meier et al. <span class="citation">(L Meier and Buhlmann <a href="#ref-Meier2008">2008</a>)</span> is taken as the average of log-likelihood of the validation data overall cross-validation sets. Another one is the maximum correlation coefficient in Yeo and Burge <span class="citation">(Yeo and Burge <a href="#ref-Yeo2004">2004</a>)</span> that is defined as:</p>
<p><span class="math display">\[
\rho_{max}=max\{\rho_{\tau}|\tau\in(0,1)\},
\]</span></p>
<p>where <span class="math inline">\(\tau\in(0,1)\)</span> is a threshold to classify the predicted probability into binary disease status and <span class="math inline">\(\rho_\tau\)</span> is the Pearson correlation coefficient between the true binary disease status and the predictive disease status with threshold <span class="math inline">\(\tau\)</span>.</p>
<p>You can use the following package to implement the model. Install the package using:</p>
<div class="sourceCode" id="cb341"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb341-1" data-line-number="1">devtools<span class="op">::</span><span class="kw">install_github</span>(<span class="st">&quot;netlify/NetlifyDS&quot;</span>)</a></code></pre></div>
<p>Load the package:</p>
<div class="sourceCode" id="cb342"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb342-1" data-line-number="1"><span class="kw">library</span>(<span class="st">&quot;NetlifyDS&quot;</span>)</a></code></pre></div>
<p>The package includes the swine disease breakout data and you can load the data by:</p>
<div class="sourceCode" id="cb343"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb343-1" data-line-number="1"><span class="kw">data</span>(<span class="st">&quot;sim1_da1&quot;</span>)</a></code></pre></div>
<p>You can use <code>cv_glasso()</code> function to tune the parameters:</p>
<div class="sourceCode" id="cb344"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb344-1" data-line-number="1"><span class="co"># the last column of sim1_da1 response variable y</span></a>
<a class="sourceLine" id="cb344-2" data-line-number="2"><span class="co"># trainx is the explanatory variable matrix</span></a>
<a class="sourceLine" id="cb344-3" data-line-number="3">trainx =<span class="st"> </span>dplyr<span class="op">::</span><span class="kw">select</span>(sim1_da1, <span class="op">-</span>y)</a>
<a class="sourceLine" id="cb344-4" data-line-number="4"><span class="co"># save response variable as as trainy</span></a>
<a class="sourceLine" id="cb344-5" data-line-number="5">trainy =<span class="st"> </span>sim1_da1<span class="op">$</span>y</a>
<a class="sourceLine" id="cb344-6" data-line-number="6"><span class="co"># get the group indicator</span></a>
<a class="sourceLine" id="cb344-7" data-line-number="7">index &lt;-<span class="st"> </span><span class="kw">gsub</span>(<span class="st">&quot;</span><span class="ch">\\</span><span class="st">..*&quot;</span>, <span class="st">&quot;&quot;</span>, <span class="kw">names</span>(trainx))</a></code></pre></div>
<p>Dummy variables from the same question are in the same group:</p>
<div class="sourceCode" id="cb345"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb345-1" data-line-number="1">index[<span class="dv">1</span><span class="op">:</span><span class="dv">50</span>]</a></code></pre></div>
<pre><code>##  [1] &quot;Q1&quot;  &quot;Q1&quot;  &quot;Q2&quot;  &quot;Q2&quot;  &quot;Q3&quot;  &quot;Q3&quot;  &quot;Q4&quot;  &quot;Q4&quot; 
##  [9] &quot;Q5&quot;  &quot;Q5&quot;  &quot;Q6&quot;  &quot;Q6&quot;  &quot;Q7&quot;  &quot;Q7&quot;  &quot;Q8&quot;  &quot;Q8&quot; 
## [17] &quot;Q9&quot;  &quot;Q9&quot;  &quot;Q10&quot; &quot;Q10&quot; &quot;Q11&quot; &quot;Q11&quot; &quot;Q12&quot; &quot;Q12&quot;
## [25] &quot;Q13&quot; &quot;Q13&quot; &quot;Q14&quot; &quot;Q14&quot; &quot;Q15&quot; &quot;Q15&quot; &quot;Q16&quot; &quot;Q16&quot;
## [33] &quot;Q17&quot; &quot;Q17&quot; &quot;Q18&quot; &quot;Q18&quot; &quot;Q19&quot; &quot;Q19&quot; &quot;Q20&quot; &quot;Q20&quot;
## [41] &quot;Q21&quot; &quot;Q21&quot; &quot;Q22&quot; &quot;Q22&quot; &quot;Q23&quot; &quot;Q23&quot; &quot;Q24&quot; &quot;Q24&quot;
## [49] &quot;Q25&quot; &quot;Q25&quot;</code></pre>
<p>Set a series of tuning parameter values. nlam is the number of values we want to tune. It is the parameter <span class="math inline">\(m\)</span> in {0.96<em>{max},0.96<sup>{2}<em>{max},0.96^{3}</em>{max},…,0.96</sup>{m}</em>{max}}$. The tuning process returns a long output and we will not report all:</p>
<div class="sourceCode" id="cb347"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb347-1" data-line-number="1"><span class="co"># Tune over 100 values</span></a>
<a class="sourceLine" id="cb347-2" data-line-number="2">nlam &lt;-<span class="st"> </span><span class="dv">100</span></a>
<a class="sourceLine" id="cb347-3" data-line-number="3"><span class="co"># set the type of prediction</span></a>
<a class="sourceLine" id="cb347-4" data-line-number="4"><span class="co"># - `link`: return the predicted link function</span></a>
<a class="sourceLine" id="cb347-5" data-line-number="5"><span class="co"># - `response`: return the predicted probability</span></a>
<a class="sourceLine" id="cb347-6" data-line-number="6"><span class="co"># number of cross-validation folds</span></a>
<a class="sourceLine" id="cb347-7" data-line-number="7">kfold &lt;-<span class="st"> </span><span class="dv">10</span></a>
<a class="sourceLine" id="cb347-8" data-line-number="8">cv_fit &lt;-<span class="st"> </span><span class="kw">cv_glasso</span>(trainx, trainy, <span class="dt">nlam =</span> nlam, <span class="dt">kfold =</span> kfold, <span class="dt">type =</span> <span class="st">&quot;link&quot;</span>)</a>
<a class="sourceLine" id="cb347-9" data-line-number="9"><span class="co"># 只展示部分结果</span></a>
<a class="sourceLine" id="cb347-10" data-line-number="10"><span class="kw">str</span>(cv_fit)</a></code></pre></div>
<p>Here we only show part of the output:</p>
<div class="sourceCode" id="cb348"><pre class="sourceCode html"><code class="sourceCode html"><a class="sourceLine" id="cb348-1" data-line-number="1">...</a>
<a class="sourceLine" id="cb348-2" data-line-number="2"> $ auc               : num [1:100] 0.573 0.567 0.535 0.484 0.514 ...</a>
<a class="sourceLine" id="cb348-3" data-line-number="3"> $ log_likelihood    : num [1:100] -554 -554 -553 -553 -552 ...</a>
<a class="sourceLine" id="cb348-4" data-line-number="4"> $ maxrho            : num [1:100] -0.0519 0.00666 0.04631 0.0486 0.06269 ...</a>
<a class="sourceLine" id="cb348-5" data-line-number="5"> $ lambda.max.auc    : Named num [1:2] 0.922 0.94</a>
<a class="sourceLine" id="cb348-6" data-line-number="6">  ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;lambda&quot; &quot;auc&quot;</a>
<a class="sourceLine" id="cb348-7" data-line-number="7"> $ lambda.1se.auc    : Named num [1:2] 16.74 0.81</a>
<a class="sourceLine" id="cb348-8" data-line-number="8">  ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;&quot; &quot;se.auc&quot;</a>
<a class="sourceLine" id="cb348-9" data-line-number="9"> $ lambda.max.loglike: Named num [1:2] 1.77 -248.86</a>
<a class="sourceLine" id="cb348-10" data-line-number="10">  ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;lambda&quot; &quot;loglike&quot;</a>
<a class="sourceLine" id="cb348-11" data-line-number="11"> $ lambda.1se.loglike: Named num [1:2] 9.45 -360.13</a>
<a class="sourceLine" id="cb348-12" data-line-number="12">  ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;lambda&quot; &quot;se.loglike&quot;</a>
<a class="sourceLine" id="cb348-13" data-line-number="13"> $ lambda.max.maxco  : Named num [1:2] 0.922 0.708</a>
<a class="sourceLine" id="cb348-14" data-line-number="14">  ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;lambda&quot; &quot;maxco&quot;</a>
<a class="sourceLine" id="cb348-15" data-line-number="15"> $ lambda.1se.maxco  : Named num [1:2] 14.216 0.504</a>
<a class="sourceLine" id="cb348-16" data-line-number="16">  ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;lambda&quot; &quot;se.maxco&quot;</a></code></pre></div>
<p>In the returned results:</p>
<ul>
<li><code>$ auc</code>: the AUC values</li>
<li><code>$ log_likelihood</code>: log-likelihood</li>
<li><code>$ maxrho</code>: maximum correlation coefficient</li>
<li><code>$ lambda.max.auc</code>: the max AUC and the corresponding value of <span class="math inline">\(\lambda\)</span></li>
<li><code>$ lambda.1se.auc</code>: one standard error to the max AUC and the corresponding <span class="math inline">\(\lambda\)</span></li>
<li><code>$ lambda.max.loglike</code>: max log-likelihood and the corresponding <span class="math inline">\(\lambda\)</span></li>
<li><code>$ lambda.1se.loglike</code>: one standard error to the max log-likelihood and the corresponding <span class="math inline">\(\lambda\)</span></li>
<li><code>$ lambda.max.maxco</code>: maximum correlation coefficient and the corresponding <span class="math inline">\(\lambda\)</span></li>
<li><code>$ lambda.1se.maxco</code>: one standard error to the maximum correlation coefficient and the corresponding <span class="math inline">\(\lambda\)</span></li>
</ul>
<p>The most common criterion is AUC. You can compare the selections from different criteria. If they all point to the same value of the tuning parameter, you can have more confidence about the choice. If they suggest very different values, then you need to concern if the tuning process is stable. You can visualize the cross validation result:</p>
<div class="sourceCode" id="cb349"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb349-1" data-line-number="1"><span class="kw">plot</span>(cv_fit)</a></code></pre></div>
<p><img src="IDS_files/figure-html/unnamed-chunk-137-1.svg" width="672" /></p>
<p>The x-axis is the value of the tuning parameter, the y-axis is AUC. The two dash lines are the value of <span class="math inline">\(\lambda\)</span> for max AUC and the value for the one standard deviation to the max AUC. Once you choose the value of the tuning parameter, you can use <code>fitglasso()</code> to fit the model. For example, we can fit the model using the parameter value that gives the max AUC, which is <span class="math inline">\(\lambda=0.922\)</span>:</p>
<div class="sourceCode" id="cb350"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb350-1" data-line-number="1">fitgl &lt;-<span class="st"> </span><span class="kw">fitglasso</span>(trainx, trainy, <span class="dt">lambda =</span> <span class="fl">0.922</span>, <span class="dt">na_action =</span> na.pass)</a></code></pre></div>
<div class="sourceCode" id="cb351"><pre class="sourceCode html"><code class="sourceCode html"><a class="sourceLine" id="cb351-1" data-line-number="1">Lambda: 0.922  nr.var: 229</a></code></pre></div>
<p>You can use <code>coef()</code> to get the estimates of coefficients:</p>
<div class="sourceCode" id="cb352"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb352-1" data-line-number="1"><span class="kw">coef</span>(fitgl)</a></code></pre></div>
<pre class="pre"><code>               0.922
Intercept -5.318e+01
Q1.A       1.757e+00
Q1.B       1.719e+00
Q2.A       2.170e+00
Q2.B       6.939e-01
Q3.A       2.102e+00
Q3.B       1.359e+00
...</code></pre>
<p>Use <code>predict_glasso()</code> to predict new samples:</p>
<div class="sourceCode" id="cb354"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb354-1" data-line-number="1">prey &lt;-<span class="st"> </span><span class="kw">predict_glasso</span>(fitgl, trainx)</a></code></pre></div>

</div>
</div>
</div>
<!-- </div> -->
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-albert1984">
<p>A, Albert, and Anderson A. J. 1984. “On the Existence of the Maximum Likelihood Estimates in Logistic Regression Models.” <em>Biometrika</em> 71 (1): 1–10.</p>
</div>
<div id="ref-Meier2008">
<p>L Meier, S van de Geer, and P Buhlmann. 2008. “The Group Lasso for Logistic Regression.” <em>J. R. Stat. Soc. Ser. B Stat. Methodol</em> 70: 53–71.</p>
</div>
<div id="ref-Tibshirani1996">
<p>R, Tibshirani. 1996. “Regression Shrinkage and Selection via the Lasso.” <em>Journal of the Royal Statistical Society Series B (Methodological)</em> 58 (1): 267–88.</p>
</div>
<div id="ref-Wed1976">
<p>Wedderburn, R. W. M. 1976. “On the Existence and Uniqueness of the Maximum Likelihood Estimates for Certain Generalized Linear Models.” <em>Biometrika</em> 63: 27–32.</p>
</div>
<div id="ref-Yeo2004">
<p>Yeo, G.W., and C.B. Burge. 2004. “Maximum Entropy Modeling of Short Sequence Motifs with Applications to Rna Splicing Signals.” <em>Journal of Computational Biology</em>, November, 475–94.</p>
</div>
<div id="ref-Kim2006">
<p>Y. Kim, J. Kim, and Y. Kim. 2006. “Blockwise Sparse Regression.” <em>Statist. Sin</em> 16: 375–90.</p>
</div>
<div id="ref-Yuan2007">
<p>Yuan, M., and Y. Lin. 2007. “Model Selection and Estimation in Regression with Grouped Variables.” <em>J. R. Stat. Soc. Ser. B Stat. Methodol</em> 68: 49–67.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="elastic-net.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="treemodel.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/lunr.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": true,
"twitter": true,
"linkedin": true,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/happyrabbit/IntroDataScience/10-Regularization.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["IDS.pdf", "IDS.epub", "IDS.mobi"],
"toc": {
"collapse": "section",
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
