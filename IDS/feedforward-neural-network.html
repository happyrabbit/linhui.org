<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>12.3 Feedforward Neural Network | Introduction to Data Science</title>
  <meta name="description" content="12.3 Feedforward Neural Network | Introduction to Data Science" />
  <meta name="generator" content="bookdown 0.15 and GitBook 2.6.7" />

  <meta property="og:title" content="12.3 Feedforward Neural Network | Introduction to Data Science" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="http://scientistcafe.com/IDS/" />
  
  <meta property="og:description" content="12.3 Feedforward Neural Network | Introduction to Data Science" />
  <meta name="github-repo" content="happyrabbit/IntroDataScience" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="12.3 Feedforward Neural Network | Introduction to Data Science" />
  
  <meta name="twitter:description" content="12.3 Feedforward Neural Network | Introduction to Data Science" />
  

<meta name="author" content="Hui Lin and Ming Li" />


<meta name="date" content="2020-01-09" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="projection-pursuit-regression.html"/>
<link rel="next" href="convolutional-neural-network.html"/>
<script src="libs/jquery/jquery.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="http://scientistcafe.com">Homepage</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="goal-of-the-book.html"><a href="goal-of-the-book.html"><i class="fa fa-check"></i>Goal of the Book</a></li>
<li class="chapter" data-level="" data-path="who-this-book-is-for.html"><a href="who-this-book-is-for.html"><i class="fa fa-check"></i>Who This Book Is For</a></li>
<li class="chapter" data-level="" data-path="what-this-book-covers.html"><a href="what-this-book-covers.html"><i class="fa fa-check"></i>What This Book Covers</a></li>
<li class="chapter" data-level="" data-path="conventions.html"><a href="conventions.html"><i class="fa fa-check"></i>Conventions</a></li>
<li class="chapter" data-level="" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="about-the-authors.html"><a href="about-the-authors.html"><i class="fa fa-check"></i>About the Authors</a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="blind-men-and-an-elephant.html"><a href="blind-men-and-an-elephant.html"><i class="fa fa-check"></i><b>1.1</b> Blind men and an elephant</a><ul>
<li class="chapter" data-level="1.1.1" data-path="blind-men-and-an-elephant.html"><a href="blind-men-and-an-elephant.html#data-science-roleskill-tracks"><i class="fa fa-check"></i><b>1.1.1</b> Data science role/skill tracks</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="what-should-data-science-do.html"><a href="what-should-data-science-do.html"><i class="fa fa-check"></i><b>1.2</b> What should data science do?</a><ul>
<li class="chapter" data-level="1.2.1" data-path="what-should-data-science-do.html"><a href="what-should-data-science-do.html#lets-dream-big"><i class="fa fa-check"></i><b>1.2.1</b> Let’s dream big</a></li>
<li class="chapter" data-level="1.2.2" data-path="what-should-data-science-do.html"><a href="what-should-data-science-do.html#what-kind-of-questions-can-data-science-solve"><i class="fa fa-check"></i><b>1.2.2</b> What kind of questions can data science solve?</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="structure-data-science-team.html"><a href="structure-data-science-team.html"><i class="fa fa-check"></i><b>1.3</b> Structure data science team</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="soft-skills-for-data-scientists.html"><a href="soft-skills-for-data-scientists.html"><i class="fa fa-check"></i><b>2</b> Soft Skills for Data Scientists</a><ul>
<li class="chapter" data-level="2.1" data-path="comparison-between-statistician-and-data-scientist.html"><a href="comparison-between-statistician-and-data-scientist.html"><i class="fa fa-check"></i><b>2.1</b> Comparison between Statistician and Data Scientist</a></li>
<li class="chapter" data-level="2.2" data-path="beyond-data-and-analytics.html"><a href="beyond-data-and-analytics.html"><i class="fa fa-check"></i><b>2.2</b> Beyond Data and Analytics</a></li>
<li class="chapter" data-level="2.3" data-path="three-pillars-of-knowledge.html"><a href="three-pillars-of-knowledge.html"><i class="fa fa-check"></i><b>2.3</b> Three Pillars of Knowledge</a></li>
<li class="chapter" data-level="2.4" data-path="data-science-project-cycle.html"><a href="data-science-project-cycle.html"><i class="fa fa-check"></i><b>2.4</b> Data Science Project Cycle</a><ul>
<li class="chapter" data-level="2.4.1" data-path="data-science-project-cycle.html"><a href="data-science-project-cycle.html#types-of-data-science-projects"><i class="fa fa-check"></i><b>2.4.1</b> Types of Data Science Projects</a></li>
<li class="chapter" data-level="2.4.2" data-path="data-science-project-cycle.html"><a href="data-science-project-cycle.html#at-the-planning-stage"><i class="fa fa-check"></i><b>2.4.2</b> At the Planning Stage</a></li>
<li class="chapter" data-level="2.4.3" data-path="data-science-project-cycle.html"><a href="data-science-project-cycle.html#at-the-modeling-stage"><i class="fa fa-check"></i><b>2.4.3</b> At the Modeling Stage</a></li>
<li class="chapter" data-level="2.4.4" data-path="data-science-project-cycle.html"><a href="data-science-project-cycle.html#at-the-production-stage"><i class="fa fa-check"></i><b>2.4.4</b> At the Production Stage</a></li>
<li class="chapter" data-level="2.4.5" data-path="data-science-project-cycle.html"><a href="data-science-project-cycle.html#summary"><i class="fa fa-check"></i><b>2.4.5</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="common-mistakes-in-data-science.html"><a href="common-mistakes-in-data-science.html"><i class="fa fa-check"></i><b>2.5</b> Common Mistakes in Data Science</a><ul>
<li class="chapter" data-level="2.5.1" data-path="common-mistakes-in-data-science.html"><a href="common-mistakes-in-data-science.html#problem-formulation-stage"><i class="fa fa-check"></i><b>2.5.1</b> Problem Formulation Stage</a></li>
<li class="chapter" data-level="2.5.2" data-path="common-mistakes-in-data-science.html"><a href="common-mistakes-in-data-science.html#problem-planning-stage"><i class="fa fa-check"></i><b>2.5.2</b> Problem Planning Stage</a></li>
<li class="chapter" data-level="2.5.3" data-path="common-mistakes-in-data-science.html"><a href="common-mistakes-in-data-science.html#modeling-stage"><i class="fa fa-check"></i><b>2.5.3</b> Modeling Stage</a></li>
<li class="chapter" data-level="2.5.4" data-path="common-mistakes-in-data-science.html"><a href="common-mistakes-in-data-science.html#production-stage"><i class="fa fa-check"></i><b>2.5.4</b> Production Stage</a></li>
<li class="chapter" data-level="2.5.5" data-path="common-mistakes-in-data-science.html"><a href="common-mistakes-in-data-science.html#summary-1"><i class="fa fa-check"></i><b>2.5.5</b> Summary</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="introduction-to-the-data.html"><a href="introduction-to-the-data.html"><i class="fa fa-check"></i><b>3</b> Introduction to The Data</a><ul>
<li class="chapter" data-level="3.1" data-path="customer-data-for-a-clothing-company.html"><a href="customer-data-for-a-clothing-company.html"><i class="fa fa-check"></i><b>3.1</b> Customer Data for A Clothing Company</a></li>
<li class="chapter" data-level="3.2" data-path="customer-satisfaction-survey-data-from-airline-company.html"><a href="customer-satisfaction-survey-data-from-airline-company.html"><i class="fa fa-check"></i><b>3.2</b> Customer Satisfaction Survey Data from Airline Company</a></li>
<li class="chapter" data-level="3.3" data-path="swinediseasedata.html"><a href="swinediseasedata.html"><i class="fa fa-check"></i><b>3.3</b> Swine Disease Breakout Data</a></li>
<li class="chapter" data-level="3.4" data-path="mnist-dataset.html"><a href="mnist-dataset.html"><i class="fa fa-check"></i><b>3.4</b> MNIST Dataset</a></li>
<li class="chapter" data-level="3.5" data-path="imdb-dataset.html"><a href="imdb-dataset.html"><i class="fa fa-check"></i><b>3.5</b> IMDB Dataset</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="big-data-cloud-platform.html"><a href="big-data-cloud-platform.html"><i class="fa fa-check"></i><b>4</b> Big Data Cloud Platform</a><ul>
<li class="chapter" data-level="4.1" data-path="power-of-cluster-of-computers.html"><a href="power-of-cluster-of-computers.html"><i class="fa fa-check"></i><b>4.1</b> Power of Cluster of Computers</a></li>
<li class="chapter" data-level="4.2" data-path="evolution-of-cluster-computing.html"><a href="evolution-of-cluster-computing.html"><i class="fa fa-check"></i><b>4.2</b> Evolution of Cluster Computing</a><ul>
<li class="chapter" data-level="4.2.1" data-path="evolution-of-cluster-computing.html"><a href="evolution-of-cluster-computing.html#hadoop"><i class="fa fa-check"></i><b>4.2.1</b> Hadoop</a></li>
<li class="chapter" data-level="4.2.2" data-path="evolution-of-cluster-computing.html"><a href="evolution-of-cluster-computing.html#spark"><i class="fa fa-check"></i><b>4.2.2</b> Spark</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="CloudEnvironment.html"><a href="CloudEnvironment.html"><i class="fa fa-check"></i><b>4.3</b> Introduction of Cloud Environment</a><ul>
<li class="chapter" data-level="4.3.1" data-path="CloudEnvironment.html"><a href="CloudEnvironment.html#open-account-and-create-a-cluster"><i class="fa fa-check"></i><b>4.3.1</b> Open Account and Create a Cluster</a></li>
<li class="chapter" data-level="4.3.2" data-path="CloudEnvironment.html"><a href="CloudEnvironment.html#r-notebook"><i class="fa fa-check"></i><b>4.3.2</b> R Notebook</a></li>
<li class="chapter" data-level="4.3.3" data-path="CloudEnvironment.html"><a href="CloudEnvironment.html#markdown-cells"><i class="fa fa-check"></i><b>4.3.3</b> Markdown Cells</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="leverage-spark-using-r-notebook.html"><a href="leverage-spark-using-r-notebook.html"><i class="fa fa-check"></i><b>4.4</b> Leverage Spark Using R Notebook</a></li>
<li class="chapter" data-level="4.5" data-path="databases-and-sql.html"><a href="databases-and-sql.html"><i class="fa fa-check"></i><b>4.5</b> Databases and SQL</a><ul>
<li class="chapter" data-level="4.5.1" data-path="databases-and-sql.html"><a href="databases-and-sql.html#history"><i class="fa fa-check"></i><b>4.5.1</b> History</a></li>
<li class="chapter" data-level="4.5.2" data-path="databases-and-sql.html"><a href="databases-and-sql.html#database-table-and-view"><i class="fa fa-check"></i><b>4.5.2</b> Database, Table and View</a></li>
<li class="chapter" data-level="4.5.3" data-path="databases-and-sql.html"><a href="databases-and-sql.html#basic-sql-statement"><i class="fa fa-check"></i><b>4.5.3</b> Basic SQL Statement</a></li>
<li class="chapter" data-level="4.5.4" data-path="databases-and-sql.html"><a href="databases-and-sql.html#advanced-topics-in-database"><i class="fa fa-check"></i><b>4.5.4</b> Advanced Topics in Database</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="data-pre-processing.html"><a href="data-pre-processing.html"><i class="fa fa-check"></i><b>5</b> Data Pre-processing</a><ul>
<li class="chapter" data-level="5.1" data-path="data-cleaning.html"><a href="data-cleaning.html"><i class="fa fa-check"></i><b>5.1</b> Data Cleaning</a></li>
<li class="chapter" data-level="5.2" data-path="missing-values.html"><a href="missing-values.html"><i class="fa fa-check"></i><b>5.2</b> Missing Values</a><ul>
<li class="chapter" data-level="5.2.1" data-path="missing-values.html"><a href="missing-values.html#impute-missing-values-with-medianmode"><i class="fa fa-check"></i><b>5.2.1</b> Impute missing values with median/mode</a></li>
<li class="chapter" data-level="5.2.2" data-path="missing-values.html"><a href="missing-values.html#k-nearest-neighbors"><i class="fa fa-check"></i><b>5.2.2</b> K-nearest neighbors</a></li>
<li class="chapter" data-level="5.2.3" data-path="missing-values.html"><a href="missing-values.html#bagging-tree"><i class="fa fa-check"></i><b>5.2.3</b> Bagging Tree</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="centering-and-scaling.html"><a href="centering-and-scaling.html"><i class="fa fa-check"></i><b>5.3</b> Centering and Scaling</a></li>
<li class="chapter" data-level="5.4" data-path="resolve-skewness.html"><a href="resolve-skewness.html"><i class="fa fa-check"></i><b>5.4</b> Resolve Skewness</a></li>
<li class="chapter" data-level="5.5" data-path="resolve-outliers.html"><a href="resolve-outliers.html"><i class="fa fa-check"></i><b>5.5</b> Resolve Outliers</a></li>
<li class="chapter" data-level="5.6" data-path="collinearity.html"><a href="collinearity.html"><i class="fa fa-check"></i><b>5.6</b> Collinearity</a></li>
<li class="chapter" data-level="5.7" data-path="sparse-variables.html"><a href="sparse-variables.html"><i class="fa fa-check"></i><b>5.7</b> Sparse Variables</a></li>
<li class="chapter" data-level="5.8" data-path="re-encode-dummy-variables.html"><a href="re-encode-dummy-variables.html"><i class="fa fa-check"></i><b>5.8</b> Re-encode Dummy Variables</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="data-wrangling.html"><a href="data-wrangling.html"><i class="fa fa-check"></i><b>6</b> Data Wrangling</a><ul>
<li class="chapter" data-level="6.1" data-path="read-and-write-data.html"><a href="read-and-write-data.html"><i class="fa fa-check"></i><b>6.1</b> Read and write data</a><ul>
<li class="chapter" data-level="6.1.1" data-path="read-and-write-data.html"><a href="read-and-write-data.html#readr"><i class="fa fa-check"></i><b>6.1.1</b> <code>readr</code></a></li>
<li class="chapter" data-level="6.1.2" data-path="read-and-write-data.html"><a href="read-and-write-data.html#data.table-enhanced-data.frame"><i class="fa fa-check"></i><b>6.1.2</b> <code>data.table</code>— enhanced <code>data.frame</code></a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="summarize-data.html"><a href="summarize-data.html"><i class="fa fa-check"></i><b>6.2</b> Summarize data</a><ul>
<li class="chapter" data-level="6.2.1" data-path="summarize-data.html"><a href="summarize-data.html#apply-lapply-and-sapply-in-base-r"><i class="fa fa-check"></i><b>6.2.1</b> <code>apply()</code>, <code>lapply()</code> and <code>sapply()</code> in base R</a></li>
<li class="chapter" data-level="6.2.2" data-path="summarize-data.html"><a href="summarize-data.html#dplyr-package"><i class="fa fa-check"></i><b>6.2.2</b> <code>dplyr</code> package</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="tidy-and-reshape-data.html"><a href="tidy-and-reshape-data.html"><i class="fa fa-check"></i><b>6.3</b> Tidy and Reshape Data</a><ul>
<li class="chapter" data-level="6.3.1" data-path="tidy-and-reshape-data.html"><a href="tidy-and-reshape-data.html#reshape2-package"><i class="fa fa-check"></i><b>6.3.1</b> <code>reshape2</code> package</a></li>
<li class="chapter" data-level="6.3.2" data-path="tidy-and-reshape-data.html"><a href="tidy-and-reshape-data.html#tidyr-package"><i class="fa fa-check"></i><b>6.3.2</b> <code>tidyr</code> package</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="modeltuningstrategy.html"><a href="modeltuningstrategy.html"><i class="fa fa-check"></i><b>7</b> Model Tuning Strategy</a><ul>
<li class="chapter" data-level="7.1" data-path="systematic-error-and-random-error.html"><a href="systematic-error-and-random-error.html"><i class="fa fa-check"></i><b>7.1</b> Systematic Error and Random Error</a><ul>
<li class="chapter" data-level="7.1.1" data-path="systematic-error-and-random-error.html"><a href="systematic-error-and-random-error.html#vbtradeoff"><i class="fa fa-check"></i><b>7.1.1</b> Variance-Bias Trade-Off</a></li>
<li class="chapter" data-level="7.1.2" data-path="systematic-error-and-random-error.html"><a href="systematic-error-and-random-error.html#measurement-error-in-the-response"><i class="fa fa-check"></i><b>7.1.2</b> Measurement Error in the Response</a></li>
<li class="chapter" data-level="7.1.3" data-path="systematic-error-and-random-error.html"><a href="systematic-error-and-random-error.html#measurement-error-in-the-independent-variables"><i class="fa fa-check"></i><b>7.1.3</b> Measurement Error in the Independent Variables</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="data-splitting-and-resampling.html"><a href="data-splitting-and-resampling.html"><i class="fa fa-check"></i><b>7.2</b> Data Splitting and Resampling</a><ul>
<li class="chapter" data-level="7.2.1" data-path="data-splitting-and-resampling.html"><a href="data-splitting-and-resampling.html#data-splitting"><i class="fa fa-check"></i><b>7.2.1</b> Data Splitting</a></li>
<li class="chapter" data-level="7.2.2" data-path="data-splitting-and-resampling.html"><a href="data-splitting-and-resampling.html#resampling"><i class="fa fa-check"></i><b>7.2.2</b> Resampling</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="measuring-performance.html"><a href="measuring-performance.html"><i class="fa fa-check"></i><b>8</b> Measuring Performance</a><ul>
<li class="chapter" data-level="8.1" data-path="regression-model-performance.html"><a href="regression-model-performance.html"><i class="fa fa-check"></i><b>8.1</b> Regression Model Performance</a></li>
<li class="chapter" data-level="8.2" data-path="classification-model-performance.html"><a href="classification-model-performance.html"><i class="fa fa-check"></i><b>8.2</b> Classification Model Performance</a><ul>
<li class="chapter" data-level="8.2.1" data-path="classification-model-performance.html"><a href="classification-model-performance.html#confusion-matrix"><i class="fa fa-check"></i><b>8.2.1</b> Confusion Matrix</a></li>
<li class="chapter" data-level="8.2.2" data-path="classification-model-performance.html"><a href="classification-model-performance.html#kappa-statistic"><i class="fa fa-check"></i><b>8.2.2</b> Kappa Statistic</a></li>
<li class="chapter" data-level="8.2.3" data-path="classification-model-performance.html"><a href="classification-model-performance.html#roc"><i class="fa fa-check"></i><b>8.2.3</b> ROC</a></li>
<li class="chapter" data-level="8.2.4" data-path="classification-model-performance.html"><a href="classification-model-performance.html#gain-and-lift-charts"><i class="fa fa-check"></i><b>8.2.4</b> Gain and Lift Charts</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="regression-models.html"><a href="regression-models.html"><i class="fa fa-check"></i><b>9</b> Regression Models</a><ul>
<li class="chapter" data-level="9.1" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html"><i class="fa fa-check"></i><b>9.1</b> Ordinary Least Squares</a></li>
<li class="chapter" data-level="9.2" data-path="multivariate-adaptive-regression-splines.html"><a href="multivariate-adaptive-regression-splines.html"><i class="fa fa-check"></i><b>9.2</b> Multivariate Adaptive Regression Splines</a></li>
<li class="chapter" data-level="9.3" data-path="generalized-linear-model.html"><a href="generalized-linear-model.html"><i class="fa fa-check"></i><b>9.3</b> Generalized Linear Model</a></li>
<li class="chapter" data-level="9.4" data-path="pcr-and-pls.html"><a href="pcr-and-pls.html"><i class="fa fa-check"></i><b>9.4</b> PCR and PLS</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="regularization-methods.html"><a href="regularization-methods.html"><i class="fa fa-check"></i><b>10</b> Regularization Methods</a><ul>
<li class="chapter" data-level="10.1" data-path="ridge-regression.html"><a href="ridge-regression.html"><i class="fa fa-check"></i><b>10.1</b> Ridge Regression</a></li>
<li class="chapter" data-level="10.2" data-path="lasso.html"><a href="lasso.html"><i class="fa fa-check"></i><b>10.2</b> LASSO</a></li>
<li class="chapter" data-level="10.3" data-path="variable-selection-property-of-the-lasso.html"><a href="variable-selection-property-of-the-lasso.html"><i class="fa fa-check"></i><b>10.3</b> Variable selection property of the lasso</a></li>
<li class="chapter" data-level="10.4" data-path="elastic-net.html"><a href="elastic-net.html"><i class="fa fa-check"></i><b>10.4</b> Elastic Net</a></li>
<li class="chapter" data-level="10.5" data-path="penalized-generalized-linear-model.html"><a href="penalized-generalized-linear-model.html"><i class="fa fa-check"></i><b>10.5</b> Penalized Generalized Linear Model</a><ul>
<li class="chapter" data-level="10.5.1" data-path="penalized-generalized-linear-model.html"><a href="penalized-generalized-linear-model.html#introduction-to-glmnet-package"><i class="fa fa-check"></i><b>10.5.1</b> Introduction to <code>glmnet</code> package</a></li>
<li class="chapter" data-level="10.5.2" data-path="penalized-generalized-linear-model.html"><a href="penalized-generalized-linear-model.html#penalized-logistic-regression"><i class="fa fa-check"></i><b>10.5.2</b> Penalized logistic regression</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="treemodel.html"><a href="treemodel.html"><i class="fa fa-check"></i><b>11</b> Tree-Based Methods</a><ul>
<li class="chapter" data-level="11.1" data-path="splitting-criteria.html"><a href="splitting-criteria.html"><i class="fa fa-check"></i><b>11.1</b> Splitting Criteria</a></li>
<li class="chapter" data-level="11.2" data-path="tree-pruning.html"><a href="tree-pruning.html"><i class="fa fa-check"></i><b>11.2</b> Tree Pruning</a></li>
<li class="chapter" data-level="11.3" data-path="regression-and-decision-tree-basic.html"><a href="regression-and-decision-tree-basic.html"><i class="fa fa-check"></i><b>11.3</b> Regression and Decision Tree Basic</a><ul>
<li class="chapter" data-level="11.3.1" data-path="regression-and-decision-tree-basic.html"><a href="regression-and-decision-tree-basic.html#regression-tree"><i class="fa fa-check"></i><b>11.3.1</b> Regression Tree</a></li>
<li class="chapter" data-level="11.3.2" data-path="regression-and-decision-tree-basic.html"><a href="regression-and-decision-tree-basic.html#decision-tree"><i class="fa fa-check"></i><b>11.3.2</b> Decision Tree</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="bagging-tree-1.html"><a href="bagging-tree-1.html"><i class="fa fa-check"></i><b>11.4</b> Bagging Tree</a></li>
<li class="chapter" data-level="11.5" data-path="random-forest.html"><a href="random-forest.html"><i class="fa fa-check"></i><b>11.5</b> Random Forest</a></li>
<li class="chapter" data-level="11.6" data-path="gradient-boosted-machine.html"><a href="gradient-boosted-machine.html"><i class="fa fa-check"></i><b>11.6</b> Gradient Boosted Machine</a><ul>
<li class="chapter" data-level="11.6.1" data-path="gradient-boosted-machine.html"><a href="gradient-boosted-machine.html#adaptive-boosting"><i class="fa fa-check"></i><b>11.6.1</b> Adaptive Boosting</a></li>
<li class="chapter" data-level="11.6.2" data-path="gradient-boosted-machine.html"><a href="gradient-boosted-machine.html#stochastic-gradient-boosting"><i class="fa fa-check"></i><b>11.6.2</b> Stochastic Gradient Boosting</a></li>
<li class="chapter" data-level="11.6.3" data-path="gradient-boosted-machine.html"><a href="gradient-boosted-machine.html#boosting-as-additive-model"><i class="fa fa-check"></i><b>11.6.3</b> Boosting as Additive Model</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="deep-learning.html"><a href="deep-learning.html"><i class="fa fa-check"></i><b>12</b> Deep Learning</a><ul>
<li class="chapter" data-level="12.1" data-path="deep-learning-introduction-and-history.html"><a href="deep-learning-introduction-and-history.html"><i class="fa fa-check"></i><b>12.1</b> Deep Learning Introduction and History</a></li>
<li class="chapter" data-level="12.2" data-path="projection-pursuit-regression.html"><a href="projection-pursuit-regression.html"><i class="fa fa-check"></i><b>12.2</b> Projection Pursuit Regression</a></li>
<li class="chapter" data-level="12.3" data-path="feedforward-neural-network.html"><a href="feedforward-neural-network.html"><i class="fa fa-check"></i><b>12.3</b> Feedforward Neural Network</a><ul>
<li class="chapter" data-level="12.3.1" data-path="feedforward-neural-network.html"><a href="feedforward-neural-network.html#logistic_reg_as_neural_network"><i class="fa fa-check"></i><b>12.3.1</b> Logistic Regression as Neural Network</a></li>
<li class="chapter" data-level="12.3.2" data-path="feedforward-neural-network.html"><a href="feedforward-neural-network.html#gradient-descent"><i class="fa fa-check"></i><b>12.3.2</b> Gradient Descent</a></li>
<li class="chapter" data-level="12.3.3" data-path="feedforward-neural-network.html"><a href="feedforward-neural-network.html#deep-neural-network"><i class="fa fa-check"></i><b>12.3.3</b> Deep Neural Network</a></li>
<li class="chapter" data-level="12.3.4" data-path="feedforward-neural-network.html"><a href="feedforward-neural-network.html#activation-function"><i class="fa fa-check"></i><b>12.3.4</b> Activation Function</a></li>
<li class="chapter" data-level="12.3.5" data-path="feedforward-neural-network.html"><a href="feedforward-neural-network.html#deal-with-overfitting"><i class="fa fa-check"></i><b>12.3.5</b> Deal with Overfitting</a></li>
<li class="chapter" data-level="12.3.6" data-path="feedforward-neural-network.html"><a href="feedforward-neural-network.html#optimization"><i class="fa fa-check"></i><b>12.3.6</b> Optimization</a></li>
<li class="chapter" data-level="12.3.7" data-path="feedforward-neural-network.html"><a href="feedforward-neural-network.html#image-recognition-using-ffnn"><i class="fa fa-check"></i><b>12.3.7</b> Image Recognition Using FFNN</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="convolutional-neural-network.html"><a href="convolutional-neural-network.html"><i class="fa fa-check"></i><b>12.4</b> Convolutional Neural Network</a></li>
<li class="chapter" data-level="12.5" data-path="recurrent-neural-network.html"><a href="recurrent-neural-network.html"><i class="fa fa-check"></i><b>12.5</b> Recurrent Neural Network</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="r-code-for-data-simulation.html"><a href="r-code-for-data-simulation.html"><i class="fa fa-check"></i><b>A</b> R code for data simulation</a><ul>
<li class="chapter" data-level="A.1" data-path="customer-data-for-clothing-company.html"><a href="customer-data-for-clothing-company.html"><i class="fa fa-check"></i><b>A.1</b> Customer Data for Clothing Company</a></li>
<li class="chapter" data-level="A.2" data-path="customer-satisfaction-survey-data-from-airline-company-1.html"><a href="customer-satisfaction-survey-data-from-airline-company-1.html"><i class="fa fa-check"></i><b>A.2</b> Customer Satisfaction Survey Data from Airline Company</a></li>
<li class="chapter" data-level="A.3" data-path="swine-disease-breakout-data.html"><a href="swine-disease-breakout-data.html"><i class="fa fa-check"></i><b>A.3</b> Swine Disease Breakout Data</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Data Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="feedforward-neural-network" class="section level2">
<h2><span class="header-section-number">12.3</span> Feedforward Neural Network</h2>
<div id="logistic_reg_as_neural_network" class="section level3">
<h3><span class="header-section-number">12.3.1</span> Logistic Regression as Neural Network</h3>
<p>Let’s look at logistic regression from the lens of neural network. For a binary classification problem, for example spam classifier, given <span class="math inline">\(m\)</span> samples <span class="math inline">\(\{(x^{(1)}, y^{(1)}),(x^{(2)}, y^{(2)}),...,(x^{(m)}, y^{(m)})\}\)</span>, we need to use the input feature <span class="math inline">\(x^{(i)}\)</span> (they may be the frequency of various words such as “money”, special characters like dollar signs, and the use of capital letters in the message etc.) to predict the output <span class="math inline">\(y^{(i)}\)</span> (if it is a spam email). Assume that for each sample <span class="math inline">\(i\)</span>, there are <span class="math inline">\(n_{x}\)</span> input features. Then we have:</p>
<p><span class="math display" id="eq:input">\[\begin{equation}
X=\left[\begin{array}{cccc}
x_{1}^{(1)} &amp; x_{1}^{(2)} &amp; \dotsb &amp; x_{1}^{(m)}\\
x_{2}^{(1)} &amp; x_{2}^{(2)} &amp; \dotsb &amp; x_{2}^{(m)}\\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots\\
x_{n_{x}}^{(1)} &amp; x_{n_{x}}^{(2)} &amp; \dots &amp; x_{n_{x}}^{(m)}
\end{array}\right]\in\mathbb{R}^{n_{x}\times m}
\tag{11.7}
\end{equation}\]</span></p>
<p><span class="math display">\[y=[y^{(1)},y^{(2)},\dots,y^{(m)}] \in \mathbb{R}^{1 \times m}\]</span></p>
<p>To predict if sample <span class="math inline">\(i\)</span> is a spam email, we first get the inactivated <strong>neuro</strong> <span class="math inline">\(z^{(i)}\)</span> by a linear transformation of the input <span class="math inline">\(x^{(i)}\)</span>, which is <span class="math inline">\(z^{(i)}=w^Tx^{(i)} + b\)</span>. Then we apply a function to “activate” the neuro <span class="math inline">\(z^{(i)}\)</span> and we call it “activation function”. In logistic regression, the activation function is sigmoid function and the “activated” <span class="math inline">\(z^{(i)}\)</span> is the prediction:</p>
<p><span class="math display">\[\hat{y}^{(i)} = \sigma(w^Tx^{(i)} + b)\]</span></p>
<p>where <span class="math inline">\(\sigma(z) = \frac{1}{1+e^{-z}}\)</span>. The following figure summarizes the process:</p>
<center>
<img src="images/dnn0.png" style="width:30.0%" />
</center>
<p>There are two types of layers. The last layer connects directly to the output. All the rest are <em>intermediate layers</em>. Depending on your definition, we call it “0-layer neural network” where the layer count only considers <em>intermediate layers</em>. To train the model, you need a cost function which is defined as equation <a href="feedforward-neural-network.html#eq:costlogistic">(11.8)</a>.</p>
<p><span class="math display" id="eq:costlogistic">\[\begin{equation}
J(w,b)=\frac{1}{m} \Sigma_{i=1}^m L(\hat{y}^{(i)}, y^{(i)})
\tag{11.8}
\end{equation}\]</span></p>
<p>where</p>
<p><span class="math display">\[L(\hat{y}^{(i)}, y^{(i)}) =  -y^{(i)}log(\hat{y}^{(i)})-(1-y^{(i)})log(1-\hat{y}^{(i)})\]</span></p>
<p>To fit the model is to minimize the cost function.</p>
</div>
<div id="gradient-descent" class="section level3">
<h3><span class="header-section-number">12.3.2</span> Gradient Descent</h3>
<p>The general approach to minimize <span class="math inline">\(J(w,b)\)</span> is by gradient descent, also known as <em>back-propagation</em>. In logistic regression, it is easy to calculate the gradient w.r.t the parameters <span class="math inline">\((w, b)\)</span> using the chain rule for differentiation. The optimization process is a forward and backward sweep over the network. Let’s look at the gradient descent for logistic regression across m sample. The non-vectorized process is as follows.</p>
<center>
<img src="images/GradientDescent.png" style="width:100.0%" />
</center>
<p>First initialize <span class="math inline">\(w_1\)</span>, <span class="math inline">\(w_2\)</span>, … , <span class="math inline">\(w_{n_x}\)</span>, and <span class="math inline">\(b\)</span>. Then plug in the initialized value to the forward and backward propagation. The forward propagation takes the current weights and calculates the prediction <span class="math inline">\(\hat{h}^{(i)}\)</span> and cost <span class="math inline">\(J^{(i)}\)</span>. The backward propagation calculates the gradient descent for the parameters. After iterating through all <span class="math inline">\(m\)</span> samples, you can calculate gradient descent for the parameters. Then update the parameter by:
<span class="math display">\[w := w - \gamma \frac{\partial J}{\partial w}\]</span>
<span class="math display">\[b := b - \gamma \frac{\partial J}{\partial b}\]</span></p>
<p>Repeat the progapation process using the updated parameter until the cost <span class="math inline">\(J\)</span> stabilizes.</p>
</div>
<div id="deep-neural-network" class="section level3">
<h3><span class="header-section-number">12.3.3</span> Deep Neural Network</h3>
<p>Before people coined the term <em>deep learning</em>, a neural network refers to <em>single hidden layer network</em>. Neural networks with more than one layers are called <em>deep learning</em>. Network with the structure in figure <a href="feedforward-neural-network.html#fig:ffnn">11.1</a> is the <strong>multiple layer perceptron (MLP)</strong> or <strong>feedforward neural network (FFNN)</strong>.</p>
<div class="figure" style="text-align: center"><span id="fig:ffnn"></span>
<img src="images/dnn_str.png" alt="Feedforward Neural Network" width="80%" />
<p class="caption">
FIGURE 11.1: Feedforward Neural Network
</p>
</div>
<p>Let’s look at a simple one-hidden-layer neural network (figure <a href="feedforward-neural-network.html#fig:onelayernn">11.2</a>). First only consider one sample. From left to right, there is an input layer with 3 features (<span class="math inline">\(x_1, x_2, x_3\)</span>), a hidden layer with four neurons and an output later to produce a prediction <span class="math inline">\(\hat{y}\)</span>.</p>
<div class="figure" style="text-align: center"><span id="fig:onelayernn"></span>
<img src="images/onelayerNN.png" alt="1-layer Neural Network" width="80%" />
<p class="caption">
FIGURE 11.2: 1-layer Neural Network
</p>
</div>
<p><strong>From input to the first hidden layer</strong></p>
<p>Each inactivated neuron on the first layer is a linear transformation of the input vector <span class="math inline">\(x\)</span>. For example, <span class="math inline">\(z^{[1]}_1 = w^{[1]T}_1x^{(i)} + b_1^{[1]}\)</span> is the first inactivated neuron for hidden layer one. <strong>We use superscript <code>[l]</code> to denote a quantity associated with the <span class="math inline">\(l^{th}\)</span> layer and the subscript <code>i</code> to denote the <span class="math inline">\(i^{th}\)</span> entry of a vector (a neuron or feature).</strong> Here <span class="math inline">\(w^{[1]}\)</span> and <span class="math inline">\(b_1^{[1]}\)</span> are the weight and bias parameters for layer 1. <span class="math inline">\(w^{[1]}\)</span> is a <span class="math inline">\(4 \times 1\)</span> vector and hence <span class="math inline">\(w^{[1]T}_1x^{(i)}\)</span> us a linear combination of the four input features. Then use a sigmoid function <span class="math inline">\(\sigma(\cdot)\)</span> to activate the neuron <span class="math inline">\(z^{[1]}_1\)</span> to get <span class="math inline">\(a^{[1]}_1\)</span>.</p>
<p><strong>From the first hidden layer to the output</strong></p>
<p>Next, do a linear combination of the activated neurons from the first layer to get inactivated output, <span class="math inline">\(z^{[2]}_1\)</span>. And then activate the neuron to get the predicted output <span class="math inline">\(\hat{y}\)</span>. The parameters to estimate in this step are <span class="math inline">\(w^{[2]}\)</span> and <span class="math inline">\(b_1^{[2]}\)</span>.</p>
<p>If you fully write out the process, it is the bottom right of figure <a href="feedforward-neural-network.html#fig:onelayernn">11.2</a>. When you implement a neural network, you need to do similar calculation four times to get the activated neurons in the first hidden layer. Doing this with a <code>for</code> loop is inefficient. So people vectorize the four equations. Take an input and compute the corresponding <span class="math inline">\(z\)</span> and <span class="math inline">\(a\)</span> as a vector. You can vectorize each step and get the following representation:</p>
<p><span class="math display">\[\begin{array}{cc}
z^{[1]}=W^{[1]}x+b^{[1]} &amp; \ \ \sigma^{[1]}(z^{[1]})=a^{[1]}\\
z^{[2]}=W^{[2]}a^{[1]}+b^{[2]} &amp; \ \ \ \ \ \sigma^{[2]}(z^{[2]})=a^{[2]}=\hat{y}
\end{array}\]</span></p>
<p><span class="math inline">\(b^{[1]}\)</span> is the column vector of the four bias parameters shown above. <span class="math inline">\(z^{[1]}\)</span> is a column vector of the four non-active neurons. When you apply an activation function to a matrix or vector, you apply it element-wise. <span class="math inline">\(W^{[1]}\)</span> is the matrix by stacking the four row-vectors:</p>
<p><span class="math display">\[W^{[1]}=\left[\begin{array}{c}
w_{1}^{[1]T}\\
w_{2}^{[1]T}\\
w_{3}^{[1]T}\\
w_{4}^{[1]T}
\end{array}\right]\]</span></p>
<p>So if you have one sample, you can go through the above forward propagation process to calculate the output <span class="math inline">\(\hat{y}\)</span> for that sample. If you have <span class="math inline">\(m\)</span> training samples, you need to repeat this process each of the <span class="math inline">\(m\)</span> samples. <strong>We use superscript <code>(i)</code> to denote a quantity associated with <span class="math inline">\(i^{th}\)</span> sample.</strong> You need to do the same calculation for all <span class="math inline">\(m\)</span> samples.</p>
<p>For i = 1 to m, do:</p>
<p><span class="math display">\[\begin{array}{cc}
z^{[1](i)}=W^{[1]}x^{(i)}+b^{[1]} &amp; \ \ \sigma^{[1]}(z^{[1](i)})=a^{[1](i)}\\
z^{[2](i)}=W^{[2]}a^{[1](i)}+b^{[2]} &amp; \ \ \ \ \ \sigma^{[2]}(z^{[2](i)})=a^{[2](i)}=\hat{y}^{(i)}
\end{array}\]</span></p>
<p>Recall that we defined the matrix X to be equal to our training samples stacked up as column vectors in equation <a href="feedforward-neural-network.html#eq:input">(11.7)</a>. We do a similar thing here to stack vectors with the superscript (i) together across <span class="math inline">\(m\)</span> samples. This way, the neural network computes the outputs on all the samples on at the same time:</p>
<p><span class="math display">\[\begin{array}{cc}
Z^{[1]}=W^{[1]}X+b^{[1]} &amp; \ \ \sigma^{[1]}(Z^{[1]})=A^{[1]}\\
Z^{[2]}=W^{[2]}A^{[1]}+b^{[2]} &amp; \ \ \ \ \ \sigma^{[2]}(Z^{[2]})=A^{[2]}=\hat{Y}
\end{array}\]</span></p>
<p>where
<span class="math display">\[X=\left[\begin{array}{cccc}
| &amp; | &amp;  &amp; |\\
x^{(1)} &amp; x^{(1)} &amp; \cdots &amp; x^{(m)}\\
| &amp; | &amp;  &amp; |
\end{array}\right],\]</span></p>
<p><span class="math display">\[A^{[l]}=\left[\begin{array}{cccc}
| &amp; | &amp;  &amp; |\\
a^{[l](1)} &amp; a^{[l](1)} &amp; \cdots &amp; a^{[l](m)}\\
| &amp; | &amp;  &amp; |
\end{array}\right]_{l=1\ or\ 2},\]</span></p>
<p><span class="math display">\[Z^{[l]}=\left[\begin{array}{cccc}
| &amp; | &amp;  &amp; |\\
z^{[l](1)} &amp; z^{[l](1)} &amp; \cdots &amp; z^{[l](m)}\\
| &amp; | &amp;  &amp; |
\end{array}\right]_{l=1\ or\ 2}\]</span></p>
<p>You can add layers like this to get a deeper neural network as shown in the bottom right of figure <a href="feedforward-neural-network.html#fig:ffnn">11.1</a>.</p>
<p>When build a neural network with many layers, one of the choices you get to make is the activation function to use in the hidden layers and the output layer. So far, we only see sigmoid activation function. But there are other choices. Intermediate layers usually use different activation function than the output layer. Let’s look at some of the common options in the next section.</p>
</div>
<div id="activation-function" class="section level3">
<h3><span class="header-section-number">12.3.4</span> Activation Function</h3>
<ul>
<li>Sigmoid and Softmax Function</li>
</ul>
<p>We have used the sigmoid (or logistic) activation function. The function is S-shape with an output value between 0 to 1. Therefore it is used as the output layer activation function to predict the probability <strong>when the response <span class="math inline">\(y\)</span> is binary</strong>. However, it is rarely used as an intermediate layer activation function. One of the main reasons is that when <span class="math inline">\(z\)</span> is away from 0, then the derivative of the function drops fast which slows down the optimization process through gradient descent. Even the fact that it is differentiable provides some convenience, the decreasing slope can cause a neural network to get stuck at the training time.</p>
<div class="figure" style="text-align: center"><span id="fig:activationsigmoid"></span>
<img src="IDS_files/figure-html/activationsigmoid-1.svg" alt="Sigmoid Function" width="80%" />
<p class="caption">
FIGURE 11.3: Sigmoid Function
</p>
</div>
<p>When the output has more than 2 categories, people use softmax function as the output layer activation function.</p>
<p><span class="math display" id="eq:softmax">\[\begin{equation}
f_i(\mathbf{z}) = \frac{e^{z_i}}{\Sigma_{j=1}^{J} e^{z_j} }
\tag{11.9}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\mathbf{z}\)</span> is a vector.</p>
<ul>
<li>Hyperbolic Tangent Function (tanh)</li>
</ul>
<p>Another activation function with a similar S-shape is the hyperbolic tangent function. It works better than the sigmoid function as the intermediate layer.<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a></p>
<p><span class="math display" id="eq:tanh">\[\begin{equation}
tanh(z) = \frac{e^{z} - e^{-z}}{e^{z} + e^{-z}}
\tag{11.10}
\end{equation}\]</span></p>
<div class="figure" style="text-align: center"><span id="fig:activationtanh"></span>
<img src="IDS_files/figure-html/activationtanh-1.svg" alt="Hyperbolic Tangent Function" width="80%" />
<p class="caption">
FIGURE 11.4: Hyperbolic Tangent Function
</p>
</div>
<p>The tanh function crosses point (0, 0) and the value of the function is between 1 and -1 which makes the mean of the activated neurons closer to 0. The sigmoid function doesn’t have that property. When you preprocess the training input data, you sometimes center the data so that the mean is 0. The tanh function is kind of doing that data processing for you which makes learning for the next layer a little easier. This activation function is used a lot in the recurrent neural networks where you want to polarize the results.</p>
<ul>
<li>Rectified Linear Unit (ReLU) Function</li>
</ul>
<p>The most popular activation function is the Rectified Linear Unit (ReLU) function. It is a piecewise function, or a half rectified function:</p>
<p><span class="math display" id="eq:relu">\[\begin{equation}
R(z) = max(0, z)
\tag{11.11}
\end{equation}\]</span></p>
<p>The derivative is 1 when z is positive and 0 when z is negative. You can define the derivative as either 0 or 1 when z is 0. When you implement this, it is unlikely that z equals to exactly 0 even it can be very close to 0.</p>
<div class="figure" style="text-align: center"><span id="fig:activationrelu"></span>
<img src="IDS_files/figure-html/activationrelu-1.svg" alt="Rectified Linear Unit Function" width="80%" />
<p class="caption">
FIGURE 11.5: Rectified Linear Unit Function
</p>
</div>
<p>The advantage of the ReLU is that when z is positive, the derivative doesn’t vanish as z getting larger. So it leads to faster computation than sigmoid or tanh. It is non-linear with an unconstrained response. However, the disadvantage is that when z is negative, the derivative is 0. It may not map the negative values appropriately. In practice, this doesn’t cause too much trouble but there is another version of ReLu called leaky ReLu that attempts to solve the dying ReLU problem. The leaky ReLu is</p>
<p><span class="math display">\[R(z)_{Leaky}=\begin{cases}
\begin{array}{c}
z\\
az
\end{array} &amp; \begin{array}{c}
z\geq0\\
z&lt;0
\end{array}\end{cases}\]</span></p>
<p>Instead of being 0 when z is negative, it adds a slight slope such as <span class="math inline">\(a=0.01\)</span> as shown in figure <a href="feedforward-neural-network.html#fig:activationleakyrelu">11.6</a> (can you see the leaky part there? : ).</p>
<div class="figure" style="text-align: center"><span id="fig:activationleakyrelu"></span>
<img src="IDS_files/figure-html/activationleakyrelu-1.svg" alt="Rectified Linear Unit Function" width="80%" />
<p class="caption">
FIGURE 11.6: Rectified Linear Unit Function
</p>
</div>
<p>You may notice that all activation functions are non-linear. Since the composition of two linear functions is still linear, using a linear activation function doesn’t help to capture more information. That is why you don’t see people use a linear activation function in the intermediate layer. One exception is when the output <span class="math inline">\(y\)</span> is continuous, you may use linear activation function at the output layer. To sum up, for intermediate layers:</p>
<ul>
<li>ReLU is usually a good choice. If you don’t know what to choose, then start with ReLU. Leaky ReLu usually works better than the ReLU but it is not used as much in practice. Either one works fine. Also, people usually use a=0.01 as the slope for leaky ReLU. You can try different parameters but most of the people a = 0.01.</li>
<li>tanh is used sometimes especially in recurrent neural network. But you nearly never see people use sigmoid function as intermediate layer activation function.</li>
</ul>
<p>For the output layer:</p>
<ul>
<li>When it is binary classification, use sigmoid with binary cross-entropy as loss function</li>
<li>When there are multiple classes, use softmax function with categorical cross-entropy as loss function</li>
<li>When the response is continuous, use identity function (i.e. y = x)</li>
</ul>
</div>
<div id="deal-with-overfitting" class="section level3">
<h3><span class="header-section-number">12.3.5</span> Deal with Overfitting</h3>
<p>The biggest problem for deep learning is overfitting.</p>
<div id="regularization" class="section level4">
<h4><span class="header-section-number">12.3.5.1</span> Regularization</h4>
<p>For logistic regression,</p>
<p><span class="math display">\[\underset{w,b}{min}J(w,b)= \frac{1}{m} \Sigma_{i=1}^{m}L(\hat{y}^{(i)}, y^{(i)}) + penalty\]</span></p>
<p>Common penalties are L1 or L2 as follows:</p>
<p><span class="math display">\[L_2\ penalty=\frac{\lambda}{2m}\parallel w \parallel_2^2 = \frac{\lambda}{2m}\Sigma_{i=1}^{n_x}w_i^2\]</span></p>
<p><span class="math display">\[L_1\ penalty = \frac{\lambda}{m}\Sigma_{i=1}^{n_x}|w|\]</span></p>
<p>For neural network,</p>
<p><span class="math display">\[J(w^{[1]},b^{[1]},\dots,w^{[L]},b^{[L]})=\frac{1}{m}\Sigma_{i=1}^{m}L(\hat{y}^{(i)},y^{(i)}) + \frac{\lambda}{2m}\Sigma_{l=1}^{L} \parallel w^{[l]} \parallel^2_F\]</span></p>
<p>where</p>
<p><span class="math display">\[\parallel w^{[l]} \parallel^2_F = \Sigma_{i=1}^{l}\Sigma_{j=1}^{l-1} (w^{[l]}_{ij})^2\]</span></p>
<p>Many people call it “Frobenius Norm” instead of L2-norm.</p>
</div>
<div id="dropout" class="section level4">
<h4><span class="header-section-number">12.3.5.2</span> Dropout</h4>
</div>
</div>
<div id="optimization" class="section level3">
<h3><span class="header-section-number">12.3.6</span> Optimization</h3>
<div id="batch-mini-batch-stochastic-gradient-descent" class="section level4">
<h4><span class="header-section-number">12.3.6.1</span> Batch, Mini-batch, Stochastic Gradient Descent</h4>
<p><span class="math display">\[\begin{array}{ccc} x= &amp; [\underbrace{x^{(1)},x^{(2)},\cdots,x^{(1000)}}/ &amp; \cdots/\cdots x^{(m)}]\\ (n_{x},m) &amp; mini-batch\ 1 \end{array}\]</span></p>
<p><span class="math display">\[\begin{array}{ccc} y= &amp; [\underbrace{y^{(1)},y^{(2)},\cdots,y^{(1000)}}/ &amp; \cdots/\cdots y^{(m)}]\\ (1,m) &amp; mini-batch\ 1 \end{array}\]</span></p>
<ul>
<li>Mini-batch size = m: batch gradient descent, too long per iteration</li>
<li>Mini-batch size = 1: stochastic gradient descent, lose speed from vectorization</li>
<li>Mini-batch size in between: mini-batch gradient descent, make progress without processing all training set, typical batch sizes are <span class="math inline">\(2^6=64\)</span>, <span class="math inline">\(2^7=128\)</span>, <span class="math inline">\(2^7=256\)</span>, <span class="math inline">\(2^8=512\)</span></li>
</ul>
</div>
<div id="optimization-algorithms" class="section level4">
<h4><span class="header-section-number">12.3.6.2</span> Optimization Algorithms</h4>
<p>In the history of deep learning, researchers proposed different optimization algorithms and showed that they worked well in a specific scenario. But the optimization algorithms didn’t generalize well to a wide range of neural networks. So you will need to try different optimizers in your application. We will introduce three commonly used optimizers here.</p>
<p><strong>Exponentially Weighted Averages</strong></p>
</div>
</div>
<div id="image-recognition-using-ffnn" class="section level3">
<h3><span class="header-section-number">12.3.7</span> Image Recognition Using FFNN</h3>
<p>In this section, we will walk through a toy example of image classification problem using <strong><code>keras</code></strong> package. We use R in the section to illustrate the process and also provide the python notebook on the book website. Please check the <a href="https://keras.rstudio.com/"><code>keras</code> R package website</a> for the most recent development. We are using the Databrick community edition with the following consideration:</p>
<ul>
<li>Minimum language barrier in coding for most users</li>
<li>Zero setup to save time using cloud environment</li>
<li>Help you get familiar with current trend of cloud computing in corporate setup</li>
</ul>
<p>Refer to section <a href="CloudEnvironment.html#CloudEnvironment">4.3</a> for how to set up an account, create a notebook (R or Python) and start a cluster.</p>
<p>What is an image as data? You can consider a digital image as a set of points on 2-d or 3-d space. Each each point has a value between 0 to 255 which is considered as a pixel. Figure <a href="feedforward-neural-network.html#fig:grayscaleimage">11.7</a> shows an example of grayscale image. It is a set of pixels on 2-d space and each pixel has a value between 0 to 255. You can process the image as a 2-d array input if you use a Convolutional Neural Network(CNN). Or, you can vectorize the array as the input for FFNN as shown in the figure.</p>
<div class="figure" style="text-align: center"><span id="fig:grayscaleimage"></span>
<img src="images/grayscaleimage.png" alt="Grayscale image is a set of pixels on 2-d space. Each pixel has a value range from 0 to 255." width="80%" />
<p class="caption">
FIGURE 11.7: Grayscale image is a set of pixels on 2-d space. Each pixel has a value range from 0 to 255.
</p>
</div>
<p>A color image is a set of pixels on 3-d space and each pixel has a value between 0 to 255. There are three 2-d panels which represent the color red, blue and green accordingly. Similarly, You can process the image as a 3-d array. Or you can vectorize the array as shown in figure <a href="feedforward-neural-network.html#fig:colorimage">11.8</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:colorimage"></span>
<img src="images/colorimage.png" alt="Color image is a set of pixels on 3-d space. Each pixel has a value range from 0 to 255." width="80%" />
<p class="caption">
FIGURE 11.8: Color image is a set of pixels on 3-d space. Each pixel has a value range from 0 to 255.
</p>
</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="2">
<li id="fn2"><p>“The tanh function is almost always strictly superior.” —- by Andrew Ng from his coursera course “Neural Networks and Deep Learning”<a href="feedforward-neural-network.html#fnref2" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="projection-pursuit-regression.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="convolutional-neural-network.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/lunr.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"facebook": true,
"twitter": true,
"linkedin": true,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"],
"github": true
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/happyrabbit/IntroDataScience/12-DeepLearning.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"download": ["IDS.pdf", "IDS.epub", "IDS.mobi"],
"toc": {
"collapse": "section",
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
