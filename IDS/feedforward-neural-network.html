<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>12.1 Feedforward Neural Network | Practitioner’s Guide to Data Science</title>
  <meta name="description" content="Introduction to Data Science" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="12.1 Feedforward Neural Network | Practitioner’s Guide to Data Science" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="http://scientistcafe.com/IDS/" />
  
  <meta property="og:description" content="Introduction to Data Science" />
  <meta name="github-repo" content="happyrabbit/IntroDataScience" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="12.1 Feedforward Neural Network | Practitioner’s Guide to Data Science" />
  
  <meta name="twitter:description" content="Introduction to Data Science" />
  

<meta name="author" content="Hui Lin and Ming Li" />


<meta name="date" content="2023-04-18" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="deeplearning.html"/>
<link rel="next" href="convolutional-neural-network.html"/>
<script src="libs/header-attrs/header-attrs.js"></script>
<script src="libs/jquery/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="http://scientistcafe.com">Homepage</a></li>

<li class="divider"></li>
<li><a href="index.html#preface" id="toc-preface">Preface</a>
<ul>
<li><a href="goal-of-the-book.html#goal-of-the-book" id="toc-goal-of-the-book">Goal of the Book</a></li>
<li><a href="what-this-book-covers.html#what-this-book-covers" id="toc-what-this-book-covers">What This Book Covers</a></li>
<li><a href="who-this-book-is-for.html#who-this-book-is-for" id="toc-who-this-book-is-for">Who This Book Is For</a></li>
<li><a href="how-to-use-this-book.html#how-to-use-this-book" id="toc-how-to-use-this-book">How to Use This Book</a>
<ul>
<li><a href="how-to-use-this-book.html#what-the-book-assumes" id="toc-what-the-book-assumes">What the Book Assumes</a></li>
<li><a href="how-to-use-this-book.html#how-to-run-r-and-python-code" id="toc-how-to-run-r-and-python-code">How to Run R and Python Code</a></li>
</ul></li>
<li><a href="complementary-reading.html#complementary-reading" id="toc-complementary-reading">Complementary Reading</a></li>
</ul></li>
<li><a href="about-the-authors.html#about-the-authors" id="toc-about-the-authors">About the Authors</a></li>
<li><a href="introduction.html#introduction" id="toc-introduction"><span class="toc-section-number">1</span> Introduction</a>
<ul>
<li><a href="a-brief-history-of-data-science.html#a-brief-history-of-data-science" id="toc-a-brief-history-of-data-science"><span class="toc-section-number">1.1</span> A Brief History of Data Science</a></li>
<li><a href="data-science-role-and-skill-tracks.html#data-science-role-and-skill-tracks" id="toc-data-science-role-and-skill-tracks"><span class="toc-section-number">1.2</span> Data Science Role and Skill Tracks</a>
<ul>
<li><a href="data-science-role-and-skill-tracks.html#engineering" id="toc-engineering"><span class="toc-section-number">1.2.1</span> Engineering</a></li>
<li><a href="data-science-role-and-skill-tracks.html#analysis" id="toc-analysis"><span class="toc-section-number">1.2.2</span> Analysis</a></li>
<li><a href="data-science-role-and-skill-tracks.html#modelinginference" id="toc-modelinginference"><span class="toc-section-number">1.2.3</span> Modeling/Inference</a></li>
</ul></li>
<li><a href="what-kind-of-questions-can-data-science-solve.html#what-kind-of-questions-can-data-science-solve" id="toc-what-kind-of-questions-can-data-science-solve"><span class="toc-section-number">1.3</span> What Kind of Questions Can Data Science Solve?</a>
<ul>
<li><a href="what-kind-of-questions-can-data-science-solve.html#prerequisites" id="toc-prerequisites"><span class="toc-section-number">1.3.1</span> Prerequisites</a></li>
<li><a href="what-kind-of-questions-can-data-science-solve.html#problem-type" id="toc-problem-type"><span class="toc-section-number">1.3.2</span> Problem Type</a></li>
</ul></li>
<li><a href="structure-of-data-science-team.html#structure-of-data-science-team" id="toc-structure-of-data-science-team"><span class="toc-section-number">1.4</span> Structure of Data Science Team</a></li>
<li><a href="data-science-roles.html#data-science-roles" id="toc-data-science-roles"><span class="toc-section-number">1.5</span> Data Science Roles</a></li>
</ul></li>
<li><a href="SoftSkillsforDataScientists.html#SoftSkillsforDataScientists" id="toc-SoftSkillsforDataScientists"><span class="toc-section-number">2</span> Soft Skills for Data Scientists</a>
<ul>
<li><a href="comparison-between-statistician-and-data-scientist.html#comparison-between-statistician-and-data-scientist" id="toc-comparison-between-statistician-and-data-scientist"><span class="toc-section-number">2.1</span> Comparison between Statistician and Data Scientist</a></li>
<li><a href="beyond-data-and-analytics.html#beyond-data-and-analytics" id="toc-beyond-data-and-analytics"><span class="toc-section-number">2.2</span> Beyond Data and Analytics</a></li>
<li><a href="three-pillars-of-knowledge.html#three-pillars-of-knowledge" id="toc-three-pillars-of-knowledge"><span class="toc-section-number">2.3</span> Three Pillars of Knowledge</a></li>
<li><a href="data-science-project-cycle.html#data-science-project-cycle" id="toc-data-science-project-cycle"><span class="toc-section-number">2.4</span> Data Science Project Cycle</a>
<ul>
<li><a href="data-science-project-cycle.html#types-of-data-science-projects" id="toc-types-of-data-science-projects"><span class="toc-section-number">2.4.1</span> Types of Data Science Projects</a></li>
<li><a href="data-science-project-cycle.html#ProblemFormulationandProjectPlanningStage" id="toc-ProblemFormulationandProjectPlanningStage"><span class="toc-section-number">2.4.2</span> Problem Formulation and Project Planning Stage</a></li>
<li><a href="data-science-project-cycle.html#project-modeling-stage" id="toc-project-modeling-stage"><span class="toc-section-number">2.4.3</span> Project Modeling Stage</a></li>
<li><a href="data-science-project-cycle.html#ModelImplementationandPostProductionStage" id="toc-ModelImplementationandPostProductionStage"><span class="toc-section-number">2.4.4</span> Model Implementation and Post Production Stage</a></li>
<li><a href="data-science-project-cycle.html#ProjectCycleSummary" id="toc-ProjectCycleSummary"><span class="toc-section-number">2.4.5</span> Project Cycle Summary</a></li>
</ul></li>
<li><a href="common-mistakes-in-data-science.html#common-mistakes-in-data-science" id="toc-common-mistakes-in-data-science"><span class="toc-section-number">2.5</span> Common Mistakes in Data Science</a>
<ul>
<li><a href="common-mistakes-in-data-science.html#problem-formulation-stage" id="toc-problem-formulation-stage"><span class="toc-section-number">2.5.1</span> Problem Formulation Stage</a></li>
<li><a href="common-mistakes-in-data-science.html#project-planning-stage" id="toc-project-planning-stage"><span class="toc-section-number">2.5.2</span> Project Planning Stage</a></li>
<li><a href="common-mistakes-in-data-science.html#project-modeling-stage-1" id="toc-project-modeling-stage-1"><span class="toc-section-number">2.5.3</span> Project Modeling Stage</a></li>
<li><a href="common-mistakes-in-data-science.html#model-implementation-and-post-production-stage" id="toc-model-implementation-and-post-production-stage"><span class="toc-section-number">2.5.4</span> Model Implementation and Post Production Stage</a></li>
<li><a href="common-mistakes-in-data-science.html#summary-of-common-mistakes" id="toc-summary-of-common-mistakes"><span class="toc-section-number">2.5.5</span> Summary of Common Mistakes</a></li>
</ul></li>
</ul></li>
<li><a href="introduction-to-the-data.html#introduction-to-the-data" id="toc-introduction-to-the-data"><span class="toc-section-number">3</span> Introduction to the Data</a>
<ul>
<li><a href="customer-data-for-a-clothing-company.html#customer-data-for-a-clothing-company" id="toc-customer-data-for-a-clothing-company"><span class="toc-section-number">3.1</span> Customer Data for a Clothing Company</a></li>
<li><a href="swinediseasedata.html#swinediseasedata" id="toc-swinediseasedata"><span class="toc-section-number">3.2</span> Swine Disease Breakout Data</a></li>
<li><a href="mnist-dataset.html#mnist-dataset" id="toc-mnist-dataset"><span class="toc-section-number">3.3</span> MNIST Dataset</a></li>
<li><a href="imdb-dataset.html#imdb-dataset" id="toc-imdb-dataset"><span class="toc-section-number">3.4</span> IMDB Dataset</a></li>
</ul></li>
<li><a href="bigdatacloudplatform.html#bigdatacloudplatform" id="toc-bigdatacloudplatform"><span class="toc-section-number">4</span> Big Data Cloud Platform</a>
<ul>
<li><a href="power-of-cluster-of-computers.html#power-of-cluster-of-computers" id="toc-power-of-cluster-of-computers"><span class="toc-section-number">4.1</span> Power of Cluster of Computers</a></li>
<li><a href="evolution-of-cluster-computing.html#evolution-of-cluster-computing" id="toc-evolution-of-cluster-computing"><span class="toc-section-number">4.2</span> Evolution of Cluster Computing</a>
<ul>
<li><a href="evolution-of-cluster-computing.html#hadoop" id="toc-hadoop"><span class="toc-section-number">4.2.1</span> Hadoop</a></li>
<li><a href="evolution-of-cluster-computing.html#spark" id="toc-spark"><span class="toc-section-number">4.2.2</span> Spark</a></li>
</ul></li>
<li><a href="CloudEnvironment.html#CloudEnvironment" id="toc-CloudEnvironment"><span class="toc-section-number">4.3</span> Introduction of Cloud Environment</a>
<ul>
<li><a href="CloudEnvironment.html#open-account-and-create-a-cluster" id="toc-open-account-and-create-a-cluster"><span class="toc-section-number">4.3.1</span> Open Account and Create a Cluster</a></li>
<li><a href="CloudEnvironment.html#r-notebook" id="toc-r-notebook"><span class="toc-section-number">4.3.2</span> R Notebook</a></li>
<li><a href="CloudEnvironment.html#markdown-cells" id="toc-markdown-cells"><span class="toc-section-number">4.3.3</span> Markdown Cells</a></li>
</ul></li>
<li><a href="leveragesparkr.html#leveragesparkr" id="toc-leveragesparkr"><span class="toc-section-number">4.4</span> Leverage Spark Using R Notebook</a></li>
<li><a href="databases-and-sql.html#databases-and-sql" id="toc-databases-and-sql"><span class="toc-section-number">4.5</span> Databases and SQL</a>
<ul>
<li><a href="databases-and-sql.html#history" id="toc-history"><span class="toc-section-number">4.5.1</span> History</a></li>
<li><a href="databases-and-sql.html#database-table-and-view" id="toc-database-table-and-view"><span class="toc-section-number">4.5.2</span> Database, Table and View</a></li>
<li><a href="databases-and-sql.html#basic-sql-statement" id="toc-basic-sql-statement"><span class="toc-section-number">4.5.3</span> Basic SQL Statement</a></li>
<li><a href="databases-and-sql.html#advanced-topics-in-database" id="toc-advanced-topics-in-database"><span class="toc-section-number">4.5.4</span> Advanced Topics in Database</a></li>
</ul></li>
</ul></li>
<li><a href="datapreprocessing.html#datapreprocessing" id="toc-datapreprocessing"><span class="toc-section-number">5</span> Data Pre-processing</a>
<ul>
<li><a href="data-cleaning.html#data-cleaning" id="toc-data-cleaning"><span class="toc-section-number">5.1</span> Data Cleaning</a></li>
<li><a href="missing-values.html#missing-values" id="toc-missing-values"><span class="toc-section-number">5.2</span> Missing Values</a>
<ul>
<li><a href="missing-values.html#impute-missing-values-with-medianmode" id="toc-impute-missing-values-with-medianmode"><span class="toc-section-number">5.2.1</span> Impute missing values with median/mode</a></li>
<li><a href="missing-values.html#k-nearest-neighbors" id="toc-k-nearest-neighbors"><span class="toc-section-number">5.2.2</span> K-nearest neighbors</a></li>
<li><a href="missing-values.html#bagging-tree" id="toc-bagging-tree"><span class="toc-section-number">5.2.3</span> Bagging Tree</a></li>
</ul></li>
<li><a href="centering-and-scaling.html#centering-and-scaling" id="toc-centering-and-scaling"><span class="toc-section-number">5.3</span> Centering and Scaling</a></li>
<li><a href="resolve-skewness.html#resolve-skewness" id="toc-resolve-skewness"><span class="toc-section-number">5.4</span> Resolve Skewness</a></li>
<li><a href="outliers.html#outliers" id="toc-outliers"><span class="toc-section-number">5.5</span> Resolve Outliers</a></li>
<li><a href="collinearity.html#collinearity" id="toc-collinearity"><span class="toc-section-number">5.6</span> Collinearity</a></li>
<li><a href="sparse-variables.html#sparse-variables" id="toc-sparse-variables"><span class="toc-section-number">5.7</span> Sparse Variables</a></li>
<li><a href="re-encode-dummy-variables.html#re-encode-dummy-variables" id="toc-re-encode-dummy-variables"><span class="toc-section-number">5.8</span> Re-encode Dummy Variables</a></li>
</ul></li>
<li><a href="datawrangline.html#datawrangline" id="toc-datawrangline"><span class="toc-section-number">6</span> Data Wrangling</a>
<ul>
<li><a href="summarize-data.html#summarize-data" id="toc-summarize-data"><span class="toc-section-number">6.1</span> Summarize Data</a>
<ul>
<li><a href="summarize-data.html#dplyr-package" id="toc-dplyr-package"><span class="toc-section-number">6.1.1</span> <code>dplyr</code> package</a></li>
<li><a href="summarize-data.html#applyfamilyinbaser" id="toc-applyfamilyinbaser"><span class="toc-section-number">6.1.2</span> <code>apply()</code>, <code>lapply()</code> and <code>sapply()</code> in base R</a></li>
</ul></li>
<li><a href="tidy-and-reshape-data.html#tidy-and-reshape-data" id="toc-tidy-and-reshape-data"><span class="toc-section-number">6.2</span> Tidy and Reshape Data</a></li>
</ul></li>
<li><a href="modeltuningstrategy.html#modeltuningstrategy" id="toc-modeltuningstrategy"><span class="toc-section-number">7</span> Model Tuning Strategy</a>
<ul>
<li><a href="vbtradeoff.html#vbtradeoff" id="toc-vbtradeoff"><span class="toc-section-number">7.1</span> Variance-Bias Trade-Off</a></li>
<li><a href="datasplittingresampling.html#datasplittingresampling" id="toc-datasplittingresampling"><span class="toc-section-number">7.2</span> Data Splitting and Resampling</a>
<ul>
<li><a href="datasplittingresampling.html#datasplitting" id="toc-datasplitting"><span class="toc-section-number">7.2.1</span> Data Splitting</a></li>
<li><a href="datasplittingresampling.html#resampling" id="toc-resampling"><span class="toc-section-number">7.2.2</span> Resampling</a></li>
</ul></li>
</ul></li>
<li><a href="measuring-performance.html#measuring-performance" id="toc-measuring-performance"><span class="toc-section-number">8</span> Measuring Performance</a>
<ul>
<li><a href="regression-model-performance.html#regression-model-performance" id="toc-regression-model-performance"><span class="toc-section-number">8.1</span> Regression Model Performance</a></li>
<li><a href="classification-model-performance.html#classification-model-performance" id="toc-classification-model-performance"><span class="toc-section-number">8.2</span> Classification Model Performance</a>
<ul>
<li><a href="classification-model-performance.html#confusion-matrix" id="toc-confusion-matrix"><span class="toc-section-number">8.2.1</span> Confusion Matrix</a></li>
<li><a href="classification-model-performance.html#kappa-statistic" id="toc-kappa-statistic"><span class="toc-section-number">8.2.2</span> Kappa Statistic</a></li>
<li><a href="classification-model-performance.html#roc" id="toc-roc"><span class="toc-section-number">8.2.3</span> ROC</a></li>
<li><a href="classification-model-performance.html#gain-and-lift-charts" id="toc-gain-and-lift-charts"><span class="toc-section-number">8.2.4</span> Gain and Lift Charts</a></li>
</ul></li>
</ul></li>
<li><a href="regression-models.html#regression-models" id="toc-regression-models"><span class="toc-section-number">9</span> Regression Models</a>
<ul>
<li><a href="ordinary-least-square.html#ordinary-least-square" id="toc-ordinary-least-square"><span class="toc-section-number">9.1</span> Ordinary Least Square</a>
<ul>
<li><a href="ordinary-least-square.html#the-magic-p-value" id="toc-the-magic-p-value"><span class="toc-section-number">9.1.1</span> The Magic P-value</a></li>
<li><a href="ordinary-least-square.html#diagnostics-for-linear-regression" id="toc-diagnostics-for-linear-regression"><span class="toc-section-number">9.1.2</span> Diagnostics for Linear Regression</a></li>
</ul></li>
<li><a href="principal-component-regression-and-partial-least-square.html#principal-component-regression-and-partial-least-square" id="toc-principal-component-regression-and-partial-least-square"><span class="toc-section-number">9.2</span> Principal Component Regression and Partial Least Square</a></li>
</ul></li>
<li><a href="regularization-methods.html#regularization-methods" id="toc-regularization-methods"><span class="toc-section-number">10</span> Regularization Methods</a>
<ul>
<li><a href="ridge-regression.html#ridge-regression" id="toc-ridge-regression"><span class="toc-section-number">10.1</span> Ridge Regression</a></li>
<li><a href="lasso.html#lasso" id="toc-lasso"><span class="toc-section-number">10.2</span> LASSO</a></li>
<li><a href="elastic-net.html#elastic-net" id="toc-elastic-net"><span class="toc-section-number">10.3</span> Elastic Net</a></li>
<li><a href="penalized-generalized-linear-model.html#penalized-generalized-linear-model" id="toc-penalized-generalized-linear-model"><span class="toc-section-number">10.4</span> Penalized Generalized Linear Model</a>
<ul>
<li><a href="penalized-generalized-linear-model.html#introduction-to-glmnet-package" id="toc-introduction-to-glmnet-package"><span class="toc-section-number">10.4.1</span> Introduction to <code>glmnet</code> package</a></li>
<li><a href="penalized-generalized-linear-model.html#penalized-logistic-regression" id="toc-penalized-logistic-regression"><span class="toc-section-number">10.4.2</span> Penalized logistic regression</a></li>
</ul></li>
</ul></li>
<li><a href="treemodel.html#treemodel" id="toc-treemodel"><span class="toc-section-number">11</span> Tree-Based Methods</a>
<ul>
<li><a href="tree-basics.html#tree-basics" id="toc-tree-basics"><span class="toc-section-number">11.1</span> Tree Basics</a></li>
<li><a href="splitting-criteria.html#splitting-criteria" id="toc-splitting-criteria"><span class="toc-section-number">11.2</span> Splitting Criteria</a>
<ul>
<li><a href="splitting-criteria.html#gini-impurity" id="toc-gini-impurity"><span class="toc-section-number">11.2.1</span> Gini impurity</a></li>
<li><a href="splitting-criteria.html#information-gain-ig" id="toc-information-gain-ig"><span class="toc-section-number">11.2.2</span> Information Gain (IG)</a></li>
<li><a href="splitting-criteria.html#information-gain-ratio-igr" id="toc-information-gain-ratio-igr"><span class="toc-section-number">11.2.3</span> Information Gain Ratio (IGR)</a></li>
<li><a href="splitting-criteria.html#sum-of-squared-error-sse" id="toc-sum-of-squared-error-sse"><span class="toc-section-number">11.2.4</span> Sum of Squared Error (SSE)</a></li>
</ul></li>
<li><a href="tree-pruning.html#tree-pruning" id="toc-tree-pruning"><span class="toc-section-number">11.3</span> Tree Pruning</a></li>
<li><a href="regression-and-decision-tree-basic.html#regression-and-decision-tree-basic" id="toc-regression-and-decision-tree-basic"><span class="toc-section-number">11.4</span> Regression and Decision Tree Basic</a>
<ul>
<li><a href="regression-and-decision-tree-basic.html#regression-tree" id="toc-regression-tree"><span class="toc-section-number">11.4.1</span> Regression Tree</a></li>
<li><a href="regression-and-decision-tree-basic.html#decision-tree" id="toc-decision-tree"><span class="toc-section-number">11.4.2</span> Decision Tree</a></li>
</ul></li>
<li><a href="bagging-tree-1.html#bagging-tree-1" id="toc-bagging-tree-1"><span class="toc-section-number">11.5</span> Bagging Tree</a></li>
<li><a href="random-forest.html#random-forest" id="toc-random-forest"><span class="toc-section-number">11.6</span> Random Forest</a></li>
<li><a href="gradient-boosted-machine.html#gradient-boosted-machine" id="toc-gradient-boosted-machine"><span class="toc-section-number">11.7</span> Gradient Boosted Machine</a>
<ul>
<li><a href="gradient-boosted-machine.html#adaptive-boosting" id="toc-adaptive-boosting"><span class="toc-section-number">11.7.1</span> Adaptive Boosting</a></li>
<li><a href="gradient-boosted-machine.html#stochastic-gradient-boosting" id="toc-stochastic-gradient-boosting"><span class="toc-section-number">11.7.2</span> Stochastic Gradient Boosting</a></li>
</ul></li>
</ul></li>
<li><a href="deeplearning.html#deeplearning" id="toc-deeplearning"><span class="toc-section-number">12</span> Deep Learning</a>
<ul>
<li><a href="feedforward-neural-network.html#feedforward-neural-network" id="toc-feedforward-neural-network"><span class="toc-section-number">12.1</span> Feedforward Neural Network</a>
<ul>
<li><a href="feedforward-neural-network.html#logisticregasneuralnetwork" id="toc-logisticregasneuralnetwork"><span class="toc-section-number">12.1.1</span> Logistic Regression as Neural Network</a></li>
<li><a href="feedforward-neural-network.html#stochastic-gradient-descent" id="toc-stochastic-gradient-descent"><span class="toc-section-number">12.1.2</span> Stochastic Gradient Descent</a></li>
<li><a href="feedforward-neural-network.html#deepneuralnetwork" id="toc-deepneuralnetwork"><span class="toc-section-number">12.1.3</span> Deep Neural Network</a></li>
<li><a href="feedforward-neural-network.html#activationfunction" id="toc-activationfunction"><span class="toc-section-number">12.1.4</span> Activation Function</a></li>
<li><a href="feedforward-neural-network.html#optimization" id="toc-optimization"><span class="toc-section-number">12.1.5</span> Optimization</a></li>
<li><a href="feedforward-neural-network.html#deal-with-overfitting" id="toc-deal-with-overfitting"><span class="toc-section-number">12.1.6</span> Deal with Overfitting</a></li>
<li><a href="feedforward-neural-network.html#ffnnexample" id="toc-ffnnexample"><span class="toc-section-number">12.1.7</span> Image Recognition Using FFNN</a></li>
</ul></li>
<li><a href="convolutional-neural-network.html#convolutional-neural-network" id="toc-convolutional-neural-network"><span class="toc-section-number">12.2</span> Convolutional Neural Network</a>
<ul>
<li><a href="convolutional-neural-network.html#convolution-layer" id="toc-convolution-layer"><span class="toc-section-number">12.2.1</span> Convolution Layer</a></li>
<li><a href="convolutional-neural-network.html#padding-layer" id="toc-padding-layer"><span class="toc-section-number">12.2.2</span> Padding Layer</a></li>
<li><a href="convolutional-neural-network.html#pooling-layer" id="toc-pooling-layer"><span class="toc-section-number">12.2.3</span> Pooling Layer</a></li>
<li><a href="convolutional-neural-network.html#convolution-over-volume" id="toc-convolution-over-volume"><span class="toc-section-number">12.2.4</span> Convolution Over Volume</a></li>
<li><a href="convolutional-neural-network.html#cnnexample" id="toc-cnnexample"><span class="toc-section-number">12.2.5</span> Image Recognition Using CNN</a></li>
</ul></li>
<li><a href="recurrent-neural-network.html#recurrent-neural-network" id="toc-recurrent-neural-network"><span class="toc-section-number">12.3</span> Recurrent Neural Network</a>
<ul>
<li><a href="recurrent-neural-network.html#rnn-model" id="toc-rnn-model"><span class="toc-section-number">12.3.1</span> RNN Model</a></li>
<li><a href="recurrent-neural-network.html#lstm" id="toc-lstm"><span class="toc-section-number">12.3.2</span> Long Short Term Memory</a></li>
<li><a href="recurrent-neural-network.html#embedding" id="toc-embedding"><span class="toc-section-number">12.3.3</span> Word Embedding</a></li>
<li><a href="recurrent-neural-network.html#rnnexample" id="toc-rnnexample"><span class="toc-section-number">12.3.4</span> Sentiment Analysis Using RNN</a></li>
</ul></li>
</ul></li>
<li><a href="appendix.html#appendix" id="toc-appendix">Appendix</a></li>
<li><a href="largelocaldata.html#largelocaldata" id="toc-largelocaldata"><span class="toc-section-number">13</span> Handling Large Local Data</a>
<ul>
<li><a href="readr.html#readr" id="toc-readr"><span class="toc-section-number">13.1</span> <code>readr</code></a></li>
<li><a href="data.table-enhanced-data.html#data.table-enhanced-data.frame" id="toc-data.table-enhanced-data.frame"><span class="toc-section-number">13.2</span> <code>data.table</code>— enhanced <code>data.frame</code></a></li>
</ul></li>
<li><a href="r-code-for-data-simulation.html#r-code-for-data-simulation" id="toc-r-code-for-data-simulation"><span class="toc-section-number">14</span> R code for data simulation</a>
<ul>
<li><a href="appendixdata1.html#appendixdata1" id="toc-appendixdata1"><span class="toc-section-number">14.1</span> Customer Data for Clothing Company</a></li>
<li><a href="appendixdata3.html#appendixdata3" id="toc-appendixdata3"><span class="toc-section-number">14.2</span> Swine Disease Breakout Data</a></li>
</ul></li>
<li><a href="references.html#references" id="toc-references">References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Practitioner’s Guide to Data Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="feedforward-neural-network" class="section level2 hasAnchor" number="12.1">
<h2><span class="header-section-number">12.1</span> Feedforward Neural Network<a href="feedforward-neural-network.html#feedforward-neural-network" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="logisticregasneuralnetwork" class="section level3 hasAnchor" number="12.1.1">
<h3><span class="header-section-number">12.1.1</span> Logistic Regression as Neural Network<a href="feedforward-neural-network.html#logisticregasneuralnetwork" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let’s look at logistic regression from the lens of the neural network. For a binary classification problem, for example spam classifier, given <span class="math inline">\(m\)</span> samples <span class="math inline">\(\{(x^{(1)}, y^{(1)}),(x^{(2)}, y^{(2)}),...,(x^{(m)}, y^{(m)})\}\)</span>, we need to use the input feature <span class="math inline">\(x^{(i)}\)</span> (they may be the frequency of various words such as “money”, special characters like dollar signs, and the use of capital letters in the message etc.) to predict the output <span class="math inline">\(y^{(i)}\)</span> (if it is a spam email). Assume that for each sample <span class="math inline">\(i\)</span>, there are <span class="math inline">\(n_{x}\)</span> input features. Then we have:</p>
<p><span class="math display" id="eq:input">\[\begin{equation}
X=\left[\begin{array}{cccc}
x_{1}^{(1)} &amp; x_{1}^{(2)} &amp; \dotsb &amp; x_{1}^{(m)}\\
x_{2}^{(1)} &amp; x_{2}^{(2)} &amp; \dotsb &amp; x_{2}^{(m)}\\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots\\
x_{n_{x}}^{(1)} &amp; x_{n_{x}}^{(2)} &amp; \dots &amp; x_{n_{x}}^{(m)}
\end{array}\right]\in\mathbb{R}^{n_{x}\times m}
\tag{12.1}
\end{equation}\]</span></p>
<p><span class="math display">\[y=[y^{(1)},y^{(2)},\dots,y^{(m)}] \in \mathbb{R}^{1 \times m}\]</span></p>
<p>To predict if sample <span class="math inline">\(i\)</span> is a spam email, we first get the inactivated <strong>neuro</strong> <span class="math inline">\(z^{(i)}\)</span> by a linear transformation of the input <span class="math inline">\(x^{(i)}\)</span>, which is <span class="math inline">\(z^{(i)}=w^Tx^{(i)} + b\)</span>. Then we apply a function to “activate” the neuro <span class="math inline">\(z^{(i)}\)</span> and we call it “activation function”. In logistic regression, the activation function is sigmoid function and the “activated” <span class="math inline">\(z^{(i)}\)</span> is the prediction:</p>
<p><span class="math display">\[\hat{y}^{(i)} = \sigma(w^Tx^{(i)} + b)\]</span></p>
<p>where <span class="math inline">\(\sigma(z) = \frac{1}{1+e^{-z}}\)</span>. The following figure summarizes the process:</p>
<center>
<img src="images/dnn0.png" style="width:30.0%" />
</center>
<p>There are two types of layers. The last layer connects directly to the output. All the rest are <em>intermediate layers</em>. Depending on your definition, we call it “0-layer neural network” where the layer count only considers <em>intermediate layers</em>. To train the model, you need a cost function which is defined as equation <a href="feedforward-neural-network.html#eq:costlogistic">(12.2)</a>.</p>
<p><span class="math display" id="eq:costlogistic">\[\begin{equation}
J(w,b)=\frac{1}{m} \Sigma_{i=1}^m L(\hat{y}^{(i)}, y^{(i)})
\tag{12.2}
\end{equation}\]</span></p>
<p>where</p>
<p><span class="math display">\[L(\hat{y}^{(i)}, y^{(i)}) =  -y^{(i)}log(\hat{y}^{(i)})-(1-y^{(i)})log(1-\hat{y}^{(i)})\]</span></p>
<p>To fit the model is to minimize the cost function.</p>
</div>
<div id="stochastic-gradient-descent" class="section level3 hasAnchor" number="12.1.2">
<h3><span class="header-section-number">12.1.2</span> Stochastic Gradient Descent<a href="feedforward-neural-network.html#stochastic-gradient-descent" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The general approach to minimize <span class="math inline">\(J(w,b)\)</span> is by gradient descent, also known as <em>back-propagation</em>. The optimization process is a forward and backward sweep over the network.</p>
<center>
<img src="images/dnn0_fb3.png" style="width:100.0%" />
</center>
<p>The forward propagation takes the current weights, calculates the prediction and cost. The backward propagation computes the gradient descent for the parameters by the chain rule for differentiation. In logistic regression, it is easy to calculate the gradient w.r.t the parameters <span class="math inline">\((w, b)\)</span>.</p>
<p>Let’s look at the Stochastic Gradient Descent (SGD) for logistic regression across <span class="math inline">\(m\)</span> samples. SGD updates one sample each time. The detailed process is as follows.</p>
<center>
<img src="images/GradientDescent.png" style="width:100.0%" />
</center>
<p>First initialize <span class="math inline">\(w_1\)</span>, <span class="math inline">\(w_2\)</span>, … , <span class="math inline">\(w_{n_x}\)</span>, and <span class="math inline">\(b\)</span>. Then plug in the initialized value to the forward and backward propagation. The forward propagation takes the current weights and calculates the prediction <span class="math inline">\(\hat{h}^{(i)}\)</span> and cost <span class="math inline">\(J^{(i)}\)</span>. The backward propagation calculates the gradient descent for the parameters. After iterating through all <span class="math inline">\(m\)</span> samples, you can calculate gradient descent for the parameters. Then update the parameter by:
<span class="math display">\[w := w - \gamma \frac{\partial J}{\partial w}\]</span>
<span class="math display">\[b := b - \gamma \frac{\partial J}{\partial b}\]</span></p>
<p>Repeat the forward and backward process using the updated parameter until the cost <span class="math inline">\(J\)</span> stabilizes.</p>
</div>
<div id="deepneuralnetwork" class="section level3 hasAnchor" number="12.1.3">
<h3><span class="header-section-number">12.1.3</span> Deep Neural Network<a href="feedforward-neural-network.html#deepneuralnetwork" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Before people coined the term <em>deep learning</em>, a neural network refers to <em>single hidden layer network</em>. Neural networks with more than one layers are called <em>deep learning</em>. Network with the structure in figure <a href="feedforward-neural-network.html#fig:ffnn">12.1</a> is the <strong>multiple layer perceptron (MLP)</strong> or <strong>feedforward neural network (FFNN)</strong>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ffnn"></span>
<img src="images/dnn_str.png" alt="Feedforward Neural Network" width="80%" />
<p class="caption">
FIGURE 12.1: Feedforward Neural Network
</p>
</div>
<p>Let’s look at a simple one-hidden-layer neural network (figure <a href="feedforward-neural-network.html#fig:onelayernn">12.2</a>). First only consider one sample. From left to right, there is an input layer with 3 features (<span class="math inline">\(x_1, x_2, x_3\)</span>), a hidden layer with four neurons and an output later to produce a prediction <span class="math inline">\(\hat{y}\)</span>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:onelayernn"></span>
<img src="images/onelayerNN.png" alt="1-layer Neural Network" width="80%" />
<p class="caption">
FIGURE 12.2: 1-layer Neural Network
</p>
</div>
<p><strong>From input to the first hidden layer</strong></p>
<p>Each inactivated neuron on the first layer is a linear transformation of the input vector <span class="math inline">\(x\)</span>. For example, <span class="math inline">\(z^{[1]}_1 = w^{[1]T}_1x^{(i)} + b_1^{[1]}\)</span> is the first inactivated neuron for hidden layer one. <strong>We use superscript <code>[l]</code> to denote a quantity associated with the <span class="math inline">\(l^{th}\)</span> layer and the subscript <code>i</code> to denote the <span class="math inline">\(i^{th}\)</span> entry of a vector (a neuron or feature).</strong> Here <span class="math inline">\(w^{[1]}\)</span> and <span class="math inline">\(b_1^{[1]}\)</span> are the weight and bias parameters for layer 1. <span class="math inline">\(w^{[1]}\)</span> is a <span class="math inline">\(4 \times 1\)</span> vector and hence <span class="math inline">\(w^{[1]T}_1x^{(i)}\)</span> is a linear combination of the four input features. Then use a sigmoid function <span class="math inline">\(\sigma(\cdot)\)</span> to activate the neuron <span class="math inline">\(z^{[1]}_1\)</span> to get <span class="math inline">\(a^{[1]}_1\)</span>.</p>
<p><strong>From the first hidden layer to the output</strong></p>
<p>Next, do a linear combination of the activated neurons from the first layer to get inactivated output, <span class="math inline">\(z^{[2]}_1\)</span>. And then activate the neuron to get the predicted output <span class="math inline">\(\hat{y}\)</span>. The parameters to estimate in this step are <span class="math inline">\(w^{[2]}\)</span> and <span class="math inline">\(b_1^{[2]}\)</span>.</p>
<p>If you fully write out the process, it is the bottom right of figure <a href="feedforward-neural-network.html#fig:onelayernn">12.2</a>. When you implement a neural network, you need to do similar calculation four times to get the activated neurons in the first hidden layer. Doing this with a <code>for</code> loop is inefficient. So people vectorize the four equations. Take an input and compute the corresponding <span class="math inline">\(z\)</span> and <span class="math inline">\(a\)</span> as a vector. You can vectorize each step and get the following representation:</p>
<p><span class="math display">\[\begin{array}{cc}
z^{[1]}=W^{[1]}x+b^{[1]} &amp; \ \ \sigma^{[1]}(z^{[1]})=a^{[1]}\\
z^{[2]}=W^{[2]}a^{[1]}+b^{[2]} &amp; \ \ \ \ \ \sigma^{[2]}(z^{[2]})=a^{[2]}=\hat{y}
\end{array}\]</span></p>
<p><span class="math inline">\(b^{[1]}\)</span> is the column vector of the four bias parameters shown above. <span class="math inline">\(z^{[1]}\)</span> is a column vector of the four non-active neurons. When you apply an activation function to a matrix or vector, you apply it element-wise. <span class="math inline">\(W^{[1]}\)</span> is the matrix by stacking the four row-vectors:</p>
<p><span class="math display">\[W^{[1]}=\left[\begin{array}{c}
w_{1}^{[1]T}\\
w_{2}^{[1]T}\\
w_{3}^{[1]T}\\
w_{4}^{[1]T}
\end{array}\right]\]</span></p>
<p>So if you have one sample, you can go through the above forward propagation process to calculate the output <span class="math inline">\(\hat{y}\)</span> for that sample. If you have <span class="math inline">\(m\)</span> training samples, you need to repeat this process each of the <span class="math inline">\(m\)</span> samples. <strong>We use superscript <code>(i)</code> to denote a quantity associated with <span class="math inline">\(i^{th}\)</span> sample.</strong> You need to do the same calculation for all <span class="math inline">\(m\)</span> samples.</p>
<p>For i = 1 to m, do:</p>
<p><span class="math display">\[\begin{array}{cc}
z^{[1](i)}=W^{[1]}x^{(i)}+b^{[1]} &amp; \ \ \sigma^{[1]}(z^{[1](i)})=a^{[1](i)}\\
z^{[2](i)}=W^{[2]}a^{[1](i)}+b^{[2]} &amp; \ \ \ \ \ \sigma^{[2]}(z^{[2](i)})=a^{[2](i)}=\hat{y}^{(i)}
\end{array}\]</span></p>
<p>Recall that we defined the matrix X to be equal to our training samples stacked up as column vectors in equation <a href="feedforward-neural-network.html#eq:input">(12.1)</a>. We do a similar thing here to stack vectors with the superscript (i) together across <span class="math inline">\(m\)</span> samples. This way, the neural network computes the outputs on all the samples on at the same time:</p>
<p><span class="math display">\[\begin{array}{cc}
Z^{[1]}=W^{[1]}X+b^{[1]} &amp; \ \ \sigma^{[1]}(Z^{[1]})=A^{[1]}\\
Z^{[2]}=W^{[2]}A^{[1]}+b^{[2]} &amp; \ \ \ \ \ \sigma^{[2]}(Z^{[2]})=A^{[2]}=\hat{Y}
\end{array}\]</span></p>
<p>where
<span class="math display">\[X=\left[\begin{array}{cccc}
| &amp; | &amp;  &amp; |\\
x^{(1)} &amp; x^{(1)} &amp; \cdots &amp; x^{(m)}\\
| &amp; | &amp;  &amp; |
\end{array}\right],\]</span></p>
<p><span class="math display">\[A^{[l]}=\left[\begin{array}{cccc}
| &amp; | &amp;  &amp; |\\
a^{[l](1)} &amp; a^{[l](1)} &amp; \cdots &amp; a^{[l](m)}\\
| &amp; | &amp;  &amp; |
\end{array}\right]_{l=1\ or\ 2},\]</span></p>
<p><span class="math display">\[Z^{[l]}=\left[\begin{array}{cccc}
| &amp; | &amp;  &amp; |\\
z^{[l](1)} &amp; z^{[l](1)} &amp; \cdots &amp; z^{[l](m)}\\
| &amp; | &amp;  &amp; |
\end{array}\right]_{l=1\ or\ 2}\]</span></p>
<p>You can add layers like this to get a deeper neural network as shown in the bottom right of figure <a href="feedforward-neural-network.html#fig:ffnn">12.1</a>.</p>
</div>
<div id="activationfunction" class="section level3 hasAnchor" number="12.1.4">
<h3><span class="header-section-number">12.1.4</span> Activation Function<a href="feedforward-neural-network.html#activationfunction" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>Sigmoid and Softmax Function</li>
</ul>
<p>We have used the sigmoid (or logistic) activation function. The function is S-shape with an output value between 0 to 1. Therefore it is used as the output layer activation function to predict the probability <strong>when the response <span class="math inline">\(y\)</span> is binary</strong>. However, it is rarely used as an intermediate layer activation function. One of the main reasons is that when <span class="math inline">\(z\)</span> is away from 0, then the derivative of the function drops fast which slows down the optimization process through gradient descent. Even with the fact that it is differentiable provides some convenience, the decreasing slope can cause a neural network to get stuck at the training time.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:activationsigmoid"></span>
<img src="IDS_files/figure-html/activationsigmoid-1.svg" alt="Sigmoid Function" width="80%" />
<p class="caption">
FIGURE 12.3: Sigmoid Function
</p>
</div>
<p>When the output has more than 2 categories, people use softmax function as the output layer activation function.</p>
<p><span class="math display" id="eq:softmax">\[\begin{equation}
f_i(\mathbf{z}) = \frac{e^{z_i}}{\Sigma_{j=1}^{J} e^{z_j} }
\tag{12.3}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\mathbf{z}\)</span> is a vector.</p>
<ul>
<li>Hyperbolic Tangent Function (tanh)</li>
</ul>
<p>Another activation function with a similar S-shape is the hyperbolic tangent function. It often works better than the sigmoid function as the intermediate layer.</p>
<p><span class="math display" id="eq:tanh">\[\begin{equation}
tanh(z) = \frac{e^{z} - e^{-z}}{e^{z} + e^{-z}}
\tag{12.4}
\end{equation}\]</span></p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:activationtanh"></span>
<img src="IDS_files/figure-html/activationtanh-1.svg" alt="Hyperbolic Tangent Function" width="80%" />
<p class="caption">
FIGURE 12.4: Hyperbolic Tangent Function
</p>
</div>
<p>The tanh function crosses point (0, 0) and the value of the function is between 1 and -1 which makes the mean of the activated neurons closer to 0. The sigmoid function doesn’t have that property. When you preprocess the training input data, you sometimes center the data so that the mean is 0. The tanh function is doing that data processing in some way which makes learning for the next layer a little easier. This activation function is used a lot in the recurrent neural networks where you want to polarize the results.</p>
<ul>
<li>Rectified Linear Unit (ReLU) Function</li>
</ul>
<p>The most popular activation function is the Rectified Linear Unit (ReLU) function. It is a piecewise function, or a half rectified function:</p>
<p><span class="math display" id="eq:relu">\[\begin{equation}
R(z) = max(0, z)
\tag{12.5}
\end{equation}\]</span></p>
<p>The derivative is 1 when z is positive and 0 when z is negative. You can define the derivative as either 0 or 1 when z is 0.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:activationrelu"></span>
<img src="IDS_files/figure-html/activationrelu-1.svg" alt="Rectified Linear Unit Function" width="80%" />
<p class="caption">
FIGURE 12.5: Rectified Linear Unit Function
</p>
</div>
<p>The advantage of the ReLU is that when z is positive, the derivative doesn’t vanish as z getting larger. So it leads to faster computation than sigmoid or tanh. It is non-linear with an unconstrained response. However, the disadvantage is that when z is negative, the derivative is 0. It may not map the negative values appropriately. In practice, this doesn’t cause too much trouble but there is another version of ReLu called leaky ReLu that attempts to solve the dying ReLU problem. The leaky ReLu is</p>
<p><span class="math display">\[R(z)_{Leaky}=\begin{cases}
\begin{array}{c}
z\\
az
\end{array} &amp; \begin{array}{c}
z\geq0\\
z&lt;0
\end{array}\end{cases}\]</span></p>
<p>Instead of being 0 when z is negative, it adds a slight slope such as <span class="math inline">\(a=0.01\)</span> as shown in figure <a href="feedforward-neural-network.html#fig:activationleakyrelu">12.6</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:activationleakyrelu"></span>
<img src="IDS_files/figure-html/activationleakyrelu-1.svg" alt="Rectified Linear Unit Function" width="80%" />
<p class="caption">
FIGURE 12.6: Rectified Linear Unit Function
</p>
</div>
<p>You may notice that all activation functions are non-linear. Since the composition of two linear functions is still linear, using a linear activation function doesn’t help to capture more information. That is why you don’t see people use a linear activation function in the intermediate layer. One exception is when the output <span class="math inline">\(y\)</span> is continuous, you may use linear activation function at the output layer. To sum up, for intermediate layers:</p>
<ul>
<li>ReLU is usually a good choice. If you don’t know what to choose, then start with ReLU. Leaky ReLu usually works better than the ReLU but it is not used as much in practice. Either one works fine. Also, people usually use a = 0.01 as the slope for leaky ReLU. You can try different parameters but most of the people a = 0.01.</li>
<li>tanh is used sometimes especially in recurrent neural network. But you nearly never see people use sigmoid function as intermediate layer activation function.</li>
</ul>
<p>For the output layer:</p>
<ul>
<li>When it is binary classification, use sigmoid with binary cross-entropy as loss function.</li>
<li>When there are multiple classes, use softmax function with categorical cross-entropy as loss function.</li>
<li>When the response is continuous, use identity function (i.e. y = x).</li>
</ul>
</div>
<div id="optimization" class="section level3 hasAnchor" number="12.1.5">
<h3><span class="header-section-number">12.1.5</span> Optimization<a href="feedforward-neural-network.html#optimization" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>So far, we have introduced the core components of deep learning architecture, layer, weight, activation function, and loss function. With the architecture in place, we need to determine how to update the network based on a loss function (a.k.a. objective function). In this section, we will look at variants of optimization algorithms that will improve the training speed.</p>
<div id="batch-mini-batch-stochastic-gradient-descent" class="section level4 hasAnchor" number="12.1.5.1">
<h4><span class="header-section-number">12.1.5.1</span> Batch, Mini-batch, Stochastic Gradient Descent<a href="feedforward-neural-network.html#batch-mini-batch-stochastic-gradient-descent" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><strong>Stochastic Gradient Descent (SGD)</strong> updates model parameters one sample each time. We showed the SGD for logistic regression across <span class="math inline">\(m\)</span> samples in section <a href="feedforward-neural-network.html#logisticregasneuralnetwork">12.1.1</a>. If you process the entire training set each time to calculate the gradient, it is <strong>Batch Gradient Descent (BGD)</strong>. The vector representation section <a href="feedforward-neural-network.html#deepneuralnetwork">12.1.3</a> using all <span class="math inline">\(m\)</span> is an example of BGD.</p>
<p>In deep learning applications, the training set is often huge, with hundreds of thousands or millions of samples. If processing all the samples can only lead to one step of gradient descent, it could be slow. An intuitive way to improve the algorithm is to make some progress before finishing the entire data set. In particular, split up the training set into smaller subsets and fit the model using one subset each time. The subsets are mini-batches. <strong>Mini-batch Gradient Descent (MGD)</strong> is to split the training set to be smaller mini-batches. For example, if the mini-batch size is 1000, the algorithm will process 1000 samples each time, calculate the gradient and update the parameters. Then it moves on to the next mini-batch set until it goes through the whole training set. We call it one pass through training set using mini-batch gradient descent or one epoch. <strong>Epoch</strong> is a common keyword in deep learning, which means a single pass through the training set. If the training set has 60,000 samples, one epoch leads to 60 gradient descent steps. Then it will start over and take another pass through the training set. It means one more decision to make, the optimal number of epochs. It is decided by looking at the trends of performance metrics on a holdout set of training data. We discussed the data splitting and sampling in section <a href="datasplittingresampling.html#datasplittingresampling">7.2</a>. People often use a single holdout set to tune the model in deep learning. But it is important to use a big enough holdout set to give high confidence in your model’s overall performance. Then you can evaluate the chosen model on your test set that is not used in the training process. MGD is what everyone in deep learning uses when training on a large data set.</p>
<p><span class="math display">\[\begin{array}{ccc} x= &amp; [\underbrace{x^{(1)},x^{(2)},\cdots,x^{(1000)}}/ &amp; \cdots/\cdots x^{(m)}]\\ (n_{x},m) &amp; mini-batch\ 1 \end{array}\]</span></p>
<p><span class="math display">\[\begin{array}{ccc} y= &amp; [\underbrace{y^{(1)},y^{(2)},\cdots,y^{(1000)}}/ &amp; \cdots/\cdots y^{(m)}]\\ (1,m) &amp; mini-batch\ 1 \end{array}\]</span></p>
<ul>
<li>Mini-batch size = m: batch gradient descent, too long per iteration</li>
<li>Mini-batch size = 1: stochastic gradient descent, lose speed from vectorization</li>
<li>Mini-batch size in between: mini-batch gradient descent, make progress without processing all training set, typical batch sizes are <span class="math inline">\(2^6=64\)</span>, <span class="math inline">\(2^7=128\)</span>, <span class="math inline">\(2^8=256\)</span>, <span class="math inline">\(2^9=512\)</span></li>
</ul>
</div>
<div id="optimization-algorithms" class="section level4 hasAnchor" number="12.1.5.2">
<h4><span class="header-section-number">12.1.5.2</span> Optimization Algorithms<a href="feedforward-neural-network.html#optimization-algorithms" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>In the history of deep learning, researchers proposed different optimization algorithms and showed that they worked well in a specific scenario. But the optimization algorithms didn’t generalize well to a wide range of neural networks. So you will need to try different optimizers in your application. We will introduce three commonly used optimizers here, and they are all based on something called exponentially weighted averages. To understand the intuition behind it, let’s look at a hypothetical example shown in figure <a href="feedforward-neural-network.html#fig:weightedagv">12.7</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:weightedagv"></span>
<img src="images/weighted_agv.png" alt="The intuition behind the weighted average gradient" width="60%" />
<p class="caption">
FIGURE 12.7: The intuition behind the weighted average gradient
</p>
</div>
<p>We have two parameters, b and w. The blue dot represents the current parameter value, and the red point is the optimum value we want to reach. The current value is close to the target vertically but far away horizontally. In this situation, we hope to have slower learning on b and faster learning on w. A way to adjust is to use the average of the gradients from different iterations instead of the current iteration to update the parameter. Vertically, since the current value is close to the target value of parameter b, the gradient of b is likely to jump between positive and negative values. The average tends to cancel out the positive and negative derivatives along the vertical direction and slow down the oscillations. Horizontally, since it is still further away from the target value of parameter w, all the gradients are likely pointing in the same direction. Using an average won’t have too much impact there. That is the fundamental idea behind many different optimizers: adjust the learning rate using a weighted average of various iterations’ gradients.</p>
<p><strong>Exponentially Weighted Averages</strong></p>
<p>Before we get to more sophisticated optimization algorithms that implement this idea, let’s first introduce the basic weighted moving average framework.</p>
<p>Suppose we have the following 100 days’ temperature data:</p>
<p><span class="math display">\[ \theta_{1}=49F, \theta_{2}=53F, \dots, \theta_{99}=70F, \theta_{100}=69F\]</span>
The weighted average is defined as:</p>
<p><span class="math display">\[V_t = \beta V_{t-1}+(1-\beta)\theta_t\]</span>
And we have:</p>
<p><span class="math display">\[
\begin{array}{c} V_{0}=0\\ V_{1}=\beta V_1 + (1-\beta)\theta_1\\ V_2=\beta V_1 + (1-\beta)\theta_2\\ \vdots \\ V_{100}= \beta V_{99} + (1-\beta)\theta_{100} \end{array}\]</span></p>
<p>For example, for <span class="math inline">\(\beta=0.95\)</span>:</p>
<p><span class="math display">\[\begin{array}{c} V_{0}=0 \\ V_{1}=0.05\theta_{1}  \\ V_{2}=0.0475\theta_{1}+0.05\theta_{2}   \end{array} \\ ......\]</span></p>
<p>The black line in the left plot of figure <a href="feedforward-neural-network.html#fig:weightedavgplot">12.8</a> is the exponentially weighted averages of simulated temperature data with <span class="math inline">\(\beta = 0.95\)</span>. <span class="math inline">\(V_t\)</span> is approximately average over the previous <span class="math inline">\(\frac{1}{1-\beta}\)</span> days. So <span class="math inline">\(\beta = 0.95\)</span> approximates a 20 days’ average. The red line corresponds to <span class="math inline">\(\beta = 0.8\)</span>, which approximates 5 days’ average. As <span class="math inline">\(\beta\)</span> increases, it averages over a larger window of the previous values, and hence the curve gets smoother. A larger <span class="math inline">\(\beta\)</span> also means that it gives the current value <span class="math inline">\(\theta_t\)</span> less weight (<span class="math inline">\(1-\beta\)</span>), and the average adapts more slowly. It is easy to see from the plot that the averages at the beginning are more biased. The bias correction can help to achieve a better estimate:</p>
<p><span class="math display">\[V_t^{corrected} = \frac{V_t}{1-\beta^t}\]</span></p>
<p><span class="math display">\[V_{1}^{corrected}=\frac{V_{1}}{1-0.95}=\theta_{1}\]</span></p>
<p><span class="math display">\[V_{2}^{corrected}=\frac{V_{2}}{1-0.95^{2}}=0.4872\theta_{1}+0.5128\theta_{2}\]</span></p>
<p>For <span class="math inline">\(\beta = 0.95\)</span>, the origional <span class="math inline">\(V_2 = 0.0475\theta_{1}+0.05\theta_{2}\)</span> which is a small fraction of both <span class="math inline">\(theta_1\)</span> and <span class="math inline">\(theta_2\)</span>. That is why it starts so much lower with big bias. After correction, <span class="math inline">\(V_{2}^{corrected} = 0.4872\theta_{1}+0.5128\theta_{2}\)</span> is a weighted average with two weights added up to 1 which revmoves the bias. Notice that as <span class="math inline">\(t\)</span> increases, <span class="math inline">\(\beta^t\)</span> converges to 0 and <span class="math inline">\(V^{corrected}\)</span> converges to <span class="math inline">\(V^t\)</span>.</p>
<p>The following code simulates a set of temperature data and applies exponentially weighted average with and without correction using various <span class="math inline">\(\beta\)</span> values (0.5, 0.8, 0.95).</p>
<div class="sourceCode" id="cb321"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb321-1"><a href="feedforward-neural-network.html#cb321-1" aria-hidden="true" tabindex="-1"></a><span class="co"># simulate the tempreture data</span></span>
<span id="cb321-2"><a href="feedforward-neural-network.html#cb321-2" aria-hidden="true" tabindex="-1"></a>a <span class="ot">=</span> <span class="sc">-</span><span class="dv">30</span><span class="sc">/</span><span class="dv">3479</span></span>
<span id="cb321-3"><a href="feedforward-neural-network.html#cb321-3" aria-hidden="true" tabindex="-1"></a>b <span class="ot">=</span> <span class="sc">-</span><span class="dv">120</span> <span class="sc">*</span> a</span>
<span id="cb321-4"><a href="feedforward-neural-network.html#cb321-4" aria-hidden="true" tabindex="-1"></a>c <span class="ot">=</span> <span class="dv">3600</span> <span class="sc">*</span> a <span class="sc">+</span> <span class="dv">80</span></span>
<span id="cb321-5"><a href="feedforward-neural-network.html#cb321-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb321-6"><a href="feedforward-neural-network.html#cb321-6" aria-hidden="true" tabindex="-1"></a>day <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">100</span>)</span>
<span id="cb321-7"><a href="feedforward-neural-network.html#cb321-7" aria-hidden="true" tabindex="-1"></a>theta <span class="ot">=</span> a <span class="sc">*</span> day<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> b <span class="sc">*</span> day <span class="sc">+</span> c <span class="sc">+</span> <span class="fu">runif</span>(<span class="fu">length</span>(day), <span class="sc">-</span><span class="dv">5</span>, <span class="dv">5</span>)</span>
<span id="cb321-8"><a href="feedforward-neural-network.html#cb321-8" aria-hidden="true" tabindex="-1"></a>theta <span class="ot">=</span> <span class="fu">round</span>(theta, <span class="dv">0</span>)</span>
<span id="cb321-9"><a href="feedforward-neural-network.html#cb321-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb321-10"><a href="feedforward-neural-network.html#cb321-10" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</span>
<span id="cb321-11"><a href="feedforward-neural-network.html#cb321-11" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(day, theta, <span class="at">cex =</span> <span class="fl">0.5</span>, <span class="at">pch =</span> <span class="dv">3</span>, <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">100</span>),</span>
<span id="cb321-12"><a href="feedforward-neural-network.html#cb321-12" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">&quot;Without Correction&quot;</span>,</span>
<span id="cb321-13"><a href="feedforward-neural-network.html#cb321-13" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Days&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Tempreture&quot;</span>)</span>
<span id="cb321-14"><a href="feedforward-neural-network.html#cb321-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb321-15"><a href="feedforward-neural-network.html#cb321-15" aria-hidden="true" tabindex="-1"></a>beta1 <span class="ot">=</span> <span class="fl">0.95</span></span>
<span id="cb321-16"><a href="feedforward-neural-network.html#cb321-16" aria-hidden="true" tabindex="-1"></a>beta2 <span class="ot">=</span> <span class="fl">0.8</span></span>
<span id="cb321-17"><a href="feedforward-neural-network.html#cb321-17" aria-hidden="true" tabindex="-1"></a>beta3 <span class="ot">=</span> <span class="fl">0.5</span></span>
<span id="cb321-18"><a href="feedforward-neural-network.html#cb321-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb321-19"><a href="feedforward-neural-network.html#cb321-19" aria-hidden="true" tabindex="-1"></a>exp_weight_avg <span class="ot">=</span> <span class="cf">function</span>(beta, theta) {</span>
<span id="cb321-20"><a href="feedforward-neural-network.html#cb321-20" aria-hidden="true" tabindex="-1"></a>  v <span class="ot">=</span> <span class="fu">rep</span>(<span class="dv">0</span>, <span class="fu">length</span>(theta))</span>
<span id="cb321-21"><a href="feedforward-neural-network.html#cb321-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb321-22"><a href="feedforward-neural-network.html#cb321-22" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(theta)) {</span>
<span id="cb321-23"><a href="feedforward-neural-network.html#cb321-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (i <span class="sc">==</span> <span class="dv">1</span>) {</span>
<span id="cb321-24"><a href="feedforward-neural-network.html#cb321-24" aria-hidden="true" tabindex="-1"></a>      v[i] <span class="ot">=</span> (<span class="dv">1</span> <span class="sc">-</span> beta) <span class="sc">*</span> theta[i]</span>
<span id="cb321-25"><a href="feedforward-neural-network.html#cb321-25" aria-hidden="true" tabindex="-1"></a>    } <span class="cf">else</span> {</span>
<span id="cb321-26"><a href="feedforward-neural-network.html#cb321-26" aria-hidden="true" tabindex="-1"></a>      v[i] <span class="ot">=</span> beta <span class="sc">*</span> v[i <span class="sc">-</span> <span class="dv">1</span>] <span class="sc">+</span> (<span class="dv">1</span> <span class="sc">-</span> beta) <span class="sc">*</span> theta[i]</span>
<span id="cb321-27"><a href="feedforward-neural-network.html#cb321-27" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb321-28"><a href="feedforward-neural-network.html#cb321-28" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb321-29"><a href="feedforward-neural-network.html#cb321-29" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(v)</span>
<span id="cb321-30"><a href="feedforward-neural-network.html#cb321-30" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb321-31"><a href="feedforward-neural-network.html#cb321-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb321-32"><a href="feedforward-neural-network.html#cb321-32" aria-hidden="true" tabindex="-1"></a>v1 <span class="ot">=</span> <span class="fu">exp_weight_avg</span>(<span class="at">beta =</span> beta1, theta)</span>
<span id="cb321-33"><a href="feedforward-neural-network.html#cb321-33" aria-hidden="true" tabindex="-1"></a>v2 <span class="ot">=</span> <span class="fu">exp_weight_avg</span>(<span class="at">beta =</span> beta2, theta)</span>
<span id="cb321-34"><a href="feedforward-neural-network.html#cb321-34" aria-hidden="true" tabindex="-1"></a>v3 <span class="ot">=</span> <span class="fu">exp_weight_avg</span>(<span class="at">beta =</span> beta3, theta)</span>
<span id="cb321-35"><a href="feedforward-neural-network.html#cb321-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb321-36"><a href="feedforward-neural-network.html#cb321-36" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(day, v1, <span class="at">col =</span> <span class="dv">1</span>)</span>
<span id="cb321-37"><a href="feedforward-neural-network.html#cb321-37" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(day, v2, <span class="at">col =</span> <span class="dv">2</span>)</span>
<span id="cb321-38"><a href="feedforward-neural-network.html#cb321-38" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(day, v3, <span class="at">col =</span> <span class="dv">3</span>)</span>
<span id="cb321-39"><a href="feedforward-neural-network.html#cb321-39" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">&quot;bottomright&quot;</span>,</span>
<span id="cb321-40"><a href="feedforward-neural-network.html#cb321-40" aria-hidden="true" tabindex="-1"></a>       <span class="fu">paste0</span>(<span class="fu">c</span>(<span class="st">&quot;beta1=&quot;</span>,<span class="st">&quot;beta2=&quot;</span>,<span class="st">&quot;beta3=&quot;</span>),</span>
<span id="cb321-41"><a href="feedforward-neural-network.html#cb321-41" aria-hidden="true" tabindex="-1"></a>              <span class="fu">c</span>(beta1, beta2, beta3)), <span class="at">col =</span> <span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>),</span>
<span id="cb321-42"><a href="feedforward-neural-network.html#cb321-42" aria-hidden="true" tabindex="-1"></a>       <span class="at">lty =</span> <span class="dv">1</span>)</span>
<span id="cb321-43"><a href="feedforward-neural-network.html#cb321-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb321-44"><a href="feedforward-neural-network.html#cb321-44" aria-hidden="true" tabindex="-1"></a>exp_weight_avg_correct <span class="ot">=</span> <span class="cf">function</span>(beta, theta) {</span>
<span id="cb321-45"><a href="feedforward-neural-network.html#cb321-45" aria-hidden="true" tabindex="-1"></a>  v <span class="ot">=</span> <span class="fu">rep</span>(<span class="dv">0</span>, <span class="fu">length</span>(theta))</span>
<span id="cb321-46"><a href="feedforward-neural-network.html#cb321-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb321-47"><a href="feedforward-neural-network.html#cb321-47" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(theta)) {</span>
<span id="cb321-48"><a href="feedforward-neural-network.html#cb321-48" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (i <span class="sc">==</span> <span class="dv">1</span>) {</span>
<span id="cb321-49"><a href="feedforward-neural-network.html#cb321-49" aria-hidden="true" tabindex="-1"></a>      v[i] <span class="ot">=</span> (<span class="dv">1</span> <span class="sc">-</span> beta) <span class="sc">*</span> theta[i]</span>
<span id="cb321-50"><a href="feedforward-neural-network.html#cb321-50" aria-hidden="true" tabindex="-1"></a>    } <span class="cf">else</span> {</span>
<span id="cb321-51"><a href="feedforward-neural-network.html#cb321-51" aria-hidden="true" tabindex="-1"></a>      v[i] <span class="ot">=</span> beta <span class="sc">*</span> v[i <span class="sc">-</span> <span class="dv">1</span>] <span class="sc">+</span> (<span class="dv">1</span> <span class="sc">-</span> beta) <span class="sc">*</span> theta[i]</span>
<span id="cb321-52"><a href="feedforward-neural-network.html#cb321-52" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb321-53"><a href="feedforward-neural-network.html#cb321-53" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb321-54"><a href="feedforward-neural-network.html#cb321-54" aria-hidden="true" tabindex="-1"></a>  v <span class="ot">=</span> v<span class="sc">/</span>(<span class="dv">1</span> <span class="sc">-</span> beta<span class="sc">^</span><span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(v)))</span>
<span id="cb321-55"><a href="feedforward-neural-network.html#cb321-55" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(v)</span>
<span id="cb321-56"><a href="feedforward-neural-network.html#cb321-56" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb321-57"><a href="feedforward-neural-network.html#cb321-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb321-58"><a href="feedforward-neural-network.html#cb321-58" aria-hidden="true" tabindex="-1"></a>v1_correct <span class="ot">=</span> <span class="fu">exp_weight_avg_correct</span>(<span class="at">beta =</span> beta1, theta)</span>
<span id="cb321-59"><a href="feedforward-neural-network.html#cb321-59" aria-hidden="true" tabindex="-1"></a>v2_correct <span class="ot">=</span> <span class="fu">exp_weight_avg_correct</span>(<span class="at">beta =</span> beta2, theta)</span>
<span id="cb321-60"><a href="feedforward-neural-network.html#cb321-60" aria-hidden="true" tabindex="-1"></a>v3_correct <span class="ot">=</span> <span class="fu">exp_weight_avg_correct</span>(<span class="at">beta =</span> beta3, theta)</span>
<span id="cb321-61"><a href="feedforward-neural-network.html#cb321-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb321-62"><a href="feedforward-neural-network.html#cb321-62" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(day, theta, <span class="at">cex =</span> <span class="fl">0.5</span>, <span class="at">pch =</span> <span class="dv">3</span>, <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">100</span>),</span>
<span id="cb321-63"><a href="feedforward-neural-network.html#cb321-63" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">&quot;With Correction&quot;</span>,</span>
<span id="cb321-64"><a href="feedforward-neural-network.html#cb321-64" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;Days&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;&quot;</span>)</span>
<span id="cb321-65"><a href="feedforward-neural-network.html#cb321-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb321-66"><a href="feedforward-neural-network.html#cb321-66" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(day, v1_correct, <span class="at">col =</span> <span class="dv">4</span>)</span>
<span id="cb321-67"><a href="feedforward-neural-network.html#cb321-67" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(day, v2_correct, <span class="at">col =</span> <span class="dv">5</span>)</span>
<span id="cb321-68"><a href="feedforward-neural-network.html#cb321-68" aria-hidden="true" tabindex="-1"></a><span class="fu">lines</span>(day, v3_correct, <span class="at">col =</span> <span class="dv">6</span>)</span>
<span id="cb321-69"><a href="feedforward-neural-network.html#cb321-69" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">&quot;bottomright&quot;</span>,</span>
<span id="cb321-70"><a href="feedforward-neural-network.html#cb321-70" aria-hidden="true" tabindex="-1"></a>       <span class="fu">paste0</span>(<span class="fu">c</span>(<span class="st">&quot;beta1=&quot;</span>,<span class="st">&quot;beta2=&quot;</span>,<span class="st">&quot;beta3=&quot;</span>),</span>
<span id="cb321-71"><a href="feedforward-neural-network.html#cb321-71" aria-hidden="true" tabindex="-1"></a>              <span class="fu">c</span>(beta1, beta2, beta3)),</span>
<span id="cb321-72"><a href="feedforward-neural-network.html#cb321-72" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="dv">4</span><span class="sc">:</span><span class="dv">6</span>), <span class="at">lty =</span> <span class="dv">1</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:weightedavgplot"></span>
<img src="IDS_files/figure-html/weightedavgplot-1.svg" alt="Exponentially weighted averages with and without corrrection" width="100%" />
<p class="caption">
FIGURE 12.8: Exponentially weighted averages with and without corrrection
</p>
</div>
<p>How do we apply the exponentially weighted average to optimization? Instead of using the gradients (<span class="math inline">\(dw\)</span> and <span class="math inline">\(db\)</span>) to update the parameter, we use the gradients’ exponentially weighted average. There are various optimizers built on top of this idea. We will look at three optimizers, Momentum, Root Mean Square Propagation (RMSprop), and Adaptive Moment Estimation (Adam).</p>
<p><strong>Momentum</strong></p>
<p>The momentum algorithm uses the exponentially weighted average of gradients to update the parameters. On iteration t, compute dw, db using samples in one mini-batch and update the parameters as follows:</p>
<p><span class="math display">\[V_{dw} = \beta V_{dw}+(1-\beta)dw\]</span></p>
<p><span class="math display">\[V_{db} = \beta V_{db}+(1-\beta)db\]</span></p>
<p><span class="math display">\[w=w-\alpha V_{dw};\ \ b=b-\alpha V_{db}\]</span></p>
<p>The learning rate <span class="math inline">\(\alpha\)</span> and weighted average parameter <span class="math inline">\(\beta\)</span> are hyperparameters. The most common and robust choice is <span class="math inline">\(\beta = 0.9\)</span>. This algorithm does not use the bias correction because the average will warm up after a dozen iterations and no longer be biased. The momentum algorithm, in general, works better than the original gradient descent without any average.</p>
<p><strong>Root Mean Square Propagation (RMSprop)</strong></p>
<p>The Root Mean Square Propagation (RMSprop) is another algorithm that applies the idea of exponentially weighted average. On iteration t, compute <span class="math inline">\(dw\)</span> and <span class="math inline">\(db\)</span> using the current mini-batch. Instead of <span class="math inline">\(V\)</span>, it calculates the weighted average of the squared gradient descents. When <span class="math inline">\(dw\)</span> and <span class="math inline">\(db\)</span> are vectors, the squaring is an element-wise operation.</p>
<p><span class="math display">\[S_{dw}=\beta S_{dw} + (1-\beta)dw^2\]</span></p>
<p><span class="math display">\[S_{db}=\beta S_{db} + (1-\beta)db^2\]</span></p>
<p>Then, update the parameters as follows:</p>
<p><span class="math display">\[w = w - \alpha \frac{dw}{\sqrt{S_{dw}}};\ b=b-\alpha \frac{db}{\sqrt{S_{db}}}\]</span></p>
<p>The RMSprop algorithm divides the learning rate for a parameter by a weighted average of recent gradients’ magnitudes for that parameter. The goal is still to adjust the learning speed. Recall the example that illustrates the intuition behind it. When parameter b is close to its target value, we want to decrease the oscillations along the vertical direction.</p>
<p><strong>Adaptive Moment Estimation (Adam)</strong></p>
<p>The Adaptive Moment Estimation (Adam) algorithm is, in some way, a combination of momentum and RMSprop. On iteration t, compute dw, db using the current mini-batch. Then calculate both V and S using the gradient descents.</p>
<p><span class="math display">\[\begin{cases} \begin{array}{c} V_{dw}=\beta_{1}V_{dw}+(1-\beta_{1})dw\\ V_{db}=\beta_{1}V_{db}+(1-\beta_{1})db \end{array} &amp; momantum\ update\ \beta_{1}\end{cases}\]</span></p>
<p><span class="math display">\[\begin{cases} \begin{array}{c} S_{dw}=\beta_{2}S_{dw}+(1-\beta_{2})dw^{2}\\ S_{db}=\beta_{2}S_{db}+(1-\beta_{2})db^{2} \end{array} &amp; RMSprop\ update\ \beta_{2}\end{cases}\]</span></p>
<p>The Adam algorithm implements bias correction.</p>
<p><span class="math display">\[\begin{cases} \begin{array}{c} V_{dw}^{corrected}=\frac{V_{dw}}{1-\beta_{1}^{t}}\\ V_{db}^{corrected}=\frac{V_{db}}{1-\beta_{1}^{t}} \end{array}\end{cases};\ \ \begin{cases} \begin{array}{c} S_{dw}^{corrected}=\frac{S_{dw}}{1-\beta_{2}^{t}}\\ S_{db}^{corrected}=\frac{S_{db}}{1-\beta_{2}^{t}} \end{array}\end{cases}\]</span></p>
<p>And it updates the parameter using both corrected V and S. <span class="math inline">\(\epsilon\)</span> here is a tiny positive number to make sure the denominator is larger than zero. The choice of <span class="math inline">\(\epsilon\)</span> doesn’t matter much, and the inventors of the Adam algorithm recommended <span class="math inline">\(\epsilon = 10^{-8}\)</span>. For hyperparameter <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span>, the common settings are <span class="math inline">\(\beta_1 = 0.9\)</span> and <span class="math inline">\(\beta_2 = 0.999\)</span>.</p>
<p><span class="math display">\[w=w-\alpha \frac{V_{dw}^{corrected}}{\sqrt{S_{dw}^{corrected}} +\epsilon};\ b=b-\alpha\frac{V_{db}^{corrected}}{\sqrt{S_{db}^{corrected}}+\epsilon}\]</span></p>
</div>
</div>
<div id="deal-with-overfitting" class="section level3 hasAnchor" number="12.1.6">
<h3><span class="header-section-number">12.1.6</span> Deal with Overfitting<a href="feedforward-neural-network.html#deal-with-overfitting" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The biggest problem for deep learning is overfitting. It happens when the model learns too much from the data. We discussed this in more detail in section <a href="vbtradeoff.html#vbtradeoff">7.1</a>. A common way to diagnose the problem is to use cross-validation (section <a href="datasplittingresampling.html#datasplittingresampling">7.2</a>). You can recognize the problem when the model fits great on the training data but gives poor predictions on the testing data. One way to prevent the model from over learning the data is to limit model complexity. There are several approaches to that.</p>
<div id="regularization" class="section level4 hasAnchor" number="12.1.6.1">
<h4><span class="header-section-number">12.1.6.1</span> Regularization<a href="feedforward-neural-network.html#regularization" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>For logistic regression, we can add a penalty term:</p>
<p><span class="math display">\[\underset{w,b}{min}J(w,b)= \frac{1}{m} \Sigma_{i=1}^{m}L(\hat{y}^{(i)}, y^{(i)}) + penalty\]</span></p>
<p>Common penalties are L1 or L2 as follows:</p>
<p><span class="math display">\[L_2\ penalty=\frac{\lambda}{2m}\parallel w \parallel_2^2 = \frac{\lambda}{2m}\Sigma_{i=1}^{n_x}w_i^2\]</span></p>
<p><span class="math display">\[L_1\ penalty = \frac{\lambda}{m}\Sigma_{i=1}^{n_x}|w|\]</span></p>
<p>For neural network,</p>
<p><span class="math display">\[J(w^{[1]},b^{[1]},\dots,w^{[L]},b^{[L]})=\frac{1}{m}\Sigma_{i=1}^{m}L(\hat{y}^{(i)},y^{(i)}) + \frac{\lambda}{2}\Sigma_{l=1}^{L} \parallel w^{[l]} \parallel^2_F\]</span></p>
<p>where</p>
<p><span class="math display">\[\parallel w^{[l]} \parallel^2_F = \Sigma_{i=1}^{n^{[l]}}\Sigma_{j=1}^{n^{[l-1]}} (w^{[l]}_{ij})^2\]</span></p>
<p>Many people call it “Frobenius Norm” instead of L2-norm.</p>
</div>
<div id="dropout" class="section level4 hasAnchor" number="12.1.6.2">
<h4><span class="header-section-number">12.1.6.2</span> Dropout<a href="feedforward-neural-network.html#dropout" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Another powerful regularization technique is “dropout.” In chapter <a href="treemodel.html#treemodel">11</a>, we mentioned that the random forest model de-correlates the trees by randomly choosing a subset of features. Dropout uses a similar idea in the parameter estimation process.</p>
<p>It temporally freezes a randomly selected subset of nodes at a specific layer in the neural network during the optimization process to reduce overfitting. When applying dropout to a particular layer and mini-batch, we pre-set a percentage, for example, 30%, and randomly remove 30% of the layer’s nodes. The output from the 30% removed nodes will be zero. During backpropagation, we update the remaining parameters for a much-diminished network. We need to do the random drop out again for the next mini-batch.</p>
<p>To normalize the output with all nodes in this layer, we need to scale up the output accordingly to the same percentage to make sure the dropped-out nodes do not impact the overall signal. Please note, the dropout process will randomly turn-off different nodes for each mini-batch. Dropout is more efficient in reducing overfitting in deep learning than L1 or L2 regularizations.</p>
</div>
</div>
<div id="ffnnexample" class="section level3 hasAnchor" number="12.1.7">
<h3><span class="header-section-number">12.1.7</span> Image Recognition Using FFNN<a href="feedforward-neural-network.html#ffnnexample" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In this section, we will walk through a toy example of image classification problem using <strong><code>keras</code></strong> package. We use R in the section to illustrate the process and also provide the python notebook on the book website. Please check the <a href="https://keras.rstudio.com/"><code>keras</code> R package website</a> for the most recent development.</p>
<p>What is an image as data? You can consider a digital image as a set of points on 2-d or 3-d space. For a grey scale image, each point has a value between 0 to 255 which is considered as a pixel. Figure <a href="feedforward-neural-network.html#fig:grayscaleimage">12.9</a> shows an example of grayscale image. It is a set of pixels on 2-d space and each pixel has a value between 0 to 255. You can process the image as a 2-d array input if you use a Convolutional Neural Network (CNN). Or, you can vectorize the 2D array into 1D vector as the input for FFNN as shown in the figure.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:grayscaleimage"></span>
<img src="images/grayscaleimage.png" alt="Grayscale image is a set of pixels on 2-d space. Each pixel has a value range from 0 to 255." width="80%" />
<p class="caption">
FIGURE 12.9: Grayscale image is a set of pixels on 2-d space. Each pixel has a value range from 0 to 255.
</p>
</div>
<p>A color image is a set of pixels on 3-d space and each pixel has a value between 0 to 255 for a specfic color format. There are three 2-d panels which represent the color red, blue and green accordingly. Similarly, You can process the image as a 3-d array. Or you can vectorize the array as shown in figure <a href="feedforward-neural-network.html#fig:colorimage">12.10</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:colorimage"></span>
<img src="images/colorimage.png" alt="Color image is a set of pixels on 3-d space. Each pixel has a value range from 0 to 255." width="80%" />
<p class="caption">
FIGURE 12.10: Color image is a set of pixels on 3-d space. Each pixel has a value range from 0 to 255.
</p>
</div>
<p>Let’s look at how to use the <code>keras</code> R package for a toy example in deep learning with the handwritten digits image dataset (i.e. MNIST). <code>keras</code> has many dependent packages, so it takes a few minutes to install.</p>
<div class="sourceCode" id="cb322"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb322-1"><a href="feedforward-neural-network.html#cb322-1" aria-hidden="true" tabindex="-1"></a><span class="fu">install.packages</span>(<span class="st">&quot;keras&quot;</span>)</span></code></pre></div>
<p>As <code>keras</code> is just an interface to popular deep learning frameworks, we have to install the deep learning backend. The default and recommended backend is TensorFlow. By calling <code>install_keras()</code>, it will install all the needed dependencies for TensorFlow.</p>
<div class="sourceCode" id="cb323"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb323-1"><a href="feedforward-neural-network.html#cb323-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(keras)</span>
<span id="cb323-2"><a href="feedforward-neural-network.html#cb323-2" aria-hidden="true" tabindex="-1"></a><span class="fu">install_keras</span>()</span></code></pre></div>
<p>You can run the code in this section in the Databrick community edition with R as the interface. Refer to section <a href="CloudEnvironment.html#CloudEnvironment">4.3</a> for how to set up an account, create a notebook (R or Python) and start a cluster. For an audience with a statistical background, using a well-managed cloud environment has the following benefit:</p>
<ul>
<li>Minimum language barrier in coding for most statisticians</li>
<li>Zero setups to save time using the cloud environment</li>
<li>Get familiar with the current trend of cloud computing in the industrial context</li>
</ul>
<p>You can also run the code on your local machine with R and the required Python packages (<code>keras</code> uses the Python TensorFlow backend engine). Different versions of Python may cause some errors when running <code>install_keras()</code>. Here are the things you could do when you encounter the Python backend issue in your local machine:</p>
<ul>
<li>Run <code>reticulate::py_config()</code> to check the current Python configuration to see if anything needs to be changed.</li>
<li>By default, <code>install_keras()</code> uses virtual environment <code>~/.virtualenvs/r-reticulate</code>. If you don’t know how to set the right environment, try to set the installation method as conda (<code>install_keras(method = "conda")</code>)</li>
<li>Refer to this document for more details on how to <a href="https://tensorflow.rstudio.com/reference/keras/install_keras/">install <code>keras</code> and the TensorFlow backend</a>.</li>
</ul>
<p>Now we are all set to explore deep learning! As simple as three lines of R code, but there are quite a lot going on behind the scene. If you are using a cloud environment, you do not need to worry about these behind scene setup and maintenance.</p>
<p>We will use the widely used MNIST handwritten digit image dataset. More information about the dataset and benchmark results from various machine learning methods can be found at <a href="http://yann.lecun.com/exdb/mnist/" class="uri">http://yann.lecun.com/exdb/mnist/</a> and <a href="https://en.wikipedia.org/wiki/MNIST_database" class="uri">https://en.wikipedia.org/wiki/MNIST_database</a>.</p>
<p>This dataset is already included in the keras/TensorFlow installation and we can simply load the dataset as described in the following cell. It takes less than a minute to load the dataset.</p>
<div class="sourceCode" id="cb324"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb324-1"><a href="feedforward-neural-network.html#cb324-1" aria-hidden="true" tabindex="-1"></a>mnist <span class="ot">&lt;-</span> <span class="fu">dataset_mnist</span>()</span></code></pre></div>
<p>The data structure of the MNIST dataset is straight forward and well prepared for R, which has two pieces:</p>
<ol style="list-style-type: decimal">
<li><p>training set: x (i.e. features): 60000x28x28 tensor which corresponds to 60000 28x28 pixel greyscale images (i.e. all the values are integers between 0 and 255 in each 28x28 matrix), and y (i.e. responses): a length 60000 vector which contains the corresponding digits with integer values between 0 and 9.</p></li>
<li><p>testing set: same as the training set, but with only 10000 images and responses. The detailed structure for the dataset can be seen with <code>str(mnist)</code> below.</p></li>
</ol>
<div class="sourceCode" id="cb325"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb325-1"><a href="feedforward-neural-network.html#cb325-1" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(mnist)</span></code></pre></div>
<div class="sourceCode" id="cb326"><pre class="sourceCode html"><code class="sourceCode html"><span id="cb326-1"><a href="feedforward-neural-network.html#cb326-1" aria-hidden="true" tabindex="-1"></a>List of 2</span>
<span id="cb326-2"><a href="feedforward-neural-network.html#cb326-2" aria-hidden="true" tabindex="-1"></a> $ train:List of 2</span>
<span id="cb326-3"><a href="feedforward-neural-network.html#cb326-3" aria-hidden="true" tabindex="-1"></a>  ..$ x: int [1:60000, 1:28, 1:28] 0 0 0 0 0 0 0 0 0 0 ...</span>
<span id="cb326-4"><a href="feedforward-neural-network.html#cb326-4" aria-hidden="true" tabindex="-1"></a>  ..$ y: int [1:60000(1d)] 5 0 4 1 9 2 1 3 1 4 ...</span>
<span id="cb326-5"><a href="feedforward-neural-network.html#cb326-5" aria-hidden="true" tabindex="-1"></a> $ test :List of 2</span>
<span id="cb326-6"><a href="feedforward-neural-network.html#cb326-6" aria-hidden="true" tabindex="-1"></a>  ..$ x: int [1:10000, 1:28, 1:28] 0 0 0 0 0 0 0 0 0 0 ...</span>
<span id="cb326-7"><a href="feedforward-neural-network.html#cb326-7" aria-hidden="true" tabindex="-1"></a>  ..$ y: int [1:10000(1d)] 7 2 1 0 4 1 4 9 5 9 ...</span></code></pre></div>
<p>Now we prepare the features (x) and the response variable (y) for both the training and testing dataset, and we can check the structure of the <code>x_train</code> and <code>y_train</code> using <code>str()</code> function.</p>
<div class="sourceCode" id="cb327"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb327-1"><a href="feedforward-neural-network.html#cb327-1" aria-hidden="true" tabindex="-1"></a>x_train <span class="ot">&lt;-</span> mnist<span class="sc">$</span>train<span class="sc">$</span>x</span>
<span id="cb327-2"><a href="feedforward-neural-network.html#cb327-2" aria-hidden="true" tabindex="-1"></a>y_train <span class="ot">&lt;-</span> mnist<span class="sc">$</span>train<span class="sc">$</span>y</span>
<span id="cb327-3"><a href="feedforward-neural-network.html#cb327-3" aria-hidden="true" tabindex="-1"></a>x_test <span class="ot">&lt;-</span> mnist<span class="sc">$</span>test<span class="sc">$</span>x</span>
<span id="cb327-4"><a href="feedforward-neural-network.html#cb327-4" aria-hidden="true" tabindex="-1"></a>y_test <span class="ot">&lt;-</span> mnist<span class="sc">$</span>test<span class="sc">$</span>y</span>
<span id="cb327-5"><a href="feedforward-neural-network.html#cb327-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb327-6"><a href="feedforward-neural-network.html#cb327-6" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(x_train)</span>
<span id="cb327-7"><a href="feedforward-neural-network.html#cb327-7" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(y_train)</span></code></pre></div>
<div class="sourceCode" id="cb328"><pre class="sourceCode html"><code class="sourceCode html"><span id="cb328-1"><a href="feedforward-neural-network.html#cb328-1" aria-hidden="true" tabindex="-1"></a>int [1:60000, 1:28, 1:28] 0 0 0 0 0 0 0 0 0 0 ...</span>
<span id="cb328-2"><a href="feedforward-neural-network.html#cb328-2" aria-hidden="true" tabindex="-1"></a>int [1:60000(1d)] 5 0 4 1 9 2 1 3 1 4 ...</span></code></pre></div>
<p>Now let’s plot a chosen 28x28 matrix as an image using R’s <code>image()</code> function. In R’s <code>image()</code> function, the way of showing an image is rotated 90 degree from the matrix representation. So there is additional steps to rearrange the matrix such that we can use <code>image()</code> function to show it in the actual orientation.</p>
<div class="sourceCode" id="cb329"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb329-1"><a href="feedforward-neural-network.html#cb329-1" aria-hidden="true" tabindex="-1"></a>index_image <span class="ot">=</span> <span class="dv">28</span>  <span class="do">## change this index to see different image.</span></span>
<span id="cb329-2"><a href="feedforward-neural-network.html#cb329-2" aria-hidden="true" tabindex="-1"></a>input_matrix <span class="ot">&lt;-</span> x_train[index_image, <span class="dv">1</span><span class="sc">:</span><span class="dv">28</span>, <span class="dv">1</span><span class="sc">:</span><span class="dv">28</span>]</span>
<span id="cb329-3"><a href="feedforward-neural-network.html#cb329-3" aria-hidden="true" tabindex="-1"></a>output_matrix <span class="ot">&lt;-</span> <span class="fu">apply</span>(input_matrix, <span class="dv">2</span>, rev)</span>
<span id="cb329-4"><a href="feedforward-neural-network.html#cb329-4" aria-hidden="true" tabindex="-1"></a>output_matrix <span class="ot">&lt;-</span> <span class="fu">t</span>(output_matrix)</span>
<span id="cb329-5"><a href="feedforward-neural-network.html#cb329-5" aria-hidden="true" tabindex="-1"></a><span class="fu">image</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">28</span>, <span class="dv">1</span><span class="sc">:</span><span class="dv">28</span>, output_matrix, <span class="at">col =</span> <span class="fu">gray.colors</span>(<span class="dv">256</span>),</span>
<span id="cb329-6"><a href="feedforward-neural-network.html#cb329-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">xlab =</span> <span class="fu">paste</span>(<span class="st">&quot;Image for digit of: &quot;</span>, y_train[index_image]),</span>
<span id="cb329-7"><a href="feedforward-neural-network.html#cb329-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">ylab =</span> <span class="st">&quot;&quot;</span>)</span></code></pre></div>
<center>
<img src="images/mnist_image3.png" style="width:70.0%" />
</center>
<p>Here is the original 28x28 matrix for the above image:</p>
<div class="sourceCode" id="cb330"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb330-1"><a href="feedforward-neural-network.html#cb330-1" aria-hidden="true" tabindex="-1"></a>dplyr<span class="sc">::</span><span class="fu">tibble</span>(input_matrix)</span></code></pre></div>
<div class="sourceCode" id="cb331"><pre class="sourceCode html"><code class="sourceCode html"><span id="cb331-1"><a href="feedforward-neural-network.html#cb331-1" aria-hidden="true" tabindex="-1"></a>## # A tibble: 28 × 1</span>
<span id="cb331-2"><a href="feedforward-neural-network.html#cb331-2" aria-hidden="true" tabindex="-1"></a>##   input_matrix[,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9] [,10] [,11]</span>
<span id="cb331-3"><a href="feedforward-neural-network.html#cb331-3" aria-hidden="true" tabindex="-1"></a>##              <span class="kw">&lt;int&gt;</span> <span class="kw">&lt;int&gt;</span> <span class="kw">&lt;int&gt;</span> <span class="kw">&lt;int&gt;</span> <span class="kw">&lt;int&gt;</span> <span class="kw">&lt;int&gt;</span> <span class="kw">&lt;int&gt;</span> <span class="kw">&lt;int&gt;</span> <span class="kw">&lt;int&gt;</span> <span class="kw">&lt;int&gt;</span> <span class="kw">&lt;int&gt;</span></span>
<span id="cb331-4"><a href="feedforward-neural-network.html#cb331-4" aria-hidden="true" tabindex="-1"></a>## 1                0     0     0     0     0     0     0     0     0     0     0</span>
<span id="cb331-5"><a href="feedforward-neural-network.html#cb331-5" aria-hidden="true" tabindex="-1"></a>## 2                0     0     0     0     0     0     0     0     0     0     0</span>
<span id="cb331-6"><a href="feedforward-neural-network.html#cb331-6" aria-hidden="true" tabindex="-1"></a>## 3                0     0     0     0     0     0     0     0     0     0     0</span>
<span id="cb331-7"><a href="feedforward-neural-network.html#cb331-7" aria-hidden="true" tabindex="-1"></a>## 4                0     0     0     0     0     0     0     0     0     0     0</span>
<span id="cb331-8"><a href="feedforward-neural-network.html#cb331-8" aria-hidden="true" tabindex="-1"></a>## 5                0     0     0     0     0     0     0     0     0     0     0</span>
<span id="cb331-9"><a href="feedforward-neural-network.html#cb331-9" aria-hidden="true" tabindex="-1"></a>## 6                0     0     0     0     0     0     0     0     0     0     9</span>
<span id="cb331-10"><a href="feedforward-neural-network.html#cb331-10" aria-hidden="true" tabindex="-1"></a>## # … with 22 more rows, and 1 more variable: input_matrix[12:28] <span class="kw">&lt;int&gt;</span></span>
<span id="cb331-11"><a href="feedforward-neural-network.html#cb331-11" aria-hidden="true" tabindex="-1"></a>...</span></code></pre></div>
<p>There are multiple deep learning methods to solve the handwritten digits problem and we will start from the simple and generic one, feedforward neural network (FFNN). FFNN contains a few fully connected layers and information is flowing from a front layer to a back layer without any feedback loop from the back layer to the front layer. It is the most common deep learning models to start with.</p>
<div id="data-preprocessing" class="section level4 hasAnchor" number="12.1.7.1">
<h4><span class="header-section-number">12.1.7.1</span> Data preprocessing<a href="feedforward-neural-network.html#data-preprocessing" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>In this section, we will walk through the needed steps of data preprocessing. For the MNIST dataset that we just loaded, some preprocessing is already done. So we have a relatively “clean” data, but before we feed the data into FFNN, we still need to do some additional preparations.</p>
<p>First, for each digits, we have a scalar response and a 28x28 integer matrix with value between 0 and 255. To use the out of box DNN functions, for each response, all the features are one row of all features. For an image in MNIST dataet, the input for one response y is a 28x28 matrix, not a single row of many columns and we need to convet the 28x28 matrix into a single row by appending every row of the matrix to the first row using <code>reshape()</code> function.</p>
<p>In addition, we also need to scale all features to be between (0, 1) or (-1, 1) or close to (-1, 1) range. Scale or normalize every feature will improve numerical stability in the optimization procedure as there are a lot of parameters to be optimized.</p>
<p>We first reshape the 28x28 image for each digit (i.e each row) into 784 columns (i.e. features), and then rescale the value to be between 0 and 1 by dividing the original pixel value by 255, as described in the cell below.</p>
<div class="sourceCode" id="cb332"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb332-1"><a href="feedforward-neural-network.html#cb332-1" aria-hidden="true" tabindex="-1"></a><span class="co"># step 1: reshape</span></span>
<span id="cb332-2"><a href="feedforward-neural-network.html#cb332-2" aria-hidden="true" tabindex="-1"></a>x_train <span class="ot">&lt;-</span> <span class="fu">array_reshape</span>(x_train,</span>
<span id="cb332-3"><a href="feedforward-neural-network.html#cb332-3" aria-hidden="true" tabindex="-1"></a>                         <span class="fu">c</span>(<span class="fu">nrow</span>(x_train), <span class="dv">784</span>))</span>
<span id="cb332-4"><a href="feedforward-neural-network.html#cb332-4" aria-hidden="true" tabindex="-1"></a>x_test <span class="ot">&lt;-</span> <span class="fu">array_reshape</span>(x_test,</span>
<span id="cb332-5"><a href="feedforward-neural-network.html#cb332-5" aria-hidden="true" tabindex="-1"></a>                        <span class="fu">c</span>(<span class="fu">nrow</span>(x_test), <span class="dv">784</span>))</span>
<span id="cb332-6"><a href="feedforward-neural-network.html#cb332-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb332-7"><a href="feedforward-neural-network.html#cb332-7" aria-hidden="true" tabindex="-1"></a><span class="co"># step 2: rescale</span></span>
<span id="cb332-8"><a href="feedforward-neural-network.html#cb332-8" aria-hidden="true" tabindex="-1"></a>x_train <span class="ot">&lt;-</span> x_train <span class="sc">/</span> <span class="dv">255</span></span>
<span id="cb332-9"><a href="feedforward-neural-network.html#cb332-9" aria-hidden="true" tabindex="-1"></a>x_test <span class="ot">&lt;-</span> x_test <span class="sc">/</span> <span class="dv">255</span></span></code></pre></div>
<p>And here is the structure of the reshaped and rescaled features for training and testing dataset. Now for each digit, there are 784 columns of features.</p>
<div class="sourceCode" id="cb333"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb333-1"><a href="feedforward-neural-network.html#cb333-1" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(x_train)</span>
<span id="cb333-2"><a href="feedforward-neural-network.html#cb333-2" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(x_test)</span></code></pre></div>
<div class="sourceCode" id="cb334"><pre class="sourceCode html"><code class="sourceCode html"><span id="cb334-1"><a href="feedforward-neural-network.html#cb334-1" aria-hidden="true" tabindex="-1"></a>num [1:60000, 1:784] 0 0 0 0 0 0 0 0 0 0 ...</span>
<span id="cb334-2"><a href="feedforward-neural-network.html#cb334-2" aria-hidden="true" tabindex="-1"></a>num [1:10000, 1:784] 0 0 0 0 0 0 0 0 0 0 ...</span></code></pre></div>
<p>In this example, though the response variable is an integer (i.e. the corresponding digits for an image), there is no order or rank for these integers and they are just an indication of one of the 10 categories. So we also convert the response variable y to be categorical.</p>
<div class="sourceCode" id="cb335"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb335-1"><a href="feedforward-neural-network.html#cb335-1" aria-hidden="true" tabindex="-1"></a>y_train <span class="ot">&lt;-</span> <span class="fu">to_categorical</span>(y_train, <span class="dv">10</span>)</span>
<span id="cb335-2"><a href="feedforward-neural-network.html#cb335-2" aria-hidden="true" tabindex="-1"></a>y_test <span class="ot">&lt;-</span> <span class="fu">to_categorical</span>(y_test, <span class="dv">10</span>)</span>
<span id="cb335-3"><a href="feedforward-neural-network.html#cb335-3" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(y_train)</span></code></pre></div>
<div class="sourceCode" id="cb336"><pre class="sourceCode html"><code class="sourceCode html"><span id="cb336-1"><a href="feedforward-neural-network.html#cb336-1" aria-hidden="true" tabindex="-1"></a>num [1:60000, 1:10] 0 1 0 0 0 0 0 0 0 0 ...</span></code></pre></div>
</div>
<div id="fit-model" class="section level4 hasAnchor" number="12.1.7.2">
<h4><span class="header-section-number">12.1.7.2</span> Fit model<a href="feedforward-neural-network.html#fit-model" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Now we are ready to fit the model. It is straight forward to build a deep neural network using keras. For this example, the number of input features is 784 (i.e. scaled value of each pixel in the 28x28 image) and the number of class for the output is 10 (i.e. one of the ten categories). So the input size for the first layer is 784 and the output size for the last layer is 10. And we can add any number of compatible layers in between.</p>
<p>In keras, it is easy to define a DNN model: (1) use <code>keras_model_sequential()</code> to initiate a model placeholder and all model structures are attached to this model object, (2) layers are added in sequence by calling the <code>layer_dense()</code> function, (3) add arbitrary layers to the model based on the sequence of calling <code>layer_dense()</code>. For a dense layer, all the nodes from the previous layer are connected with each and every node to the next layer. In <code>layer_dense()</code> function, we can define how many nodes in that layer through the <code>units</code> parameter. The activation function can be defined through the <code>activation</code> parameter. For the first layer, we also need to define the input features’ dimension through <code>input_shape</code> parameter. For our preprocessed MNIST dataset, there are 784 columns in the input data. A common way to reduce overfitting is to use the dropout method, which randomly drops a proportion of the nodes in a layer. We can define the dropout proportion through <code>layer_dropout()</code> function immediately after the <code>layer_dense()</code> function.</p>
<div class="sourceCode" id="cb337"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb337-1"><a href="feedforward-neural-network.html#cb337-1" aria-hidden="true" tabindex="-1"></a>dnn_model <span class="ot">&lt;-</span> <span class="fu">keras_model_sequential</span>()</span>
<span id="cb337-2"><a href="feedforward-neural-network.html#cb337-2" aria-hidden="true" tabindex="-1"></a>dnn_model <span class="sc">%&gt;%</span></span>
<span id="cb337-3"><a href="feedforward-neural-network.html#cb337-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> <span class="dv">256</span>, <span class="at">activation =</span> <span class="st">&#39;relu&#39;</span>, <span class="at">input_shape =</span> <span class="fu">c</span>(<span class="dv">784</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb337-4"><a href="feedforward-neural-network.html#cb337-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dropout</span>(<span class="at">rate =</span> <span class="fl">0.4</span>) <span class="sc">%&gt;%</span></span>
<span id="cb337-5"><a href="feedforward-neural-network.html#cb337-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> <span class="dv">128</span>, <span class="at">activation =</span> <span class="st">&#39;relu&#39;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb337-6"><a href="feedforward-neural-network.html#cb337-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dropout</span>(<span class="at">rate =</span> <span class="fl">0.3</span>) <span class="sc">%&gt;%</span></span>
<span id="cb337-7"><a href="feedforward-neural-network.html#cb337-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> <span class="dv">64</span>, <span class="at">activation =</span> <span class="st">&#39;relu&#39;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb337-8"><a href="feedforward-neural-network.html#cb337-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dropout</span>(<span class="at">rate =</span> <span class="fl">0.3</span>) <span class="sc">%&gt;%</span></span>
<span id="cb337-9"><a href="feedforward-neural-network.html#cb337-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> <span class="dv">10</span>, <span class="at">activation =</span> <span class="st">&#39;softmax&#39;</span>)</span></code></pre></div>
<p>The above <code>dnn_model</code> has 4 layers with first layer 256 nodes, 2nd layer 128 nodes, 3rd layer 64 nodes, and last layer 10 nodes. The activation function for the first 3 layers is <code>relu</code> and the activation function for the last layer is <code>softmax</code> which is typical for classification problems. The model detail can be obtained through summary() function. The number of parameter of each layer can be calculated as: (number of input features +1) times (numbe of nodes in the layer). For example, the first layer has (784+1)x256=200960 parameters; the 2nd layer has (256+1)x128=32896 parameters. Please note, dropout only randomly drop certain proportion of parameters for each batch, it will not reduce the number of parameters in the model. The total number of parameters for the <code>dnn_model</code> we just defined has 242762 parameters to be estimated.</p>
<div class="sourceCode" id="cb338"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb338-1"><a href="feedforward-neural-network.html#cb338-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(dnn_model)</span></code></pre></div>
<div class="sourceCode" id="cb339"><pre class="sourceCode html"><code class="sourceCode html"><span id="cb339-1"><a href="feedforward-neural-network.html#cb339-1" aria-hidden="true" tabindex="-1"></a>________________________________________________________________________________</span>
<span id="cb339-2"><a href="feedforward-neural-network.html#cb339-2" aria-hidden="true" tabindex="-1"></a>Layer (type)                        Output Shape                    Param #     </span>
<span id="cb339-3"><a href="feedforward-neural-network.html#cb339-3" aria-hidden="true" tabindex="-1"></a>================================================================================</span>
<span id="cb339-4"><a href="feedforward-neural-network.html#cb339-4" aria-hidden="true" tabindex="-1"></a>dense_1 (Dense)                     (None, 256)                     200960      </span>
<span id="cb339-5"><a href="feedforward-neural-network.html#cb339-5" aria-hidden="true" tabindex="-1"></a>________________________________________________________________________________</span>
<span id="cb339-6"><a href="feedforward-neural-network.html#cb339-6" aria-hidden="true" tabindex="-1"></a>dropout_1 (Dropout)                 (None, 256)                     0           </span>
<span id="cb339-7"><a href="feedforward-neural-network.html#cb339-7" aria-hidden="true" tabindex="-1"></a>________________________________________________________________________________</span>
<span id="cb339-8"><a href="feedforward-neural-network.html#cb339-8" aria-hidden="true" tabindex="-1"></a>dense_2 (Dense)                     (None, 128)                     32896       </span>
<span id="cb339-9"><a href="feedforward-neural-network.html#cb339-9" aria-hidden="true" tabindex="-1"></a>________________________________________________________________________________</span>
<span id="cb339-10"><a href="feedforward-neural-network.html#cb339-10" aria-hidden="true" tabindex="-1"></a>dropout_2 (Dropout)                 (None, 128)                     0           </span>
<span id="cb339-11"><a href="feedforward-neural-network.html#cb339-11" aria-hidden="true" tabindex="-1"></a>________________________________________________________________________________</span>
<span id="cb339-12"><a href="feedforward-neural-network.html#cb339-12" aria-hidden="true" tabindex="-1"></a>dense_3 (Dense)                     (None, 64)                      8256        </span>
<span id="cb339-13"><a href="feedforward-neural-network.html#cb339-13" aria-hidden="true" tabindex="-1"></a>________________________________________________________________________________</span>
<span id="cb339-14"><a href="feedforward-neural-network.html#cb339-14" aria-hidden="true" tabindex="-1"></a>dropout_3 (Dropout)                 (None, 64)                      0           </span>
<span id="cb339-15"><a href="feedforward-neural-network.html#cb339-15" aria-hidden="true" tabindex="-1"></a>________________________________________________________________________________</span>
<span id="cb339-16"><a href="feedforward-neural-network.html#cb339-16" aria-hidden="true" tabindex="-1"></a>dense_4 (Dense)                     (None, 10)                      650         </span>
<span id="cb339-17"><a href="feedforward-neural-network.html#cb339-17" aria-hidden="true" tabindex="-1"></a>================================================================================</span>
<span id="cb339-18"><a href="feedforward-neural-network.html#cb339-18" aria-hidden="true" tabindex="-1"></a>Total params: 242,762</span>
<span id="cb339-19"><a href="feedforward-neural-network.html#cb339-19" aria-hidden="true" tabindex="-1"></a>Trainable params: 242,762</span>
<span id="cb339-20"><a href="feedforward-neural-network.html#cb339-20" aria-hidden="true" tabindex="-1"></a>Non-trainable params: 0</span>
<span id="cb339-21"><a href="feedforward-neural-network.html#cb339-21" aria-hidden="true" tabindex="-1"></a>________________________________________________________________________________</span></code></pre></div>
<p>Once a model is defined, we need to compile the model with a few other hyper-parameters including (1) loss function, (2) optimizer, and (3) performance metrics. For multi-class classification problems, people usually use the <code>categorical_crossentropy</code> loss function and <code>optimizer_rmsprop()</code> as the optimizer which performs batch gradient descent.</p>
<div class="sourceCode" id="cb340"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb340-1"><a href="feedforward-neural-network.html#cb340-1" aria-hidden="true" tabindex="-1"></a>dnn_model <span class="sc">%&gt;%</span> <span class="fu">compile</span>(</span>
<span id="cb340-2"><a href="feedforward-neural-network.html#cb340-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">loss =</span> <span class="st">&#39;categorical_crossentropy&#39;</span>,</span>
<span id="cb340-3"><a href="feedforward-neural-network.html#cb340-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">optimizer =</span> <span class="fu">optimizer_rmsprop</span>(),</span>
<span id="cb340-4"><a href="feedforward-neural-network.html#cb340-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">metrics =</span> <span class="fu">c</span>(<span class="st">&#39;accuracy&#39;</span>)</span>
<span id="cb340-5"><a href="feedforward-neural-network.html#cb340-5" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<p>Now we can feed data (x and y) into the neural network that we just built to estimate all the parameters in the model. Here we define three hyperparameters: <code>epochs</code>, <code>batch_size</code>, and <code>validation_split</code>, for this model. It just takes a couple of minutes to finish.</p>
<div class="sourceCode" id="cb341"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb341-1"><a href="feedforward-neural-network.html#cb341-1" aria-hidden="true" tabindex="-1"></a>dnn_history <span class="ot">&lt;-</span> dnn_model <span class="sc">%&gt;%</span> <span class="fu">fit</span>(</span>
<span id="cb341-2"><a href="feedforward-neural-network.html#cb341-2" aria-hidden="true" tabindex="-1"></a>  x_train, y_train,</span>
<span id="cb341-3"><a href="feedforward-neural-network.html#cb341-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">epochs =</span> <span class="dv">15</span>, <span class="at">batch_size =</span> <span class="dv">128</span>,</span>
<span id="cb341-4"><a href="feedforward-neural-network.html#cb341-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">validation_split =</span> <span class="fl">0.2</span></span>
<span id="cb341-5"><a href="feedforward-neural-network.html#cb341-5" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<p>There is some useful information stored in the output object <code>dnn_history</code> and the details can be shown by using <code>str()</code>. We can plot the training and validation accuracy and loss as function of epoch by simply calling <code>plot(dnn_history)</code>.</p>
<div class="sourceCode" id="cb342"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb342-1"><a href="feedforward-neural-network.html#cb342-1" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(dnn_history)</span></code></pre></div>
<div class="sourceCode" id="cb343"><pre class="sourceCode html"><code class="sourceCode html"><span id="cb343-1"><a href="feedforward-neural-network.html#cb343-1" aria-hidden="true" tabindex="-1"></a>List of 2</span>
<span id="cb343-2"><a href="feedforward-neural-network.html#cb343-2" aria-hidden="true" tabindex="-1"></a> $ params :List of 8</span>
<span id="cb343-3"><a href="feedforward-neural-network.html#cb343-3" aria-hidden="true" tabindex="-1"></a>  ..$ metrics           : chr [1:4] &quot;loss&quot; &quot;acc&quot; &quot;val_loss&quot; &quot;val_acc&quot;</span>
<span id="cb343-4"><a href="feedforward-neural-network.html#cb343-4" aria-hidden="true" tabindex="-1"></a>  ..$ epochs            : int 15</span>
<span id="cb343-5"><a href="feedforward-neural-network.html#cb343-5" aria-hidden="true" tabindex="-1"></a>  ..$ steps             : NULL</span>
<span id="cb343-6"><a href="feedforward-neural-network.html#cb343-6" aria-hidden="true" tabindex="-1"></a>  ..$ do_validation     : logi TRUE</span>
<span id="cb343-7"><a href="feedforward-neural-network.html#cb343-7" aria-hidden="true" tabindex="-1"></a>  ..$ samples           : int 48000</span>
<span id="cb343-8"><a href="feedforward-neural-network.html#cb343-8" aria-hidden="true" tabindex="-1"></a>  ..$ batch_size        : int 128</span>
<span id="cb343-9"><a href="feedforward-neural-network.html#cb343-9" aria-hidden="true" tabindex="-1"></a>  ..$ verbose           : int 1</span>
<span id="cb343-10"><a href="feedforward-neural-network.html#cb343-10" aria-hidden="true" tabindex="-1"></a>  ..$ validation_samples: int 12000</span>
<span id="cb343-11"><a href="feedforward-neural-network.html#cb343-11" aria-hidden="true" tabindex="-1"></a> $ metrics:List of 4</span>
<span id="cb343-12"><a href="feedforward-neural-network.html#cb343-12" aria-hidden="true" tabindex="-1"></a>  ..$ acc     : num [1:15] 0.83 0.929 0.945 0.954 0.959 ...</span>
<span id="cb343-13"><a href="feedforward-neural-network.html#cb343-13" aria-hidden="true" tabindex="-1"></a>  ..$ loss    : num [1:15] 0.559 0.254 0.195 0.165 0.148 ...</span>
<span id="cb343-14"><a href="feedforward-neural-network.html#cb343-14" aria-hidden="true" tabindex="-1"></a>  ..$ val_acc : num [1:15] 0.946 0.961 0.967 0.969 0.973 ...</span>
<span id="cb343-15"><a href="feedforward-neural-network.html#cb343-15" aria-hidden="true" tabindex="-1"></a>  ..$ val_loss: num [1:15] 0.182 0.137 0.122 0.113 0.104 ...</span>
<span id="cb343-16"><a href="feedforward-neural-network.html#cb343-16" aria-hidden="true" tabindex="-1"></a> - attr(*, &quot;class&quot;)= chr &quot;keras_training_history&quot;</span></code></pre></div>
<div class="sourceCode" id="cb344"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb344-1"><a href="feedforward-neural-network.html#cb344-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(dnn_history)</span></code></pre></div>
<center>
<img src="images/dnn_history.png" style="width:70.0%" />
</center>
</div>
<div id="prediction" class="section level4 hasAnchor" number="12.1.7.3">
<h4><span class="header-section-number">12.1.7.3</span> Prediction<a href="feedforward-neural-network.html#prediction" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div class="sourceCode" id="cb345"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb345-1"><a href="feedforward-neural-network.html#cb345-1" aria-hidden="true" tabindex="-1"></a>dnn_model <span class="sc">%&gt;%</span> <span class="fu">evaluate</span>(x_test, y_test)</span></code></pre></div>
<div class="sourceCode" id="cb346"><pre class="sourceCode html"><code class="sourceCode html"><span id="cb346-1"><a href="feedforward-neural-network.html#cb346-1" aria-hidden="true" tabindex="-1"></a>##       loss   accuracy </span>
<span id="cb346-2"><a href="feedforward-neural-network.html#cb346-2" aria-hidden="true" tabindex="-1"></a>## 0.09035096 0.98100007</span></code></pre></div>
<div class="sourceCode" id="cb347"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb347-1"><a href="feedforward-neural-network.html#cb347-1" aria-hidden="true" tabindex="-1"></a>dnn_pred <span class="ot">&lt;-</span> dnn_model <span class="sc">%&gt;%</span></span>
<span id="cb347-2"><a href="feedforward-neural-network.html#cb347-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">predict</span>(x_test) <span class="sc">%&gt;%</span></span>
<span id="cb347-3"><a href="feedforward-neural-network.html#cb347-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">k_argmax</span>()</span>
<span id="cb347-4"><a href="feedforward-neural-network.html#cb347-4" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(dnn_pred)</span></code></pre></div>
<div class="sourceCode" id="cb348"><pre class="sourceCode html"><code class="sourceCode html"><span id="cb348-1"><a href="feedforward-neural-network.html#cb348-1" aria-hidden="true" tabindex="-1"></a>tf.Tensor([7 2 1 0 4 1], shape=(6,), dtype=int64)</span></code></pre></div>
<p>Let’s check a few misclassified images. A number of misclassified images can be found using the following code. And we can plot these misclassified images to see whether a human can correctly read it out.</p>
<div class="sourceCode" id="cb349"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb349-1"><a href="feedforward-neural-network.html#cb349-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Convert tf.tensor to array</span></span>
<span id="cb349-2"><a href="feedforward-neural-network.html#cb349-2" aria-hidden="true" tabindex="-1"></a>dnn_pred <span class="ot">&lt;-</span> <span class="fu">as.array</span>(dnn_pred)</span>
<span id="cb349-3"><a href="feedforward-neural-network.html#cb349-3" aria-hidden="true" tabindex="-1"></a><span class="do">## total number of mis-classcified images</span></span>
<span id="cb349-4"><a href="feedforward-neural-network.html#cb349-4" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(dnn_pred <span class="sc">!=</span> mnist<span class="sc">$</span>test<span class="sc">$</span>y)</span></code></pre></div>
<div class="sourceCode" id="cb350"><pre class="sourceCode html"><code class="sourceCode html"><span id="cb350-1"><a href="feedforward-neural-network.html#cb350-1" aria-hidden="true" tabindex="-1"></a>[1] 190</span></code></pre></div>
<div class="sourceCode" id="cb351"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb351-1"><a href="feedforward-neural-network.html#cb351-1" aria-hidden="true" tabindex="-1"></a>missed_image <span class="ot">=</span> mnist<span class="sc">$</span>test<span class="sc">$</span>x[dnn_pred <span class="sc">!=</span> mnist<span class="sc">$</span>test<span class="sc">$</span>y,,]</span>
<span id="cb351-2"><a href="feedforward-neural-network.html#cb351-2" aria-hidden="true" tabindex="-1"></a>missed_digit <span class="ot">=</span> mnist<span class="sc">$</span>test<span class="sc">$</span>y[dnn_pred <span class="sc">!=</span> mnist<span class="sc">$</span>test<span class="sc">$</span>y]</span>
<span id="cb351-3"><a href="feedforward-neural-network.html#cb351-3" aria-hidden="true" tabindex="-1"></a>missed_pred <span class="ot">=</span> dnn_pred[dnn_pred <span class="sc">!=</span> mnist<span class="sc">$</span>test<span class="sc">$</span>y]</span>
<span id="cb351-4"><a href="feedforward-neural-network.html#cb351-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb351-5"><a href="feedforward-neural-network.html#cb351-5" aria-hidden="true" tabindex="-1"></a>index_image <span class="ot">=</span> <span class="dv">34</span></span>
<span id="cb351-6"><a href="feedforward-neural-network.html#cb351-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb351-7"><a href="feedforward-neural-network.html#cb351-7" aria-hidden="true" tabindex="-1"></a><span class="do">## change this index to see different image.</span></span>
<span id="cb351-8"><a href="feedforward-neural-network.html#cb351-8" aria-hidden="true" tabindex="-1"></a>input_matrix <span class="ot">&lt;-</span> missed_image[index_image,<span class="dv">1</span><span class="sc">:</span><span class="dv">28</span>,<span class="dv">1</span><span class="sc">:</span><span class="dv">28</span>]</span>
<span id="cb351-9"><a href="feedforward-neural-network.html#cb351-9" aria-hidden="true" tabindex="-1"></a>output_matrix <span class="ot">&lt;-</span> <span class="fu">apply</span>(input_matrix, <span class="dv">2</span>, rev)</span>
<span id="cb351-10"><a href="feedforward-neural-network.html#cb351-10" aria-hidden="true" tabindex="-1"></a>output_matrix <span class="ot">&lt;-</span> <span class="fu">t</span>(output_matrix)</span>
<span id="cb351-11"><a href="feedforward-neural-network.html#cb351-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb351-12"><a href="feedforward-neural-network.html#cb351-12" aria-hidden="true" tabindex="-1"></a><span class="fu">image</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">28</span>, <span class="dv">1</span><span class="sc">:</span><span class="dv">28</span>, output_matrix, <span class="at">col =</span> <span class="fu">gray.colors</span>(<span class="dv">256</span>),</span>
<span id="cb351-13"><a href="feedforward-neural-network.html#cb351-13" aria-hidden="true" tabindex="-1"></a>    <span class="at">xlab =</span> <span class="fu">paste</span>(<span class="st">&quot;Image for digit &quot;</span>, missed_digit[index_image],</span>
<span id="cb351-14"><a href="feedforward-neural-network.html#cb351-14" aria-hidden="true" tabindex="-1"></a>        <span class="st">&quot;, wrongly predicted as &quot;</span>, missed_pred[index_image]),</span>
<span id="cb351-15"><a href="feedforward-neural-network.html#cb351-15" aria-hidden="true" tabindex="-1"></a>    <span class="at">ylab =</span> <span class="st">&quot;&quot;</span>)</span></code></pre></div>
<center>
<img src="images/misclassified_img.png" style="width:70.0%" />
</center>
<p>Now we finish this simple tutorial of using deep neural networks for handwritten digit recognition using the MNIST dataset. We illustrate how to reshape the original data into the right format and scaling; how to define a deep neural network with arbitrary number of layers; how to choose activation function, optimizer, and loss function; how to use dropout to limit overfitting; how to setup hyperparameters; and how to fit the model and using a fitted model to predict. Finally, we illustrate how to plot the accuracy/loss as functions of the epoch. It shows the end-to-end cycle of how to fit a deep neural network model.</p>
<p>On the other hand, the image can be better dealt with Convolutional Neural Network (CNN) and we are going to walk through the exact same problem using CNN in the next section.</p>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="deeplearning.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="convolutional-neural-network.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": true,
"twitter": true,
"linkedin": true,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/happyrabbit/IntroDataScience/12-DeepLearning.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["IDS.pdf", "IDS.epub", "IDS.mobi"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section",
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
