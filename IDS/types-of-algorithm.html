<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Introduction to Data Science</title>
  <meta name="description" content="Introduction to Data Science">
  <meta name="generator" content="bookdown 0.4 and GitBook 2.6.7">

  <meta property="og:title" content="Introduction to Data Science" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="http://scientistcafe.com/IDS/" />
  
  <meta property="og:description" content="Introduction to Data Science" />
  <meta name="github-repo" content="happyrabbit/IntroDataScience" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Introduction to Data Science" />
  
  <meta name="twitter:description" content="Introduction to Data Science" />
  

<meta name="author" content="Hui Lin and Ming Li">


<meta name="date" content="2017-12-07">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="types-of-learning.html">
<link rel="next" href="references.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="http://scientistcafe.com">Homepage</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Copyright Statement</a></li>
<li class="chapter" data-level="" data-path="about-the-authors.html"><a href="about-the-authors.html"><i class="fa fa-check"></i>About the Authors</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="goal-of-the-book.html"><a href="goal-of-the-book.html"><i class="fa fa-check"></i>Goal of the Book</a></li>
<li class="chapter" data-level="" data-path="who-this-book-is-for.html"><a href="who-this-book-is-for.html"><i class="fa fa-check"></i>Who this book is for</a></li>
<li class="chapter" data-level="" data-path="what-this-book-covers.html"><a href="what-this-book-covers.html"><i class="fa fa-check"></i>What this book covers</a></li>
<li class="chapter" data-level="" data-path="conventions.html"><a href="conventions.html"><i class="fa fa-check"></i>Conventions</a></li>
<li class="chapter" data-level="" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="what-is-data-science.html"><a href="what-is-data-science.html"><i class="fa fa-check"></i><b>1.1</b> What is data science?</a></li>
<li class="chapter" data-level="1.2" data-path="is-it-science-totally.html"><a href="is-it-science-totally.html"><i class="fa fa-check"></i><b>1.2</b> Is it science? Totally?</a></li>
<li class="chapter" data-level="1.3" data-path="what-kind-of-questions-can-data-science-solve.html"><a href="what-kind-of-questions-can-data-science-solve.html"><i class="fa fa-check"></i><b>1.3</b> What kind of questions can data science solve?</a><ul>
<li class="chapter" data-level="1.3.1" data-path="what-kind-of-questions-can-data-science-solve.html"><a href="what-kind-of-questions-can-data-science-solve.html#prerequisites"><i class="fa fa-check"></i><b>1.3.1</b> Prerequisites</a></li>
<li class="chapter" data-level="1.3.2" data-path="what-kind-of-questions-can-data-science-solve.html"><a href="what-kind-of-questions-can-data-science-solve.html#problem-type"><i class="fa fa-check"></i><b>1.3.2</b> Problem type</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="what-are-the-required-skills-for-data-scientist.html"><a href="what-are-the-required-skills-for-data-scientist.html"><i class="fa fa-check"></i><b>1.4</b> What are the required skills for data scientist?</a></li>
<li class="chapter" data-level="1.5" data-path="types-of-learning.html"><a href="types-of-learning.html"><i class="fa fa-check"></i><b>1.5</b> Types of Learning</a></li>
<li class="chapter" data-level="1.6" data-path="types-of-algorithm.html"><a href="types-of-algorithm.html"><i class="fa fa-check"></i><b>1.6</b> Types of Algorithm</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>2</b> References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Data Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="types-of-algorithm" class="section level2">
<h2><span class="header-section-number">1.6</span> Types of Algorithm</h2>
<p>The summary of various algorithms for data science in this section is based on Jason Brownlee’s blog “(A Tour of Machine Learning Algorithms)[<a href="http://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/" class="uri">http://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/</a>].” We added and subtracted some algorithms in each category and gave additional comments. The categorization here is based on the structure (such as tree model, Regularization Methods) or type of question to answer (such as regression). It is far less than perfect but will help to show a bigger map of different algorithms. Some can be legitimately classified into multiple categories, such as support vector machine (SVM) can be a classifier, and can also be used for regression. So you may see other ways of grouping. Also, the following summary does not list all the existing algorithms (there are just too many).</p>
<ol style="list-style-type: decimal">
<li>Regression</li>
</ol>
<p>Regression can refer to the algorithm or a particular type of problem. It is supervised learning. Regression is one of the oldest and most widely used statistical models. It is often called the statistical machine learning method. Standard regression models are:</p>
<ul>
<li>Ordinary Least Squares Regression</li>
<li>Logistic Regression</li>
<li>Multivariate Adaptive Regression Splines (MARS)</li>
<li>Locally Estimated Scatterplot Smoothing (LOESS)</li>
</ul>
<p>The least squares regression and logistic regression are traditional statistical models. Both of them are highly interpretable. MARS is similar to neural networks and partial least squares (PLS) in the respect that they all use surrogate features instead of original predictors.</p>
<p>They differ in how to create the surrogate features. PLS and neural networks use linear combinations of the original predictors as surrogate features.<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a> MARS creates two contrasted versions of a predictor by a truncation point. And LOESS is a non-parametric model, usually only used in visualization.</p>
<ol start="2" style="list-style-type: decimal">
<li>Similarity-based Algorithms</li>
</ol>
<p>This type of model is based on a similarity measure. There are three main steps: (1) compare the new sample with the existing ones; (2) search for the closest sample; (3) and let the response of the nearest sample be used as the prediction.</p>
<ul>
<li>K-Nearest Neighbour [KNN]</li>
<li>Learning Vector Quantization [LVQ]</li>
<li>Self-Organizing Map [SOM]</li>
</ul>
<p>The biggest advantage of this type of model is that they are intuitive. K-Nearest Neighbour is generally the most popular algorithm in this set. The other two are less common. The key to similarity based algorithms is to find an appropriate distance metric for your data.</p>
<ol start="3" style="list-style-type: decimal">
<li>Feature Selection Algorithms</li>
</ol>
<p>The primary purpose of feature selection is to exclude non-information or redundant variables and also reduce dimension. Although it is possible that all the independent variables are significant for explaining the response. But more often, the response is only related to a portion of the predictors. We will expand the feature selection in detail later.</p>
<ul>
<li>Filter method</li>
<li>Wrapper method</li>
<li>Embedded method</li>
</ul>
<p>Filter method focuses on the relationship between a single feature and a target variable. It evaluates each feature (or an independent variable) before modeling and selects “important” variables.</p>
<p>Wrapper method removes the variable according to particular law and finds the feature combination that optimizes the model fitting by evaluating a set of feature combinations. In essence, it is a searching algorithm.</p>
<p>Embedding method is part of the machine learning model. Some model has built-in variable selection function such as lasso, and decision tree.</p>
<ol start="4" style="list-style-type: decimal">
<li>Regularization Method</li>
</ol>
<p>This method itself is not a complete model, but rather an add-on to other models (such as regression models). It appends a penalty function on the criteria used by the original model to estimate the variables (such as likelihood function or sum of squared error). In this way, it penalizes model complexity and contracts the model parameters. That is why people call them “shrinkage method.” This approach is advantageous in practice.</p>
<ul>
<li>Ridge Regression</li>
<li>Least Absolute Shrinkage and Selection Operator (LASSO)</li>
<li>Elastic Net</li>
</ul>
<ol start="5" style="list-style-type: decimal">
<li>Decision Tree</li>
</ol>
<p>Decision trees are no doubt one of the most popular machine learning algorithms. Thanks to all kinds of software, implementation is a no brainer which requires nearly zero understanding of the mechanism. The followings are some of the common trees:</p>
<ul>
<li>Classification and Regression Tree (CART)</li>
<li>Iterative Dichotomiser 3 (ID3)</li>
<li>C4.5</li>
<li>Random Forest</li>
<li>Gradient Boosting Machines (GBM)</li>
</ul>
<ol start="6" style="list-style-type: decimal">
<li>Bayesian Models</li>
</ol>
<p>People usually confuse Bayes theorem with Bayesian models. Bayes theorem is an implication of probability theory which gives Bayesian data analysis its name.</p>
<p><span class="math display">\[Pr(\theta|y)=\frac{Pr(y|\theta)Pr(\theta)}{Pr(y)}\]</span></p>
<p>The actual Bayesian model is not identical to Bayes theorem. Given a likelihood, parameters to estimate, and a prior for each parameter, a Bayesian model treats the estimates as a purely logical consequence of those assumptions. The resulting estimates are the posterior distribution which is the relative plausibility of different parameter values, conditional on the observations. The Bayesian model here is not strictly in the sense of Bayesian but rather model using Bayes theorem.</p>
<ul>
<li>Naïve Bayes</li>
<li>Averaged One-Dependence Estimators (AODE)</li>
<li>Bayesian Belief Network (BBN)</li>
</ul>
<ol start="7" style="list-style-type: decimal">
<li>Kernel Methods</li>
</ol>
<p>The most common kernel method is the support vector machine (SVM). This type of algorithm maps the input data to a higher order vector space where classification or regression problems are easier to solve.</p>
<ul>
<li>Support Vector Machine (SVM)</li>
<li>Radial Basis Function (RBF)</li>
<li>Linear Discriminate Analysis (LDA)</li>
</ul>
<ol start="8" style="list-style-type: decimal">
<li>Clustering Methods</li>
</ol>
<p>Like regression, when people mention clustering, sometimes they mean a class of problems, sometimes a class of algorithms. The clustering algorithm usually clusters similar samples to categories in a centroidal or hierarchical manner. The two are the most common clustering methods:</p>
<ul>
<li>K-Means</li>
<li>Hierarchical Clustering</li>
</ul>
<ol start="9" style="list-style-type: decimal">
<li>Association Rule</li>
</ol>
<p>The basic idea of an association rule is: when events occur together more often than one would expect from their individual rates of occurrence, such co- occurrence is an interesting pattern. The most used algorithms are:</p>
<ul>
<li>Apriori algorithm</li>
<li>Eclat algorithm</li>
</ul>
<ol start="10" style="list-style-type: decimal">
<li>Artificial Neural Network</li>
</ol>
<p>The term neural network has evolved to encompass a repertoire of models and learning methods. There has been lots of hype around the model family making them seem magical and mysterious. A neural network is a two-stage regression or classification model. The basic idea is that it uses linear combinations of the original predictors as surrogate features, and then the new features are put through non-linear activation functions to get hidden units in the 2nd stage. When there are multiple hidden layers, it is called deep learning, another over hyped term. Among varieties of neural network models, the most widely used “vanilla” net is the single hidden layer back-propagation network.</p>
<ul>
<li>Perceptron Neural Network</li>
<li>Back Propagation</li>
<li>Hopield Network</li>
<li>Self-Organizing Map (SOM)</li>
<li>Learning Vector Quantization (LVQ)</li>
</ul>
<ol start="11" style="list-style-type: decimal">
<li>Deep Learning</li>
</ol>
<p>The name is a little misleading. As mentioned before, it is multilayer neural network. It is hyped tremendously especially after AlphaGO defeated Li Shishi at the board game Go. We don’t have too much experience with the application of deep learning and are not in the right position to talk more about it. Here are some of the common algorithms:</p>
<!--Many of the deep learning algorithms are semi-supervised learning algorithms that deal with large data sets with a few unlabeled samples. -->
<!-- Alex's comments:
There are many different types of deep learning applications, some are supervised (computer vision image classification), some are reinforment learning (I think I read that a lot of game AIs use this, such as bots to beat games like super mario), and can also be used to build features in an unsupervised way (autoencoders).  There is a lot of hype in this topic but I think deep learning differentiates itself from earlier ANN because of many recent advacements that make it possible to have very large and very deep networks: backprop, new ways to initialize weights, dropout etc.-->
<ul>
<li>Restricted Boltzmann Machine (RBN)</li>
<li>Deep Belief Networks (DBN)</li>
<li>Convolutional Network</li>
<li>Stacked Autoencoders</li>
<li>Long short-term memory (LSTM)</li>
</ul>
<ol start="12" style="list-style-type: decimal">
<li>Dimensionality Reduction</li>
</ol>
<p>Its purpose is to construct new features that have significant physical or statistical characteristics, such as capturing as much of the variance as possible.</p>
<ul>
<li>Principle Component Analysis (PCA)</li>
<li>Partial Least Square Regression (PLS)</li>
<li>Multi-Dimensional Scaling (MDS)</li>
<li>Exploratory Factor Analysis (EFA)</li>
</ul>
<p>PCA attempts to find uncorrelated linear combinations of original variables that can explain the variance to the greatest extent possible. EFA also tries to explain as much variance as possible in a lower dimension. MDS maps the observed similarity to a low dimension, such as a two-dimensional plane. Instead of extracting underlying components or latent factors, MDS attempts to find a lower-dimensional map that best preserves all the observed similarities between items. So it needs to define a similarity measure as in clustering methods.</p>
<ol start="13" style="list-style-type: decimal">
<li>Ensemble Methods</li>
</ol>
<p>Ensemble method made its debut in the 1990s. The idea is to build a prediction model by combining the strengths of a collection of simpler base models. Bagging, originally proposed by Leo Breiman, is one of the earliest ensemble methods. After that, people developed Random Forest <span class="citation">(T <a href="#ref-Ho1998">1998</a>; Y and D <a href="#ref-amit1997">1997</a>)</span> and Boosting method <span class="citation">(L <a href="#ref-Valiant1984">1984</a>; M and L <a href="#ref-KV1989">1989</a>)</span>. This is a class of powerful and effective algorithms.</p>
<ul>
<li>Bootstrapped Aggregation (Bagging)</li>
<li>Random Forest</li>
<li>Gradient Boosting Machine (GBM)</li>
</ul>
<div class="figure">
<img src="images/AlogrithmTypes.png" />

</div>
<!--
Uncertainty:

- Partial knowledge of state of the world: such as income, social media behavior, competitor’s offer
- Noisy observations: missing information, measurement with error (food taken), self-justification bias (nobody watches the cat video……)
- Phenomena not covered by our model: linear assumption, normal assumption
- Inherent stochasticity: even at a higher level, the modeling limitations of complicated systems are such that one might as well view the world as inherently stochastic.
-->
<!--
## General Process of Data Science 

<img src="http://scientistcafe.com/book/Figure/GeneralProcessEN.png" width="850px" height="400px" />
-->

</div>
<!-- </div> -->
<h3> References</h3>
<div id="refs" class="references">
<div id="ref-Ho1998">
<p>T, Ho. 1998. “The Random Subspace Method for Constructing Decision Forests.” <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em> 13: 340–54.</p>
</div>
<div id="ref-amit1997">
<p>Y, Amit, and Geman D. 1997. “Shape Quantization and Recognition with Randomized Trees.” <em>Neural Computation</em> 9: 1545–88.</p>
</div>
<div id="ref-Valiant1984">
<p>L, Valiant. 1984. “A Theory of the Learnable.” <em>Communications of the ACM</em> 27: 1134–42.</p>
</div>
<div id="ref-KV1989">
<p>M, Kearns, and Valiant L. 1989. “Cryptographic Limitations on Learning Boolean Formulae and Finite Automata.”</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>To be clear on neural networks, the linear combinations of predictors are put through non-linear activation functions, deeper neural networks have many layers of non-linear transformation<a href="types-of-algorithm.html#fnref1">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="types-of-learning.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="references.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/happyrabbit/IntroDataScience/01-Introduction.Rmd",
"text": "Edit"
},
"download": ["IDS.pdf", "IDS.epub", "IDS.mobi"],
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
