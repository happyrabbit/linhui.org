\documentclass[12pt,]{krantz}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
    \setmonofont[Mapping=tex-ansi,Scale=0.7]{Source Code Pro}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[unicode=true]{hyperref}
\PassOptionsToPackage{usenames,dvipsnames}{color} % color is loaded by hyperref
\hypersetup{
            pdftitle={Introduction to Data Science},
            pdfauthor={Hui Lin and Ming Li},
            colorlinks=true,
            linkcolor=Maroon,
            citecolor=Blue,
            urlcolor=Blue,
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{natbib}
\bibliographystyle{apalike}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{{#1}}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{{#1}}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{{#1}}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{{#1}}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{{#1}}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{{#1}}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{{#1}}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{{#1}}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{{#1}}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{{#1}}}
\newcommand{\ImportTok}[1]{{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{{#1}}}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{{#1}}}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{{#1}}}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{{#1}}}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{{#1}}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{{#1}}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{{#1}}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{{#1}}}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{{#1}}}}
\newcommand{\BuiltInTok}[1]{{#1}}
\newcommand{\ExtensionTok}[1]{{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{{#1}}}
\newcommand{\RegionMarkerTok}[1]{{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{{#1}}}}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{{#1}}}}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{{#1}}}}
\newcommand{\NormalTok}[1]{{#1}}
\usepackage{longtable,booktabs}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\usepackage{booktabs}
\usepackage{longtable}
\usepackage[bf,singlelinecheck=off]{caption}

%\setmainfont[UprightFeatures={SmallCapsFont=AlegreyaSC-Regular}]{Alegreya}

\usepackage{framed,color}
\definecolor{shadecolor}{RGB}{248,248,248}

\renewcommand{\textfraction}{0.05}
\renewcommand{\topfraction}{0.8}
\renewcommand{\bottomfraction}{0.8}
\renewcommand{\floatpagefraction}{0.75}

\renewenvironment{quote}{\begin{VF}}{\end{VF}}
\let\oldhref\href
\renewcommand{\href}[2]{#2\footnote{\url{#1}}}

\ifxetex
  \usepackage{letltxmacro}
  \setlength{\XeTeXLinkMargin}{1pt}
  \LetLtxMacro\SavedIncludeGraphics\includegraphics
  \def\includegraphics#1#{% #1 catches optional stuff (star/opt. arg.)
    \IncludeGraphicsAux{#1}%
  }%
  \newcommand*{\IncludeGraphicsAux}[2]{%
    \XeTeXLinkBox{%
      \SavedIncludeGraphics#1{#2}%
    }%
  }%
\fi

\makeatletter
\newenvironment{kframe}{%
\medskip{}
\setlength{\fboxsep}{.8em}
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

% \renewenvironment{Shaded}{\begin{kframe}}{\end{kframe}}

\newenvironment{rmdblock}[1]
  {
  \begin{itemize}
  \renewcommand{\labelitemi}{
    \raisebox{-.7\height}[0pt][0pt]{
      {\setkeys{Gin}{width=3em,keepaspectratio}\includegraphics{images/#1}}
    }
  }
  \setlength{\fboxsep}{1em}
  \begin{kframe}
  \item
  }
  {
  \end{kframe}
  \end{itemize}
  }
\newenvironment{rmdnote}
  {\begin{rmdblock}{note}}
  {\end{rmdblock}}
\newenvironment{rmdcaution}
  {\begin{rmdblock}{caution}}
  {\end{rmdblock}}
\newenvironment{rmdimportant}
  {\begin{rmdblock}{important}}
  {\end{rmdblock}}
\newenvironment{rmdtip}
  {\begin{rmdblock}{tip}}
  {\end{rmdblock}}
\newenvironment{rmdwarning}
  {\begin{rmdblock}{warning}}
  {\end{rmdblock}}

\usepackage{makeidx}
\makeindex

\urlstyle{tt}

\usepackage{amsthm}
\makeatletter
\def\thm@space@setup{%
  \thm@preskip=8pt plus 2pt minus 4pt
  \thm@postskip=\thm@preskip
}
\makeatother

\frontmatter

\title{Introduction to Data Science}
\author{Hui Lin and Ming Li}
\date{2018-02-03}

\usepackage{amsthm}
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{lemma}{Lemma}[chapter]
\theoremstyle{definition}
\newtheorem{definition}{Definition}[chapter]
\newtheorem{corollary}{Corollary}[chapter]
\newtheorem{proposition}{Proposition}[chapter]
\theoremstyle{definition}
\newtheorem{example}{Example}[chapter]
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\begin{document}
\maketitle

%\cleardoublepage\newpage\thispagestyle{empty}\null
%\cleardoublepage\newpage\thispagestyle{empty}\null
%\cleardoublepage\newpage
\thispagestyle{empty}
\begin{center}
%\includegraphics{images/dedication.pdf}
\end{center}

\setlength{\abovedisplayskip}{-5pt}
\setlength{\abovedisplayshortskip}{-5pt}

{
\hypersetup{linkcolor=black}
\setcounter{tocdepth}{2}
\tableofcontents
}
\listoftables
\listoffigures
\chapter*{Preface}\label{preface}


During the first couple years of our career as data scientists, we were
bewildered by all kinds of data science hype. There is a lack of
definition of many basic terminologies such as ``Big Data'' and ``Data
Science.'' How big is big? If someone ran into you asked what data
science was all about, what would you tell them? What is the difference
between the sexy role ``Data Scientist'' and the traditional ``Data
Analyst''? How suddenly came all kinds of machine algorithms? All those
struck us as confusing and vague as real-world data scientists! But we
always felt that there was something real there. After applying data
science for many years, we explored it more and had a much better idea
about data science. And this book is our endeavor to make data science
to a more legitimate field.

\section*{Goal of the Book}\label{goal-of-the-book}


This is an introductory book to data science with a specific focus on
the application. Data Science is a cross-disciplinary subject involving
hands-on experience and business problem-solving exposures. The majority
of existing introduction books on data science are about the modeling
techniques and the implementation of models using R or Python. However,
they fail to introduce data science in a context of the industrial
environment. Moreover, a crucial part, the art of data science in
practice, is often missing. This book intends to fill the gap.

Some key features of this book are as follows:

\begin{itemize}
\item
  It is comprehensive. It covers not only technical skills but also soft
  skills and big data environment in the industry.
\item
  It is hands-on. We provide the data and repeatable R and Python code.
  You can repeat the analysis in the book using the data and code
  provided. We also suggest you perform the analyses with your data
  whenever possible. You can only learn data science by doing it!
\item
  It is based on context. We put methods in the context of industrial
  data science questions.
\item
  Where appropriate, we point you to more advanced materials on models
  to dive deeper
\end{itemize}

\section*{Who This Book Is For}\label{who-this-book-is-for}


Non-mathematical readers will appreciate the emphasis on problem-solving
with real data across a wide variety of applications and the
reproducibility of the companion R and python code.

Readers should know basic statistical ideas, such as correlation and
linear regression analysis. While the text is biased against complex
equations, a mathematical background is needed for advanced topics.

\section*{What This Book Covers}\label{what-this-book-covers}


Based on industry experience, this book outlines the real world scenario
and points out pitfalls data science practitioners should avoid. It also
covers big data cloud platform and the art of data science such as soft
skills. We use R as the main tool and provide code for both R and
Python.

\section*{Conventions}\label{conventions}


\section*{Acknowledgements}\label{acknowledgements}


\chapter*{About the Authors}\label{about-the-authors}


\textbf{Hui Lin} is currently Data Scientist at DowDuPont. She is a
leader in the company at applying advanced data science to enhance
Marketing and Sales Effectiveness. She has been providing statistical
leadership for a broad range of predictive analytics and market research
analysis since 2013. She is the co-founder of Central Iowa R User Group,
blogger of scientistcafe.com and 2018 Program Chair of ASA Statistics in
Marketing Section. She enjoys making analytics accessible to a broad
audience and teaches tutorials and workshops for practitioners on data
science. She holds MS and Ph.D.~in statistics from Iowa State
University, BS in mathematical statistics from Beijing Normal
University.

\textbf{Ming Li} is currently a Senior Data Scientist at Amazon and an
Adjunct Faculty of Department of Marketing and Business Analytics in
Texas A\&M University - Commerce. He is the Chair of Quality \&
Productivity Section of ASA for 2017. He was a Data Scientist at Walmart
and a Statistical Leader at General Electric Global Research Center. He
obtained his Ph.D.~in Statistics from Iowa State University at 2010.
With deep statistics background and a few years' experience in data
science, he has trained and mentored numerous junior data scientist with
different background such as statistician, programmer, software
developer, database administrator and business analyst. He is also an
Instructor of Amazon's internal Machine Learning University and was one
of the key founding member of Walmart's Analytics Rotational Program
which bridges the skill gaps between new hires and productive data
scientists.(

\mainmatter

\chapter{Introduction}\label{introduction}

Interest in data science is at an all-time high and has exploded in
popularity in the last couple of years. Data scientists today are from
various backgrounds. If someone ran into you asked what data science was
all about, what would you tell them? It is not easy to answer. Data
science is one of the areas where if you ask ten people you get ten
different answers. It is not well-defined as an academic subject but
broadly used in the industry. Media has been hyping about ``Data
Science'' ``Big Data'' and ``Artificial Intelligence'' over the fast few
years. With the data science hype picking up stream, many professionals
changed their titles to ``Data Scientist'' without any of the necessary
qualifications. Your first reaction to all of this might be some
combination of skepticism and confusion. We want to address this up
front that: we had that exact reaction. To make things clear, let's
start with the fundamental question.

\section{What is data science?}\label{what-is-data-science}

David Donoho \citep{data50} summarizes in ``50 Years of Data Science''
the main recurring ``Memes'' about data sciences:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The `Big Data' Meme
\item
  The `Skills' Meme
\item
  The `Jobs' Meme
\end{enumerate}

Everyone should have heard about big data. Data science trainees now
need the skills to cope with such big data sets. What are those skills?
You may hear about: Hadoop, a system using Map/Reduce to process large
data sets distributed across a cluster of computers. The new skills are
for dealing with organizational artifacts of large-scale cluster
computing but not for better solving the real problem. A lot of data on
its own is worthless. It isn't the size of the data that's important.
It's what you do with it. The big data skills that so many are touting
today are not skills for better solving the real problem of inference
from data.

We are transiting to universal connectivity with a deluge of data
filling telecom servers. But these facts don't immediately create a
science. The statisticians and computer scientists have been laying the
groundwork for data science for at least 50 years. Today's data science
is an enlargement and combination of statistics and computer science
rather than a brand new discipline.

Data Science doesn't come out of the blue. Its predecessor is data
analysis. Back in 1962, John Tukey wrote in ``The Future of Data
Analysis'':

\begin{quote}
For a long time I have thought I was a statistician, interested in
inferences from the particular to the general. But as I have watched
mathematical statistics evolve, I have had cause to wonder and to doubt.
\ldots{}All in all, I have come to feel that my central interest is in
data analysis, which I take to include, among other things: procedures
for analyzing data, techniques for interpreting the results of such
procedures, ways of planning the gathering of data to make its analysis
easier, more precise or more accurate, and all the machinery and results
of (mathematical) statistics which apply to analyzing data.
\end{quote}

It deeply shocked his academic readers. Aren't you supposed to present
something mathematically precise, such as definitions, theorems, and
proofs? If we use one sentence to summarize what John said, it is:

\begin{quote}
data analysis is more than mathematics.
\end{quote}

In September 2015, the University of Michigan made plans to invest \$100
million over the next five years in a new Data Science Initiative (DSI)
that will enhance opportunities for student and faculty researchers
across the university to tap into the enormous potential of big data.
How does DSI define Data science? Their website gives us an idea:

\begin{quote}
``This coupling of scientific discovery and practice involves the
collection, management, processing, analysis, visualization, and
interpretation of vast amounts of heterogeneous data associated with a
diverse array of scientific, translational, and interdisciplinary
applications.''
\end{quote}

How about data scientist? Here is a list of somewhat whimsical
definitions for a ``data scientist'':

\begin{itemize}
\tightlist
\item
  ``A data scientist is a data analyst who lives in California''
\item
  ``A data scientist is someone who is better at statistics than any
  software engineer and better at software engineering than any
  statistician.''
\item
  ``A data scientist is a statistician who lives in San Francisco.''
\item
  ``Data Science is statistics on a Mac.''
\end{itemize}

There is lots of confusion between Data Scientist, Statistician,
Business/Financial/Risk(etc.) Analyst and BI professional due to the
apparent intersections among skillsets. We see data science as a
discipline to make sense of data. The techniques and methodologies of
data science stem from the fields of computer science and statistics.
One of the most well-cited diagrams describing the area comes from Drew
Conway where he suggested data science is the intersection of hacking
skills, math and stats knowledge, and substantial expertise. This
diagram might be a bit of an oversimplification, but it's a great start.

There are almost as many definitions of data science as there are data
scientists. Instead of listing some of these definitions, it might be
more informative to let the subject matter define the field.

Let's start from a brief history of data science. If you hit up the
Google Trends website which shows search keyword information over time
and check the term ``data science,'' you will find the history of data
science goes back a little further than 2004. From the way media
describes it, you may feel machine learning algorithms were just
invented last month, and there was never ``big'' data before Google.
That is not true. There are new and exciting developments of data
science, but many of the techniques we are using are based on decades of
work by statisticians, computer scientists, mathematicians and
scientists of all types.

\begin{figure}[htbp]
\centering
\includegraphics{images/DataScienceTimeline.png}
\caption{Data Science Timeline}
\end{figure}

In the early 19th century when Legendre and Gauss came up the least
squares method for linear regression, only physicists would use it to
fit linear regression. Now, even non-technical people can fit linear
regressions using excel. In 1936 Fisher came up with linear discriminant
analysis. In the 1940s, we had another widely used model -- logistic
regression. In the 1970s, Nelder and Wedderburn formulated ``generalized
linear model (GLM)'' which:

\begin{quote}
``generalized linear regression by allowing the linear model to be
related to the response variable via a link function and by allowing the
magnitude of the variance of each measurement to be a function of its
predicted value.'' {[}from Wikipedia{]}
\end{quote}

By the end of the 1970s, there was a range of analytical models and most
of them were linear because computers were not powerful enough to fit
non-linear model until the 1980s.

In 1984 Breiman et al. introduced classification and regression tree
(CART) which is one of the oldest and most utilized classification and
regression techniques. After that Ross Quinlan came up with more tree
algorithms such as ID3, C4.5, and C5.0. In the 1990s, ensemble
techniques (methods that combine many models' predictions) began to
appear. Bagging is a general approach that uses bootstrapping in
conjunction with any regression or classification model to construct an
ensemble. Based on the ensemble idea, Breiman came up with random forest
in 2001. Later, Yoav Freund and Robert Schapire came up with the
AdaBoost.M1 algorithm. Benefiting from the increasing availability of
digitized information, and the possibility to distribute that via the
internet, the toolbox has been expanding fast. The applications include
business, health, biology, social science, politics, etc.

John Tukey identified four forces driving data analysis (there was no
``data science'' then):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The formal theories of math and statistics
\item
  Acceleration of developments in computers and display devices
\item
  The challenge, in many fields, of more and ever larger bodies of data
\item
  The emphasis on quantification in an ever wider variety of disciplines
\end{enumerate}

Tukey's 1962 list is surprisingly modern. Let's inspect those points in
today's context. There is always a time gap between a theory and its
application. We had the theories much earlier than application.
Fortunately, for the past 50 years, statisticians have been laying the
theoretical groundwork for constructing ``data science'' today. The
development of computers enables us to calculate much faster and deliver
results in a friendly and intuitive way. The striking transition to the
internet of things generates vast amounts of commercial data. Industries
have also sensed the value of exploiting that data. Data science seems
certain to be a major preoccupation of commercial life in coming
decades. All the four forces John identified exist today and have been
driving data science.

\section{What kind of questions can data science
solve?}\label{what-kind-of-questions-can-data-science-solve}

\subsection{Prerequisites}\label{prerequisites}

Data science is not a panacea, and data scientists are not magicians.
There are problems data science can't help. It is best to make a
judgment as early in the analytical cycle as possible. Tell your clients
honestly and clearly when you figure data analytics can't give the
answer they want. What kind of questions can data science solve? What
are the requirements for our question?

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Your question needs to be specific enough
\end{enumerate}

Look at two examples:

\begin{itemize}
\tightlist
\item
  Question 1: How can I increase product sales?
\item
  Question 2: Is the new promotional tool introduced at the beginning of
  this year boosting the annual sales of P1197 in Iowa and Wisconsin?
  (P1197 is an impressive corn seed product from DuPont Pioneer)
\end{itemize}

It is easy to see the difference between the two questions. Question 1
is a grammatically correct question, but it is proper for data analysis
to answer. Why? It is too general. What is the response variable here?
Product sales? Which product? Is it annual sales or monthly sales? What
are the candidate predictors? You nearly can't get any useful
information from the questions. In contrast, question 2 is much more
specific. From the analysis point of view, the response variable is
clearly ``annual sales of P1197 in Iowa and Wisconsin''. Even we don't
know all the predictors, but the variable of interest is ``the new
promotional tool introduced early this year.'' We want to study the
impact of the promotion of the sales. You can start from there and move
on to figure out other variables need to include in the model by further
communication.

As a data scientist, you may start with something general and unspecific
like question 1 and eventually get to question 2. Effective
communication and in-depth domain knowledge about the business problem
are essential to convert a general business question into a solvable
analytical problem. Domain knowledge helps data scientist communicate
with the language the other people can understand and obtain the
required information.

However, defining the question and variables involved don't guarantee
that you can answer it. I have encountered a well-defined supply chain
problem. My client asked about the stock needed for a product in a
particular area. Why can not this question be answered? I did fit a
Multivariate Adaptive Regression Spline (MARS) model and thought I found
a reasonable solution. But it turned out later that the data they gave
me was inaccurate. In some areas, only estimates of past supply figures
were available. The lesson lends itself to the next point.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  You need to have sound and relevant data
\end{enumerate}

One cannot make a silk purse out of a sow's ear. Data scientists need
data, sound and relevant data. The supply problem is a case in point.
There was relevant data, but not sound. All the later analytics based on
that data was a building on sand. Of course, data nearly almost have
noise, but it has to be in a certain range. Generally speaking, the
accuracy requirement for the independent variables of interest and
response variable is higher than others. In question 2, it is data
related to the ``new promotion'' and ``sales of P1197''.

The data has to be helpful for the question. If you want to predict
which product consumers are most likely to buy in the next three months,
you need to have historical purchasing data: the last buying time, the
amount of invoice, coupons and so on. Information about customers'
credit card number, ID number, the email address is not going to help.

Often the quality of the data is more important than the quantity, but
the quantity cannot be overlooked. In the premise of guaranteeing
quality, usually the more data, the better. If you have a specific and
reasonable question, also sound and relevant data, then congratulations,
you can start playing data science!

\subsection{Problem type}\label{problem-type}

Many of the data science books classify the various models from a
technical point of view. Such as supervised vs.~unsupervised models,
linear vs.~nonlinear models, parametric models vs.~non-parametric
models, and so on. Here we will continue on ``problem-oriented'' track.
We first introduce different groups of real problems and then present
which models can be used to answer the corresponding category of
questions.

\begin{figure}[htbp]
\centering
\includegraphics{images/DataScienceQuestion.png}
\caption{Data Science Questions}
\end{figure}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Comparison
\end{enumerate}

The first common problem is to compare different groups. Such as: Is A
better in some way than B? Or more comparisons: Is there any difference
among A, B, C in a certain aspect? Here are some examples:

\begin{itemize}
\tightlist
\item
  Are the purchasing amounts different between consumers receiving
  coupons and those without coupons?
\item
  Are males more inclined to buy our products than females?
\item
  Are there any differences in customer satisfaction in different
  business districts?
\item
  Do the mice receiving a drug have a faster weight gain than the
  control group?
\item
  Do soybeans carrying a particular gene contain more oil than the
  control group?
\end{itemize}

For those problems, it is usually to start exploring from the summary
statistics and visualization by groups. After a preliminary
visualization, you can test the differences between treatment and
control group statistically. The commonly used statistical tests are
chi-square test, t-test, and ANOVA. There are also methods using
Bayesian methods. In biology industry, such as new drug development,
crop breeding, mixed effect models are the dominant technique.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Description
\end{enumerate}

In the problem such as customer segmentation, after you cluster the
sample, the next step is to figure out the profile of each class by
comparing the descriptive statistics of the various variables. Questions
of this kind are:

\begin{itemize}
\tightlist
\item
  Is the income of the family's annual observations unbiased?
\item
  What is the mean/variance of the monthly sales volume of a product in
  different regions?
\item
  What is the difference in the magnitude of the variable? (Decide
  whether the data needs to be standardized)
\item
  What is the prediction variable in the model?
\item
  What is the age distribution of the respondents?
\end{itemize}

Data description is often used to check data, find the appropriate data
preprocessing method, and demonstrate the model results.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Clustering
\end{enumerate}

Clustering is a widespread problem, which is usually related to
classification. Clustering answers questions like:

\begin{itemize}
\tightlist
\item
  Which consumers have similar product preferences? (Marketing)
\item
  Which printer performs similar pattern to the broken ones? (Quality
  Control)
\item
  How many different kinds of employees are there in the company? (Human
  Resources)
\item
  How many different themes are there in the corpus? (Natural Language
  Processing)
\end{itemize}

Note that clustering is unsupervised learning. The most common
clustering algorithms include K-Means and Hierarchical Clustering.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Classification
\end{enumerate}

Usually, a labeled sample set is used as a training set to train the
classifier. Then the classifier is used to predict the category of a
future sample. Here are some example questions:

\begin{itemize}
\tightlist
\item
  Is this customer going to buy our product? (yes/no)
\item
  Is there a risk that a lender does not repay?
\item
  Who is the author of this book?
\item
  Is this spam email?
\end{itemize}

There are hundreds of classifiers. In practice, we do not have to try
all the models as long as we fit in several of the best models in most
cases.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  Regression
\end{enumerate}

In general, regression deals with the problem of ``how much is it?'' and
return a numerical answer. In some cases, it is necessary to coerce the
model results to be 0, or round the result to the nearest integer. It is
the most common problem.

\begin{itemize}
\tightlist
\item
  What will be the temperature tomorrow?
\item
  What will be the company's sales in the fourth quarter of this year?
\item
  How long will the engine work?
\item
  How much beer should we prepare for this event?
\end{itemize}

\section{Data Scientist Skill Set}\label{data-scientist-skill-set}

We talked about the bewildering definitions of data scientist. What are
the required skills for a data scientist?

\begin{itemize}
\tightlist
\item
  Educational Background
\end{itemize}

Most of the data scientists today have undergraduate or higher degree
from one of the following areas: computer science, electronic
engineering, mathematics or statistics. According to a 2017 survey, 25\%
of US data scientists have a Ph.D.~degree, 64\% have a Master's degree,
and 11\% are Bachelors.

\begin{itemize}
\tightlist
\item
  Database Skills
\end{itemize}

Data scientists in the industry need to use SQL to pull data from the
database. So it is necessary to be familiar with how data is structured
and how to do basic data manipulation using SQL. Many
statistics/mathematics students do not have experience with SQL in
school. Don't worry. If you are proficient in one programming language,
it is easy to pick up SQL. The main purpose of graduate school should be
to develop the ability to learn and analytical thinking rather than the
technical skills. Even the technical skills are necessary to enter the
professional area. Most of the skills needed at work are not taught in
school.

\begin{itemize}
\tightlist
\item
  Programming Skills
\end{itemize}

Programming skills are critical for data scientists. According to a 2017
survey from
\href{http://www.burtchworks.com/2017/06/19/2017-sas-r-python-flash-survey-results/}{Burtch
Works}, 97\% of the data scientists today using R or Python. We will
provide exemplary code for both in this book. There is not one
``have-to-use'' tool. The goal is to solve the problem not which tool to
choose. However, a good tool needs to be flexible and scalable.

\begin{itemize}
\tightlist
\item
  Modeling Skills
\end{itemize}

Data scientists need to know statistical and machine learning models.
There is no clear line separating these two. Many statistical models are
also machine learning models, vice versa. Generally speaking, a data
scientist is familiar with basic statistical tests such as t-test,
chi-square test, and analysis of variance. They can explain the
difference between Spearman rank correlation and Pearson correlation, be
aware of basic sampling schemes, such as Simple Random Sampling,
Stratified Random Sampling, and Multi-Stage Sampling. Know commonly used
probability distributions such as Normal distribution, Binomial
distribution, Poisson distribution, F distribution, T distribution, and
Chi-square distribution. Experimental design plays a significant role in
the biological study. Understanding the main tenants of Bayesian methods
is necessary (at least be able to write the Bayes theorem on the
whiteboard and know what does it mean). Know the difference between
supervised and unsupervised learning. Understand commonly used cluster
algorithms, classifiers, and regression models. Some powerful tools in
predictive analytics are tree models (such as random forest and
AdaBoost) and penalized model (such as lasso and SVM). Data scientist
working on social science (such as consumer awareness surveys), also
needs to know the latent variable model, such as exploratory factor
analysis, confirmatory factor analysis, structural equation model.

Is the list getting a little scary? It can get even longer. Don't worry
if you don't know all of them now. You will learn as you go. Standard
mathematics, statistics or computer science training in graduate school
can get you started. But you have to learn lots of new skills after
school. Learning is happening increasingly outside of formal educational
settings and in unsupervised environments. An excellent data scientist
must be a lifetime learner. Fortunately, technological advantages
provide new tools and opportunities for lifetime learners, MOOC, online
data science workshops and various online tutorials. So above all, being
a \textbf{life-time learner} is the most critical.

\begin{itemize}
\tightlist
\item
  Soft Skills
\end{itemize}

In addition to technical knowledge, there are some critical soft skills.
These include the ability to translate practical problems into data
problems, excellent communication skill, attention to detail,
storytelling and so on. We will discuss it in a later chapter in more
detail.

\begin{figure}[htbp]
\centering
\includegraphics{images/SkillEN.png}
\caption{Data Scientist Skill Set}
\end{figure}

\section{Types of Learning}\label{types-of-learning}

There are three broad groups of styles: supervised learning,
reinforcement learning, and unsupervised learning.

In supervised learning, each observation of the predictor measurement(s)
corresponds to a response measurement. There are two flavors of
supervised learning: regression and classification. In regression, the
response is a real number such as the total net sales in 2017, or the
yield of corn next year. The goal is to approximate the response
measurement as much as possible. In classification, the response is a
class label, such as dichotomous response such as yes/no. The response
can also have more than two categories, such as four segments of
customers. A supervised learning model is a function that maps some
input variables with corresponding parameters to a response y. Modeling
tuning is to adjust the value of parameters to make the mapping fit the
given response. In other words, it is to minimize the discrepancy
between given response and the model output. When the response y is a
real value, it is intuitive to define discrepancy as the squared
difference between model output and given the response. When y is
categorical, there are other ways to measure the difference, such as AUC
or information gain.

In reinforcement learning, the correct input/output pairs are not
present. The model will learn from a sequence of actions and select the
action maximizing the expected sum of the future rewards. There is a
discount factor that makes future rewards less valuable than current
rewards. Reinforcement learning is difficult for the following reasons:

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\item
  The rewards are not instant. If the action sequence is long, it is
  hard to know which action was wrong.
\item
  The rewards are occasional. Each reward does not supply much
  information, so its impact of parameter change is limited. Typically,
  it is not likely to learn a large number of parameters using
  reinforcement learning. However, it is possible for supervised and
  unsupervised learning. The number of parameters in reinforcement
  learning usually range from dozens to maybe 1,000, but not millions.
\end{enumerate}

In unsupervised learning, there is no response variable. For a long
time, the machine learning community overlooked unsupervised learning
except for one called clustering. Moreover, many researchers thought
that clustering was the only form of unsupervised learning. One reason
is that it is hard to define the goal of unsupervised learning
explicitly. Unsupervised learning can be used to do the following:

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\item
  Identify a good internal representation or pattern of the input that
  is useful for subsequent supervised or reinforcement learning, such as
  finding clusters.
\item
  It is a dimension reduction tool that is to provide compact, low
  dimensional representations of the input, such as factor analysis.
\item
  Provide a reduced number of uncorrelated learned features from
  original variables, such as principal component regression.
\end{enumerate}

\begin{figure}[htbp]
\centering
\includegraphics{images/LearningStyles.png}
\caption{Machine Learning Styles}
\end{figure}

\section{Types of Algorithm}\label{types-of-algorithm}

The categorization here is based on the structure (such as tree model,
Regularization Methods) or type of question to answer (such as
regression).\footnote{The summary of various algorithms for data science
  in this section is based on Jason Brownlee's blog ``(A Tour of Machine
  Learning
  Algorithms){[}\url{http://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/}{]}.''
  We added and subtracted some algorithms in each category and gave
  additional comments.} It is far less than perfect but will help to
show a bigger map of different algorithms. Some can be legitimately
classified into multiple categories, such as support vector machine
(SVM) can be a classifier, and can also be used for regression. So you
may see other ways of grouping. Also, the following summary does not
list all the existing algorithms (there are just too many).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Regression
\end{enumerate}

Regression can refer to the algorithm or a particular type of problem.
It is supervised learning. Regression is one of the oldest and most
widely used statistical models. It is often called the statistical
machine learning method. Standard regression models are:

\begin{itemize}
\tightlist
\item
  Ordinary Least Squares Regression
\item
  Logistic Regression
\item
  Multivariate Adaptive Regression Splines (MARS)
\item
  Locally Estimated Scatterplot Smoothing (LOESS)
\end{itemize}

The least squares regression and logistic regression are traditional
statistical models. Both of them are highly interpretable. MARS is
similar to neural networks and partial least squares (PLS) in the
respect that they all use surrogate features instead of original
predictors.

They differ in how to create the surrogate features. PLS and neural
networks use linear combinations of the original predictors as surrogate
features \footnote{To be clear on neural networks, the linear
  combinations of predictors are put through non-linear activation
  functions, deeper neural networks have many layers of non-linear
  transformation}. MARS creates two contrasted versions of a predictor
by a truncation point. And LOESS is a non-parametric model, usually only
used in visualization.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Similarity-based Algorithms
\end{enumerate}

This type of model is based on a similarity measure. There are three
main steps: (1) compare the new sample with the existing ones; (2)
search for the closest sample; (3) and let the response of the nearest
sample be used as the prediction.

\begin{itemize}
\tightlist
\item
  K-Nearest Neighbour {[}KNN{]}
\item
  Learning Vector Quantization {[}LVQ{]}
\item
  Self-Organizing Map {[}SOM{]}
\end{itemize}

The biggest advantage of this type of model is that they are intuitive.
K-Nearest Neighbour is generally the most popular algorithm in this set.
The other two are less common. The key to similarity-based algorithms is
to find an appropriate distance metric for your data.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Feature Selection Algorithms
\end{enumerate}

The primary purpose of feature selection is to exclude non-information
or redundant variables and also reduce dimension. Although it is
possible that all the independent variables are significant for
explaining the response. But more often, the response is only related to
a portion of the predictors. We will expand the feature selection in
detail later.

\begin{itemize}
\tightlist
\item
  Filter method
\item
  Wrapper method
\item
  Embedded method
\end{itemize}

Filter method focuses on the relationship between a single feature and a
target variable. It evaluates each feature (or an independent variable)
before modeling and selects ``important'' variables.

Wrapper method removes the variable according to particular law and
finds the feature combination that optimizes the model fitting by
evaluating a set of feature combinations. In essence, it is a searching
algorithm.

Embedding method is part of the machine learning model. Some model has
built-in variable selection function such as lasso, and decision tree.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Regularization Method
\end{enumerate}

This method itself is not a complete model, but rather an add-on to
other models (such as regression models). It appends a penalty function
on the criteria used by the original model to estimate the variables
(such as likelihood function or the sum of squared error). In this way,
it penalizes model complexity and contracts the model parameters. That
is why people call them ``shrinkage method.'' This approach is
advantageous in practice.

\begin{itemize}
\tightlist
\item
  Ridge Regression
\item
  Least Absolute Shrinkage and Selection Operator (LASSO)
\item
  Elastic Net
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  Decision Tree
\end{enumerate}

Decision trees are no doubt one of the most popular machine learning
algorithms. Thanks to all kinds of software, implementation is a
no-brainer which requires nearly zero understanding of the mechanism.
The followings are some of the common trees:

\begin{itemize}
\tightlist
\item
  Classification and Regression Tree (CART)
\item
  Iterative Dichotomiser 3 (ID3)
\item
  C4.5
\item
  Random Forest
\item
  Gradient Boosting Machines (GBM)
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\tightlist
\item
  Bayesian Models
\end{enumerate}

People usually confuse Bayes theorem with Bayesian models. Bayes theorem
is an implication of probability theory which gives Bayesian data
analysis its name.

\[Pr(\theta|y)=\frac{Pr(y|\theta)Pr(\theta)}{Pr(y)}\]

The actual Bayesian model is not identical to Bayes theorem. Given a
likelihood, parameters to estimate, and a prior for each parameter, a
Bayesian model treats the estimates as a purely logical consequence of
those assumptions. The resulting estimates are the posterior
distribution which is the relative plausibility of different parameter
values, conditional on the observations. The Bayesian model here is not
strictly in the sense of Bayesian but rather model using Bayes theorem.

\begin{itemize}
\tightlist
\item
  Naïve Bayes
\item
  Averaged One-Dependence Estimators (AODE)
\item
  Bayesian Belief Network (BBN)
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{6}
\tightlist
\item
  Kernel Methods
\end{enumerate}

The most common kernel method is the support vector machine (SVM). This
type of algorithm maps the input data to a higher order vector space
where classification or regression problems are easier to solve.

\begin{itemize}
\tightlist
\item
  Support Vector Machine (SVM)
\item
  Radial Basis Function (RBF)
\item
  Linear Discriminate Analysis (LDA)
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{7}
\tightlist
\item
  Clustering Methods
\end{enumerate}

Like regression, when people mention clustering, sometimes they mean a
class of problems, sometimes a class of algorithms. The clustering
algorithm usually clusters similar samples to categories in a centroidal
or hierarchical manner. The two are the most common clustering methods:

\begin{itemize}
\tightlist
\item
  K-Means
\item
  Hierarchical Clustering
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{8}
\tightlist
\item
  Association Rule
\end{enumerate}

The basic idea of an association rule is: when events occur together
more often than one would expect from their rates of occurrence, such
co-occurrence is an interesting pattern. The most used algorithms are:

\begin{itemize}
\tightlist
\item
  Apriori algorithm
\item
  Eclat algorithm
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{9}
\tightlist
\item
  Artificial Neural Network
\end{enumerate}

The term neural network has evolved to encompass a repertoire of models
and learning methods. There has been lots of hype around the model
family making them seem magical and mysterious. A neural network is a
two-stage regression or classification model. The basic idea is that it
uses linear combinations of the original predictors as surrogate
features, and then the new features are put through non-linear
activation functions to get hidden units in the 2nd stage. When there
are multiple hidden layers, it is called deep learning, another over
hyped term. Among varieties of neural network models, the most widely
used ``vanilla'' net is the single hidden layer back-propagation
network.

\begin{itemize}
\tightlist
\item
  Perceptron Neural Network
\item
  Back Propagation
\item
  Hopield Network
\item
  Self-Organizing Map (SOM)
\item
  Learning Vector Quantization (LVQ)
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{10}
\tightlist
\item
  Deep Learning
\end{enumerate}

The name is a little misleading. As mentioned before, it is multilayer
neural network. It is hyped tremendously especially after AlphaGO
defeated Li Shishi at the board game Go. We don't have too much
experience with the application of deep learning and are not in the
right position to talk more about it. Here are some of the common
algorithms:

\begin{itemize}
\tightlist
\item
  Restricted Boltzmann Machine (RBN)
\item
  Deep Belief Networks (DBN)
\item
  Convolutional Network
\item
  Stacked Autoencoders
\item
  Long short-term memory (LSTM)
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{11}
\tightlist
\item
  Dimensionality Reduction
\end{enumerate}

Its purpose is to construct new features that have significant physical
or statistical characteristics, such as capturing as much of the
variance as possible.

\begin{itemize}
\tightlist
\item
  Principle Component Analysis (PCA)
\item
  Partial Least Square Regression (PLS)
\item
  Multi-Dimensional Scaling (MDS)
\item
  Exploratory Factor Analysis (EFA)
\end{itemize}

PCA attempts to find uncorrelated linear combinations of original
variables that can explain the variance to the greatest extent possible.
EFA also tries to explain as much variance as possible in a lower
dimension. MDS maps the observed similarity to a low dimension, such as
a two-dimensional plane. Instead of extracting underlying components or
latent factors, MDS attempts to find a lower-dimensional map that best
preserves all the observed similarities between items. So it needs to
define a similarity measure as in clustering methods.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{12}
\tightlist
\item
  Ensemble Methods
\end{enumerate}

Ensemble method made its debut in the 1990s. The idea is to build a
prediction model by combining the strengths of a collection of simpler
base models. Bagging, originally proposed by Leo Breiman, is one of the
earliest ensemble methods. After that, people developed Random Forest
\citep{Ho1998, amit1997} and Boosting method
\citep{Valiant1984, KV1989}. This is a class of powerful and effective
algorithms.

\begin{itemize}
\tightlist
\item
  Bootstrapped Aggregation (Bagging)
\item
  Random Forest
\item
  Gradient Boosting Machine (GBM)
\end{itemize}

\begin{figure}[htbp]
\centering
\includegraphics{images/AlogrithmTypes.png}
\caption{Machines Learning Algorithms}
\end{figure}

\chapter{Introduction to the data}\label{introduction-to-the-data}

Before tackling analytics problem, we start by introducing data to be
analyzed in later chapters.

\section{Customer Data for Clothing
Company}\label{customer-data-for-clothing-company}

Our first data set represents customers of a clothing company who sells
products in stores and online. This data is typical of what one might
get from a company's marketing data base (the data base will have more
data than the one we show here). This data includes 1000 customers for
whom we have 3 types of data:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Demography

  \begin{itemize}
  \tightlist
  \item
    \texttt{age}: age of the respondent
  \item
    \texttt{gender}: male/female
  \item
    \texttt{house}: 0/1 variable indicating if the customer owns a house
    or not
  \end{itemize}
\item
  Sales in the past year

  \begin{itemize}
  \tightlist
  \item
    \texttt{store\_exp}: expense in store
  \item
    \texttt{online\_exp}: expense online
  \item
    \texttt{store\_trans}: times of store purchase
  \item
    \texttt{online\_trans}: times of online purchase
  \end{itemize}
\item
  Survey on product preference
\end{enumerate}

It is common for companies to survey their customers and draw insights
to guide future marketing activities. The survey is as below:

How strongly do you agree or disagree with the following statements:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Strong disagree
\item
  Disagree
\item
  Neither agree nor disagree
\item
  Agree
\item
  Strongly agree
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Q1. I like to buy clothes from different brands
\item
  Q2. I buy almost all my clothes from some of my favorite brands
\item
  Q3. I like to buy premium brands
\item
  Q4. Quality is the most important factor in my purchasing decision
\item
  Q5. Style is the most important factor in my purchasing decision
\item
  Q6. I prefer to buy clothes in store
\item
  Q7. I prefer to buy clothes online
\item
  Q8. Price is important
\item
  Q9. I like to try different styles
\item
  Q10. I like to make a choice by myself and don't need too much of
  others' suggestions
\end{itemize}

There are 4 segments of customers:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Price
\item
  Conspicuous
\item
  Quality
\item
  Style
\end{enumerate}

Let's check it:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{str}\NormalTok{(sim.dat,}\DataTypeTok{vec.len=}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 'data.frame':    1000 obs. of  19 variables:
##  $ age         : int  57 63 59 60 51 59 57 57 ...
##  $ gender      : Factor w/ 2 levels "Female","Male": 1 1 2 2 2 2 2 2 ...
##  $ income      : num  120963 122008 114202 113616 ...
##  $ house       : Factor w/ 2 levels "No","Yes": 2 2 2 2 2 2 2 2 ...
##  $ store_exp   : num  529 478 491 348 ...
##  $ online_exp  : num  304 110 279 142 ...
##  $ store_trans : int  2 4 7 10 4 4 5 11 ...
##  $ online_trans: int  2 2 2 2 4 5 3 5 ...
##  $ Q1          : int  4 4 5 5 4 4 4 5 ...
##  $ Q2          : int  2 1 2 2 1 2 1 2 ...
##  $ Q3          : int  1 1 1 1 1 1 1 1 ...
##  $ Q4          : int  2 2 2 3 3 2 2 3 ...
##  $ Q5          : int  1 1 1 1 1 1 1 1 ...
##  $ Q6          : int  4 4 4 4 4 4 4 4 ...
##  $ Q7          : int  1 1 1 1 1 1 1 1 ...
##  $ Q8          : int  4 4 4 4 4 4 4 4 ...
##  $ Q9          : int  2 1 1 2 2 1 1 2 ...
##  $ Q10         : int  4 4 4 4 4 4 4 4 ...
##  $ segment     : Factor w/ 4 levels "Conspicuous",..: 2 2 2 2 2 2 2 2 ...
\end{verbatim}

\section{Customer Satisfaction Survey Data from Airline
Company}\label{customer-satisfaction-survey-data-from-airline-company}

This data set is from a customer satisfaction survey for three airline
companies. There are \texttt{N=1000} respondents and 15 questions. The
market researcher asked respondents to recall the experience with
different airline companies and assign a score (1-9) to each airline
company for all the 15 questions. The higher the score, the more
satisfied the customer to the specific item. The 15 questions are of 4
types (the variable names are in the parentheses):

\begin{itemize}
\tightlist
\item
  How satisfied are you with your\_\_\_\_\_\_?
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Ticketing

  \begin{itemize}
  \tightlist
  \item
    Ease of making reservation（Easy\_Reservation）
  \item
    Availability of preferred seats（Preferred\_Seats）
  \item
    Variety of flight options（Flight\_Options）
  \item
    Ticket prices（Ticket\_Prices）
  \end{itemize}
\item
  Aircraft

  \begin{itemize}
  \tightlist
  \item
    Seat comfort（Seat\_Comfort）
  \item
    Roominess of seat area（Seat\_Roominess）
  \item
    Availability of Overhead（Overhead\_Storage）
  \item
    Cleanliness of aircraft（Clean\_Aircraft）
  \end{itemize}
\item
  Service

  \begin{itemize}
  \tightlist
  \item
    Courtesy of flight attendant（Courtesy）
  \item
    Friendliness（Friendliness）
  \item
    Helpfulness（Helpfulness）
  \item
    Food and drinks（Service）
  \end{itemize}
\item
  General

  \begin{itemize}
  \tightlist
  \item
    Overall satisfaction（Satisfaction）
  \item
    Purchase again（Fly\_Again）
  \item
    Willingness to recommend（Recommend）
  \end{itemize}
\end{enumerate}

Now check the data frame we have:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{str}\NormalTok{(rating,}\DataTypeTok{vec.len=}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Classes 'tbl_df', 'tbl' and 'data.frame':    3000 obs. of  17 variables:
##  $ Easy_Reservation: int  6 5 6 5 4 5 6 4 ...
##  $ Preferred_Seats : int  5 7 6 6 5 6 6 6 ...
##  $ Flight_Options  : int  4 7 5 5 3 4 6 3 ...
##  $ Ticket_Prices   : int  5 6 6 5 6 5 5 5 ...
##  $ Seat_Comfort    : int  5 6 7 7 6 6 6 4 ...
##  $ Seat_Roominess  : int  7 8 6 8 7 8 6 5 ...
##  $ Overhead_Storage: int  5 5 7 6 5 4 4 4 ...
##  $ Clean_Aircraft  : int  7 6 7 7 7 7 6 4 ...
##  $ Courtesy        : int  5 6 6 4 2 5 5 4 ...
##  $ Friendliness    : int  4 6 6 6 3 4 5 5 ...
##  $ Helpfulness     : int  6 5 6 4 4 5 5 4 ...
##  $ Service         : int  6 5 6 5 3 5 5 5 ...
##  $ Satisfaction    : int  6 7 7 5 4 6 5 5 ...
##  $ Fly_Again       : int  6 6 6 7 4 5 3 4 ...
##  $ Recommend       : int  3 6 5 5 4 5 6 5 ...
##  $ ID              : int  1 2 3 4 5 6 7 8 ...
##  $ Airline         : chr  "AirlineCo.1" "AirlineCo.1" "AirlineCo.1" ...
\end{verbatim}

\chapter{Data Pre-processing}\label{data-pre-processing}

Many data analysis related books focus on models, algorithms and
statistical inferences. However, in practice, raw data is usually not
directly used for modeling. Data preprocessing is the process of
converting raw data into clean data that is proper for modeling. A model
fails for various reasons. One is that the modeler doesn't correctly
preprocess data before modeling. Data preprocessing can significantly
impact model results, such as imputing missing value and handling with
outliers. So data preprocessing is a very critical part.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.90000\textwidth]{images/DataPre-processing.png}
\caption{Data Pre-processing Outline}
\end{figure}

In real life, depending on the stage of data cleanup, data has the
following types:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Raw data
\item
  Technically correct data
\item
  Data that is proper for the model
\item
  Summarized data
\item
  Data with fixed format
\end{enumerate}

The raw data is the first-hand data that analysts pull from the
database, market survey responds from your clients, the experimental
results collected by the R \& D department, and so on. These data may be
very rough, and R sometimes can't read them directly. The table title
could be multi-line, or the format does not meet the requirements:

\begin{itemize}
\tightlist
\item
  Use 50\% to represent the percentage rather than 0.5, so R will read
  it as a character;
\item
  The missing value of the sales is represented by ``-'' instead of
  space so that R will treat the variable as character or factor type;
\item
  The data is in a slideshow document, or the spreadsheet is not
  ``.csv'' but ``.xlsx''
\item
  \ldots{}
\end{itemize}

Most of the time, you need to clean the data so that R can import them.
Some data format requires a specific package. Technically correct data
is the data, after preliminary cleaning or format conversion, that R (or
another tool you use) can successfully import it.

Assume we have loaded the data into R with reasonable column names,
variable format and so on. That does not mean the data is entirely
correct. There may be some observations that do not make sense, such as
age is negative, the discount percentage is greater than 1, or data is
missing. Depending on the situation, there may be a variety of problems
with the data. It is necessary to clean the data before modeling.
Moreover, different models have different requirements on the data. For
example, some model may require the variables are of consistent scale;
some may be susceptible to outliers or collinearity, some may not be
able to handle categorical variables and so on. The modeler has to
preprocess the data to make it proper for the specific model.

Sometimes we need to aggregate the data. For example, add up the daily
sales to get annual sales of a product at different locations. In
customer segmentation, it is common practice to build a profile for each
segment. It requires calculating some statistics such as average age,
average income, age standard deviation, etc. Data aggregation is also
necessary for presentation, or for data visualization.

The final table results for clients need to be in a nicer format than
what used in the analysis. Usually, data analysts will take the results
from data scientists and adjust the format, such as labels, cell color,
highlight. It is important for a data scientist to make sure the results
look consistent which makes the next step easier for data analysts.

It is highly recommended to store each step of the data and the R code,
making the whole process as repeatable as possible. The R markdown
reproducible report will be extremely helpful for that. If the data
changes, it is easy to rerun the process. In the remainder of this
chapter, we will show the most common data preprocessing methods.

Load the R packages first:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{source}\NormalTok{(}\StringTok{"https://raw.githubusercontent.com/happyrabbit/CE_JSM2017/master/Rcode/00-course-setup.R"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\section{Data Cleaning}\label{data-cleaning}

After you load the data, the first thing is to check how many variables
are there, the type of variables, the distributions, and data errors.
Let's read and check the data:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sim.dat <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"https://raw.githubusercontent.com/happyrabbit/DataScientistR/master/Data/SegData.csv "}\NormalTok{)}
\KeywordTok{summary}\NormalTok{(sim.dat)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       age           gender        income      
##  Min.   : 16.0   Female:554   Min.   : 41776  
##  1st Qu.: 25.0   Male  :446   1st Qu.: 85832  
##  Median : 36.0                Median : 93869  
##  Mean   : 38.8                Mean   :113543  
##  3rd Qu.: 53.0                3rd Qu.:124572  
##  Max.   :300.0                Max.   :319704  
##                               NA's   :184     
##  house       store_exp       online_exp  
##  No :432   Min.   : -500   Min.   :  69  
##  Yes:568   1st Qu.:  205   1st Qu.: 420  
##            Median :  329   Median :1942  
##            Mean   : 1357   Mean   :2120  
##            3rd Qu.:  597   3rd Qu.:2441  
##            Max.   :50000   Max.   :9479  
##                                          
##   store_trans     online_trans        Q1     
##  Min.   : 1.00   Min.   : 1.0   Min.   :1.0  
##  1st Qu.: 3.00   1st Qu.: 6.0   1st Qu.:2.0  
##  Median : 4.00   Median :14.0   Median :3.0  
##  Mean   : 5.35   Mean   :13.6   Mean   :3.1  
##  3rd Qu.: 7.00   3rd Qu.:20.0   3rd Qu.:4.0  
##  Max.   :20.00   Max.   :36.0   Max.   :5.0  
##                                              
##        Q2             Q3             Q4      
##  Min.   :1.00   Min.   :1.00   Min.   :1.00  
##  1st Qu.:1.00   1st Qu.:1.00   1st Qu.:2.00  
##  Median :1.00   Median :1.00   Median :3.00  
##  Mean   :1.82   Mean   :1.99   Mean   :2.76  
##  3rd Qu.:2.00   3rd Qu.:3.00   3rd Qu.:4.00  
##  Max.   :5.00   Max.   :5.00   Max.   :5.00  
##                                              
##        Q5             Q6             Q7      
##  Min.   :1.00   Min.   :1.00   Min.   :1.00  
##  1st Qu.:1.75   1st Qu.:1.00   1st Qu.:2.50  
##  Median :4.00   Median :2.00   Median :4.00  
##  Mean   :2.94   Mean   :2.45   Mean   :3.43  
##  3rd Qu.:4.00   3rd Qu.:4.00   3rd Qu.:4.00  
##  Max.   :5.00   Max.   :5.00   Max.   :5.00  
##                                              
##        Q8            Q9            Q10      
##  Min.   :1.0   Min.   :1.00   Min.   :1.00  
##  1st Qu.:1.0   1st Qu.:2.00   1st Qu.:1.00  
##  Median :2.0   Median :4.00   Median :2.00  
##  Mean   :2.4   Mean   :3.08   Mean   :2.32  
##  3rd Qu.:3.0   3rd Qu.:4.00   3rd Qu.:3.00  
##  Max.   :5.0   Max.   :5.00   Max.   :5.00  
##                                             
##         segment   
##  Conspicuous:200  
##  Price      :250  
##  Quality    :200  
##  Style      :350  
##                   
##                   
## 
\end{verbatim}

Are there any problems? Questionnaire response Q1-Q10 seem reasonable,
the minimum is 1 and maximum is 5. Recall that the questionnaire score
is 1-5. The number of store transactions (store\_trans) and online
transactions (store\_trans) make sense too. Things need to pay attention
are:

\begin{itemize}
\tightlist
\item
  There are some missing values.
\item
  There are outliers for store expenses (\texttt{store\_exp}). The
  maximum value is 50000. Who would spend \$50000 a year buying clothes?
  Is it an imputation error?
\item
  There is a negative value ( -500) in \texttt{store\_exp} which is not
  logical.
\item
  Someone is 300 years old.
\end{itemize}

How to deal with that? Depending on the real situation, if the sample
size is large enough, it will not hurt to delete those problematic
samples. Here we have 1000 observations. Since marketing survey is
usually expensive, it is better to set these values as missing and
impute them instead of deleting the rows.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# set problematic values as missings}
\NormalTok{sim.dat$age[}\KeywordTok{which}\NormalTok{(sim.dat$age>}\DecValTok{100}\NormalTok{)]<-}\OtherTok{NA}
\NormalTok{sim.dat$store_exp[}\KeywordTok{which}\NormalTok{(sim.dat$store_exp<}\DecValTok{0}\NormalTok{)]<-}\OtherTok{NA}
\CommentTok{# see the results}
\KeywordTok{summary}\NormalTok{(}\KeywordTok{subset}\NormalTok{(sim.dat,}\DataTypeTok{select=}\KeywordTok{c}\NormalTok{(}\StringTok{"age"}\NormalTok{,}\StringTok{"income"}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       age           income      
##  Min.   :16.0   Min.   : 41776  
##  1st Qu.:25.0   1st Qu.: 85832  
##  Median :36.0   Median : 93869  
##  Mean   :38.6   Mean   :113543  
##  3rd Qu.:53.0   3rd Qu.:124572  
##  Max.   :69.0   Max.   :319704  
##  NA's   :1      NA's   :184
\end{verbatim}

Now we will deal with the missing values in the data.

\section{Missing Values}\label{missing-values}

You can write a whole book about missing value. This section will only
show some of the most commonly used methods without getting too deep
into the topic. Chapter 7 of the book by De Waal, Pannekoek and Scholtus
\citep{Ton2011} makes a concise overview of some of the existing
imputation methods. The choice of specific method depends on the actual
situation. There is no best way.

One question to ask before imputation: Is there any auxiliary
information? Being aware of any auxiliary information is critical. For
example, if the system set customer who did not purchase as missing,
then the real purchasing amount should be 0. Is missing a random
occurrence? If so, it may be reasonable to impute with mean or median.
If not, is there a potential mechanism for the missing data? For
example, older people are more reluctant to disclose their ages in the
questionnaire, so that the absence of age is not completely random. In
this case, the missing values need to be estimated using the
relationship between age and other independent variables. For example,
use variables such as whether they have children, income, and other
survey questions to build a model to predict age.

Also, the purpose of modeling is important for selecting imputation
methods. If the goal is to interpret the parameter estimate or
statistical inference, then it is important to study the missing
mechanism carefully and to estimate the missing values using non-missing
information as much as possible. If the goal is to predict, people
usually will not study the absence mechanism rigorously (but sometimes
the mechanism is obvious). If the absence mechanism is not clear, treat
it as missing at random and use mean, median, or k-nearest neighbor to
impute. Since statistical inference is sensitive to missing values,
researchers from survey statistics have conducted in-depth studies of
various imputation schemes which focus on valid statistical inference.
The problem of missing values in the prediction model is different from
that in the traditional survey. Therefore, there are not many papers on
missing value imputation in the prediction model. Those who want to
study further can refer to Saar-Tsechansky and Provost's comparison of
different imputation methods \citep{missing1} and De Waal, Pannekoek and
Scholtus' book \citep{Ton2011}.

\subsection{Impute missing values with
median/mode}\label{impute-missing-values-with-medianmode}

In the case of missing at random, a common method is to impute with the
mean (continuous variable) or median (categorical variables). You can
use \texttt{impute()} function in \texttt{imputeMissings} package.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# save the result as another object}
\NormalTok{demo_imp<-}\KeywordTok{impute}\NormalTok{(sim.dat,}\DataTypeTok{method=}\StringTok{"median/mode"}\NormalTok{)}
\CommentTok{# check the first 5 columns, there is no missing values in other columns}
\KeywordTok{summary}\NormalTok{(demo_imp[,}\DecValTok{1}\NormalTok{:}\DecValTok{5}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       age          gender        income      
##  Min.   :16.0   Female:554   Min.   : 41776  
##  1st Qu.:25.0   Male  :446   1st Qu.: 87896  
##  Median :36.0                Median : 93869  
##  Mean   :38.6                Mean   :109923  
##  3rd Qu.:53.0                3rd Qu.:119456  
##  Max.   :69.0                Max.   :319704  
##  house       store_exp    
##  No :432   Min.   :  156  
##  Yes:568   1st Qu.:  205  
##            Median :  330  
##            Mean   : 1358  
##            3rd Qu.:  597  
##            Max.   :50000
\end{verbatim}

After imputation, \texttt{demo\_imp} has no missing value. This method
is straightforward and widely used. The disadvantage is that it does not
take into account the relationship between the variables. When there is
a significant proportion of missing, it will distort the data. In this
case, it is better to consider the relationship between variables and
study the missing mechanism. In the example here, the missing variables
are numeric. If the missing variable is a categorical/factor variable,
the \texttt{impute()} function will impute with the mode.

You can also use \texttt{preProcess()} function, but it is only for
numeric variables, and can not impute categorical variables. Since
missing values here are numeric, we can use the \texttt{preProcess()}
function. The result is the same as the \texttt{impute()} function.
\texttt{PreProcess()} is a powerful function that can link to a variety
of data preprocessing methods. We will use the function later for other
data preprocessing.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{imp<-}\KeywordTok{preProcess}\NormalTok{(sim.dat,}\DataTypeTok{method=}\StringTok{"medianImpute"}\NormalTok{)}
\NormalTok{demo_imp2<-}\KeywordTok{predict}\NormalTok{(imp,sim.dat)}
\KeywordTok{summary}\NormalTok{(demo_imp2[,}\DecValTok{1}\NormalTok{:}\DecValTok{5}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       age          gender        income      
##  Min.   :16.0   Female:554   Min.   : 41776  
##  1st Qu.:25.0   Male  :446   1st Qu.: 87896  
##  Median :36.0                Median : 93869  
##  Mean   :38.6                Mean   :109923  
##  3rd Qu.:53.0                3rd Qu.:119456  
##  Max.   :69.0                Max.   :319704  
##  house       store_exp    
##  No :432   Min.   :  156  
##  Yes:568   1st Qu.:  205  
##            Median :  330  
##            Mean   : 1358  
##            3rd Qu.:  597  
##            Max.   :50000
\end{verbatim}

\subsection{K-nearest neighbors}\label{k-nearest-neighbors}

K-nearest neighbor (KNN) will find the k closest samples (Euclidian
distance) in the training set and impute the mean of those
``neighbors.''

Use \texttt{preProcess()} to conduct KNN:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{imp<-}\KeywordTok{preProcess}\NormalTok{(sim.dat,}\DataTypeTok{method=}\StringTok{"knnImpute"}\NormalTok{,}\DataTypeTok{k=}\DecValTok{5}\NormalTok{)}
\CommentTok{# need to use predict() to get KNN result}
\NormalTok{demo_imp<-}\KeywordTok{predict}\NormalTok{(imp,sim.dat)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Error in `[.data.frame`(old, , non_missing_cols, drop = FALSE) : 
  undefined columns selected
\end{verbatim}

Now we get an error saying ``undefined columns selected.'' It is because
\texttt{sim.dat} has non-numeric variables. The \texttt{preProcess()} in
the first line will automatically ignore non-numeric columns, so there
is no error. However, there is a problem when using \texttt{predict()}
to get the result. Removing those variable will solve the problem.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# find factor columns}
\NormalTok{imp<-}\KeywordTok{preProcess}\NormalTok{(sim.dat,}\DataTypeTok{method=}\StringTok{"knnImpute"}\NormalTok{,}\DataTypeTok{k=}\DecValTok{5}\NormalTok{)}
\NormalTok{idx<-}\KeywordTok{which}\NormalTok{(}\KeywordTok{lapply}\NormalTok{(sim.dat,class)==}\StringTok{"factor"}\NormalTok{)}
\NormalTok{demo_imp<-}\KeywordTok{predict}\NormalTok{(imp,sim.dat[,-idx])}
\KeywordTok{summary}\NormalTok{(demo_imp[,}\DecValTok{1}\NormalTok{:}\DecValTok{3}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       age             income         store_exp     
##  Min.   :-1.591   Min.   :-1.440   Min.   :-0.433  
##  1st Qu.:-0.957   1st Qu.:-0.537   1st Qu.:-0.416  
##  Median :-0.182   Median :-0.376   Median :-0.371  
##  Mean   : 0.000   Mean   : 0.024   Mean   : 0.000  
##  3rd Qu.: 1.016   3rd Qu.: 0.215   3rd Qu.:-0.274  
##  Max.   : 2.144   Max.   : 4.136   Max.   :17.527
\end{verbatim}

\texttt{lapply(data,class)} can return a list of column class. Here the
data frame is \texttt{sim.dat}, and the following code will give the
list of column class:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# only show the first three elements}
\KeywordTok{lapply}\NormalTok{(sim.dat,class)[}\DecValTok{1}\NormalTok{:}\DecValTok{3}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $age
## [1] "integer"
## 
## $gender
## [1] "factor"
## 
## $income
## [1] "numeric"
\end{verbatim}

Comparing the KNN result with the previous median imputation, the two
are very different. This is because when you tell the
\texttt{preProcess()} function to use KNN (the option
\texttt{method\ ="\ knnImpute"}), it will automatically standardize the
data. Another way is to use Bagging tree (in the next section). Note
that KNN can not impute samples with the entire row missing. The reason
is straightforward. Since the algorithm uses the average of its
neighbors if none of them has a value, what does it apply to calculate
the mean? Let's append a new row with all values missing to the original
data frame to get a new object called \texttt{temp}. Then apply KNN to
\texttt{temp} and see what happens:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{temp<-}\KeywordTok{rbind}\NormalTok{(sim.dat,}\KeywordTok{rep}\NormalTok{(}\OtherTok{NA}\NormalTok{,}\KeywordTok{ncol}\NormalTok{(sim.dat)))}
\NormalTok{imp<-}\KeywordTok{preProcess}\NormalTok{(sim.dat,}\DataTypeTok{method=}\StringTok{"knnImpute"}\NormalTok{,}\DataTypeTok{k=}\DecValTok{5}\NormalTok{)}
\NormalTok{idx<-}\KeywordTok{which}\NormalTok{(}\KeywordTok{lapply}\NormalTok{(temp,class)==}\StringTok{"factor"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{demo_imp<-}\KeywordTok{predict}\NormalTok{(imp,temp[,-idx])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Error in FUN(newX[, i], ...) : 
  cannot impute when all predictors are missing in the new data point
\end{verbatim}

There is an error saying ``cannot impute when all predictors are missing
in the new data point''. It is easy to fix by finding and removing the
problematic row:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{idx<-}\KeywordTok{apply}\NormalTok{(temp,}\DecValTok{1}\NormalTok{,function(x) }\KeywordTok{sum}\NormalTok{(}\KeywordTok{is.na}\NormalTok{(x)) )}
\KeywordTok{as.vector}\NormalTok{(}\KeywordTok{which}\NormalTok{(idx==}\KeywordTok{ncol}\NormalTok{(temp)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1001
\end{verbatim}

It shows that row 1001 is problematic. You can go ahead to delete it.

\subsection{Bagging Tree}\label{bagging-tree}

Bagging (Bootstrap aggregating) was originally proposed by Leo Breiman.
It is one of the earliest ensemble methods \citep{bag1}. When used in
missing value imputation, it will use the remaining variables as
predictors to train a bagging tree and then use the tree to predict the
missing values. Although theoretically, the method is powerful, the
computation is much more intense than KNN. In practice, there is a
trade-off between computation time and the effect. If a median or mean
meet the modeling needs, even bagging tree may improve the accuracy a
little, but the upgrade is so marginal that it does not deserve the
extra time. The bagging tree itself is a model for regression and
classification. Here we use \texttt{preProcess()} to impute
\texttt{sim.dat}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{imp<-}\KeywordTok{preProcess}\NormalTok{(sim.dat,}\DataTypeTok{method=}\StringTok{"bagImpute"}\NormalTok{)}
\NormalTok{demo_imp<-}\KeywordTok{predict}\NormalTok{(imp,sim.dat)}
\KeywordTok{summary}\NormalTok{(demo_imp[,}\DecValTok{1}\NormalTok{:}\DecValTok{5}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
      age           gender        income       house       store_exp      
 Min.   :16.00   Female:554   Min.   : 41776   No :432   Min.   :  155.8  
 1st Qu.:25.00   Male  :446   1st Qu.: 86762   Yes:568   1st Qu.:  205.1  
 Median :36.00                Median : 94739             Median :  329.0  
 Mean   :38.58                Mean   :114665             Mean   : 1357.7  
 3rd Qu.:53.00                3rd Qu.:123726             3rd Qu.:  597.3  
 Max.   :69.00                Max.   :319704             Max.   :50000.0  
\end{verbatim}

\section{Centering and Scaling}\label{centering-and-scaling}

It is the most straightforward data transformation. It centers and
scales a variable to mean 0 and standard deviation 1. It ensures that
the criterion for finding linear combinations of the predictors is based
on how much variation they explain and therefore improves the numerical
stability. Models involving finding linear combinations of the
predictors to explain response/predictors variation need data centering
and scaling, such as PCA \citep{pca1}, PLS \citep{PLS1} and EFA
\citep{EFA1}. You can quickly write code yourself to conduct this
transformation.

Let's standardize the variable \texttt{income} from \texttt{sim.dat}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{income<-sim.dat$income}
\CommentTok{# calculate the mean of income}
\NormalTok{mux<-}\KeywordTok{mean}\NormalTok{(income,}\DataTypeTok{na.rm=}\NormalTok{T)}
\CommentTok{# calculate the standard deviation of income}
\NormalTok{sdx<-}\KeywordTok{sd}\NormalTok{(income,}\DataTypeTok{na.rm=}\NormalTok{T)}
\CommentTok{# centering}
\NormalTok{tr1<-income-mux}
\CommentTok{# scaling}
\NormalTok{tr2<-tr1/sdx}
\end{Highlighting}
\end{Shaded}

Or the function \texttt{preProcess()} in package \texttt{caret} can
apply this transformation to a set of predictors.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sdat<-}\KeywordTok{subset}\NormalTok{(sim.dat,}\DataTypeTok{select=}\KeywordTok{c}\NormalTok{(}\StringTok{"age"}\NormalTok{,}\StringTok{"income"}\NormalTok{))}
\CommentTok{# set the "method" option}
\NormalTok{trans<-}\KeywordTok{preProcess}\NormalTok{(sdat,}\DataTypeTok{method=}\KeywordTok{c}\NormalTok{(}\StringTok{"center"}\NormalTok{,}\StringTok{"scale"}\NormalTok{))}
\CommentTok{# use predict() function to get the final result}
\NormalTok{transformed<-}\KeywordTok{predict}\NormalTok{(trans,sdat)}
\end{Highlighting}
\end{Shaded}

Now the two variables are in the same scale:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(transformed)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       age             income     
##  Min.   :-1.591   Min.   :-1.44  
##  1st Qu.:-0.957   1st Qu.:-0.56  
##  Median :-0.182   Median :-0.39  
##  Mean   : 0.000   Mean   : 0.00  
##  3rd Qu.: 1.016   3rd Qu.: 0.22  
##  Max.   : 2.144   Max.   : 4.14  
##  NA's   :1        NA's   :184
\end{verbatim}

Sometimes you only need to scale the variable. For example, if the model
adds a penalty to the parameter estimates (such as \(L_2\) penalty is
ridge regression and \(L_1\) penalty in LASSO), the variables need to
have a similar scale to ensure a fair variable selection. I am a heavy
user of this kind of penalty-based model in my work, and I used the
following quantile transformation:

\[
x_{ij}^{*}=\frac{x_{ij}-quantile(x_{.j},0.01)}{quantile(x_{.j}-0.99)-quantile(x_{-j},0.01)}
\]

The reason to use 99\% and 1\% quantile instead of maximum and minimum
values is to resist the impact of outliers.

It is easy to write a function to do it:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{qscale<-function(dat)\{}
  \NormalTok{for (i in }\DecValTok{1}\NormalTok{:}\KeywordTok{ncol}\NormalTok{(dat))\{}
    \NormalTok{up<-}\KeywordTok{quantile}\NormalTok{(dat[,i],}\FloatTok{0.99}\NormalTok{)}
    \NormalTok{low<-}\KeywordTok{quantile}\NormalTok{(dat[,i],}\FloatTok{0.01}\NormalTok{)}
    \NormalTok{diff<-up-low}
    \NormalTok{dat[,i]<-(dat[,i]-low)/diff}
  \NormalTok{\}}
  \KeywordTok{return}\NormalTok{(dat)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

In order to illustrate, let's apply it to some variables from
`demo\_imp2:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{demo_imp3<-}\KeywordTok{qscale}\NormalTok{(}\KeywordTok{subset}\NormalTok{(demo_imp2,}\DataTypeTok{select=}\KeywordTok{c}\NormalTok{(}\StringTok{"income"}\NormalTok{,}\StringTok{"store_exp"}\NormalTok{,}\StringTok{"online_exp"}\NormalTok{)))}
\KeywordTok{summary}\NormalTok{(demo_imp3)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      income          store_exp        online_exp     
##  Min.   :-0.0578   Min.   :-0.003   Min.   :-0.0060  
##  1st Qu.: 0.1574   1st Qu.: 0.004   1st Qu.: 0.0427  
##  Median : 0.1852   Median : 0.023   Median : 0.2537  
##  Mean   : 0.2601   Mean   : 0.177   Mean   : 0.2784  
##  3rd Qu.: 0.3046   3rd Qu.: 0.063   3rd Qu.: 0.3229  
##  Max.   : 1.2386   Max.   : 7.477   Max.   : 1.2988
\end{verbatim}

After transformation, most of the variables are between 0-1.

\section{Resolve Skewness}\label{resolve-skewness}

\href{https://en.wikipedia.org/wiki/Skewness}{Skewness} is defined to be
the third standardized central moment. The formula for the sample
skewness statistics is:
\[ skewness=\frac{\sum(x_{i}-\bar{x})^{3}}{(n-1)v^{3/2}}\]
\[v=\frac{\sum(x_{i}-\bar{x})^{2}}{(n-1)}\] Skewness=0 means that the
destribution is symmetric, i.e.~the probability of falling on either
side of the distribution's mean is equal.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# need skewness() function from e1071 package}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1000}\NormalTok{)}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{),}\DataTypeTok{oma=}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\CommentTok{# random sample 1000 chi-square distribution with df=2}
\CommentTok{# right skew}
\NormalTok{x1<-}\KeywordTok{rchisq}\NormalTok{(}\DecValTok{1000}\NormalTok{,}\DecValTok{2}\NormalTok{, }\DataTypeTok{ncp =} \DecValTok{0}\NormalTok{)}
\CommentTok{# get left skew variable x2 from x1}
\NormalTok{x2<-}\KeywordTok{max}\NormalTok{(x1)-x1}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{density}\NormalTok{(x2),}\DataTypeTok{main=}\KeywordTok{paste}\NormalTok{(}\StringTok{"left skew, skewnwss ="}\NormalTok{,}\KeywordTok{round}\NormalTok{(}\KeywordTok{skewness}\NormalTok{(x2),}\DecValTok{2}\NormalTok{)), }\DataTypeTok{xlab=}\StringTok{"X2"}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{density}\NormalTok{(x1),}\DataTypeTok{main=}\KeywordTok{paste}\NormalTok{(}\StringTok{"right skew, skewness ="}\NormalTok{,}\KeywordTok{round}\NormalTok{(}\KeywordTok{skewness}\NormalTok{(x1),}\DecValTok{2}\NormalTok{)), }\DataTypeTok{xlab=}\StringTok{"X1"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{IDS_files/figure-latex/skew-1} 

}

\caption{Shewed Distribution}\label{fig:skew}
\end{figure}

You can easily tell if a distribution is skewed by simple
visualization(Figure\ref{fig:skew}). There are different ways may help
to remove skewness such as log, square root or inverse. However, it is
often difficult to determine from plots which transformation is most
appropriate for correcting skewness. The Box-Cox procedure automatically
identified a transformation from the family of power transformations
that are indexed by a parameter \(\lambda\)\citep{BOXCOX1}.

\[
x^{*}=\begin{cases}
\begin{array}{c}
\frac{x^{\lambda}-1}{\lambda}\\
log(x)
\end{array} & \begin{array}{c}
if\ \lambda\neq0\\
if\ \lambda=0
\end{array}\end{cases}
\]

It is easy to see that this family includes log transformation
(\(\lambda=0\)), square transformation (\(\lambda=2\)), square root
(\(\lambda=0.5\)), inverse (\(\lambda=-1\)) and others in-between. We
can still use function \texttt{preProcess()} in package \texttt{caret}
to apply this transformation by chaning the \texttt{method} argument.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{describe}\NormalTok{(sim.dat)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##              vars    n      mean       sd  median
## age             1  999     38.58    14.19    36.0
## gender*         2 1000      1.45     0.50     1.0
## income          3  816 113543.07 49842.29 93868.7
## house*          4 1000      1.57     0.50     2.0
## store_exp       5  999   1358.71  2775.17   329.8
## online_exp      6 1000   2120.18  1731.22  1941.9
## store_trans     7 1000      5.35     3.70     4.0
## online_trans    8 1000     13.55     7.96    14.0
## Q1              9 1000      3.10     1.45     3.0
## Q2             10 1000      1.82     1.17     1.0
## Q3             11 1000      1.99     1.40     1.0
## Q4             12 1000      2.76     1.16     3.0
## Q5             13 1000      2.94     1.28     4.0
## Q6             14 1000      2.45     1.44     2.0
## Q7             15 1000      3.43     1.46     4.0
## Q8             16 1000      2.40     1.15     2.0
## Q9             17 1000      3.08     1.12     4.0
## Q10            18 1000      2.32     1.14     2.0
## segment*       19 1000      2.70     1.15     3.0
##                trimmed      mad      min    max  range
## age              37.67    16.31    16.00     69     53
## gender*           1.43     0.00     1.00      2      1
## income       104841.94 28989.47 41775.64 319704 277929
## house*            1.58     0.00     1.00      2      1
## store_exp       845.14   197.47   155.81  50000  49844
## online_exp     1874.51  1015.21    68.82   9479   9411
## store_trans       4.89     2.97     1.00     20     19
## online_trans     13.42    10.38     1.00     36     35
## Q1                3.13     1.48     1.00      5      4
## Q2                1.65     0.00     1.00      5      4
## Q3                1.75     0.00     1.00      5      4
## Q4                2.83     1.48     1.00      5      4
## Q5                3.05     0.00     1.00      5      4
## Q6                2.43     1.48     1.00      5      4
## Q7                3.54     0.00     1.00      5      4
## Q8                2.36     1.48     1.00      5      4
## Q9                3.23     0.00     1.00      5      4
## Q10               2.27     1.48     1.00      5      4
## segment*          2.75     1.48     1.00      4      3
##               skew kurtosis      se
## age           0.47    -1.18    0.45
## gender*       0.22    -1.95    0.02
## income        1.69     2.57 1744.83
## house*       -0.27    -1.93    0.02
## store_exp     8.08   115.04   87.80
## online_exp    1.18     1.31   54.75
## store_trans   1.11     0.69    0.12
## online_trans  0.03    -0.98    0.25
## Q1           -0.12    -1.36    0.05
## Q2            1.13    -0.32    0.04
## Q3            1.06    -0.40    0.04
## Q4           -0.18    -1.46    0.04
## Q5           -0.60    -1.40    0.04
## Q6            0.11    -1.89    0.05
## Q7           -0.90    -0.79    0.05
## Q8            0.21    -1.33    0.04
## Q9           -0.68    -1.10    0.04
## Q10           0.39    -1.23    0.04
## segment*     -0.20    -1.41    0.04
\end{verbatim}

It is easy to see the skewed variables. If \texttt{mean} and
\texttt{trimmed} differ a lot, there is very likely outliers. By
default, \texttt{trimmed} reports mean by dropping the top and bottom
10\%. It can be adjusted by setting argument \texttt{trim=}. It is clear
that \texttt{store\_exp} has outliers.

As an example, we will apply Box-Cox transformation on
\texttt{store\_trans} and \texttt{online\_trans}:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# select the two columns and save them as dat_bc}
\NormalTok{dat_bc<-}\KeywordTok{subset}\NormalTok{(sim.dat,}\DataTypeTok{select=}\KeywordTok{c}\NormalTok{(}\StringTok{"store_trans"}\NormalTok{,}\StringTok{"online_trans"}\NormalTok{))}
\NormalTok{(trans<-}\KeywordTok{preProcess}\NormalTok{(dat_bc,}\DataTypeTok{method=}\KeywordTok{c}\NormalTok{(}\StringTok{"BoxCox"}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Created from 1000 samples and 2 variables
## 
## Pre-processing:
##   - Box-Cox transformation (2)
##   - ignored (0)
## 
## Lambda estimates for Box-Cox transformation:
## 0.1, 0.7
\end{verbatim}

The last line of the output shows the estimates of \(\lambda\) for each
variable. As before, use \texttt{predict()} to get the transformed
result:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{transformed<-}\KeywordTok{predict}\NormalTok{(trans,dat_bc)}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{),}\DataTypeTok{oma=}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\KeywordTok{hist}\NormalTok{(dat_bc$store_trans,}\DataTypeTok{main=}\StringTok{"Before Transformation"}\NormalTok{,}\DataTypeTok{xlab=}\StringTok{"store_trans"}\NormalTok{)}
\KeywordTok{hist}\NormalTok{(transformed$store_trans,}\DataTypeTok{main=}\StringTok{"After Transformation"}\NormalTok{,}\DataTypeTok{xlab=}\StringTok{"store_trans"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{IDS_files/figure-latex/bc-1} 

}

\caption{Box-Cox Transformation}\label{fig:bc}
\end{figure}

Before the transformation, the \texttt{stroe\_trans} is skewed right.
The situation is significantly improved after (figure\ref{fig:bc}).
\texttt{BoxCoxTrans\ ()} can also conduct Box-Cox transform. But note
that \texttt{BoxCoxTrans\ ()} can only be applied to a single variable,
and it is not possible to transform difference columns in a data frame
at the same time.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(trans<-}\KeywordTok{BoxCoxTrans}\NormalTok{(dat_bc$store_trans))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Box-Cox Transformation
## 
## 1000 data points used to estimate Lambda
## 
## Input data summary:
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##    1.00    3.00    4.00    5.35    7.00   20.00 
## 
## Largest/Smallest: 20 
## Sample Skewness: 1.11 
## 
## Estimated Lambda: 0.1 
## With fudge factor, Lambda = 0 will be used for transformations
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{transformed<-}\KeywordTok{predict}\NormalTok{(trans,dat_bc$store_trans)}
\KeywordTok{skewness}\NormalTok{(transformed)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] -0.2155
\end{verbatim}

The estimate of \(\lambda\) is the same as before (0.1). The skewness of
the original observation is 1.1, and -0.2 after transformation. Although
it is not strictly 0, it is greatly improved.

\section{Resolve Outliers}\label{resolve-outliers}

Even under certain assumptions we can statistically define outliers, it
can be hard to define in some situations. Box plot, histogram and some
other basic visualizations can be used to initially check whether there
are outliers. For example, we can visualize numerical non-survey
variables in \texttt{sim.dat}:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# select numerical non-survey data}
\NormalTok{sdat<-}\KeywordTok{subset}\NormalTok{(sim.dat,}\DataTypeTok{select=}\KeywordTok{c}\NormalTok{(}\StringTok{"age"}\NormalTok{,}\StringTok{"income"}\NormalTok{,}\StringTok{"store_exp"}\NormalTok{,}\StringTok{"online_exp"}\NormalTok{,}\StringTok{"store_trans"}\NormalTok{,}\StringTok{"online_trans"} \NormalTok{))}
\CommentTok{# use scatterplotMatrix() function from car package}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{oma=}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\KeywordTok{scatterplotMatrix}\NormalTok{(sdat,}\DataTypeTok{diagonal=}\StringTok{"boxplot"}\NormalTok{,}\DataTypeTok{smoother=}\OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{IDS_files/figure-latex/scm-1} 

}

\caption{Use basic visualization to check outliers}\label{fig:scm}
\end{figure}

As figure \ref{fig:scm} shows, \texttt{store\_exp} has outliers. It is
also easy to observe the pair relationship from the plot. \texttt{age}
is negatively correlated with \texttt{online\_trans} but positively
correlated with \texttt{store\_trans}. It seems that older people tend
to purchase from the local store. The amount of expense is positively
correlated with income. Scatterplot matrix like this can reveal lots of
information before modeling.

In addition to visualization, there are some statistical methods to
define outliers, such as the commonly used Z-score. The Z-score for
variable \(\mathbf{Y}\) is defined as:

\[Z_{i}=\frac{Y_{i}-\bar{Y}}{s}\]

where \(\bar{Y}\) and \(s\) are mean and standard deviation for \(Y\).
Z-score is a measurement of the distance between each observation and
the mean. This method may be misleading, especially when the sample size
is small. Iglewicz and Hoaglin proposed to use the modified Z-score to
determine the outlier\citep{mad1}：

\[M_{i}=\frac{0.6745(Y_{i}-\bar{Y})}{MAD}\]

Where MAD is the median of a series of \(|Y_ {i} - \bar{Y}|\), called
the median of the absolute dispersion. Iglewicz and Hoaglin suggest that
the points with the Z-score greater than 3.5 corrected above are
possible outliers. Let's apply it to \texttt{income}:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# calculate median of the absolute dispersion for income}
\NormalTok{ymad<-}\KeywordTok{mad}\NormalTok{(}\KeywordTok{na.omit}\NormalTok{(sdat$income))}
\CommentTok{# calculate z-score}
\NormalTok{zs<-(sdat$income-}\KeywordTok{mean}\NormalTok{(}\KeywordTok{na.omit}\NormalTok{(sdat$income)))/ymad}
\CommentTok{# count the number of outliers}
\KeywordTok{sum}\NormalTok{(}\KeywordTok{na.omit}\NormalTok{(zs>}\FloatTok{3.5}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 59
\end{verbatim}

According to modified Z-score, variable income has 59 outliers. Refer to
\citep{mad1} for other ways of detecting outliers.

The impact of outliers depends on the model. Some models are sensitive
to outliers, such as linear regression, logistic regression. Some are
pretty robust to outliers, such as tree models, support vector machine.
Also, the outlier is not wrong data. It is real observation so cannot be
deleted at will. If a model is sensitive to outliers, we can use
\emph{spatial sign transformation} \citep{ssp} to minimize the problem.
It projects the original sample points to the surface of a sphere by:

\[x_{ij}^{*}=\frac{x_{ij}}{\sqrt{\sum_{j=1}^{p}x_{ij}^{2}}}\]

where \(x_{ij}\) represents the \(i^{th}\) observation and \(j^{th}\)
variable. As shown in the equation, every observation for sample \(i\)
is divided by its square mode. The denominator is the Euclidean distance
to the center of the p-dimensional predictor space. Three things to pay
attention here:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  It is important to center and scale the predictor data before using
  this transformation
\item
  Unlike centering or scaling, this manipulation of the predictors
  transforms them as a group
\item
  If there are some variables to remove (for example, highly correlated
  variables), do it before the transformation
\end{enumerate}

Function \texttt{spatialSign()} \texttt{caret} package can conduct the
transformation. Take \texttt{income} and \texttt{age} as an example:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# KNN imputation}
\NormalTok{sdat<-sim.dat[,}\KeywordTok{c}\NormalTok{(}\StringTok{"income"}\NormalTok{,}\StringTok{"age"}\NormalTok{)]}
\NormalTok{imp<-}\KeywordTok{preProcess}\NormalTok{(sdat,}\DataTypeTok{method=}\KeywordTok{c}\NormalTok{(}\StringTok{"knnImpute"}\NormalTok{),}\DataTypeTok{k=}\DecValTok{5}\NormalTok{)}
\NormalTok{sdat<-}\KeywordTok{predict}\NormalTok{(imp,sdat)}
\NormalTok{transformed <-}\StringTok{ }\KeywordTok{spatialSign}\NormalTok{(sdat)}
\NormalTok{transformed <-}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{(transformed)}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{),}\DataTypeTok{oma=}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\KeywordTok{plot}\NormalTok{(income ~}\StringTok{ }\NormalTok{age,}\DataTypeTok{data =} \NormalTok{sdat,}\DataTypeTok{col=}\StringTok{"blue"}\NormalTok{,}\DataTypeTok{main=}\StringTok{"Before"}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(income ~}\StringTok{ }\NormalTok{age,}\DataTypeTok{data =} \NormalTok{transformed,}\DataTypeTok{col=}\StringTok{"blue"}\NormalTok{,}\DataTypeTok{main=}\StringTok{"After"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{IDS_files/figure-latex/sst-1} 

}

\caption{Spatial sign transformation}\label{fig:sst}
\end{figure}

Some readers may have found that the above code does not seem to
standardize the data before transformation. Recall the introduction of
KNN, \texttt{preProcess()} with \texttt{method="knnImpute"} by default
will standardize data.

\section{Collinearity}\label{collinearity}

It is probably the technical term known by the most un-technical people.
When two predictors are very strongly correlated, including both in a
model may lead to confusion or problem with a singular matrix. There is
an excellent function in \texttt{corrplot} package with the same name
\texttt{corrplot()} that can visualize correlation structure of a set of
predictors. The function has the option to reorder the variables in a
way that reveals clusters of highly correlated ones.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# select non-survey numerical variables}
\NormalTok{sdat<-}\KeywordTok{subset}\NormalTok{(sim.dat,}\DataTypeTok{select=}\KeywordTok{c}\NormalTok{(}\StringTok{"age"}\NormalTok{,}\StringTok{"income"}\NormalTok{,}\StringTok{"store_exp"}\NormalTok{,}\StringTok{"online_exp"}\NormalTok{,}\StringTok{"store_trans"}\NormalTok{,}\StringTok{"online_trans"} \NormalTok{))}
\CommentTok{# use bagging imputation here}
\NormalTok{imp<-}\KeywordTok{preProcess}\NormalTok{(sdat,}\DataTypeTok{method=}\StringTok{"bagImpute"}\NormalTok{)}
\NormalTok{sdat<-}\KeywordTok{predict}\NormalTok{(imp,sdat)}
\CommentTok{# get the correlation matrix}
\NormalTok{correlation<-}\KeywordTok{cor}\NormalTok{(sdat)}
\CommentTok{# plot }
\KeywordTok{par}\NormalTok{(}\DataTypeTok{oma=}\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\KeywordTok{corrplot.mixed}\NormalTok{(correlation,}\DataTypeTok{order=}\StringTok{"hclust"}\NormalTok{,}\DataTypeTok{tl.pos=}\StringTok{"lt"}\NormalTok{,}\DataTypeTok{upper=}\StringTok{"ellipse"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{IDS_files/figure-latex/corp-1} 

}

\caption{Correlation Matrix}\label{fig:corp}
\end{figure}

Here use \texttt{corrplot.mixed()} function to visualize the correlation
matrix (figure \ref{fig:corp}). The closer the correlation is to 0, the
lighter the color is and the closer the shape is to a circle. The
elliptical means the correlation is not equal to 0 (because we set the
\texttt{upper\ =\ "ellipse"}), the greater the correlation, the narrower
the ellipse. Blue represents a positive correlation; red represents a
negative correlation. The direction of the ellipse also changes with the
correlation. The correlation coefficient is shown in the lower triangle
of the matrix. The variables relationship from previous scatter matrix
(figure @ref(fig: scm)) are clear here: the negative correlation between
age and online shopping, the positive correlation between income and
amount of purchasing. Some correlation is very strong ( such as the
correlation between \texttt{online\_trans} and\texttt{age} is -0.85)
which means the two variables contain duplicate information.

Section 3.5 of ``Applied Predictive Modeling'' \citep{APM} presents a
heuristic algorithm to remove a minimum number of predictors to ensure
all pairwise correlations are below a certain threshold:

\begin{quote}
\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\tightlist
\item
  Calculate the correlation matrix of the predictors.
\item
  Determine the two predictors associated with the largest absolute
  pairwise correlation (call them predictors A and B).
\item
  Determine the average correlation between A and the other variables.
  Do the same for predictor B.
\item
  If A has a larger average correlation, remove it; otherwise, remove
  predictor B.
\item
  Repeat Step 2-4 until no absolute correlations are above the
  threshold.
\end{enumerate}
\end{quote}

The \texttt{findCorrelation()} function in package \texttt{caret} will
apply the above algorithm.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(highCorr<-}\KeywordTok{findCorrelation}\NormalTok{(}\KeywordTok{cor}\NormalTok{(sdat),}\DataTypeTok{cutoff=}\NormalTok{.}\DecValTok{75}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1
\end{verbatim}

It returns the index of columns need to be deleted. It tells us that we
need to remove the first column to make sure the correlations are all
below 0.75.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# delete highly correlated columns}
\NormalTok{sdat<-sdat[-highCorr]}
\CommentTok{# check the new correlation matrix}
\KeywordTok{cor}\NormalTok{(sdat)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##               income store_exp online_exp store_trans
## income        1.0000    0.6004     0.5199      0.7070
## store_exp     0.6004    1.0000     0.5350      0.5399
## online_exp    0.5199    0.5350     1.0000      0.4421
## store_trans   0.7070    0.5399     0.4421      1.0000
## online_trans -0.3573   -0.1367     0.2256     -0.4368
##              online_trans
## income            -0.3573
## store_exp         -0.1367
## online_exp         0.2256
## store_trans       -0.4368
## online_trans       1.0000
\end{verbatim}

The absolute value of the elements in the correlation matrix after
removal are all below 0.75. How strong does a correlation have to get,
before you should start worrying about multicollinearity? There is no
easy answer to that question. You can treat the threshold as a tuning
parameter and pick one that gives you best prediction accuracy.

\section{Sparse Variables}\label{sparse-variables}

Other than the highly related predictors, predictors with degenerate
distributions can cause the problem too. Removing those variables can
significantly improve some models' performance and stability (such as
linear regression and logistic regression but the tree based model is
impervious to this type of predictors). One extreme example is a
variable with a single value which is called zero-variance variable.
Variables with very low frequency of unique values are near-zero
variance predictors. In general, detecting those variables follows two
rules:

\begin{itemize}
\tightlist
\item
  The fraction of unique values over the sample size
\item
  The ratio of the frequency of the most prevalent value to the
  frequency of the second most prevalent value.
\end{itemize}

\texttt{nearZeroVar()} function in the \texttt{caret} package can filter
near-zero variance predictors according to the above rules. In order to
show the useage of the function, let's arbitaryly add some problematic
variables to the origional data \texttt{sim.dat}:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# make a copy}
\NormalTok{zero_demo<-sim.dat}
\CommentTok{# add two sparse variable}
\CommentTok{# zero1 only has one unique value}
\CommentTok{# zero2 is a vector with the first element 1 and the rest are 0s}
\NormalTok{zero_demo$zero1<-}\KeywordTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{,}\KeywordTok{nrow}\NormalTok{(zero_demo))}
\NormalTok{zero_demo$zero2<-}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\KeywordTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{,}\KeywordTok{nrow}\NormalTok{(zero_demo)-}\DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

The function will return a vector of integers indicating which columns
to remove:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{nearZeroVar}\NormalTok{(zero_demo,}\DataTypeTok{freqCut =} \DecValTok{95}\NormalTok{/}\DecValTok{5}\NormalTok{, }\DataTypeTok{uniqueCut =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

As expected, it returns the two columns we generated. You can go ahead
to remove them. Note the two arguments in the function
\texttt{freqCut\ =} and \texttt{uniqueCut\ =} are corresponding to the
previous two rules.

\begin{itemize}
\tightlist
\item
  \texttt{freqCut}: the cutoff for the ratio of the most common value to
  the second most common value
\item
  \texttt{uniqueCut}: the cutoff for the percentage of distinct values
  out of the number of total samples
\end{itemize}

\section{Re-encode Dummy Variables}\label{re-encode-dummy-variables}

A dummy variable is a binary variable (0/1) to represent subgroups of
the sample. Sometimes we need to recode categories to smaller bits of
information named ``dummy variables.'' For example, some questionnaires
have five options for each question, A, B, C, D, and E. After you get
the data, you will usually convert the corresponding categorical
variables for each question into five nominal variables, and then use
one of the options as the baseline.

Let's encode \texttt{gender} and \texttt{house} from \texttt{sim.dat} to
dummy variables. There are two ways to implement this. The first is to
use \texttt{class.ind()} from \texttt{nnet} package. However, it only
works on one variable at a time.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dumVar<-nnet::}\KeywordTok{class.ind}\NormalTok{(sim.dat$gender)}
\KeywordTok{head}\NormalTok{(dumVar)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      Female Male
## [1,]      1    0
## [2,]      1    0
## [3,]      0    1
## [4,]      0    1
## [5,]      0    1
## [6,]      0    1
\end{verbatim}

Since it is redundant to keep both, we need to remove one of them when
modeling. Another more powerful function is \texttt{dummyVars()} from
\texttt{caret}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dumMod<-}\KeywordTok{dummyVars}\NormalTok{(~gender+house+income,}
                  \DataTypeTok{data=}\NormalTok{sim.dat,}
                  \CommentTok{# use "origional variable name + level" as new name}
                  \DataTypeTok{levelsOnly=}\NormalTok{F)}
\KeywordTok{head}\NormalTok{(}\KeywordTok{predict}\NormalTok{(dumMod,sim.dat))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   gender.Female gender.Male house.No house.Yes income
## 1             1           0        0         1 120963
## 2             1           0        0         1 122008
## 3             0           1        0         1 114202
## 4             0           1        0         1 113616
## 5             0           1        0         1 124253
## 6             0           1        0         1 107661
\end{verbatim}

\texttt{dummyVars()} can also use formula format. The variable on the
right-hand side can be both categorical and numeric. For a numerical
variable, the function will keep the variable unchanged. The advantage
is that you can apply the function to a data frame without removing
numerical variables. Other than that, the function can create
interaction term:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dumMod<-}\KeywordTok{dummyVars}\NormalTok{(~gender+house+income+income:gender,}
                  \DataTypeTok{data=}\NormalTok{sim.dat,}
                  \DataTypeTok{levelsOnly=}\NormalTok{F)}
\KeywordTok{head}\NormalTok{(}\KeywordTok{predict}\NormalTok{(dumMod,sim.dat))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   gender.Female gender.Male house.No house.Yes income
## 1             1           0        0         1 120963
## 2             1           0        0         1 122008
## 3             0           1        0         1 114202
## 4             0           1        0         1 113616
## 5             0           1        0         1 124253
## 6             0           1        0         1 107661
##   gender.Female:income gender.Male:income
## 1               120963                  0
## 2               122008                  0
## 3                    0             114202
## 4                    0             113616
## 5                    0             124253
## 6                    0             107661
\end{verbatim}

If you think the impact income levels on purchasing behavior is
different for male and female, then you may add the interaction term
between \texttt{income} and \texttt{gender}. You can do this by adding
\texttt{income:\ gender} in the formula.

\section{Python Computing}\label{python-computing}

\textbf{Environmental Setup}

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from} \NormalTok{IPython.core.interactiveshell }\ImportTok{import} \NormalTok{InteractiveShell}
\NormalTok{InteractiveShell.ast_node_interactivity }\OperatorTok{=} \StringTok{"all"}

\ImportTok{import} \NormalTok{numpy }\ImportTok{as} \NormalTok{np}
\ImportTok{import} \NormalTok{scipy }\ImportTok{as} \NormalTok{sp}
\ImportTok{import} \NormalTok{pandas }\ImportTok{as} \NormalTok{pd}
\ImportTok{import} \NormalTok{math}

\ImportTok{from} \NormalTok{sklearn.preprocessing }\ImportTok{import} \NormalTok{Imputer}
\ImportTok{from} \NormalTok{sklearn.preprocessing }\ImportTok{import} \NormalTok{StandardScaler}

\ImportTok{from} \NormalTok{pandas.plotting }\ImportTok{import} \NormalTok{scatter_matrix}
\ImportTok{import} \NormalTok{matplotlib.pyplot }\ImportTok{as} \NormalTok{plt}
\end{Highlighting}
\end{Shaded}

\subsection{Data Cleaning}\label{data-cleaning-1}

\chapter{Data Wrangling}\label{data-wrangling}

This chapter focuses on some of the most frequently used data
manipulations and shows how to implement them in R and Python. It is
critical to explore the data with descriptive statistics (mean, standard
deviation, etc.) and data visualization before analysis. Transform data
so that the data structure is in line with the requirements of the
model. You also need to summarize the results after analysis.

\section{Data Wrangling Using R}\label{data-wrangling-using-r}

\subsection{Read and write data}\label{read-and-write-data}

\subsubsection{\texorpdfstring{\texttt{readr}}{readr}}\label{readr}

You must be familiar with \texttt{read.csv()}, \texttt{read.table()} and
\texttt{write.csv()} in base R. Here we will introduce a more efficient
package from RStudio in 2015 for reading and writing data:
\texttt{readr} package. The corresponding functions are
\texttt{read\_csv()}, \texttt{read\_table()} and \texttt{write\_csv()}.
The commands look quite similar, but \texttt{readr} is different in the
following respects:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  It is 10x faster. The trick is that \texttt{readr} uses C++ to process
  the data quickly.
\item
  It doesn't change the column names. The names can start with a number
  and ``\texttt{.}'' will not be substituted to ``\texttt{\_}''. For
  example:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(readr)}
\KeywordTok{read_csv}\NormalTok{(}\StringTok{"2015,2016,2017}
\StringTok{1,2,3}
\StringTok{4,5,6"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 2 x 3
##   `2015` `2016` `2017`
##    <int>  <int>  <int>
## 1      1      2      3
## 2      4      5      6
\end{verbatim}
\item
  \texttt{readr} functions do not convert strings to factors by default,
  are able to parse dates and times and can automatically determine the
  data types in each column.
\item
  The killing character, in my opinion, is that \texttt{readr} provides
  \textbf{progress bar}. What makes you feel worse than waiting is not
  knowing how long you have to wait.
\end{enumerate}

\includegraphics{images/prograssbar.png}~

The major functions of readr is to turn flat files into data frames:

\begin{itemize}
\tightlist
\item
  \texttt{read\_csv()}: reads comma delimited files
\item
  \texttt{read\_csv2()}: reads semicolon separated files (common in
  countries where \texttt{,} is used as the decimal place)
\item
  \texttt{read\_tsv()}: reads tab delimited files
\item
  \texttt{read\_delim()}: reads in files with any delimiter
\item
  \texttt{read\_fwf()}: reads fixed width files. You can specify fields
  either by their widths with \texttt{fwf\_widths()} or their position
  with \texttt{fwf\_positions()}\\
\item
  \texttt{read\_table()}: reads a common variation of fixed width files
  where columns are separated by white space
\item
  \texttt{read\_log()}: reads Apache style log files
\end{itemize}

The good thing is that those functions have similar syntax. Once you
learn one, the others become easy. Here we will focus on
\texttt{read\_csv()}.

The most important information for \texttt{read\_csv()} is the path to
your data:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(readr)}
\NormalTok{sim.dat <-}\StringTok{ }\KeywordTok{read_csv}\NormalTok{(}\StringTok{"https://raw.githubusercontent.com/happyrabbit/DataScientistR/master/Data/SegData.csv "}\NormalTok{)}
\KeywordTok{head}\NormalTok{(sim.dat)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 19
##     age gender income house store_exp online_exp
##   <int>  <chr>  <dbl> <chr>     <dbl>      <dbl>
## 1    57 Female 120963   Yes     529.1      303.5
## 2    63 Female 122008   Yes     478.0      109.5
## 3    59   Male 114202   Yes     490.8      279.2
## 4    60   Male 113616   Yes     347.8      141.7
## 5    51   Male 124253   Yes     379.6      112.2
## 6    59   Male 107661   Yes     338.3      195.7
## # ... with 13 more variables: store_trans <int>,
## #   online_trans <int>, Q1 <int>, Q2 <int>, Q3 <int>,
## #   Q4 <int>, Q5 <int>, Q6 <int>, Q7 <int>, Q8 <int>,
## #   Q9 <int>, Q10 <int>, segment <chr>
\end{verbatim}

The function reads the file to R as a \texttt{tibble}. You can consider
\texttt{tibble} as next iteration of the data frame. They are different
with data frame for the following aspects:

\begin{itemize}
\tightlist
\item
  It never changes an input's type (i.e., no more
  \texttt{stringsAsFactors\ =\ FALSE}!)
\item
  It never adjusts the names of variables
\item
  It has a refined print method that shows only the first 10 rows and
  all the columns that fit on the screen. You can also control the
  default print behavior by setting options.
\end{itemize}

Refer to \url{http://r4ds.had.co.nz/tibbles.html} for more information
about `tibble'.

When you run \texttt{read\_csv()} it prints out a column specification
that gives the name and type of each column. To better understanding how
\texttt{readr} works, it is helpful to type in some baby data set and
check the results:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dat=}\KeywordTok{read_csv}\NormalTok{(}\StringTok{"2015,2016,2017}
\StringTok{100,200,300}
\StringTok{canola,soybean,corn"}\NormalTok{)}
\KeywordTok{print}\NormalTok{(dat)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 2 x 3
##   `2015`  `2016` `2017`
##    <chr>   <chr>  <chr>
## 1    100     200    300
## 2 canola soybean   corn
\end{verbatim}

You can also add comments on the top and tell R to skip those lines:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dat=}\KeywordTok{read_csv}\NormalTok{(}\StringTok{"# I will never let you know that}
\StringTok{          # my favorite food is carrot}
\StringTok{          Date,Food,Mood}
\StringTok{          Monday,carrot,happy}
\StringTok{          Tuesday,carrot,happy}
\StringTok{          Wednesday,carrot,happy}
\StringTok{          Thursday,carrot,happy}
\StringTok{          Friday,carrot,happy}
\StringTok{          Saturday,carrot,extremely happy}
\StringTok{          Sunday,carrot,extremely happy"}\NormalTok{, }\DataTypeTok{skip =} \DecValTok{2}\NormalTok{)}
\KeywordTok{print}\NormalTok{(dat)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 7 x 3
##        Date   Food            Mood
##       <chr>  <chr>           <chr>
## 1    Monday carrot           happy
## 2   Tuesday carrot           happy
## 3 Wednesday carrot           happy
## 4  Thursday carrot           happy
## 5    Friday carrot           happy
## 6  Saturday carrot extremely happy
## 7    Sunday carrot extremely happy
\end{verbatim}

If you don't have column names, set \texttt{col\_names\ =\ FALSE} then R
will assign names ``\texttt{X1}'',``\texttt{X2}''\ldots{} to the
columns:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dat=}\KeywordTok{read_csv}\NormalTok{(}\StringTok{"Saturday,carrot,extremely happy}
\StringTok{          Sunday,carrot,extremely happy"}\NormalTok{, }\DataTypeTok{col_names=}\OtherTok{FALSE}\NormalTok{)}
\KeywordTok{print}\NormalTok{(dat)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 2 x 3
##         X1     X2              X3
##      <chr>  <chr>           <chr>
## 1 Saturday carrot extremely happy
## 2   Sunday carrot extremely happy
\end{verbatim}

You can also pass \texttt{col\_names} a character vector which will be
used as the column names. Try to replace \texttt{col\_names=FALSE} with
\texttt{col\_names=c("Date","Food","Mood")} and see what happen.

As mentioned before, you can use \texttt{read\_csv2()} to read semicolon
separated files:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dat=}\KeywordTok{read_csv2}\NormalTok{(}\StringTok{"Saturday; carrot; extremely happy }\CharTok{\textbackslash{}n}\StringTok{ Sunday; carrot; extremely happy"}\NormalTok{, }\DataTypeTok{col_names=}\OtherTok{FALSE}\NormalTok{)}
\KeywordTok{print}\NormalTok{(dat)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 2 x 3
##         X1     X2              X3
##      <chr>  <chr>           <chr>
## 1 Saturday carrot extremely happy
## 2   Sunday carrot extremely happy
\end{verbatim}

Here ``\texttt{\textbackslash{}n}'' is a convenient shortcut for adding
a new line.

You can use \texttt{read\_tsv()} to read tab delimited files：

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dat=}\KeywordTok{read_tsv}\NormalTok{(}\StringTok{"every}\CharTok{\textbackslash{}t}\StringTok{man}\CharTok{\textbackslash{}t}\StringTok{is}\CharTok{\textbackslash{}t}\StringTok{a}\CharTok{\textbackslash{}t}\StringTok{poet}\CharTok{\textbackslash{}t}\StringTok{when}\CharTok{\textbackslash{}t}\StringTok{he}\CharTok{\textbackslash{}t}\StringTok{is}\CharTok{\textbackslash{}t}\StringTok{in}\CharTok{\textbackslash{}t}\StringTok{love}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{, }\DataTypeTok{col_names =} \OtherTok{FALSE}\NormalTok{)}
\KeywordTok{print}\NormalTok{(dat)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 10
##      X1    X2    X3    X4    X5    X6    X7    X8
##   <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr>
## 1 every   man    is     a  poet  when    he    is
## # ... with 2 more variables: X9 <chr>, X10 <chr>
\end{verbatim}

Or more generally, you can use \texttt{read\_delim()} and assign
separating character：

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dat=}\KeywordTok{read_delim}\NormalTok{(}\StringTok{"THE|UNBEARABLE|RANDOMNESS|OF|LIFE}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{, }\DataTypeTok{delim =} \StringTok{"|"}\NormalTok{, }\DataTypeTok{col_names =} \OtherTok{FALSE}\NormalTok{)}
\KeywordTok{print}\NormalTok{(dat)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 5
##      X1         X2         X3    X4    X5
##   <chr>      <chr>      <chr> <chr> <chr>
## 1   THE UNBEARABLE RANDOMNESS    OF  LIFE
\end{verbatim}

Another situation you will often run into is the missing value. In
marketing survey, people like to use ``99'' to represent missing. You
can tell R to set all observation with value ``99'' as missing when you
read the data:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dat=}\KeywordTok{read_csv}\NormalTok{(}\StringTok{"Q1,Q2,Q3}
\StringTok{               5, 4,99"}\NormalTok{,}\DataTypeTok{na=}\StringTok{"99"}\NormalTok{)}
\KeywordTok{print}\NormalTok{(dat)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 3
##      Q1    Q2    Q3
##   <int> <int> <chr>
## 1     5     4  <NA>
\end{verbatim}

For writing data back to disk, you can use \texttt{write\_csv()} and
\texttt{write\_tsv()}. The following two characters of the two functions
increase the chances of the output file being read back in correctly:

\begin{itemize}
\tightlist
\item
  Encode strings in UTF-8
\item
  Save dates and date-times in ISO8601 format so they are easily parsed
  elsewhere
\end{itemize}

For example:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{write_csv}\NormalTok{(sim.dat, }\StringTok{"sim_dat.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

For other data types, you can use the following packages:

\begin{itemize}
\tightlist
\item
  \texttt{Haven}: SPSS, Stata and SAS data
\item
  \texttt{Readxl} and \texttt{xlsx}: excel data(.xls and .xlsx)
\item
  \texttt{DBI}: given data base, such as RMySQL, RSQLite and
  RPostgreSQL, read data directly from the database using SQL
\end{itemize}

Some other useful materials:

\begin{itemize}
\tightlist
\item
  For getting data from the internet, you can refer to the book ``XML
  and Web Technologies for Data Sciences with R''.\\
\item
  \href{https://cran.r-project.org/doc/manuals/r-release/R-data.html\#Acknowledgements}{R
  data import/export manual}
\item
  \texttt{rio} package：\url{https://github.com/leeper/rio}
\end{itemize}

\subsubsection{\texorpdfstring{\texttt{data.table}--- enhanced
\texttt{data.frame}}{data.table--- enhanced data.frame}}\label{data.table-enhanced-data.frame}

What is \texttt{data.table}? It is an R package that provides an
enhanced version of \texttt{data.frame}. The most used object in R is
\texttt{data\ frame}. Before we move on, let's briefly review some basic
characters and manipulations of data.frame:

\begin{itemize}
\tightlist
\item
  It is a set of rows and columns.
\item
  Each row is of the same length and data type
\item
  Every column is of the same length but can be of differing data types
\item
  It has characteristics of both a matrix and a list
\item
  It uses \texttt{{[}{]}} to subset data
\end{itemize}

We will use the clothes customer data to illustrate. There are two
dimensions in \texttt{{[}{]}}. The first one indicates the row and
second one indicates column. It uses a comma to separate them.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# read data}
\NormalTok{sim.dat<-readr::}\KeywordTok{read_csv}\NormalTok{(}\StringTok{"https://raw.githubusercontent.com/happyrabbit/DataScientistR/master/Data/SegData.csv"}\NormalTok{)}
\CommentTok{# subset the first two rows}
\NormalTok{sim.dat[}\DecValTok{1}\NormalTok{:}\DecValTok{2}\NormalTok{,]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 2 x 19
##     age gender income house store_exp online_exp
##   <int>  <chr>  <dbl> <chr>     <dbl>      <dbl>
## 1    57 Female 120963   Yes     529.1      303.5
## 2    63 Female 122008   Yes     478.0      109.5
## # ... with 13 more variables: store_trans <int>,
## #   online_trans <int>, Q1 <int>, Q2 <int>, Q3 <int>,
## #   Q4 <int>, Q5 <int>, Q6 <int>, Q7 <int>, Q8 <int>,
## #   Q9 <int>, Q10 <int>, segment <chr>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# subset the first two rows and column 3 and 5}
\NormalTok{sim.dat[}\DecValTok{1}\NormalTok{:}\DecValTok{2}\NormalTok{,}\KeywordTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{,}\DecValTok{5}\NormalTok{)]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 2 x 2
##   income store_exp
##    <dbl>     <dbl>
## 1 120963     529.1
## 2 122008     478.0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# get all rows with age>70}
\NormalTok{sim.dat[sim.dat$age>}\DecValTok{70}\NormalTok{,]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 19
##     age gender income house store_exp online_exp
##   <int>  <chr>  <dbl> <chr>     <dbl>      <dbl>
## 1   300   Male 208017   Yes      5077       6053
## # ... with 13 more variables: store_trans <int>,
## #   online_trans <int>, Q1 <int>, Q2 <int>, Q3 <int>,
## #   Q4 <int>, Q5 <int>, Q6 <int>, Q7 <int>, Q8 <int>,
## #   Q9 <int>, Q10 <int>, segment <chr>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# get rows with age> 60 and gender is Male}
\CommentTok{# select column 3 and 4}
\NormalTok{sim.dat[sim.dat$age>}\DecValTok{68} \NormalTok{&}\StringTok{ }\NormalTok{sim.dat$gender ==}\StringTok{ "Male"}\NormalTok{, }\DecValTok{3}\NormalTok{:}\DecValTok{4}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 2 x 2
##   income house
##    <dbl> <chr>
## 1 119552    No
## 2 208017   Yes
\end{verbatim}

Remember that there are usually different ways to conduct the same
manipulation. For example, the following code presents three ways to
calculate an average number of online transactions for male and female:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{tapply}\NormalTok{(sim.dat$online_trans, sim.dat$gender, mean )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Female   Male 
##  15.38  11.26
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{aggregate}\NormalTok{(online_trans ~}\StringTok{ }\NormalTok{gender, }\DataTypeTok{data =} \NormalTok{sim.dat, mean)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   gender online_trans
## 1 Female        15.38
## 2   Male        11.26
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(dplyr)}
\NormalTok{sim.dat%>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(gender)%>%}
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{Avg_online_trans=}\KeywordTok{mean}\NormalTok{(online_trans))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 2 x 2
##   gender Avg_online_trans
##    <chr>            <dbl>
## 1 Female            15.38
## 2   Male            11.26
\end{verbatim}

There is no gold standard to choose a specific function to manipulate
data. The goal is to solve the real problem, not the tool itself. So
just use whatever tool that is convenient for you.

The way to use \texttt{{[}{]}} is straightforward. But the manipulations
are limited. If you need more complicated data reshaping or aggregation,
there are other packages to use such as \texttt{dplyr},
\texttt{reshape2}, \texttt{tidyr} etc. But the usage of those packages
are not as straightforward as \texttt{{[}{]}}. You often need to change
functions. Keeping related operations together, such as subset, group,
update, join etc, will allow for:

\begin{itemize}
\tightlist
\item
  concise, consistent and readable syntax irrespective of the set of
  operations you would like to perform to achieve your end goal
\item
  performing data manipulation fluidly without the cognitive burden of
  having to change among different functions
\item
  by knowing precisely the data required for each operation, you can
  automatically optimize operations effectively
\end{itemize}

\texttt{data.table} is the package for that. If you are not familiar
with other data manipulating packages and are interested in reducing
programming time tremendously, then this package is for you.

Other than extending the function of \texttt{{[}{]}},
\texttt{data.table} has the following advantages:

Offers fast import, subset, grouping, update, and joins for large data
files It is easy to turn data frame to data table Can behave just like a
data frame

You need to install and load the package:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# If you haven't install it, use the code to instal}
\CommentTok{# install.packages("data.table")}
\CommentTok{# load packagw}
\KeywordTok{library}\NormalTok{(data.table)}
\end{Highlighting}
\end{Shaded}

Use \texttt{data.table()} to covert the existing data frame
\texttt{sim.dat} to data table:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dt <-}\StringTok{ }\KeywordTok{data.table}\NormalTok{(sim.dat)}
\KeywordTok{class}\NormalTok{(dt)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "data.table" "data.frame"
\end{verbatim}

Calculate mean for counts of online transactions:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dt[, }\KeywordTok{mean}\NormalTok{(online_trans)]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 13.55
\end{verbatim}

You can't do the same thing using data frame:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sim.dat[,}\KeywordTok{mean}\NormalTok{(online_trans)]}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Error in mean(online_trans) : object 'online_trans' not found}
\end{Highlighting}
\end{Shaded}

If you want to calculate mean by group as before, set ``\texttt{by\ =}''
argument:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dt[ , }\KeywordTok{mean}\NormalTok{(online_trans), by =}\StringTok{ }\NormalTok{gender]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    gender    V1
## 1: Female 15.38
## 2:   Male 11.26
\end{verbatim}

You can group by more than one variables. For example, group by
``\texttt{gender}'' and ``\texttt{house}'':

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dt[ , }\KeywordTok{mean}\NormalTok{(online_trans), by =}\StringTok{ }\NormalTok{.(gender, house)]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    gender house     V1
## 1: Female   Yes 11.312
## 2:   Male   Yes  8.772
## 3: Female    No 19.146
## 4:   Male    No 16.486
\end{verbatim}

Assign column names for aggregated variables:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dt[ , .(}\DataTypeTok{avg =} \KeywordTok{mean}\NormalTok{(online_trans)), by =}\StringTok{ }\NormalTok{.(gender, house)]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    gender house    avg
## 1: Female   Yes 11.312
## 2:   Male   Yes  8.772
## 3: Female    No 19.146
## 4:   Male    No 16.486
\end{verbatim}

\texttt{data.table} can accomplish all operations that
\texttt{aggregate()} and \texttt{tapply()}can do for data frame.

\begin{itemize}
\tightlist
\item
  General setting of \texttt{data.table}
\end{itemize}

Different from data frame, there are three arguments for data table:

\includegraphics{images/datable1.png}

It is analogous to SQL. You don't have to know SQL to learn data table.
But experience with SQL will help you understand data table. In SQL, you
select column \texttt{j} (use command \texttt{SELECT}) for row
\texttt{i} (using command \texttt{WHERE}). \texttt{GROUP\ BY} in SQL
will assign the variable to group the observations.

\includegraphics{images/rSQL.png}

Let's review our previous code:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dt[ , }\KeywordTok{mean}\NormalTok{(online_trans), by =}\StringTok{ }\NormalTok{gender]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    gender    V1
## 1: Female 15.38
## 2:   Male 11.26
\end{verbatim}

The code above is equal to the following SQL：

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{SELECT}  \NormalTok{gender, }\FunctionTok{avg}\NormalTok{(online_trans) }\KeywordTok{FROM} \NormalTok{sim.dat }\KeywordTok{GROUP} \KeywordTok{BY} \NormalTok{gender}
\end{Highlighting}
\end{Shaded}

R code:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dt[ , .(}\DataTypeTok{avg =} \KeywordTok{mean}\NormalTok{(online_trans)), by =}\StringTok{ }\NormalTok{.(gender, house)]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    gender house    avg
## 1: Female   Yes 11.312
## 2:   Male   Yes  8.772
## 3: Female    No 19.146
## 4:   Male    No 16.486
\end{verbatim}

is equal to SQL：

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{SELECT} \NormalTok{gender, house, }\FunctionTok{avg}\NormalTok{(online_trans) }\KeywordTok{AS} \FunctionTok{avg} \KeywordTok{FROM} \NormalTok{sim.dat }\KeywordTok{GROUP} \KeywordTok{BY} \NormalTok{gender, house}
\end{Highlighting}
\end{Shaded}

R code：

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dt[ age <}\StringTok{ }\DecValTok{40}\NormalTok{, .(}\DataTypeTok{avg =} \KeywordTok{mean}\NormalTok{(online_trans)), by =}\StringTok{ }\NormalTok{.(gender, house)]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    gender house   avg
## 1:   Male   Yes 14.46
## 2: Female   Yes 18.14
## 3:   Male    No 18.24
## 4: Female    No 20.10
\end{verbatim}

is equal to SQL：

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{SELECT} \NormalTok{gender, house, }\FunctionTok{avg}\NormalTok{(online_trans) }\KeywordTok{AS} \FunctionTok{avg} \KeywordTok{FROM} \NormalTok{sim.dat }\KeywordTok{WHERE} \NormalTok{age < }\DecValTok{40} \KeywordTok{GROUP} \KeywordTok{BY} \NormalTok{gender, house}
\end{Highlighting}
\end{Shaded}

You can see the analogy between \texttt{data.table} and \texttt{SQL}.
Now let's focus on operations in data table.

\begin{itemize}
\tightlist
\item
  select row
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# select rows with age<20 and income > 80000}
\NormalTok{dt[age <}\StringTok{ }\DecValTok{20} \NormalTok{&}\StringTok{ }\NormalTok{income >}\StringTok{ }\DecValTok{80000}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    age gender income house store_exp online_exp
## 1:  19 Female  83535    No     227.7       1491
## 2:  18 Female  89416   Yes     209.5       1926
## 3:  19 Female  92813    No     186.7       1042
##    store_trans online_trans Q1 Q2 Q3 Q4 Q5 Q6 Q7 Q8 Q9
## 1:           1           22  2  1  1  2  4  1  4  2  4
## 2:           3           28  2  1  1  1  4  1  4  2  4
## 3:           2           18  3  1  1  2  4  1  4  3  4
##    Q10 segment
## 1:   1   Style
## 2:   1   Style
## 3:   1   Style
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# select the first two rows}
\NormalTok{dt[}\DecValTok{1}\NormalTok{:}\DecValTok{2}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    age gender income house store_exp online_exp
## 1:  57 Female 120963   Yes     529.1      303.5
## 2:  63 Female 122008   Yes     478.0      109.5
##    store_trans online_trans Q1 Q2 Q3 Q4 Q5 Q6 Q7 Q8 Q9
## 1:           2            2  4  2  1  2  1  4  1  4  2
## 2:           4            2  4  1  1  2  1  4  1  4  1
##    Q10 segment
## 1:   4   Price
## 2:   4   Price
\end{verbatim}

\begin{itemize}
\tightlist
\item
  select column
\end{itemize}

Selecting columns in \texttt{data.table} don't need \texttt{\$}:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# select column “age” but return it as a vector}
\CommentTok{# the argument for row is empty so the result will return all observations}
\NormalTok{ans <-}\StringTok{ }\NormalTok{dt[, age]}
\KeywordTok{head}\NormalTok{(ans)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 57 63 59 60 51 59
\end{verbatim}

To return \texttt{data.table} object, put column names in
\texttt{list()}:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Select age and online_exp columns and return as a data.table instead}
\NormalTok{ans <-}\StringTok{ }\NormalTok{dt[, }\KeywordTok{list}\NormalTok{(age, online_exp)]}
\KeywordTok{head}\NormalTok{(ans)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    age online_exp
## 1:  57      303.5
## 2:  63      109.5
## 3:  59      279.2
## 4:  60      141.7
## 5:  51      112.2
## 6:  59      195.7
\end{verbatim}

Or you can also put column names in \texttt{.()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ans <-}\StringTok{ }\NormalTok{dt[, .(age, online_exp)]}
\CommentTok{# head(ans)}
\end{Highlighting}
\end{Shaded}

To select all columns from ``\texttt{age}'' to ``\texttt{income}'':

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ans <-}\StringTok{ }\NormalTok{dt[, age:income, with =}\StringTok{ }\OtherTok{FALSE}\NormalTok{]}
\KeywordTok{head}\NormalTok{(ans,}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    age gender income
## 1:  57 Female 120963
## 2:  63 Female 122008
\end{verbatim}

Delete columns using \texttt{-} or \texttt{!}:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# delete columns from  age to online_exp}
\NormalTok{ans <-}\StringTok{ }\NormalTok{dt[, -(age:online_exp), with =}\StringTok{ }\OtherTok{FALSE}\NormalTok{]}
\NormalTok{ans <-}\StringTok{ }\NormalTok{dt[, !(age:online_exp), with =}\StringTok{ }\OtherTok{FALSE}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  tabulation
\end{itemize}

In data table. \texttt{.N} means to count。

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# row count}
\NormalTok{dt[, .N] }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1000
\end{verbatim}

If you assign the group variable, then it will count by groups:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# counts by gender}
\NormalTok{dt[, .N, by=}\StringTok{ }\NormalTok{gender]  }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    gender   N
## 1: Female 554
## 2:   Male 446
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# for those younger than 30, count by gender}
 \NormalTok{dt[age <}\StringTok{ }\DecValTok{30}\NormalTok{, .(}\DataTypeTok{count=}\NormalTok{.N), by=}\StringTok{ }\NormalTok{gender] }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    gender count
## 1: Female   292
## 2:   Male    86
\end{verbatim}

Order table:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# get records with the highest 5 online expense:}
\KeywordTok{head}\NormalTok{(dt[}\KeywordTok{order}\NormalTok{(-online_exp)],}\DecValTok{5}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    age gender income house store_exp online_exp
## 1:  40 Female 217600    No      7024       9479
## 2:  41 Female     NA   Yes      3787       8638
## 3:  36   Male 228550   Yes      3280       8221
## 4:  31 Female 159508   Yes      5177       8006
## 5:  43 Female 190407   Yes      4695       7876
##    store_trans online_trans Q1 Q2 Q3 Q4 Q5 Q6 Q7 Q8 Q9
## 1:          10            6  1  4  5  4  3  4  4  1  4
## 2:          14           10  1  4  4  4  4  4  4  1  4
## 3:           8           12  1  4  5  4  4  4  4  1  4
## 4:          11           13  1  4  4  4  4  4  4  1  4
## 5:           6           11  1  4  5  4  4  4  4  1  4
##    Q10     segment
## 1:   2 Conspicuous
## 2:   2 Conspicuous
## 3:   1 Conspicuous
## 4:   2 Conspicuous
## 5:   2 Conspicuous
\end{verbatim}

Since data table keep some characters of data frame, they share some
operations:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dt[}\KeywordTok{order}\NormalTok{(-online_exp)][}\DecValTok{1}\NormalTok{:}\DecValTok{5}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    age gender income house store_exp online_exp
## 1:  40 Female 217600    No      7024       9479
## 2:  41 Female     NA   Yes      3787       8638
## 3:  36   Male 228550   Yes      3280       8221
## 4:  31 Female 159508   Yes      5177       8006
## 5:  43 Female 190407   Yes      4695       7876
##    store_trans online_trans Q1 Q2 Q3 Q4 Q5 Q6 Q7 Q8 Q9
## 1:          10            6  1  4  5  4  3  4  4  1  4
## 2:          14           10  1  4  4  4  4  4  4  1  4
## 3:           8           12  1  4  5  4  4  4  4  1  4
## 4:          11           13  1  4  4  4  4  4  4  1  4
## 5:           6           11  1  4  5  4  4  4  4  1  4
##    Q10     segment
## 1:   2 Conspicuous
## 2:   2 Conspicuous
## 3:   1 Conspicuous
## 4:   2 Conspicuous
## 5:   2 Conspicuous
\end{verbatim}

You can also order the table by more than one variable. The following
code will order the table by \texttt{gender}, then order within
\texttt{gender} by \texttt{online\_exp}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dt[}\KeywordTok{order}\NormalTok{(gender, -online_exp)][}\DecValTok{1}\NormalTok{:}\DecValTok{5}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    age gender income house store_exp online_exp
## 1:  40 Female 217600    No      7024       9479
## 2:  41 Female     NA   Yes      3787       8638
## 3:  31 Female 159508   Yes      5177       8006
## 4:  43 Female 190407   Yes      4695       7876
## 5:  50 Female 263858   Yes      5814       7449
##    store_trans online_trans Q1 Q2 Q3 Q4 Q5 Q6 Q7 Q8 Q9
## 1:          10            6  1  4  5  4  3  4  4  1  4
## 2:          14           10  1  4  4  4  4  4  4  1  4
## 3:          11           13  1  4  4  4  4  4  4  1  4
## 4:           6           11  1  4  5  4  4  4  4  1  4
## 5:          11           11  1  4  5  4  4  4  4  1  4
##    Q10     segment
## 1:   2 Conspicuous
## 2:   2 Conspicuous
## 3:   2 Conspicuous
## 4:   2 Conspicuous
## 5:   1 Conspicuous
\end{verbatim}

\begin{itemize}
\tightlist
\item
  Use \texttt{fread()} to import dat
\end{itemize}

Other than \texttt{read.csv} in base R, we have introduced `read\_csv'
in `readr'. \texttt{read\_csv} is much faster and will provide progress
bar which makes user feel much better (at least make me feel better).
\texttt{fread()} in \texttt{data.table} further increase the efficiency
of reading data. The following are three examples of reading the same
data file \texttt{topic.csv}. The file includes text data scraped from
an agriculture forum with 209670 rows and 6 columns:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{system.time}\NormalTok{(topic<-}\KeywordTok{read.csv}\NormalTok{(}\StringTok{"https://raw.githubusercontent.com/happyrabbit/DataScientistR/master/Data/topic.csv"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
  \NormalTok{user  system elapsed }
  \NormalTok{4.313   0.027   4.340}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{system.time}\NormalTok{(topic<-readr::}\KeywordTok{read_csv}\NormalTok{(}\StringTok{"https://raw.githubusercontent.com/happyrabbit/DataScientistR/master/Data/topic.csv"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
   \NormalTok{user  system elapsed }
  \NormalTok{0.267   0.008   0.274 }
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{system.time}\NormalTok{(topic<-data.table::}\KeywordTok{fread}\NormalTok{(}\StringTok{"https://raw.githubusercontent.com/happyrabbit/DataScientistR/master/Data/topic.csv"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
   \NormalTok{user  system elapsed }
  \NormalTok{0.217   0.005   0.221 }
\end{Highlighting}
\end{Shaded}

It is clear that \texttt{read\_csv()} is much faster than
\texttt{read.csv()}. \texttt{fread()} is a little faster than
\texttt{read\_csv()}. As the size increasing, the difference will become
for significant. Note that \texttt{fread()} will read file as
\texttt{data.table} by default.

\subsection{Summarize data}\label{summarize-data}

\subsubsection{\texorpdfstring{\texttt{apply()}, \texttt{lapply()} and
\texttt{sapply()} in base
R}{apply(), lapply() and sapply() in base R}}\label{apply-lapply-and-sapply-in-base-r}

There are some powerful functions to summarize data in base R, such as
\texttt{apply()}, \texttt{lapply()} and \texttt{sapply()}. They do the
same basic things and are all from ``apply'' family: apply functions
over parts of data. They differ in two important respects:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  the type of object they apply to
\item
  the type of result they will return
\end{enumerate}

When do we use \texttt{apply()}? When we want to apply a function to
margins of an array or matrix. That means our data need to be
structured. The operations can be very flexible. It returns a vector or
array or list of values obtained by applying a function to margins of an
array or matrix.

For example you can compute row and column sums for a matrix:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{## simulate a matrix}
\NormalTok{x <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(}\DataTypeTok{x1 =}\DecValTok{1}\NormalTok{:}\DecValTok{8}\NormalTok{, }\DataTypeTok{x2 =} \KeywordTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{:}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{:}\DecValTok{5}\NormalTok{))}
\KeywordTok{dimnames}\NormalTok{(x)[[}\DecValTok{1}\NormalTok{]] <-}\StringTok{ }\NormalTok{letters[}\DecValTok{1}\NormalTok{:}\DecValTok{8}\NormalTok{]}
\KeywordTok{apply}\NormalTok{(x, }\DecValTok{2}\NormalTok{, mean)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  x1  x2 
## 4.5 3.0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{col.sums <-}\StringTok{ }\KeywordTok{apply}\NormalTok{(x, }\DecValTok{2}\NormalTok{, sum)}
\NormalTok{row.sums <-}\StringTok{ }\KeywordTok{apply}\NormalTok{(x, }\DecValTok{1}\NormalTok{, sum)}
\end{Highlighting}
\end{Shaded}

You can also apply other functions:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ma <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{:}\DecValTok{4}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{6}\NormalTok{:}\DecValTok{8}\NormalTok{), }\DataTypeTok{nrow =} \DecValTok{2}\NormalTok{)}
\NormalTok{ma}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1] [,2] [,3] [,4]
## [1,]    1    3    1    7
## [2,]    2    4    6    8
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{apply}\NormalTok{(ma, }\DecValTok{1}\NormalTok{, table)  }\CommentTok{#--> a list of length 2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [[1]]
## 
## 1 3 7 
## 2 1 1 
## 
## [[2]]
## 
## 2 4 6 8 
## 1 1 1 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{apply}\NormalTok{(ma, }\DecValTok{1}\NormalTok{, stats::quantile) }\CommentTok{# 5 x n matrix with rownames}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1] [,2]
## 0%      1  2.0
## 25%     1  3.5
## 50%     2  5.0
## 75%     4  6.5
## 100%    7  8.0
\end{verbatim}

Results can have different lengths for each call. This is a trickier
example. What will you get?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{## Example with different lengths for each call}
\NormalTok{z <-}\StringTok{ }\KeywordTok{array}\NormalTok{(}\DecValTok{1}\NormalTok{:}\DecValTok{24}\NormalTok{, }\DataTypeTok{dim =} \DecValTok{2}\NormalTok{:}\DecValTok{4}\NormalTok{)}
\NormalTok{zseq <-}\StringTok{ }\KeywordTok{apply}\NormalTok{(z, }\DecValTok{1}\NormalTok{:}\DecValTok{2}\NormalTok{, function(x) }\KeywordTok{seq_len}\NormalTok{(}\KeywordTok{max}\NormalTok{(x)))}
\NormalTok{zseq         ## a 2 x 3 matrix}
\KeywordTok{typeof}\NormalTok{(zseq) ## list}
\KeywordTok{dim}\NormalTok{(zseq) ## 2 3}
\NormalTok{zseq[}\DecValTok{1}\NormalTok{,]}
\KeywordTok{apply}\NormalTok{(z, }\DecValTok{3}\NormalTok{, function(x) }\KeywordTok{seq_len}\NormalTok{(}\KeywordTok{max}\NormalTok{(x)))}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  \texttt{lapply()} applies a function over a list, data.frame or vector
  and returns a list of the same length.
\item
  \texttt{sapply()} is a user-friendly version and wrapper of
  \texttt{lapply()}. By default it returns a vector, matrix or if
  \texttt{simplify\ =\ "array"}, an array if appropriate.
  \texttt{apply(x,\ f,\ simplify\ =\ FALSE,\ USE.NAMES\ =\ FALSE)} is
  the same as \texttt{lapply(x,\ f)}. If \texttt{simplify=TRUE}, then it
  will return a \texttt{data.frame} instead of \texttt{list}.
\end{itemize}

Let's use some data with context to help you better understand the
functions.

\begin{itemize}
\tightlist
\item
  Get the mean and standard deviation of all numerical variables in the
  dataset.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Read data}
\NormalTok{sim.dat<-}\KeywordTok{read.csv}\NormalTok{(}\StringTok{"https://raw.githubusercontent.com/happyrabbit/DataScientistR/master/Data/SegData.csv"}\NormalTok{)}
\CommentTok{# Get numerical variables}
\NormalTok{sdat<-sim.dat[,!}\KeywordTok{lapply}\NormalTok{(sim.dat,class)==}\StringTok{"factor"}\NormalTok{]}
\NormalTok{## Try the following code with apply() function}
\NormalTok{## apply(sim.dat,2,class)}
\NormalTok{## What is the problem?}
\end{Highlighting}
\end{Shaded}

The data frame \texttt{sdat} only includes numeric columns. Now we can
go head and use \texttt{apply()} to get mean and standard deviation for
each column:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{apply}\NormalTok{(sdat, }\DataTypeTok{MARGIN=}\DecValTok{2}\NormalTok{,function(x) }\KeywordTok{mean}\NormalTok{(}\KeywordTok{na.omit}\NormalTok{(x)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          age       income    store_exp   online_exp 
##    3.884e+01    1.135e+05    1.357e+03    2.120e+03 
##  store_trans online_trans           Q1           Q2 
##    5.350e+00    1.355e+01    3.101e+00    1.823e+00 
##           Q3           Q4           Q5           Q6 
##    1.992e+00    2.763e+00    2.945e+00    2.448e+00 
##           Q7           Q8           Q9          Q10 
##    3.434e+00    2.396e+00    3.085e+00    2.320e+00
\end{verbatim}

Here we defined a function using \texttt{function(x)\ mean(na.omit(x))}.
It is a very simple function. It tells R to ignore the missing value
when calculating the mean. \texttt{MARGIN=2} tells R to apply the
function to each column. It is not hard to guess what \texttt{MARGIN=1}
mean. The result show that the average online expense is much higher
than store expense. You can also compare the average scores across
different questions. The command to calculate standard deviation is very
similar. The only difference is to change \texttt{mean()} to
\texttt{sd()}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{apply}\NormalTok{(sdat, }\DataTypeTok{MARGIN=}\DecValTok{2}\NormalTok{,function(x) }\KeywordTok{sd}\NormalTok{(}\KeywordTok{na.omit}\NormalTok{(x)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          age       income    store_exp   online_exp 
##       16.417    49842.287     2774.400     1731.224 
##  store_trans online_trans           Q1           Q2 
##        3.696        7.957        1.450        1.168 
##           Q3           Q4           Q5           Q6 
##        1.402        1.155        1.284        1.439 
##           Q7           Q8           Q9          Q10 
##        1.456        1.154        1.118        1.136
\end{verbatim}

Even the average online expense is higher than store expense, the
standard deviation for store expense is much higher than online expense
which indicates there is very likely some big/small purchase in store.
We can check it quickly:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(sdat$store_exp)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##    -500     205     329    1360     597   50000
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(sdat$online_exp)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##      69     420    1940    2120    2440    9480
\end{verbatim}

There are some odd values in store expense. The minimum value is -500
which is a wrong imputation which indicates that you should preprocess
data before analyzing it. Checking those simple statistics will help you
better understand your data. It then gives you some idea how to
preprocess and analyze them. How about using \texttt{lapply()} and
\texttt{sapply()}?

Run the following code and compare the results:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{lapply}\NormalTok{(sdat, function(x) }\KeywordTok{sd}\NormalTok{(}\KeywordTok{na.omit}\NormalTok{(x)))}
\KeywordTok{sapply}\NormalTok{(sdat, function(x) }\KeywordTok{sd}\NormalTok{(}\KeywordTok{na.omit}\NormalTok{(x)))}
\KeywordTok{sapply}\NormalTok{(sdat, function(x) }\KeywordTok{sd}\NormalTok{(}\KeywordTok{na.omit}\NormalTok{(x)), }\DataTypeTok{simplify =} \OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{\texorpdfstring{\texttt{dplyr}
package}{dplyr package}}\label{dplyr-package}

\texttt{dplyr} provides a flexible grammar of data manipulation focusing
on tools for working with data frames (hence the \texttt{d} in the
name). It is faster and more friendly:

\begin{itemize}
\tightlist
\item
  It identifies the most important data manipulations and make they easy
  to use from R
\item
  It performs faster for in-memory data by writing key pieces in C++
  using \texttt{Rcpp}
\item
  The interface is the same for data frame, data table or database.
\end{itemize}

We will illustrate the following functions in order:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Display
\item
  Subset
\item
  Summarize
\item
  Create new variable
\item
  Merge
\end{enumerate}

\textbf{Display}

\begin{itemize}
\tightlist
\item
  \texttt{tbl\_df()}: Convert the data to \texttt{tibble} which offers
  better checking and printing capabilities than traditional data
  frames. It will adjust output width according to fit the current
  window.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(dplyr)}
\KeywordTok{tbl_df}\NormalTok{(sim.dat)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  \texttt{glimpse()}: This is like a transposed version of
  \texttt{tbl\_df()}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{glimpse}\NormalTok{(sim.dat)}
\end{Highlighting}
\end{Shaded}

\textbf{Subset}

Get rows with \texttt{income} more than 300000:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(magrittr)}
\KeywordTok{filter}\NormalTok{(sim.dat, income >}\DecValTok{300000}\NormalTok{) %>%}
\StringTok{  }\KeywordTok{tbl_df}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 4 x 19
##     age gender income  house store_exp online_exp
##   <int> <fctr>  <dbl> <fctr>     <dbl>      <dbl>
## 1    40   Male 301398    Yes      4840       3618
## 2    33   Male 319704    Yes      5998       4396
## 3    41   Male 317476    Yes      3030       4180
## 4    37 Female 315697    Yes      6549       4284
## # ... with 13 more variables: store_trans <int>,
## #   online_trans <int>, Q1 <int>, Q2 <int>, Q3 <int>,
## #   Q4 <int>, Q5 <int>, Q6 <int>, Q7 <int>, Q8 <int>,
## #   Q9 <int>, Q10 <int>, segment <fctr>
\end{verbatim}

Here we meet a new operator \texttt{\%\textgreater{}\%}. It is called
``Pipe operator'' which pipes a value forward into an expression or
function call. What you get in the left operation will be the first
argument or the only argument in the right operation.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x %>%}\StringTok{ }\KeywordTok{f}\NormalTok{(y) =}\StringTok{ }\KeywordTok{f}\NormalTok{(x, y)}
\NormalTok{y %>%}\StringTok{ }\KeywordTok{f}\NormalTok{(x, ., z) =}\StringTok{ }\KeywordTok{f}\NormalTok{(x, y, z )}
\end{Highlighting}
\end{Shaded}

It is an operator from \texttt{magrittr} which can be really beneficial.
Look at the following code. Can you tell me what it does?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ave_exp <-}\StringTok{ }\KeywordTok{filter}\NormalTok{( }
  \KeywordTok{summarise}\NormalTok{(}
    \KeywordTok{group_by}\NormalTok{( }
      \KeywordTok{filter}\NormalTok{(}
        \NormalTok{sim.dat, }
        \NormalTok{!}\KeywordTok{is.na}\NormalTok{(income)}
      \NormalTok{), }
      \NormalTok{segment}
    \NormalTok{), }
    \DataTypeTok{ave_online_exp =} \KeywordTok{mean}\NormalTok{(online_exp), }
    \DataTypeTok{n =} \KeywordTok{n}\NormalTok{()}
  \NormalTok{), }
  \NormalTok{n >}\StringTok{ }\DecValTok{200}
\NormalTok{) }
\end{Highlighting}
\end{Shaded}

Now look at the identical code using ``\texttt{\%\textgreater{}\%}'':

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ave_exp <-}\StringTok{ }\NormalTok{sim.dat %>%}\StringTok{ }
\StringTok{ }\KeywordTok{filter}\NormalTok{(!}\KeywordTok{is.na}\NormalTok{(income)) %>%}\StringTok{ }
\StringTok{ }\KeywordTok{group_by}\NormalTok{(segment) %>%}\StringTok{ }
\StringTok{ }\KeywordTok{summarise}\NormalTok{( }
   \DataTypeTok{ave_online_exp =} \KeywordTok{mean}\NormalTok{(online_exp), }
   \DataTypeTok{n =} \KeywordTok{n}\NormalTok{() ) %>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(n >}\StringTok{ }\DecValTok{200}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Isn't it much more straightforward now? Let's read it:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Delete observations from \texttt{sim.dat} with missing income values
\item
  Group the data from step 1 by variable \texttt{segment}
\item
  Calculate mean of online expense for each segment and save the result
  as a new variable named \texttt{ave\_online\_exp}
\item
  Calculate the size of each segment and saved it as a new variable
  named \texttt{n}
\item
  Get segments with size larger than 200
\end{enumerate}

You can use \texttt{distinct()} to delete duplicated rows.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dplyr::}\KeywordTok{distinct}\NormalTok{(sim.dat)}
\end{Highlighting}
\end{Shaded}

\texttt{sample\_frac()} will randomly select some rows with a specified
percentage. \texttt{sample\_n()} can randomly select rows with a
specified number.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dplyr::}\KeywordTok{sample_frac}\NormalTok{(sim.dat, }\FloatTok{0.5}\NormalTok{, }\DataTypeTok{replace =} \OtherTok{TRUE}\NormalTok{) }
\NormalTok{dplyr::}\KeywordTok{sample_n}\NormalTok{(sim.dat, }\DecValTok{10}\NormalTok{, }\DataTypeTok{replace =} \OtherTok{TRUE}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\texttt{slice()} will select rows by position:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dplyr::}\KeywordTok{slice}\NormalTok{(sim.dat, }\DecValTok{10}\NormalTok{:}\DecValTok{15}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

It is equivalent to \texttt{sim.dat{[}10:15,{]}}.

\texttt{top\_n()} will select the order top n entries:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dplyr::}\KeywordTok{top_n}\NormalTok{(sim.dat,}\DecValTok{2}\NormalTok{,income)}
\end{Highlighting}
\end{Shaded}

If you want to select columns instead of rows, you can use
\texttt{select()}. The following are some sample codes:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# select by column name}
\NormalTok{dplyr::}\KeywordTok{select}\NormalTok{(sim.dat,income,age,store_exp)}

\CommentTok{# select columns whose name contains a character string}
\NormalTok{dplyr::}\KeywordTok{select}\NormalTok{(sim.dat, }\KeywordTok{contains}\NormalTok{(}\StringTok{"_"}\NormalTok{))}

\CommentTok{# select columns whose name ends with a character string}
\CommentTok{# similar there is "starts_with"}
\NormalTok{dplyr::}\KeywordTok{select}\NormalTok{(sim.dat, }\KeywordTok{ends_with}\NormalTok{(}\StringTok{"e"}\NormalTok{))}

\CommentTok{# select columns Q1,Q2,Q3,Q4 and Q5}
\KeywordTok{select}\NormalTok{(sim.dat, }\KeywordTok{num_range}\NormalTok{(}\StringTok{"Q"}\NormalTok{, }\DecValTok{1}\NormalTok{:}\DecValTok{5}\NormalTok{)) }

\CommentTok{# select columns whose names are in a group of names}
\NormalTok{dplyr::}\KeywordTok{select}\NormalTok{(sim.dat, }\KeywordTok{one_of}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"age"}\NormalTok{, }\StringTok{"income"}\NormalTok{)))}

\CommentTok{# select columns between age and online_exp}
\NormalTok{dplyr::}\KeywordTok{select}\NormalTok{(sim.dat, age:online_exp)}

\CommentTok{# select all columns except for age}
\NormalTok{dplyr::}\KeywordTok{select}\NormalTok{(sim.dat, -age)}
\end{Highlighting}
\end{Shaded}

\textbf{Summarize}

A standard marketing problem is customer segmentation. It usually starts
with designing survey and collecting data. Then run a cluster analysis
on the data to get customer segments. Once we have different segments,
the next is to understand how each group of customer look like by
summarizing some key metrics. For example, we can do the following data
aggregation for different segments of clothes customers.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sim.dat%>%}
\StringTok{  }\KeywordTok{group_by}\NormalTok{(segment)%>%}
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{Age=}\KeywordTok{round}\NormalTok{(}\KeywordTok{mean}\NormalTok{(}\KeywordTok{na.omit}\NormalTok{(age)),}\DecValTok{0}\NormalTok{),}
      \DataTypeTok{FemalePct=}\KeywordTok{round}\NormalTok{(}\KeywordTok{mean}\NormalTok{(gender==}\StringTok{"Female"}\NormalTok{),}\DecValTok{2}\NormalTok{),}
      \DataTypeTok{HouseYes=}\KeywordTok{round}\NormalTok{(}\KeywordTok{mean}\NormalTok{(house==}\StringTok{"Yes"}\NormalTok{),}\DecValTok{2}\NormalTok{),}
      \DataTypeTok{store_exp=}\KeywordTok{round}\NormalTok{(}\KeywordTok{mean}\NormalTok{(}\KeywordTok{na.omit}\NormalTok{(store_exp),}\DataTypeTok{trim=}\FloatTok{0.1}\NormalTok{),}\DecValTok{0}\NormalTok{),}
      \DataTypeTok{online_exp=}\KeywordTok{round}\NormalTok{(}\KeywordTok{mean}\NormalTok{(online_exp),}\DecValTok{0}\NormalTok{),}
      \DataTypeTok{store_trans=}\KeywordTok{round}\NormalTok{(}\KeywordTok{mean}\NormalTok{(store_trans),}\DecValTok{1}\NormalTok{),}
      \DataTypeTok{online_trans=}\KeywordTok{round}\NormalTok{(}\KeywordTok{mean}\NormalTok{(online_trans),}\DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 4 x 8
##       segment   Age FemalePct HouseYes store_exp
##        <fctr> <dbl>     <dbl>    <dbl>     <dbl>
## 1 Conspicuous    42      0.32     0.86      4990
## 2       Price    60      0.45     0.94       501
## 3     Quality    35      0.47     0.34       301
## 4       Style    24      0.81     0.27       200
## # ... with 3 more variables: online_exp <dbl>,
## #   store_trans <dbl>, online_trans <dbl>
\end{verbatim}

Now, let's peel the onion in order.

The first line \texttt{sim.dat} is easy. It is the data you want to work
on. The second line \texttt{group\_by(segment)} tells R that in the
following steps you want to summarise by variable \texttt{segment}. Here
we only summarize data by one categorical variable, but you can group by
multiple variables, such as \texttt{group\_by(segment,\ house)}. The
third argument \texttt{summarise} tells R the manipulation(s) to do.
Then list the exact actions inside \texttt{summarise()} . For example,
\texttt{Age=round(mean(na.omit(age)),0)} tell R the following things:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Calculate the mean of column \texttt{age} ignoring missing value for
  each customer segment
\item
  Round the result to the specified number of decimal places
\item
  Store the result in a new variable named \texttt{Age}
\end{enumerate}

The rest of the command above is similar. In the end, we calculate the
following for each segment:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{Age}: average age for each segment
\item
  \texttt{FemalePct}: percentage for each segment
\item
  \texttt{HouseYes}: percentage of people who own a house
\item
  \texttt{stroe\_exp}: average expense in store
\item
  \texttt{online\_exp}: average expense online
\item
  \texttt{store\_trans}: average times of transactions in the store
\item
  \texttt{online\_trans}: average times of online transactions
\end{enumerate}

There is a lot of information you can extract from those simple
averages.

\begin{itemize}
\item
  Conspicuous: average age is about 40. It is a group of middle-age
  wealthy people. 1/3 of them are female, and 2/3 are male. They buy
  regardless the price. Almost all of them own house (0.86). It makes us
  wonder what is wrong with the rest 14\%?
\item
  Price: They are older people with average age 60. Nearly all of them
  own a house(0.94). They are less likely to purchase online
  (store\_trans=6 while online\_trans=3). It is the only group that is
  less likely to buy online.
\item
  Quality: The average age is 35. They are not way different with
  Conspicuous regarding age. But they spend much less. The percentages
  of male and female are similar. They prefer online shopping. More than
  half of them don't own a house (0.66).
\item
  Style: They are young people with average age 24. The majority of them
  are female (0.81). Most of them don't own a house (0.73). They are
  very likely to be digital natives and prefer online shopping.
\end{itemize}

You may notice that Style group purchase more frequently online
(\texttt{online\_trans}) but the expense (\texttt{online\_exp}) is not
higher. It makes us wonder what is the average expense each time, so you
have a better idea about the price range of the group.

The analytical process is aggregated instead of independent steps. The
current step will shed new light on what to do next. Sometimes you need
to go back to fix something in the previous steps. Let's check average
one-time online and instore purchase amounts:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sim.dat%>%}
\KeywordTok{group_by}\NormalTok{(segment)%>%}
\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{avg_online=}\KeywordTok{round}\NormalTok{(}\KeywordTok{sum}\NormalTok{(online_exp)/}\KeywordTok{sum}\NormalTok{(online_trans),}\DecValTok{2}\NormalTok{),}
    \DataTypeTok{avg_store=}\KeywordTok{round}\NormalTok{(}\KeywordTok{sum}\NormalTok{(store_exp)/}\KeywordTok{sum}\NormalTok{(store_trans),}\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 4 x 3
##       segment avg_online avg_store
##        <fctr>      <dbl>     <dbl>
## 1 Conspicuous     442.27     479.2
## 2       Price      69.28      81.3
## 3     Quality     126.05     105.1
## 4       Style      92.83     121.1
\end{verbatim}

Price group has the lowest averaged one-time purchase. The Conspicuous
group will pay the highest price. When we build customer profile in real
life, we will also need to look at the survey summarization. You may be
surprised how much information simple data manipulations can provide.

Another comman task is to check which column has missing values. It
requires the program to look at each column in the data. In this case
you can use \texttt{summarise\_all}:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# apply function anyNA() to each column}
\CommentTok{# you can also assign a function vector such as: c("anyNA","is.factor")}
\NormalTok{dplyr::}\KeywordTok{summarise_all}\NormalTok{(sim.dat, }\KeywordTok{funs_}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"anyNA"}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     age gender income house store_exp online_exp
## 1 FALSE  FALSE   TRUE FALSE     FALSE      FALSE
##   store_trans online_trans    Q1    Q2    Q3    Q4
## 1       FALSE        FALSE FALSE FALSE FALSE FALSE
##      Q5    Q6    Q7    Q8    Q9   Q10 segment
## 1 FALSE FALSE FALSE FALSE FALSE FALSE   FALSE
\end{verbatim}

The above code returns a vector indicating if there is any value missing
in each column.

\textbf{Create new variable}

There are often situations where you need to create new variables. For
example, adding online and store expense to get total expense. In this
case, you will apply \textbf{window function} to the columns and return
a column with the same length. \texttt{mutate()} can do it for you and
append one or more new columns:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dplyr::}\KeywordTok{mutate}\NormalTok{(sim.dat, }\DataTypeTok{total_exp =} \NormalTok{store_exp +}\StringTok{ }\NormalTok{online_exp)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      age gender income house store_exp online_exp
## 1     57 Female 120963   Yes     529.1     303.51
## 2     63 Female 122008   Yes     478.0     109.53
## 3     59   Male 114202   Yes     490.8     279.25
## 4     60   Male 113616   Yes     347.8     141.67
## 5     51   Male 124253   Yes     379.6     112.24
## 6     59   Male 107661   Yes     338.3     195.69
## 7     57   Male 120483   Yes     482.5     284.54
## 8     57   Male 110542   Yes     340.7     135.26
## 9     61 Female 132061   Yes     608.2     142.55
## 10    60   Male 105049   Yes     470.3     163.47
## 11    58   Male 107197   Yes     366.6     170.13
## 12    59   Male     NA   Yes     674.9     310.27
## 13    64   Male 119020   Yes     613.9     160.85
## 14    57 Female     NA   Yes     737.0     224.53
## 15    64   Male 114539   Yes     402.5     241.83
## 16    61 Female     NA   Yes     615.1     238.10
## 17    57 Female 133078   Yes     429.4     262.66
## 18    63   Male 115709   Yes     552.6     187.52
## 19    57 Female 113211    No     540.3     254.58
## 20    57 Female 129774    No     384.3     311.03
## 21    58   Male     NA   Yes     372.4     296.83
## 22    58   Male 124357   Yes     535.5     205.54
## 23    59 Female 123117   Yes     481.3     157.29
## 24    61 Female     NA   Yes     546.2     161.23
## 25    60 Female     NA   Yes     411.1     123.78
## 26    56   Male     NA   Yes     492.7     128.02
## 27    58   Male 127887   Yes     519.3     142.63
## 28    64   Male 115925   Yes     627.7     240.98
## 29    63 Female     NA   Yes     511.1     203.49
## 30    56 Female 112621   Yes     699.3     223.44
## 31    57   Male     NA   Yes     530.6     217.92
## 32    59 Female 121773   Yes     532.1     176.88
## 33    63   Male 126903   Yes     601.6     257.84
## 34    55   Male 128254   Yes     595.3     248.06
## 35    60 Female     NA   Yes     403.6     236.96
## 36    64 Female     NA   Yes     611.7     102.24
## 37    57 Female 118170   Yes     482.7     183.36
## 38    57 Female 119148   Yes     412.9     246.87
## 39    58   Male 125844   Yes     473.8     261.67
## 40    55   Male 128194   Yes     595.7     156.93
## 41    59 Female     NA   Yes     548.3     186.50
## 42    61   Male     NA   Yes     597.2     209.68
## 43    59 Female 122338   Yes     455.3     205.16
## 44    58 Female 114519   Yes     493.5     271.67
## 45    62   Male 123459   Yes     561.7     209.23
## 46    59   Male 125626   Yes     518.2     261.31
## 47    56   Male 100583   Yes     633.0     195.95
## 48    66   Male     NA   Yes     510.3     198.46
## 49    62 Female 128606   Yes     499.7     244.05
## 50    54   Male 114337   Yes     518.0     217.25
## 51    59   Male 121889   Yes     490.4     120.95
## 52    57 Female 125566   Yes     506.2     155.84
## 53    52 Female     NA   Yes     529.3     193.48
## 54    56   Male 111594   Yes     567.7     195.55
## 55    60 Female 109520   Yes     594.7     212.07
## 56    55 Female     NA   Yes     321.2     203.29
## 57    64 Female 124792   Yes     594.9     190.81
## 58    56   Male 117325   Yes     525.8     211.02
## 59    58 Female     NA   Yes     392.1     226.47
## 60    58   Male 121033   Yes     451.6     157.40
## 61    62 Female 117474   Yes     483.6     213.19
## 62    62 Female 119597   Yes     697.5     169.40
## 63    58 Female 116507   Yes     527.7     192.62
## 64    58   Male 125974   Yes     350.5     151.46
## 65    61   Male 111027   Yes     606.8     278.53
## 66    64   Male     NA   Yes     689.6     239.12
## 67    59   Male 125334   Yes     524.4     190.49
## 68    56 Female 115479   Yes     438.6     109.31
## 69    58 Female     NA    No     293.1     253.49
## 70    62   Male 116434   Yes     360.1     274.21
## 71    62 Female     NA   Yes     534.9     181.33
## 72    61 Female 127498   Yes     644.1     238.12
## 73    54   Male 117859    No     570.4     262.01
## 74    59   Male 133587   Yes     624.6     206.62
## 75    59   Male     NA   Yes     610.5     171.44
## 76    62 Female 130151   Yes     704.6     225.27
## 77    60   Male 125086   Yes     388.2     163.91
## 78    62   Male     NA   Yes     377.2     259.30
## 79    59   Male     NA   Yes     723.8     179.83
## 80    58   Male 118653   Yes     659.0     208.69
## 81    61   Male     NA   Yes     358.2     315.42
## 82    61 Female 117817   Yes     607.8     207.02
## 83    61   Male     NA   Yes     409.4     341.66
## 84    57   Male     NA   Yes     382.9     171.65
## 85    55 Female 127716   Yes     494.9     183.64
## 86    65   Male     NA   Yes     633.5     202.92
## 87    58   Male     NA   Yes     528.4     202.01
## 88    62   Male 121172   Yes     652.8     185.31
## 89    59   Male 115396   Yes     622.8     192.09
## 90    60 Female 123867   Yes     441.3     166.77
## 91    64   Male 113301   Yes     460.0     252.77
## 92    61   Male 133464   Yes     627.1     151.89
## 93    56 Female 119472   Yes     515.7     197.70
## 94    62 Female 119461   Yes     532.3     176.85
## 95    58 Female 119233   Yes     568.6     253.94
## 96    58 Female     NA   Yes     420.7     208.86
## 97    60 Female     NA   Yes     478.0     198.95
## 98    59 Female     NA    No     434.7     239.83
## 99    53 Female 121089    No     556.9     163.28
## 100   63   Male 125302   Yes     544.1     233.97
## 101   61   Male     NA   Yes     416.5     235.55
## 102   56 Female     NA   Yes     427.0     279.10
## 103   56   Male 106712   Yes     410.6     132.02
## 104   64 Female     NA   Yes     302.4     112.59
## 105   60   Male 127820   Yes     423.2     255.74
## 106   62   Male 129258    No     555.3     142.39
## 107   66   Male 117317   Yes     499.1     158.88
## 108   67 Female 113786   Yes     504.5     193.11
## 109   60 Female 115340   Yes     538.2     144.37
## 110   63 Female 122790   Yes     341.8     158.82
## 111   60 Female 126579   Yes     257.9     222.29
## 112   67   Male 126651   Yes     430.9     177.61
## 113   58   Male     NA    No     302.8     181.84
## 114   60 Female     NA   Yes     582.6     220.78
## 115   65 Female 111526   Yes     406.4     189.85
## 116   59 Female 122550    No     620.8     185.13
## 117   61   Male 116833   Yes     311.9     254.31
## 118   65   Male 127422   Yes     567.8     199.96
## 119   64 Female 130856   Yes     399.4     217.99
## 120   56 Female 124593   Yes     212.9     192.38
## 121   60   Male 117161   Yes     465.0     246.81
## 122   60   Male 112290   Yes     430.5     136.68
## 123   68   Male 124419   Yes     389.2     206.57
## 124   55 Female     NA   Yes     475.6     163.12
## 125   62   Male     NA   Yes     518.9     229.38
## 126   54   Male 127204   Yes     597.5     155.14
## 127   59 Female 116489   Yes     469.3     205.45
## 128   60   Male 121120    No     452.6     256.68
## 129   60 Female 121269   Yes     471.1     233.39
## 130   62   Male     NA   Yes     406.9     159.74
## 131   59   Male 132961   Yes     518.5     208.18
## 132   59   Male     NA   Yes     471.2     252.30
## 133   59   Male     NA   Yes     390.8     217.02
## 134   60 Female 128337   Yes     518.9     202.17
## 135   62   Male 121456   Yes     410.1      68.82
## 136   60   Male 125824   Yes     510.7     335.14
## 137   61   Male 120403   Yes     519.1     251.42
## 138   54 Female 108161   Yes     377.9     176.64
## 139   59 Female 129715   Yes     566.9     130.03
## 140   55   Male     NA   Yes     482.0     212.63
## 141   55   Male 121293    No     413.7     213.29
## 142   62   Male 124251   Yes     427.9      94.00
## 143   54 Female     NA   Yes     375.4     130.70
## 144   61   Male     NA   Yes     566.9     224.30
## 145   59 Female 116347   Yes     428.8      79.40
## 146   61   Male 114550   Yes     648.0     172.58
## 147   62 Female 122186   Yes     512.3     215.44
## 148   59 Female 124382   Yes     498.0     244.65
## 149   62 Female 119771   Yes     464.3     237.74
## 150   64 Female     NA   Yes     499.3     208.47
## 151   64   Male 114683   Yes     521.7     203.82
## 152   59   Male     NA   Yes     306.2     161.16
## 153   60 Female 113287   Yes     677.1     243.17
## 154   57   Male 129904   Yes     675.8     240.78
## 155   61   Male 118475   Yes     209.3     142.54
## 156   56   Male     NA   Yes     324.6     183.98
## 157   56 Female 126413   Yes     602.6     129.45
## 158   66 Female     NA   Yes     517.0     152.26
## 159   63   Male     NA   Yes     593.8     225.88
## 160   65 Female 123786   Yes     493.2     196.80
## 161   56   Male 111408   Yes     630.6     275.48
## 162   53 Female     NA   Yes     526.5     145.41
## 163   62 Female 129469   Yes     688.1     189.40
## 164   63 Female     NA   Yes     314.4     145.21
## 165   56   Male     NA   Yes     468.0     150.58
## 166   59   Male 126272   Yes     377.3     234.59
## 167   62 Female 108396   Yes     446.1     238.23
## 168   58   Male     NA   Yes     405.5     119.23
## 169   54   Male 108699   Yes     574.9     245.28
## 170   59 Female 112985   Yes     456.9     181.70
## 171   61   Male     NA   Yes     701.4     254.80
## 172   61 Female     NA   Yes     608.4     301.55
## 173   61 Female     NA   Yes     539.7     223.32
## 174   56   Male     NA   Yes     517.9     233.06
## 175   57 Female 131449   Yes     390.1     147.85
## 176   59   Male 115900   Yes     528.1     186.37
## 177   56 Female     NA   Yes     602.2     191.31
## 178   62 Female 110510   Yes     473.7     253.33
## 179   61 Female     NA   Yes     458.8     156.64
## 180   57   Male     NA   Yes     317.5     187.41
## 181   63 Female 124568   Yes     471.6     202.65
## 182   58   Male 119448   Yes     466.1     249.35
## 183   59 Female 123706   Yes     314.3     212.45
## 184   63 Female 134146   Yes     549.9     241.30
## 185   54   Male 111547   Yes     540.6     189.29
## 186   61   Male 119540   Yes     700.9     247.79
## 187   59   Male 123976   Yes     526.7     198.84
## 188   59   Male 119461   Yes     587.8     181.95
## 189   63 Female 134418   Yes     488.7     254.34
## 190   61 Female 121046   Yes     330.4     168.52
## 191   60 Female 119772   Yes     747.0     127.50
## 192   60 Female 106708    No     418.6     244.42
## 193   56   Male     NA   Yes     419.7     192.37
## 194   63   Male     NA   Yes     584.1     320.96
## 195   58   Male 116516   Yes     588.9     110.15
## 196   61   Male 124903   Yes     390.9     109.30
## 197   58   Male 111364   Yes     402.1     179.17
## 198   57 Female 119470   Yes     515.5     153.16
## 199   63   Male 118613   Yes     571.6     230.38
## 200   60 Female 118609   Yes     645.1     169.31
## 201   65   Male  99409   Yes     526.5     270.56
## 202   57 Female 124554   Yes     307.6     310.26
## 203   53   Male     NA   Yes     586.2     195.13
## 204   69   Male 119552    No     603.7     246.00
## 205   59 Female 118707   Yes     618.8     192.68
## 206   60   Male 106360   Yes     658.0     199.56
## 207   60   Male 134577   Yes     339.3     174.01
## 208   60   Male     NA   Yes     562.6     210.15
## 209   62   Male     NA   Yes     503.2     206.86
## 210   67 Female 127121   Yes     348.6     200.55
## 211   64   Male     NA   Yes     426.7     240.03
## 212   62 Female     NA   Yes     639.7     216.00
## 213   65   Male 123845    No     554.0     184.05
## 214   66   Male 133819   Yes     401.0     307.50
## 215   59   Male 104521   Yes     461.7     182.08
## 216   57 Female 114557    No     427.0     247.15
## 217   64   Male     NA   Yes     343.7     254.74
## 218   60   Male 121940   Yes     480.9     230.48
## 219   60 Female     NA   Yes     422.2     270.14
## 220   55   Male     NA   Yes     483.4     203.28
## 221   57 Female     NA   Yes     641.1     187.33
## 222   58 Female 126646   Yes     611.7     222.44
## 223   58 Female 117707   Yes     416.7     236.34
## 224   58   Male 145345   Yes     690.2     164.24
## 225   60 Female 130677   Yes     615.5     171.37
## 226   57 Female 118732   Yes     486.9     224.39
## 227   58   Male 126062   Yes     682.2     208.31
## 228   55   Male 115303   Yes     451.6     213.64
## 229   62   Male     NA   Yes     521.2     273.49
## 230   60   Male 118625   Yes     540.2     238.25
## 231   60   Male 114025   Yes     412.0     252.69
## 232   56   Male 107039   Yes     538.2     275.54
## 233   59 Female     NA   Yes     518.5     161.04
## 234   60 Female 127042   Yes     534.5     273.48
## 235   55   Male 123608   Yes     568.6     253.36
## 236   58   Male 141349   Yes     528.5     269.11
## 237   59   Male 104866   Yes     565.7     144.76
## 238   55   Male     NA   Yes     345.3     207.98
## 239   58   Male 120409   Yes     270.0     198.48
## 240   55 Female 123505   Yes     502.7     250.53
## 241   59   Male 114339   Yes     452.9     309.75
## 242   62 Female 115346   Yes     566.8      77.93
## 243   61   Male     NA   Yes     557.1     279.21
## 244   57 Female     NA   Yes     513.0     112.25
## 245   57   Male     NA   Yes     334.0     261.33
## 246   61 Female 121937   Yes     563.4     177.00
## 247   63 Female     NA   Yes     521.4     264.49
## 248   63 Female     NA   Yes     615.4     226.39
## 249   54   Male 127794   Yes     579.2     172.49
## 250   65   Male 126479   Yes     519.5     157.88
## 251   40   Male     NA   Yes    3562.2    7264.49
## 252   39   Male 180891   Yes    3955.5    4677.19
## 253   38   Male 190941   Yes    5058.1    4499.61
## 254   36   Male 106483   Yes    3891.6    3401.05
## 255   40 Female 227057   Yes    5416.7    2784.96
## 256   45 Female 232140   Yes    3079.0    4515.88
## 257   42 Female     NA   Yes    3905.9    3435.70
## 258   42   Male     NA   Yes    5095.1    2869.87
## 259   41   Male     NA    No    4918.6    6585.00
## 260   40   Male 245176   Yes    5509.3    4742.39
## 261   43   Male     NA   Yes    3835.7    4931.95
## 262   45   Male 197653    No    4541.7    6692.99
## 263   40   Male 301398   Yes    4840.5    3618.21
## 264   38 Female 189199   Yes    5120.1    3398.28
## 265   42   Male     NA    No    6841.4    6070.81
## 266   37   Male 260037   Yes    3724.7    4714.18
## 267   41   Male 186456   Yes    4530.4    5148.72
## 268   55 Female 222738    No    4642.8    4636.42
## 269   45   Male 189379   Yes    3031.9    6259.35
## 270   48   Male 176785    No    6879.0    4208.66
## 271   42 Female 197429   Yes    6033.3    2628.74
## 272   36 Female     NA   Yes    5740.8    2752.77
## 273   47   Male  88844   Yes    4747.8    3655.00
## 274   38   Male 267565   Yes    5335.1    6052.44
## 275   38 Female 179326   Yes    4365.1    4309.18
## 276   42   Male     NA   Yes    5263.2    3924.34
## 277   35 Female 167613   Yes    5375.2    4026.04
## 278   34   Male 176475   Yes    6621.8    4288.32
## 279   42   Male 184352   Yes    6319.1    3765.25
## 280   36 Female 190846   Yes    4806.6    6590.34
## 281   33 Female 217051   Yes    3645.5    2529.51
## 282   39 Female     NA   Yes    3103.4    3070.81
## 283   50 Female 263858   Yes    5813.8    7448.73
## 284   39   Male  91326   Yes    4960.4    3078.77
## 285   44 Female     NA   Yes    4842.8    6227.72
## 286   44 Female 206791   Yes    5010.1    4343.07
## 287   42   Male 141526   Yes    5106.5    6590.50
## 288  300   Male 208017   Yes    5076.8    6053.49
## 289   38 Female 164507   Yes    3916.9    5764.12
## 290   49   Male 292446    No    4693.3    4361.46
## 291   31 Female 159508   Yes    5177.1    8005.93
## 292   34   Male 156882   Yes    4792.7    6627.55
## 293   41   Male     NA   Yes    5302.9    2444.81
## 294   36 Female 189099   Yes    5155.1    4362.71
## 295   48   Male 124586    No    5185.1    6382.02
## 296   36   Male 171877   Yes    5266.5    4713.01
## 297   33   Male 194787   Yes    3224.3    7563.34
## 298   33   Male 319704   Yes    5998.3    4395.92
## 299   49   Male 144164   Yes    6210.9    5148.91
## 300   41   Male 217153   Yes    5081.3    5712.58
## 301   41   Male 135441   Yes    5841.7    2747.00
## 302   36   Male 271401   Yes    4964.9    5478.38
## 303   47   Male 196925   Yes    4525.9    5995.04
## 304   41   Male 172960   Yes    4573.1    5484.87
## 305   39   Male 220098    No    4357.9    4825.81
## 306   37   Male 148274   Yes    4500.0    5083.95
## 307   30 Female 176389   Yes    5698.2    5966.45
## 308   40   Male 258034   Yes    5459.8    4437.28
## 309   48   Male 243304   Yes    4126.0    4016.95
## 310   41   Male 199560    No    3458.3    3757.81
## 311   47   Male 254559   Yes    5139.5    3630.69
## 312   44   Male 295423    No    5460.1    5598.66
## 313   32   Male 160867   Yes    5511.8    7579.47
## 314   41   Male 183991   Yes    4807.5    3598.30
## 315   39 Female     NA   Yes    5907.9    5462.78
## 316   40   Male 129256   Yes    4257.4    4668.66
## 317   38   Male 154789   Yes    5598.6    4183.51
## 318   41   Male     NA   Yes    5887.7    3512.45
## 319   53 Female 255254   Yes    4739.7    5385.38
## 320   32   Male     NA   Yes    4251.6    2421.31
## 321   39 Female 106557   Yes    4542.8    5558.27
## 322   45 Female 220074    No    5972.7    5743.44
## 323   43   Male     NA   Yes    4502.9    6887.86
## 324   43   Male 107347   Yes    4465.8    3976.20
## 325   32   Male  73952   Yes    5381.3    4469.66
## 326   37 Female 127373   Yes    5229.7     446.57
## 327   38 Female 232699   Yes    6397.6    5551.22
## 328   48   Male 153518   Yes    4599.7    5291.27
## 329   32 Female 238803   Yes    5065.5    7231.27
## 330   48 Female 192712   Yes    6326.8    2638.37
## 331   41 Female 246952   Yes    3311.1    7178.20
## 332   45   Male 137559    No    6019.4    3425.59
## 333   33   Male 207886   Yes    4309.9    5302.00
## 334   42   Male 168775   Yes    4393.0    5029.97
## 335   40   Male 174716   Yes    4630.6    4760.96
## 336   36   Male 147095   Yes    6434.3    3970.56
## 337   43   Male 185241   Yes    4852.4    4489.94
## 338   40   Male 261916   Yes    4316.9    6350.51
## 339   42   Male 207789    No    3653.7    4952.69
## 340   48   Male 205101    No    5021.9    3391.52
## 341   44   Male 234536   Yes    7074.4    6972.33
## 342   34   Male 131651    No    4848.6    4602.80
## 343   47 Female 180859   Yes    6582.4    3642.05
## 344   40   Male 240776    No    5061.4    4545.37
## 345   40   Male 280653   Yes    5084.8    4167.11
## 346   40 Female 148750   Yes    6807.0    5522.96
## 347   40 Female 217600    No    7023.7    9479.44
## 348   41 Female 247215   Yes    4230.7    6632.27
## 349   37   Male 187063   Yes    5931.7    1942.18
## 350   40   Male 245544   Yes    4935.1    5087.34
## 351   50   Male 198177   Yes    4514.0    1910.46
## 352   36   Male 268528   Yes    6115.8    5396.88
## 353   34   Male 159307    No    4992.9    4036.44
## 354   40   Male 225217   Yes    4791.5    5682.11
## 355   41   Male 158261   Yes    4581.6    4322.50
## 356   35 Female 225379    No    4123.7    3652.63
## 357   48   Male     NA   Yes    4457.0    5722.16
## 358   30   Male 234691    No    5165.8    5535.35
## 359   43 Female 229674    No    5615.0    4860.80
## 360   44   Male     NA   Yes    5201.9    2882.03
## 361   42 Female 173379   Yes    5650.2    5332.80
## 362   34   Male     NA   Yes    5866.4    6598.33
## 363   42   Male 139589   Yes    2364.9    5965.12
## 364   40   Male     NA   Yes    6905.4    6878.97
## 365   47   Male 140226    No    4387.1    5211.13
## 366   43 Female 217073   Yes    5199.9    6469.65
## 367   35   Male 184216   Yes    2715.1    5972.83
## 368   45   Male  84897    No    4424.8    5621.56
## 369   53   Male 163901   Yes    6513.3    6233.20
## 370   47   Male     NA   Yes    5372.6    2759.34
## 371   30 Female     NA   Yes    4361.2    6665.26
## 372   40 Female 149416    No    3515.4    6135.30
## 373   34   Male 257558   Yes    4602.5    5342.44
## 374   41   Male     NA   Yes    5679.4    3924.42
## 375   48 Female     NA   Yes    4807.5    4996.92
## 376   42   Male 293010   Yes    5114.8    4798.79
## 377   33   Male 174461   Yes    3916.8    7322.93
## 378   43 Female 190407   Yes    4694.9    7875.56
## 379   50 Female 110149   Yes    5102.8    5478.86
## 380   38 Female 247626   Yes    5731.8    5340.25
## 381   42   Male 205641   Yes    5294.6    4751.09
## 382   51   Male 119454   Yes    4717.1    5245.62
## 383   43   Male 200491   Yes    5411.8    4309.57
## 384   37   Male 197546   Yes    5287.4    4221.31
## 385   46   Male 162741   Yes    4922.5    5346.71
## 386   42   Male 177699   Yes    5135.4    7255.36
## 387   46   Male     NA   Yes    4117.3    6144.90
## 388   43   Male 130536   Yes    5080.4    4775.69
## 389   47 Female 133038   Yes    5114.7    4685.07
## 390   34 Female 210712   Yes    5257.7    4838.53
## 391   44   Male 166319   Yes    5568.4    5608.00
## 392   41   Male 200071   Yes    7431.2    5726.34
## 393   30   Male     NA   Yes    5130.7    4546.08
## 394   43 Female 244905   Yes    4345.6    5832.68
## 395   38   Male 110137   Yes    4364.8    4809.25
## 396   38   Male 220746   Yes    5318.0    4468.87
## 397   41   Male 198365   Yes    4279.4    2208.08
## 398   38 Female 271750   Yes    5275.0    3813.21
## 399   37   Male 139063   Yes    4070.7    7595.58
## 400   47   Male 228502   Yes    4597.4    4299.84
## 401   33   Male 102390   Yes    4070.8    5683.64
## 402   36   Male 228550   Yes    3279.6    8220.56
## 403   42   Male 163590   Yes    4068.0    4190.33
## 404   44   Male 170380   Yes    4455.9    7750.84
## 405   51 Female 172848   Yes    5931.8    4522.54
## 406   38 Female 181307   Yes    3142.8    3806.81
## 407   41   Male 317476   Yes    3029.8    4179.67
## 408   34   Male 252651   Yes    5310.3    4312.21
## 409   39   Male 259350   Yes   50000.0    3172.26
## 410   44 Female     NA   Yes    4671.9    3766.91
## 411   43   Male 222222    No    4867.5    6509.25
## 412   41 Female     NA   Yes    4369.2    4327.90
## 413   33 Female 150413   Yes    5827.1    3857.16
## 414   39 Female 227953    No    5639.0    6060.18
## 415   41 Female     NA   Yes    3786.7    8638.24
## 416   41   Male 157409   Yes    5008.3    5758.64
## 417   40   Male     NA   Yes    4147.8    1275.30
## 418   42 Female 164408   Yes    5389.0    6134.82
## 419   39   Male 236079   Yes    4487.8    4509.85
## 420   32   Male 170209   Yes    4696.6    4840.47
## 421   48   Male     NA   Yes    4960.4    5512.78
## 422   43   Male     NA   Yes    4120.6    5030.21
## 423   39   Male 254858   Yes    6655.0    3198.15
## 424   32   Male     NA   Yes    7600.8    2911.39
## 425   36   Male 140316   Yes    6040.7    5157.12
## 426   41 Female     NA    No    5022.7    3757.21
## 427   40   Male 224713   Yes    4242.0    5394.22
## 428   42 Female 178619   Yes    5753.2    4099.51
## 429   37   Male 247904   Yes    5448.9    5307.12
## 430   40 Female 255187   Yes    2709.9    2033.85
## 431   46   Male 217732   Yes    3593.9    4398.09
## 432   40   Male 127078   Yes    4471.9    4951.41
## 433   37   Male 208428   Yes    6347.3    5592.74
## 434   42   Male 138567   Yes    4541.1    4949.03
## 435   35 Female 130109   Yes    6155.5    6201.71
## 436   31 Female 179653   Yes    4762.2    3879.54
## 437   42 Female     NA   Yes    3971.5    2802.65
## 438   39   Male  92395   Yes    4822.5    5251.31
## 439   42   Male 186606   Yes    4714.9    4882.97
## 440   44 Female 204052    No    5784.6    2623.46
## 441   41   Male 217297    No    5773.5    4641.28
## 442   38   Male     NA   Yes    5397.9    2587.65
## 443   37 Female 315697   Yes    6549.0    4284.06
## 444   29 Female 247093   Yes    4189.7    6875.14
## 445   41   Male 192074   Yes    7014.7    4817.97
## 446   42   Male 206799   Yes    5480.6    4970.82
## 447   33   Male 251581   Yes    5342.9    5952.70
## 448   37   Male 118144   Yes    6376.3    7152.00
## 449   37 Female 171941   Yes    6676.6    1559.85
## 450   33 Female 197351   Yes    4304.7    3590.20
## 451   30 Female  66317    No     211.0    1864.98
## 452   36 Female  73212    No     278.8    1941.53
## 453   36 Female     NA    No     330.1    1795.23
## 454   40 Female  94279    No     364.6    1521.96
## 455   21 Female  71473    No     349.7    2182.01
## 456   19   Male  79270   Yes     346.6    2124.64
## 457   25 Female  69245   Yes     380.8    2189.33
## 458   38 Female  60945   Yes     296.7    2160.57
## 459   40   Male     NA    No     255.2    2076.83
## 460   33 Female  66346    No     189.1    2005.69
## 461   31 Female  63881   Yes     269.5    1916.12
## 462   35   Male  68918   Yes     201.9    2418.43
## 463   31   Male  77909   Yes     293.2    1745.86
## 464   31 Female  68035    No     381.3    1793.15
## 465   45 Female  41776    No     305.8    2370.60
## 466   35 Female  68203    No     327.6    1701.01
## 467   41 Female  72446   Yes     258.5    1877.09
## 468   33 Female  62383   Yes     314.0    1680.48
## 469   35 Female  85725    No     354.3    1926.27
## 470   33   Male     NA    No     265.7    1892.56
## 471   30   Male  57171   Yes     298.7    2208.77
## 472   32   Male  86840    No     348.1    2029.03
## 473   36   Male     NA   Yes     203.3    2202.51
## 474   34 Female  82647    No     155.8    2214.11
## 475   39 Female  63417    No     362.5    1976.33
## 476   45 Female  54187    No     347.4    2010.68
## 477   27   Male  67893    No     291.5    1752.33
## 478   39   Male  69959    No     251.9    1811.25
## 479   37 Female  62437   Yes     242.2    1846.21
## 480   45 Female  73319    No     266.5    2029.11
## 481   33 Female     NA    No     278.6    1868.19
## 482   43   Male  74626    No     342.8    1933.08
## 483   38   Male  78948    No     268.9    1889.85
## 484   42 Female  77492   Yes     343.3    1852.36
## 485   37   Male  80766    No     340.7    2033.19
## 486   33   Male  67274   Yes     248.2    2075.62
## 487   28   Male  73423   Yes     325.3    2278.97
## 488   41 Female  58605    No     344.8    1660.44
## 489   24 Female  60377    No     338.7    2328.17
## 490   39   Male  86035   Yes     433.6    2205.64
## 491   52 Female  70661   Yes     310.5    1877.82
## 492   33   Male  70123    No     345.3    1843.23
## 493   41   Male  64594   Yes     251.9    1984.13
## 494   28   Male  65864   Yes     330.4    1783.44
## 495   47   Male  52253    No     304.4    1765.03
## 496   49 Female  47103    No     330.9    2191.16
## 497   38   Male  76596    No     257.9    1813.50
## 498   32 Female  78075    No     276.9    2126.37
## 499   35 Female     NA    No     331.7    2240.67
## 500   25 Female  66516   Yes     341.3    2368.39
## 501   39 Female  89220    No     341.9    1573.80
## 502   35   Male  52292    No     221.1    1912.28
## 503   30 Female     NA    No     381.8    2093.38
## 504   30 Female     NA    No     313.1    2153.10
## 505   26 Female     NA    No     263.4    1880.14
## 506   16   Male  66303    No     275.5    2182.04
## 507   32   Male     NA   Yes     316.8    2143.11
## 508   26 Female  65785   Yes     265.5    1855.81
## 509   37 Female  57463    No     339.2    2219.61
## 510   36 Female  64393    No     227.1    1949.13
## 511   38   Male     NA    No     291.1    2153.61
## 512   47   Male  61984    No     340.2    1942.69
## 513   28   Male  89322    No     342.4    1840.74
## 514   42 Female     NA    No     353.4    2765.04
## 515   34 Female  65354    No     225.5    1718.06
## 516   30   Male  65413   Yes     257.3    1735.10
## 517   44   Male  75326    No     249.4    1964.44
## 518   37 Female  76209   Yes     272.2    2035.49
## 519   32   Male  72854    No     282.6    1964.17
## 520   34 Female  71996    No     358.0    2040.19
## 521   35   Male  60287    No     314.2    1950.29
## 522   33   Male  71336    No     241.3    2278.14
## 523   42 Female  71264    No     425.0    1893.20
## 524   44   Male  51281   Yes     274.7    1993.12
## 525   34 Female  73234    No     349.5    2081.45
## 526   40   Male  89468   Yes     290.8    2258.78
## 527   31 Female  74734   Yes     253.3    1941.31
## 528   36   Male     NA   Yes     306.8    2073.36
## 529   40 Female  62844    No     356.3    2081.35
## 530   37   Male  70207    No     305.1    2208.15
## 531   31 Female  71249    No     265.4    1743.05
## 532   26   Male  78942   Yes     299.3    1633.25
## 533   36 Female     NA    No     303.1    2287.73
## 534   41   Male  81855    No     256.3    1805.31
## 535   42 Female  62933   Yes     373.4    1973.84
## 536   36   Male  72641    No     313.4    2063.71
## 537   32 Female  62881    No     270.5    2174.11
## 538   34 Female  60522    No     299.3    2054.17
## 539   43   Male  44160    No     307.6    2103.38
## 540   26   Male  78241    No     430.2    2091.47
## 541   46   Male  60178   Yes     323.4    2099.71
## 542   32   Male  74317    No     308.7    2140.08
## 543   35 Female  79197    No     335.0    2140.34
## 544   37   Male  68483    No     212.3    2107.29
## 545   31   Male  58372    No     291.3    1566.87
## 546   35 Female  86467    No     234.0    1914.89
## 547   31 Female  71550    No     310.9    2166.57
## 548   39 Female  75350    No     341.2    1882.17
## 549   46   Male  78974   Yes     241.0    1576.39
## 550   39   Male  61539    No     269.6    2055.00
## 551   40   Male  71487   Yes     393.0    2358.35
## 552   46 Female  60048    No     299.5    2128.48
## 553   35 Female  70807    No     219.1    2167.16
## 554   44 Female     NA   Yes     295.1    1842.93
## 555   21   Male  59310   Yes     345.3    2167.41
## 556   47   Male     NA    No     297.8    1800.30
## 557   42   Male  71756    No     315.4    1746.90
## 558   30   Male  87802    No     311.0    1981.42
## 559   38 Female  76713    No     247.0    2333.38
## 560   34   Male  56927   Yes     325.7    2138.25
## 561   43 Female     NA    No     349.9    2016.89
## 562   46   Male  88624    No     229.0    2137.06
## 563   45 Female  73858    No     306.1    2048.52
## 564   38 Female     NA   Yes     317.6    2308.71
## 565   29   Male  64288    No     367.0    2157.29
## 566   36 Female  77865   Yes     328.2    2159.56
## 567   34   Male  68452    No     237.4    2052.20
## 568   32   Male     NA    No     316.2    2002.27
## 569   37   Male  56282   Yes     252.1    1878.03
## 570   41 Female  57480    No     388.9    2001.53
## 571   48 Female  58364    No     251.0    2097.39
## 572   28 Female  71145    No     316.8    1974.00
## 573   35 Female     NA    No     260.5    1874.39
## 574   30   Male  81359   Yes     331.9    1854.31
## 575   45   Male  48177   Yes     334.3    1896.10
## 576   43   Male     NA   Yes     311.9    2199.05
## 577   39   Male  76509    No     317.3    1808.93
## 578   31 Female  70626    No     359.9    1844.58
## 579   30 Female  75600    No     217.3    2068.10
## 580   33 Female     NA    No     355.3    2034.00
## 581   39   Male  81161   Yes     324.7    2123.18
## 582   40   Male  58719    No     263.1    2070.46
## 583   32   Male  83430    No     235.8    1800.12
## 584   34 Female  57762    No     343.1    1863.79
## 585   37   Male     NA    No     280.3    2000.45
## 586   34 Female  69856    No     229.7    1900.11
## 587   27   Male  72062    No     257.2    1914.92
## 588   24   Male     NA   Yes     363.4    2253.95
## 589   35 Female     NA    No     313.6    1800.93
## 590   33 Female  75974   Yes     306.3    1950.07
## 591   31 Female  80539    No     225.9    1975.76
## 592   33   Male     NA    No     388.6    1929.75
## 593   48   Male     NA    No     264.9    1662.88
## 594   38   Male  66167    No     341.0    2619.88
## 595   25   Male  66317    No     277.2    2211.91
## 596   43 Female  58386    No     238.7    2203.26
## 597   36   Male  94470    No     326.2    1734.84
## 598   27   Male  57293   Yes     299.4    2001.80
## 599   38   Male  74080    No     293.0    1995.77
## 600   33   Male  81900   Yes     309.9    2047.93
## 601   49 Female  74314    No     304.2    1582.86
## 602   39 Female  51058   Yes     344.2    2241.17
## 603   39   Male  56819   Yes     303.8    2184.07
## 604   43   Male  73348   Yes     309.3    2175.90
## 605   37   Male     NA    No     227.8    2003.87
## 606   46   Male  71334   Yes     302.9    2155.14
## 607   27 Female  67714    No     259.4    1960.02
## 608   34   Male  74599    No     285.9    1895.79
## 609   36 Female     NA   Yes     254.9    1662.15
## 610   35 Female  54162    No     426.9    1824.69
## 611   44   Male     NA    No     412.5    1761.98
## 612   32 Female  76320   Yes     291.0    2250.24
## 613   35 Female  73300    No     272.4    2059.88
## 614   39 Female  78567   Yes     348.6    2148.27
## 615   40   Male  62515   Yes     346.8    1806.37
## 616   30   Male  80747   Yes     430.2    1795.68
## 617   31 Female  64390    No     298.1    2279.46
## 618   39   Male  94275    No     244.8    1972.00
## 619   35   Male  49074   Yes     255.4    2226.45
## 620   46 Female  88945   Yes     229.0    1881.82
## 621   27   Male  73825   Yes     388.4    2430.97
## 622   36 Female  55672    No     192.4    2102.23
## 623   30   Male  75981    No     314.3    1900.10
## 624   37   Male     NA    No     323.1    1834.27
## 625   30   Male     NA   Yes     394.7    2110.50
## 626   33   Male  85878    No     177.7    2232.62
## 627   37   Male  66628   Yes     333.9    2044.02
## 628   41 Female  69055    No     350.7    2224.51
## 629   25   Male  77831    No     309.0    2062.95
## 630   30 Female  66344    No     187.5    1812.31
## 631   44   Male  66028   Yes     266.6    2173.57
## 632   22   Male  85489    No     329.8    1883.90
## 633   31   Male  59617    No     241.2    2346.53
## 634   38   Male  70418   Yes     257.5    2095.49
## 635   23 Female  79905    No     245.2    2026.54
## 636   38   Male  79704    No     244.5    1931.56
## 637   24 Female     NA   Yes     247.6    1817.30
## 638   23   Male  66349    No     273.3    2054.91
## 639   34   Male  53946   Yes     370.5    2305.34
## 640   31 Female  54923   Yes     319.2    1990.26
## 641   35   Male  57121    No     340.1    2010.43
## 642   28   Male  70499    No     349.5    2126.77
## 643   40   Male  78799    No     292.0    1824.83
## 644   29 Female     NA    No     323.9    2080.41
## 645   30 Female  73364   Yes     380.6    1890.62
## 646   42   Male  77909   Yes     297.5    2052.50
## 647   37 Female  73123   Yes     314.7    2147.49
## 648   32   Male  68784    No     268.3    2330.25
## 649   42   Male  59126   Yes     324.7    1817.53
## 650   40 Female  76242    No     236.9    2098.14
## 651   23 Female  89178    No     205.6    2506.17
## 652   25 Female  87159   Yes     212.5    2069.52
## 653   23   Male     NA    No     193.0    1251.46
## 654   23 Female  89670    No     202.1    1263.24
## 655   26   Male  89644    No     203.3    1033.70
## 656   27 Female  93462    No     192.9    2367.26
## 657   25 Female  88802    No     202.3    1807.88
## 658   26 Female  92634    No     222.5    2268.71
## 659   20 Female  87992    No     185.5    1644.05
## 660   26 Female  83869    No     193.1    1931.37
## 661   21 Female     NA    No     161.8    1797.49
## 662   23 Female  86274    No     198.8    1210.98
## 663   22 Female  91369   Yes     198.7    2150.58
## 664   24   Male  89268    No     193.2    1513.56
## 665   29 Female  88348   Yes     216.3    2006.22
## 666   20 Female  93821    No     198.9    2884.71
## 667   22 Female  91472    No     188.2    2310.93
## 668   27 Female  82967    No     186.7    1824.53
## 669   24 Female  85943    No     212.3    2387.95
## 670   22 Female 102655   Yes     205.7    1871.73
## 671   24 Female  94031   Yes     198.9    1651.71
## 672   22 Female  92386    No     195.7    2600.20
## 673   24 Female  90311    No     200.8    2935.10
## 674   28 Female  90749    No     211.1    1496.41
## 675   26 Female  95957   Yes     196.6    2542.99
## 676   23 Female  89872    No     185.3    1942.55
## 677   24 Female  90547   Yes     194.6    2172.44
## 678   24 Female  87898    No     213.6     873.71
## 679   26 Female  82729    No     194.6    1664.86
## 680   23   Male     NA    No     210.3    1975.99
## 681   22 Female  92494    No     201.1    1919.88
## 682   24 Female     NA    No     203.7    1796.36
## 683   28 Female  90793   Yes     182.5    1524.65
## 684   27 Female  84747    No     190.9    2082.58
## 685   23 Female  89610    No     203.2    1734.34
## 686   23 Female  87122   Yes     193.0     992.96
## 687   27 Female     NA   Yes     205.4    2532.57
## 688   21 Female  86341    No     177.2    2603.03
## 689   25 Female  93571    No     217.2    2229.67
## 690   28 Female  94912    No     200.2    2801.92
## 691   23 Female  91559   Yes     200.4    1896.10
## 692   25 Female  91939    No     223.0    1889.79
## 693   21 Female  84933    No     195.5    2103.33
## 694   22   Male  92386    No     204.2    2237.61
## 695   23 Female  99408    No     200.3    2493.13
## 696   19 Female  83535    No     227.7    1490.72
## 697   23 Female  97857    No     188.2    1551.87
## 698   23 Female  92648    No     197.3    2432.04
## 699   25 Female  90828   Yes     198.1    3072.36
## 700   25 Female  95034    No     202.2    1834.32
## 701   23 Female  78323    No     210.1    2108.84
## 702   26 Female  84832    No     203.4    1236.20
## 703   23 Female  93241   Yes     210.5    1870.61
## 704   26 Female  93656   Yes     203.1    2379.60
## 705   24   Male  87635    No     197.1    1424.30
## 706   24 Female     NA    No     175.9    2412.85
## 707   24 Female     NA   Yes     191.5    1551.18
## 708   22 Female  90081    No     194.1    2035.32
## 709   23 Female  88983    No     202.2    1167.69
## 710   26 Female     NA    No     183.1    2391.23
## 711   23 Female  92176   Yes     213.1    2183.85
## 712   23 Female     NA    No     199.4    2488.37
## 713   26 Female  93115   Yes     203.9    2382.82
## 714   24 Female  81848   Yes     191.0    2334.48
## 715   22 Female  87477    No     195.3    1636.81
## 716   24 Female  92506    No     191.5    1929.63
## 717   26 Female     NA    No     189.6    1107.59
## 718   22 Female  91886    No     199.8    1492.39
## 719   26 Female     NA    No     214.1    2130.79
## 720   23 Female  88386    No     195.1    1740.06
## 721   24 Female  97019    No     188.1    2554.30
## 722   26 Female  96718    No     215.9    1949.94
## 723   24 Female  91834    No     210.3    2448.73
## 724   23 Female     NA   Yes     192.7    2444.43
## 725   26 Female  92346    No     200.6    1388.04
## 726   24 Female  92081   Yes     178.5    2259.28
## 727   25   Male  92088    No     189.3    2082.66
## 728   26   Male  94010    No     203.5    1737.52
## 729   22 Female  90477    No     195.9    2684.44
## 730   25 Female  89653   Yes     222.6    1121.35
## 731   27   Male  93193    No     178.7    2467.77
## 732   26 Female  89056   Yes     193.4    1778.74
## 733   25 Female  88515    No     198.6    1528.57
## 734   24   Male  88930    No     220.7    2069.27
## 735   24 Female  93318   Yes     198.9    2610.17
## 736   23   Male  85384   Yes     192.2    1381.04
## 737   25   Male  86249    No     185.1    3049.03
## 738   23 Female  81764    No     205.7    1040.90
## 739   22   Male  87850    No     201.8    1952.33
## 740   22 Female  93525    No     190.3    2161.74
## 741   30 Female     NA    No     203.9    1812.20
## 742   23 Female  87669   Yes     197.7    2030.82
## 743   24 Female  87565    No     203.6    2072.28
## 744   23 Female  95831    No     199.9    2208.52
## 745   26 Female  93259    No     206.7    2358.84
## 746   22 Female  93569    No     202.4    1575.05
## 747   23 Female  86881   Yes     204.6    2636.66
## 748   26 Female     NA   Yes     178.3    1458.33
## 749   26   Male  97518    No     198.5    1379.55
## 750   22 Female     NA    No     183.8    1812.87
## 751   24   Male  94164    No     201.2    1905.86
## 752   25 Female  87545    No     196.0    1430.16
## 753   25 Female  87509    No     192.5    3075.19
## 754   26 Female  85853   Yes     196.8    1603.84
## 755   27   Male  92512    No     202.8    2179.37
## 756   26 Female  85161    No     203.5    2009.07
## 757   29 Female  85068    No     183.4    1909.02
## 758   24 Female  93131    No     193.0    1791.48
## 759   25   Male  88544    No     192.8    2371.81
## 760   23 Female  87554   Yes     217.2    2037.90
## 761   25 Female  94443    No     206.5    2092.59
## 762   25   Male  87891    No     190.9    1544.12
## 763   33 Female  91748    No     188.8    2216.19
## 764   25   Male  97369    No     166.2    2229.26
## 765   26   Male  86821   Yes     201.2    1560.17
## 766   26 Female 100226    No     209.6    2174.18
## 767   25 Female  88409   Yes     182.4    1304.25
## 768   26 Female  94658    No     212.4    1572.38
## 769   24 Female 100292    No     181.8    2033.15
## 770   25 Female  90913    No     187.3    1669.90
## 771   26 Female  89461    No     200.5    2449.80
## 772   27 Female  99464    No     204.4    1768.27
## 773   26 Female 105529   Yes     186.9    2349.93
## 774   26 Female  93885    No     203.1    2145.69
## 775   25 Female  84813    No     206.0    2273.40
## 776   24 Female  99718    No     203.2    1635.11
## 777   21 Female  93695    No     209.1    1985.93
## 778   23 Female  84628    No     204.8    1753.82
## 779   26 Female  92447    No     192.5    1601.50
## 780   23 Female  90074    No     198.2    1990.16
## 781   27   Male  89414    No     198.4    2207.23
## 782   21   Male  84611    No     185.8    2150.92
## 783   24   Male  98964    No     206.1    1289.37
## 784   24 Female     NA    No     192.1    2197.87
## 785   24 Female  93071    No     202.1    1515.48
## 786   23   Male 100481   Yes     190.5    2214.41
## 787   25 Female  92883    No     208.1    2556.11
## 788   25 Female  80405    No    -500.0    1897.56
## 789   25 Female  88985   Yes     191.9    2735.55
## 790   20   Male  82467    No     205.3    2673.26
## 791   25 Female  93555   Yes     196.8    2152.20
## 792   25 Female  85530    No     209.8    2072.35
## 793   26 Female  93400    No     182.2    1934.79
## 794   23 Female     NA   Yes     195.6    2191.55
## 795   24 Female  82529    No     220.2    2160.57
## 796   24 Female  87792    No     193.3    1829.13
## 797   21 Female  91852    No     200.3    1630.41
## 798   21 Female 101811    No     205.6    1663.38
## 799   25 Female  87634    No     212.1    2495.04
## 800   22 Female     NA    No     215.5    1395.86
## 801   28 Female     NA    No     207.3    2615.89
## 802   27 Female  92935    No     203.8    1885.18
## 803   24 Female  89301    No     223.6    2726.38
## 804   23 Female     NA    No     197.9    2313.23
## 805   21 Female  90184   Yes     197.0    1835.95
## 806   24 Female     NA    No     197.1    1922.65
## 807   24 Female  89718   Yes     188.2    2199.11
## 808   27 Female  88689   Yes     211.2    1673.35
## 809   22 Female  93584    No     206.4    1801.51
## 810   24 Female  89054    No     208.2    2021.27
## 811   26 Female  91257   Yes     195.9    1628.03
## 812   24 Female  95187    No     184.6    1743.79
## 813   26   Male  83997    No     192.3    1822.91
## 814   22 Female  94830    No     199.3    1955.97
## 815   29   Male  87054    No     193.6    2800.16
## 816   23   Male     NA    No     192.2    1531.26
## 817   24   Male  97373    No     185.7    2178.25
## 818   21 Female     NA   Yes     196.6    2250.53
## 819   26 Female     NA    No     200.9    1529.23
## 820   18 Female  89416   Yes     209.5    1926.47
## 821   24 Female  79865    No     212.4    2293.59
## 822   24 Female     NA   Yes     204.8    2007.49
## 823   24 Female  93991    No     210.0    1932.58
## 824   25 Female  89760    No     206.9    1832.92
## 825   23 Female  91634    No     202.1    2082.89
## 826   26   Male  96071   Yes     191.6    1663.72
## 827   26 Female  84007    No     208.1    2181.24
## 828   21 Female  83531    No     215.1    1257.70
## 829   23 Female  90214    No     198.7    2134.74
## 830   23 Female  92445   Yes     187.9    1540.97
## 831   26 Female  88502    No     199.3    1680.43
## 832   25 Female  86094    No     199.8    2297.09
## 833   19 Female  92813    No     186.7    1041.54
## 834   24 Female  95866   Yes     198.1    2098.53
## 835   25 Female  94382    No     210.5    1958.94
## 836   26 Female  92755    No     218.7    1638.54
## 837   26   Male  88378    No     199.5    1504.64
## 838   27   Male  89076    No     204.5    2991.98
## 839   23   Male  94024    No     192.1    1772.65
## 840   25 Female 100952    No     198.8     746.72
## 841   25 Female  84472    No     211.7    1173.95
## 842   24 Female  89810    No     198.9    2665.06
## 843   26 Female  91373   Yes     216.4    2194.18
## 844   25 Female  93085    No     209.4    1167.05
## 845   26 Female  87590   Yes     186.6    2407.72
## 846   24 Female  89558    No     211.7    1910.56
## 847   25 Female  85957    No     186.3    1856.43
## 848   21 Female  89582   Yes     198.1    1986.69
## 849   26 Female  84049   Yes     196.6    1400.31
## 850   26   Male     NA    No     217.2    2018.28
## 851   29   Male  90753   Yes     207.4    2086.91
## 852   26 Female  92267    No     198.7    1122.82
## 853   25 Female  92906    No     204.4    1245.15
## 854   26   Male  95905    No     195.4    2164.48
## 855   25 Female  85401    No     204.1    1614.14
## 856   25 Female  91776    No     201.2    2439.70
## 857   21 Female  88008    No     194.1    1708.93
## 858   27 Female  90734   Yes     210.3    2265.74
## 859   23 Female  86666   Yes     189.6    2127.42
## 860   24 Female  88266   Yes     208.2    1585.71
## 861   24 Female  94957    No     216.2    1239.82
## 862   26 Female  84550    No     209.7    1459.45
## 863   26 Female  85205   Yes     210.1    1933.98
## 864   23   Male  89640    No     194.0    1738.68
## 865   21   Male  87527    No     210.5    2113.96
## 866   25 Female     NA    No     211.1    2412.76
## 867   25   Male     NA    No     201.1    1676.81
## 868   24 Female  88349   Yes     193.6    2528.62
## 869   25 Female  90856    No     178.4    1754.47
## 870   23 Female  82922    No     219.9    1994.64
## 871   23 Female  89201    No     219.8    1376.26
## 872   22 Female  98708    No     207.7    2029.09
## 873   29 Female  77346    No     203.9    2129.71
## 874   20 Female  80078    No     185.9    1334.98
## 875   23 Female  93774    No     191.1    1576.85
## 876   26 Female  98187   Yes     189.5    2033.07
## 877   24   Male  92192    No     203.0    2000.24
## 878   27 Female  92132    No     182.8     996.29
## 879   25 Female  90718   Yes     197.7    1356.00
## 880   24   Male  86815    No     192.6    1225.00
## 881   23 Female  83075    No   30000.0    1678.91
## 882   24 Female  82550    No     195.5    2627.20
## 883   24 Female  96434    No     209.3    2255.37
## 884   25 Female  90856    No     203.8    2228.48
## 885   26   Male  87497   Yes     194.9    1161.85
## 886   24 Female  86970    No     200.1    1680.50
## 887   24 Female  93899   Yes     211.5    1988.68
## 888   23   Male  94526   Yes     212.7    2367.53
## 889   25 Female  93275    No     194.7    1929.53
## 890   25 Female  95425    No     205.1    2729.22
## 891   21 Female  91260    No     198.3    1737.75
## 892   22   Male  82007    No     200.3    1070.51
## 893   24 Female  88170    No     199.9    1284.37
## 894   23   Male  87804    No     202.1    1878.80
## 895   28 Female  92103    No     213.6    1598.35
## 896   27   Male  86813    No     191.1    2047.26
## 897   27 Female  92033    No     209.0    2412.89
## 898   26 Female  90950    No     197.1    1872.80
## 899   25 Female     NA    No     197.7    1713.57
## 900   24 Female  96891    No     208.2    3317.68
## 901   24 Female  81280   Yes     200.6    1583.43
## 902   22 Female  96508    No     203.1    1945.31
## 903   23 Female  95593    No     191.4    2081.22
## 904   24   Male  94335    No     207.6    1760.71
## 905   25 Female  98297    No     178.2    1787.44
## 906   26 Female  87400    No     208.8    2087.68
## 907   26 Female  79641    No     200.0    1558.82
## 908   26 Female  89721   Yes     191.1    1838.62
## 909   24 Female  91569    No     213.4    2196.67
## 910   23 Female  85771    No     191.0    2029.90
## 911   22 Female  88315    No     199.9    2477.18
## 912   26 Female  91320    No     193.3    2424.61
## 913   27   Male  90735    No     184.9    2891.32
## 914   27 Female  91906    No     210.5    2030.71
## 915   24 Female  92975    No     212.8    2329.11
## 916   25 Female  93852    No     192.1    2113.64
## 917   26 Female  92238    No     214.5    1230.39
## 918   24   Male  89175   Yes     181.7    1106.88
## 919   28 Female     NA    No     195.9    2661.26
## 920   26 Female  91589   Yes     216.5    1770.59
## 921   25 Female  82520   Yes     215.6    1788.27
## 922   29 Female  87591    No     205.0    2921.28
## 923   27 Female  90303    No     198.9    1870.39
## 924   26 Female  98462    No     185.7    1906.87
## 925   22   Male  91553    No     200.7    1777.50
## 926   22 Female  93934   Yes     208.8    1520.62
## 927   25   Male  88732    No     204.2    2162.41
## 928   23 Female  87819    No     214.4    2913.31
## 929   21 Female  89840    No     217.2    1357.66
## 930   20 Female  99267   Yes     194.2    2476.18
## 931   27 Female  94708    No     196.4    1773.61
## 932   23 Female  89509    No     176.8    2749.34
## 933   21 Female  95848   Yes     210.3    1907.57
## 934   24 Female  80674    No     208.3    1805.54
## 935   23 Female  95800    No     213.7    1645.49
## 936   25 Female  96685    No     209.7    1385.90
## 937   23 Female  87160    No   30000.0    3298.91
## 938   25 Female  90943    No     202.3    1820.34
## 939   22 Female  88924    No     195.5    2171.47
## 940   20 Female  88201   Yes     200.8    2263.93
## 941   27 Female  94575    No     190.0    2370.88
## 942   24 Female  90619    No     194.2    1930.61
## 943   26 Female  94013   Yes     205.2    1357.30
## 944   23 Female  91746   Yes     198.4    2071.73
## 945   28 Female  95897    No     183.0    1514.87
## 946   23 Female  88709   Yes     186.0    2037.08
## 947   27 Female  85255   Yes     198.5    1609.47
## 948   22   Male 100969   Yes     219.5    2171.74
## 949   23 Female  84444   Yes     181.4    1140.41
## 950   28 Female  83810   Yes     181.0    2888.90
## 951   30 Female  95001    No     182.5    2443.99
## 952   25 Female  82673    No     203.7    2554.74
## 953   23 Female  88359   Yes     200.2    2142.99
## 954   21 Female  99124    No     215.1    2006.19
## 955   26 Female  98547   Yes     189.5    2697.73
## 956   22   Male  94905   Yes     207.1    2171.02
## 957   26   Male  92472    No     180.4    1207.09
## 958   22 Female     NA   Yes     187.2    2742.98
## 959   27 Female  87088    No     207.2    2676.69
## 960   22 Female  90801    No     188.2     470.05
## 961   24 Female  94831   Yes     193.2    1440.43
## 962   28   Male  89949    No     200.2    1859.17
## 963   22 Female     NA    No     194.9    1322.05
## 964   21 Female  89963    No     206.4    3027.63
## 965   22 Female  94185    No     210.1    2344.93
## 966   25 Female     NA    No     218.2    1420.93
## 967   26 Female     NA    No     192.7    1800.12
## 968   24   Male  91300    No     207.1    2188.52
## 969   24 Female  98840    No     196.9    1798.61
## 970   26 Female  87102    No     184.6    1115.42
## 971   26 Female  84162    No     189.8    1441.61
## 972   23   Male  88609   Yes     214.2    2846.34
## 973   22 Female     NA   Yes     202.5    2319.09
## 974   24 Female  84310   Yes     194.2    1424.43
## 975   26   Male  86864    No     189.6    1471.70
## 976   22   Male  81595    No     189.9    1832.26
## 977   26 Female  86016    No     201.2    3210.12
## 978   23   Male  86794    No     202.6    2093.19
## 979   26 Female  95342    No     199.0    2036.47
## 980   28 Female  88251    No     220.3    2097.26
## 981   27 Female  92442   Yes     202.8    1800.57
## 982   21 Female  88723   Yes     202.4    1525.74
## 983   27   Male 100162   Yes     209.5     995.36
## 984   26 Female  84919   Yes     186.4    2216.21
## 985   28 Female  88605   Yes     209.8    2124.79
## 986   27   Male  94157    No     213.9    2618.90
## 987   24 Female  92570   Yes     201.5    2093.85
## 988   22 Female     NA   Yes     190.9    1752.03
## 989   26 Female  86991    No     207.2    1528.81
## 990   23   Male  96072    No     193.9    1128.63
## 991   25   Male  88152   Yes     201.9    2675.73
## 992   22   Male  80839    No     196.0    2621.30
## 993   27 Female  93987    No     202.7    1684.63
## 994   24 Female  94993    No     199.9    2216.16
## 995   21   Male  95579    No     192.6    2489.26
## 996   23 Female  88040    No     206.0    1885.09
## 997   25 Female  91034    No     198.7    2437.62
## 998   26 Female 100724    No     195.0    2360.36
## 999   23   Male  88926    No     190.7    1724.03
## 1000  26 Female  94769    No     199.7    2482.66
##      store_trans online_trans Q1 Q2 Q3 Q4 Q5 Q6 Q7 Q8
## 1              2            2  4  2  1  2  1  4  1  4
## 2              4            2  4  1  1  2  1  4  1  4
## 3              7            2  5  2  1  2  1  4  1  4
## 4             10            2  5  2  1  3  1  4  1  4
## 5              4            4  4  1  1  3  1  4  1  4
## 6              4            5  4  2  1  2  1  4  1  4
## 7              5            3  4  1  1  2  1  4  1  4
## 8             11            5  5  2  1  3  1  4  1  4
## 9              6            1  4  1  1  2  1  4  1  4
## 10            12            1  4  2  1  3  1  4  1  4
## 11             5            4  4  1  1  3  1  4  1  4
## 12             6            2  4  1  1  3  1  4  1  4
## 13             7            4  5  1  1  3  1  4  1  4
## 14             7            3  4  2  1  3  1  4  1  4
## 15             5            5  4  2  1  2  1  4  1  4
## 16             5            1  5  2  1  2  1  4  1  4
## 17             5            3  5  2  1  2  1  4  1  4
## 18             5            2  5  2  1  3  1  4  1  4
## 19             7            2  4  2  1  3  1  4  1  4
## 20             4            2  5  2  1  2  1  4  1  5
## 21            11            2  5  1  1  3  1  4  1  4
## 22             7            3  4  2  1  2  1  4  1  4
## 23             6            1  5  2  1  2  1  4  1  4
## 24             8            3  5  1  1  2  1  4  1  4
## 25            11            3  4  1  1  3  1  4  1  4
## 26             7            1  5  1  1  3  1  4  1  4
## 27             6            2  4  1  1  3  1  4  1  4
## 28             4            4  5  1  1  2  1  4  1  4
## 29             4            3  5  1  1  2  1  4  1  4
## 30             9            2  5  2  1  2  1  4  1  4
## 31             4            2  4  2  1  3  1  4  1  4
## 32             2            3  5  2  1  2  1  4  1  4
## 33             6            4  4  1  1  3  1  4  1  4
## 34             8            3  4  1  1  2  1  4  1  4
## 35             9            4  4  2  1  3  1  4  1  4
## 36             6            4  5  2  1  2  1  4  1  4
## 37             6            2  4  2  1  3  1  4  1  4
## 38             6            5  5  1  1  3  1  4  1  4
## 39             5            4  4  2  1  2  1  4  1  4
## 40             6            2  4  1  1  2  1  4  1  4
## 41             2            3  4  1  1  3  1  4  1  4
## 42             5            1  5  1  1  3  1  4  1  4
## 43             9            4  5  1  1  2  1  4  1  3
## 44             4            5  4  1  1  3  1  4  1  4
## 45             9            1  4  1  1  3  1  4  1  4
## 46             8            2  5  1  1  3  1  4  1  4
## 47            11            3  5  1  1  3  1  4  1  4
## 48             9            4  5  2  1  3  1  4  1  4
## 49             5            2  4  1  1  2  1  4  1  4
## 50             6            2  4  1  1  2  1  4  1  4
## 51             6            1  5  2  1  3  1  4  1  4
## 52             5            7  4  1  1  2  1  4  1  4
## 53            11            3  5  2  1  2  1  4  1  4
## 54             4            4  4  1  1  2  1  4  1  4
## 55             9            2  5  2  1  3  1  4  1  4
## 56             6            4  5  1  1  2  1  4  1  4
## 57             5            2  4  1  1  3  1  4  1  4
## 58             7            4  5  1  1  2  1  4  1  4
## 59             7            2  5  1  1  3  1  4  1  4
## 60             7            4  4  1  1  2  1  4  1  4
## 61             6            6  5  2  1  2  1  4  1  4
## 62             4            5  4  2  1  2  1  4  1  4
## 63            11            2  5  1  1  2  1  4  1  4
## 64             9            3  5  2  1  2  1  4  1  4
## 65             3            5  4  2  1  2  1  4  1  4
## 66             8            2  4  1  1  3  1  4  1  4
## 67             5            6  4  1  1  2  1  4  1  4
## 68             7            7  5  1  1  3  1  4  1  4
## 69             6            1  4  1  1  3  1  4  1  4
## 70            10            3  5  2  1  2  1  4  1  4
## 71             3            2  4  1  1  3  1  4  1  4
## 72             1            5  5  2  1  2  1  4  1  4
## 73            10            3  5  2  1  3  1  4  1  3
## 74             4            2  5  1  1  2  1  4  1  4
## 75             6            1  4  2  1  2  1  4  1  4
## 76             2            1  4  2  1  3  1  4  1  4
## 77             5            3  5  2  1  3  1  4  1  4
## 78             8            2  5  2  1  2  1  4  1  4
## 79             8            2  5  2  1  3  1  4  1  4
## 80             6            2  5  1  1  2  1  4  1  4
## 81             4            5  5  1  1  3  1  4  1  4
## 82             5            3  5  2  1  3  1  4  1  5
## 83             7            5  4  2  1  3  1  4  1  4
## 84            10            1  5  1  1  3  1  4  1  4
## 85             7            3  5  2  1  2  1  4  1  4
## 86             5            2  4  1  1  2  1  4  1  4
## 87             8            4  4  2  1  3  1  4  1  4
## 88             5            1  4  1  1  3  1  4  1  4
## 89             6            3  4  2  1  3  1  4  1  4
## 90             7            2  5  2  1  2  1  4  1  4
## 91             2            3  5  1  1  3  1  4  1  4
## 92             4            3  4  1  1  3  1  4  1  4
## 93             8            6  5  2  1  3  1  4  1  5
## 94             4            5  4  1  1  2  1  4  1  4
## 95             7            1  4  2  1  3  1  4  1  4
## 96             7            2  5  2  1  2  1  4  1  4
## 97             7            2  5  2  1  2  1  4  1  3
## 98             4            2  5  2  1  3  1  4  1  4
## 99             5            2  4  1  1  2  1  4  1  4
## 100            8            5  5  1  1  2  1  4  1  4
## 101            4            3  4  1  1  3  1  4  1  4
## 102            3            1  5  1  1  3  1  4  1  4
## 103            5            3  4  1  1  2  1  4  1  4
## 104            6            3  5  2  1  2  1  4  1  4
## 105            5            5  5  1  1  3  1  4  1  4
## 106            5            4  5  2  1  3  1  4  1  4
## 107           10            2  5  2  1  2  1  4  1  4
## 108            4            1  4  1  1  3  1  4  1  4
## 109            6            1  5  1  1  3  1  4  1  4
## 110            6            6  5  2  1  2  1  4  1  4
## 111            3            2  5  1  1  2  1  4  1  4
## 112            6            4  4  1  1  3  1  4  1  4
## 113            9            4  4  1  1  2  1  4  1  4
## 114            7            2  4  1  1  3  1  4  1  4
## 115            6            4  5  1  1  2  1  4  1  4
## 116            7            1  4  1  1  2  1  4  1  4
## 117            7            2  5  1  1  3  1  4  1  4
## 118            8            2  4  2  1  2  1  4  1  4
## 119            6            4  5  1  1  2  1  4  1  4
## 120            7            1  5  2  1  3  1  4  1  4
## 121            4            2  5  1  1  2  1  4  1  4
## 122            7            3  4  2  1  3  1  4  1  4
## 123            5            1  5  1  1  3  1  4  1  4
## 124            8            4  5  2  1  2  1  4  1  4
## 125            7            2  5  2  1  3  1  4  1  4
## 126            4            3  4  2  1  3  1  4  1  4
## 127            6            2  4  2  1  2  1  4  1  4
## 128           11            1  4  2  1  3  1  4  1  4
## 129            5            2  5  1  1  3  1  4  1  4
## 130            8            4  4  1  1  3  1  4  1  4
## 131           10            1  4  2  1  3  1  5  1  4
## 132            6            3  5  2  1  2  1  4  1  4
## 133            1            2  5  2  1  2  1  4  1  4
## 134            6            5  5  2  1  3  1  4  1  4
## 135            5            4  4  1  1  2  1  4  1  4
## 136            4            4  4  2  1  3  1  4  1  4
## 137            6            7  5  1  1  3  1  4  1  4
## 138            5            3  4  1  1  2  1  4  1  4
## 139            9            1  4  1  1  3  1  4  1  4
## 140            5            2  5  2  1  2  1  4  1  4
## 141            9            3  5  2  1  2  1  4  1  4
## 142            6            2  5  1  1  2  1  5  1  4
## 143           10            1  4  2  1  3  1  4  1  4
## 144            4            2  5  1  1  2  1  4  1  4
## 145            6            2  4  2  1  3  1  4  1  4
## 146            5            2  4  1  1  3  1  4  1  4
## 147            7            3  5  1  1  3  1  4  1  4
## 148            3            3  5  1  1  2  1  4  1  4
## 149            3            2  5  1  1  2  1  4  1  5
## 150            7            3  5  2  1  2  1  4  1  4
## 151            6            1  4  1  1  2  1  4  1  4
## 152            8            3  5  1  1  2  1  4  1  4
## 153           10            5  5  1  1  3  1  4  1  4
## 154            6            1  5  2  1  3  1  4  1  4
## 155            7            3  4  2  1  2  1  4  1  4
## 156            3            4  4  2  1  3  1  4  1  4
## 157            8            3  4  1  1  2  1  4  1  4
## 158            5            4  5  1  1  3  1  4  1  4
## 159            5            2  5  2  1  3  1  4  1  4
## 160            2            3  4  1  1  3  1  4  1  4
## 161            8            2  5  2  1  3  1  4  1  4
## 162            5            3  5  2  1  2  1  4  1  4
## 163            4            2  5  1  1  2  1  4  1  4
## 164            7            4  4  1  1  2  1  4  1  4
## 165            4            5  4  1  1  2  1  4  1  4
## 166           11            2  4  1  1  3  1  4  1  4
## 167            9            3  5  2  1  2  1  4  1  4
## 168            8            3  5  1  1  2  1  4  1  4
## 169            5            4  4  1  1  2  1  4  1  4
## 170            5            1  5  1  1  3  1  4  1  4
## 171            6            6  5  1  1  3  1  4  1  4
## 172            9            2  4  1  1  2  1  4  1  4
## 173            6            2  4  2  1  2  1  4  1  4
## 174            5            2  4  1  1  3  1  4  1  4
## 175            7            2  4  1  1  2  1  4  1  4
## 176            5            4  4  2  1  2  1  4  1  4
## 177            7            1  5  2  1  2  1  4  1  4
## 178            7            1  5  1  1  2  1  4  1  4
## 179            8            4  5  1  1  3  1  4  1  4
## 180            5            3  4  1  1  2  1  4  1  4
## 181            4            3  5  1  1  3  1  4  1  4
## 182            5            2  5  2  1  3  1  4  1  4
## 183            4            4  5  2  1  2  1  4  1  4
## 184            8            4  5  2  1  3  1  4  1  4
## 185            6            3  5  2  1  3  1  4  1  4
## 186            7            3  5  2  1  3  1  4  1  4
## 187            2            3  4  2  1  3  1  4  1  4
## 188            7            7  5  2  1  2  1  4  1  4
## 189            8            3  5  2  1  3  1  4  1  4
## 190            5            1  4  1  1  2  1  4  1  4
## 191            9            2  4  1  1  3  1  4  1  4
## 192            5            3  4  2  1  3  1  4  1  4
## 193            3            1  4  1  1  2  1  4  1  4
## 194            5            3  5  2  1  3  1  4  1  4
## 195            8            7  5  1  1  3  1  4  1  4
## 196            4            2  5  1  1  3  1  4  1  4
## 197            7            3  5  2  1  3  1  4  1  4
## 198            9            4  5  2  1  2  1  4  1  4
## 199            5            3  5  2  1  3  1  4  1  4
## 200            8            3  4  1  1  3  1  4  1  4
## 201            3            5  5  2  1  3  1  4  1  4
## 202           11            5  5  2  1  2  1  4  1  4
## 203            9            1  4  2  1  2  1  4  1  4
## 204            3            3  4  2  1  3  1  4  1  4
## 205            5            5  4  1  1  2  1  4  1  4
## 206           10            4  4  2  1  3  1  4  1  4
## 207            3            3  5  2  1  3  1  4  1  4
## 208            4            6  4  1  1  2  1  4  1  4
## 209           10            6  5  2  1  2  1  4  1  4
## 210            5            3  4  1  1  2  1  4  1  4
## 211            4            1  4  2  1  2  1  4  1  4
## 212            5            2  5  2  1  3  1  4  1  4
## 213            9            7  4  2  1  3  1  4  1  4
## 214            6            2  4  1  1  3  1  4  1  4
## 215            6            3  4  1  1  3  1  4  1  4
## 216            3            5  4  2  1  3  1  4  1  4
## 217           11            3  4  1  1  2  1  4  1  4
## 218            5            2  4  2  1  2  1  4  1  4
## 219            5            4  4  1  1  2  1  4  1  4
## 220            5            4  4  1  1  2  1  4  1  4
## 221           10            1  4  1  1  2  1  4  1  4
## 222            3            6  4  2  1  3  1  4  1  4
## 223            6            2  4  1  1  3  1  4  1  4
## 224            6            4  5  2  1  3  1  4  1  5
## 225            6            5  5  2  1  2  1  4  1  4
## 226            4            2  5  1  1  2  1  4  1  4
## 227            6            2  4  1  1  3  1  4  1  4
## 228            6            4  5  1  1  3  1  4  1  4
## 229            3            1  5  2  1  3  1  4  1  4
## 230            4            4  4  1  1  3  1  4  1  4
## 231            4            2  4  2  1  3  1  4  1  4
## 232            6            2  4  1  1  2  1  4  1  4
## 233            5            2  5  1  1  2  1  4  1  4
## 234            9            4  4  2  1  3  1  4  1  4
## 235            4            6  4  2  1  3  1  4  1  4
## 236            7            2  5  2  1  3  1  4  1  4
## 237            6            1  5  2  1  2  1  4  1  4
## 238           10            4  4  2  1  2  1  4  1  4
## 239            3            5  5  2  1  3  1  4  1  4
## 240            3            2  4  2  1  2  1  4  1  4
## 241            2            2  4  1  1  3  1  4  1  4
## 242            7            1  5  1  1  3  1  4  1  4
## 243            5            2  4  1  1  3  1  5  1  4
## 244           12            2  5  2  1  2  1  4  1  4
## 245            4            4  4  1  1  2  1  4  1  4
## 246           10            3  5  1  1  2  1  4  1  4
## 247            5            2  5  2  1  3  1  4  1  4
## 248            7            4  5  2  1  2  1  4  1  4
## 249            3            5  5  1  1  3  1  4  1  4
## 250            8            3  4  1  1  3  1  4  1  4
## 251           12            8  1  4  5  4  4  4  4  1
## 252            7            8  1  4  5  4  4  4  4  1
## 253            9           12  1  4  5  4  4  4  4  1
## 254           18            9  1  4  4  4  4  4  4  1
## 255           10            8  1  4  5  4  4  4  4  1
## 256            7           10  1  4  4  4  4  4  4  1
## 257           10           15  1  4  4  4  4  4  4  1
## 258            7           14  1  4  4  4  4  3  4  1
## 259           11           13  1  4  5  4  4  4  4  1
## 260           12           12  1  4  4  4  4  4  4  1
## 261            6           16  1  4  4  4  4  4  4  1
## 262           15            6  1  4  4  4  4  4  4  1
## 263           10           11  1  4  4  4  4  4  4  1
## 264           11            9  1  4  5  4  4  4  4  1
## 265           12            8  1  4  5  4  4  4  4  1
## 266            4           13  1  4  4  4  4  4  4  1
## 267            7            8  1  4  4  4  4  4  4  1
## 268            5           12  1  4  5  5  4  4  4  1
## 269            8            8  1  4  5  4  4  4  4  1
## 270           10           15  1  4  5  4  4  4  4  1
## 271           11            8  1  4  4  4  4  4  4  1
## 272           12           16  1  4  4  4  4  4  4  1
## 273           13           12  1  4  4  4  4  4  4  1
## 274            8           10  1  4  5  4  4  4  4  1
## 275            6           12  1  4  5  4  4  4  4  1
## 276           10           15  1  4  4  4  4  4  4  1
## 277            8            9  1  4  4  4  4  4  4  1
## 278           13           13  1  4  5  4  4  4  4  1
## 279           10           14  1  4  4  4  4  4  4  1
## 280           12            8  1  4  4  4  4  4  4  1
## 281           14           14  1  4  4  4  4  4  4  1
## 282           10           11  1  4  5  4  4  4  4  1
## 283           11           11  1  4  5  4  4  4  4  1
## 284           12            7  1  4  4  4  4  4  4  1
## 285           11           13  1  4  4  4  4  4  4  1
## 286            9            7  1  4  5  4  4  4  4  1
## 287           13           14  1  4  4  4  4  4  4  1
## 288           12           11  1  4  5  4  4  4  4  1
## 289           11           10  1  4  4  4  4  4  4  1
## 290           10           16  1  4  5  4  4  4  4  1
## 291           11           13  1  4  4  4  4  4  4  1
## 292            9           10  1  4  4  4  4  4  4  1
## 293           11           12  1  4  4  4  4  4  4  1
## 294           10           13  1  4  4  4  4  4  4  1
## 295           16           10  1  4  5  4  4  4  4  1
## 296           14            8  1  4  5  4  4  4  4  1
## 297           10           15  1  4  5  4  4  4  4  1
## 298            9           11  1  4  4  4  4  4  4  1
## 299           13           12  1  4  4  4  4  4  4  1
## 300           15           12  1  4  4  4  4  4  4  1
## 301           12           13  1  4  4  4  4  4  4  1
## 302            8           11  1  4  4  4  4  4  4  1
## 303           13            9  1  4  4  4  4  4  4  1
## 304            8            9  1  4  4  4  4  4  4  1
## 305           11           17  1  4  5  4  4  4  4  1
## 306            9           12  1  4  4  4  4  4  4  1
## 307           12           12  1  4  5  4  4  4  4  1
## 308           10            9  1  5  5  4  4  4  4  1
## 309           17           10  1  4  4  4  4  4  4  1
## 310           14           12  1  4  4  4  4  4  4  1
## 311           13            7  1  4  5  4  4  4  4  1
## 312           11           12  1  4  4  4  4  4  4  1
## 313            8            5  1  4  4  4  4  4  4  1
## 314           14           12  1  4  5  4  4  4  4  1
## 315           14           12  1  4  5  4  4  4  4  1
## 316           13           12  1  4  4  4  4  4  4  1
## 317            6           12  1  4  4  4  4  4  4  1
## 318            6           10  1  4  5  4  4  4  4  1
## 319            9           13  1  4  4  4  4  4  4  1
## 320           11           15  1  4  5  4  4  4  4  1
## 321            5           12  1  4  5  4  4  4  4  1
## 322           13           11  1  5  5  4  4  4  4  1
## 323           13           12  1  4  4  4  4  4  4  1
## 324           12           13  1  4  4  4  4  4  4  1
## 325           16           10  1  4  5  4  4  4  4  1
## 326           11           10  1  4  4  4  4  4  4  1
## 327           16            7  1  4  4  4  4  4  4  1
## 328           14            9  1  4  4  4  4  4  5  1
## 329           10           15  1  4  5  4  4  4  4  1
## 330           12            8  1  4  4  4  4  4  4  1
## 331           10           16  1  4  4  4  4  4  4  1
## 332           12            7  1  4  5  4  4  4  4  1
## 333           14            8  1  4  4  4  4  4  4  1
## 334           11            4  1  4  4  4  4  4  4  1
## 335           12            8  1  4  5  4  4  4  4  1
## 336            9           17  1  4  5  4  4  4  4  1
## 337           12           10  1  4  4  4  4  4  4  1
## 338           11            7  1  4  5  4  4  4  4  1
## 339           15           11  1  4  5  4  4  4  4  1
## 340           11            5  1  4  5  4  4  4  4  1
## 341            8            7  1  4  5  4  4  4  4  1
## 342           13           17  1  4  5  4  4  4  4  1
## 343           12           12  1  4  5  4  4  4  4  1
## 344            8           13  1  4  4  4  4  4  4  1
## 345           11            7  1  4  4  4  4  4  4  1
## 346           11           12  1  4  4  4  4  4  4  1
## 347           10            6  1  4  5  4  3  4  4  1
## 348           13           14  1  4  4  4  4  4  4  1
## 349           18           11  1  4  4  4  4  4  4  1
## 350           10           10  1  4  4  4  4  4  4  1
## 351           15           19  1  4  4  4  4  4  4  1
## 352            8           13  1  4  4  4  4  4  4  1
## 353            8            8  1  4  5  4  4  4  4  1
## 354           13            9  1  4  5  4  4  4  4  1
## 355           19           11  1  4  4  4  4  4  4  1
## 356           12           12  1  4  5  4  4  4  4  1
## 357            7           18  1  4  5  4  4  4  4  1
## 358            9            8  1  4  5  4  4  4  4  1
## 359           12           14  1  4  4  4  4  4  4  1
## 360           13            7  1  4  5  4  4  4  4  1
## 361           10           13  1  4  4  4  4  4  4  1
## 362            4           10  1  4  4  4  4  4  4  1
## 363           14           14  1  4  5  4  4  4  4  1
## 364           10           13  1  4  5  4  4  4  4  1
## 365           12           10  1  4  4  4  4  4  4  1
## 366           13            6  1  4  4  4  4  4  4  1
## 367           18            8  1  5  4  4  4  4  4  1
## 368           11            7  1  4  4  4  4  4  4  1
## 369            9           13  1  4  4  4  4  4  4  1
## 370           11           13  1  4  5  4  4  4  4  1
## 371            9           12  1  4  4  4  4  4  4  1
## 372            5           14  1  4  5  4  4  4  4  1
## 373            8           14  1  4  4  4  4  4  4  1
## 374           12            8  1  4  4  4  4  4  4  1
## 375           12            9  1  4  4  4  3  4  4  1
## 376           12            9  1  4  5  4  4  4  4  1
## 377            7           14  1  4  5  4  4  4  4  1
## 378            6           11  1  4  5  4  4  4  4  1
## 379           11           14  1  4  4  4  4  4  4  1
## 380           12            7  1  4  5  4  4  4  4  1
## 381           10           14  1  4  5  4  4  4  4  1
## 382           12           11  1  4  4  4  4  4  4  1
## 383            8           23  1  4  4  4  4  4  4  1
## 384            8           11  1  4  5  4  4  4  4  1
## 385           11           13  1  4  4  4  4  4  4  1
## 386            5            6  1  4  4  4  4  4  4  1
## 387           15            9  1  4  5  4  4  4  4  1
## 388           13           15  1  4  5  4  4  5  4  1
## 389           13            9  1  4  4  4  4  4  4  1
## 390           13           10  1  4  4  4  4  4  4  1
## 391            8           14  1  4  4  4  4  4  4  1
## 392            8           15  1  4  5  4  4  4  4  1
## 393            9            7  1  4  4  4  4  4  4  1
## 394           10           14  1  4  5  4  4  4  4  1
## 395           17           10  1  4  5  4  4  4  4  1
## 396           14           18  1  4  4  4  4  4  4  1
## 397            7           14  1  4  4  4  4  4  4  1
## 398            5           15  1  4  4  4  4  4  4  1
## 399           16           13  1  4  4  4  4  4  4  1
## 400            9           18  1  4  5  4  4  4  4  1
## 401           13           11  1  4  4  4  4  4  4  1
## 402            8           12  1  4  5  4  4  4  4  1
## 403           12           11  1  4  5  4  4  4  4  1
## 404            8            9  1  4  5  4  4  4  4  1
## 405           10           11  1  4  5  4  4  4  4  1
## 406            8           13  1  4  4  4  4  4  4  1
## 407           11           12  1  4  5  4  4  4  4  1
## 408           10           12  1  4  5  4  4  4  4  1
## 409           13           11  1  4  4  4  4  4  4  1
## 410            7            7  1  4  5  4  4  4  4  1
## 411           20           12  1  4  5  4  4  4  4  1
## 412            6           13  1  4  4  4  4  4  4  1
## 413           18           12  1  4  5  4  4  4  4  1
## 414            9           11  1  4  4  4  4  4  4  1
## 415           14           10  1  4  4  4  4  4  4  1
## 416           11            4  1  4  5  4  4  4  4  1
## 417           16            5  1  4  4  4  4  4  4  1
## 418            7            8  1  4  4  4  4  4  4  1
## 419            6            9  1  4  5  4  4  4  4  1
## 420           13            9  1  4  5  4  4  4  4  1
## 421            9            8  1  4  5  4  4  4  4  1
## 422           11           14  1  4  4  4  4  4  4  1
## 423           10           15  1  4  4  4  4  4  4  1
## 424           12            8  1  4  4  4  4  4  4  1
## 425            9           10  1  4  5  4  4  4  4  1
## 426            9           11  1  4  5  4  4  4  4  1
## 427           12            9  1  4  5  4  4  4  4  1
## 428            8           11  1  4  4  4  4  4  4  1
## 429            7           10  1  4  4  4  4  4  4  1
## 430           10            8  1  4  4  4  4  4  4  1
## 431           17            7  1  4  5  4  4  4  4  1
## 432            9           10  1  4  5  4  4  4  4  1
## 433           16            7  1  4  4  4  5  4  4  1
## 434            8           14  1  4  5  4  4  4  4  1
## 435            9           13  1  4  5  4  4  4  4  1
## 436           15           11  1  4  4  4  4  4  4  1
## 437           10           11  1  4  5  4  4  4  4  1
## 438            8            5  1  4  5  4  4  4  4  1
## 439           11            6  1  4  5  4  4  4  4  1
## 440           14           12  1  4  4  4  4  4  4  1
## 441           11           12  1  4  4  4  4  4  4  1
## 442           10            9  1  4  4  4  4  4  4  1
## 443           13           11  1  4  5  4  4  4  4  1
## 444           13           15  1  4  5  4  4  4  4  1
## 445           14            8  1  4  4  4  4  4  4  1
## 446            7           12  1  4  4  4  4  4  4  1
## 447           11           20  1  4  5  4  4  4  4  1
## 448           13            9  1  4  4  4  4  4  4  1
## 449           11           10  1  4  5  4  4  4  4  1
## 450            9           15  1  4  5  4  4  3  4  1
## 451            1           15  4  2  2  4  2  2  4  2
## 452            2           14  5  1  3  4  3  2  4  1
## 453            4           16  4  1  2  4  2  1  4  1
## 454            3           16  5  1  2  4  2  2  4  2
## 455            2           15  5  2  3  4  2  1  4  2
## 456            4           27  4  2  2  4  2  2  4  1
## 457            4           10  4  1  3  4  3  2  4  2
## 458            3           10  4  1  2  4  3  1  4  1
## 459            4           18  4  1  2  4  2  2  4  2
## 460            1           18  4  1  2  4  3  1  4  2
## 461            3           14  5  1  3  4  2  2  4  1
## 462            5           16  5  2  3  4  2  2  4  1
## 463            2           25  4  2  2  4  2  1  3  1
## 464            2           19  4  2  3  4  3  1  4  1
## 465            2           14  4  2  2  4  3  2  4  1
## 466            1           23  4  2  3  4  2  2  4  2
## 467            4           14  4  2  2  4  3  2  4  1
## 468            3           18  4  1  2  4  2  1  4  1
## 469            4           13  4  1  2  4  3  2  4  2
## 470            2           12  4  1  2  4  2  2  4  1
## 471            4           23  4  2  3  4  3  2  4  2
## 472            3           21  4  1  3  4  3  1  4  2
## 473            2           15  5  1  2  4  2  2  4  2
## 474            1           16  5  2  3  4  3  1  4  2
## 475            4           16  5  1  3  4  3  1  4  2
## 476            2           17  4  2  2  4  3  1  4  1
## 477            2           17  4  2  2  4  2  2  4  2
## 478            6           16  5  1  2  4  3  1  4  2
## 479            5           18  5  2  3  4  3  2  4  2
## 480            1           21  5  1  2  4  2  2  4  1
## 481            3           18  4  2  3  4  3  1  4  1
## 482            4           10  4  1  2  4  3  2  4  2
## 483            2           11  4  2  3  4  3  1  4  2
## 484            1            8  5  1  2  4  3  1  4  1
## 485            5           10  4  2  3  4  3  2  4  1
## 486            3           13  5  2  2  4  2  2  4  2
## 487            1           16  5  2  2  4  2  1  4  1
## 488            5           12  5  2  2  4  3  2  4  1
## 489            3           26  4  2  3  4  3  2  4  2
## 490            3           14  4  1  2  4  2  2  4  1
## 491            4           20  4  1  3  4  3  2  4  1
## 492            2           18  4  1  2  4  2  1  4  2
## 493            4           27  5  2  2  4  3  2  4  2
## 494            3           23  4  2  2  4  2  1  4  2
## 495            2           18  5  1  2  4  3  1  4  2
## 496            3           16  5  2  3  4  2  1  4  1
## 497            5           19  5  2  2  4  3  1  4  2
## 498            4           19  5  1  2  4  2  1  4  1
## 499            4           14  5  2  3  4  2  1  4  2
## 500            2           19  5  2  2  4  3  1  4  2
## 501            2           14  4  1  3  4  3  2  4  1
## 502            3           18  5  1  3  4  2  1  4  2
## 503            3           18  5  1  2  4  2  2  4  2
## 504            6           16  5  1  2  4  3  2  4  1
## 505            2           15  5  2  2  4  3  2  4  1
## 506            1           15  5  1  2  4  3  2  4  2
## 507            3           17  4  1  2  4  2  2  4  2
## 508            4           15  4  2  3  4  3  2  4  2
## 509            2           17  5  2  3  4  3  2  4  1
## 510            4           20  4  2  2  4  2  2  4  1
## 511            3           14  5  2  3  4  2  2  4  1
## 512            2           13  4  2  3  4  2  2  4  2
## 513            5           16  5  2  2  4  2  2  4  2
## 514            3           11  4  1  3  4  3  2  4  2
## 515            1           17  5  1  3  4  3  2  4  1
## 516            4           13  5  1  2  4  2  2  4  2
## 517            2           22  5  2  2  4  3  2  4  1
## 518            2           15  4  2  3  4  2  2  4  2
## 519            1           22  4  1  2  4  2  2  4  1
## 520            5           21  4  1  2  4  2  1  4  1
## 521            5           15  4  1  3  4  3  2  4  2
## 522            3           18  4  1  2  4  2  1  4  1
## 523            2            6  5  2  3  4  3  2  4  1
## 524            4            8  5  2  3  4  3  1  4  2
## 525            4           21  4  1  2  4  2  2  4  1
## 526            4           15  4  2  2  4  3  2  4  1
## 527            4           17  4  1  3  4  3  2  4  1
## 528            1           17  4  1  3  4  3  1  4  2
## 529            4           17  5  1  3  4  3  2  4  1
## 530            3           18  4  2  3  4  2  2  4  1
## 531            2            8  4  1  3  4  2  2  4  2
## 532            5           12  5  2  3  4  2  2  4  1
## 533            4           13  4  1  2  4  3  2  4  2
## 534            3           12  4  2  2  4  2  1  4  1
## 535            5           18  5  2  3  4  3  2  4  1
## 536            3           13  5  2  2  4  3  1  4  2
## 537            6           17  5  1  3  4  2  2  4  2
## 538            3           16  4  1  2  4  2  2  4  2
## 539            2           26  4  1  2  4  2  1  4  1
## 540            3           14  4  2  2  4  2  1  4  2
## 541            1           17  4  2  3  4  2  2  4  2
## 542            1           18  4  2  3  4  3  2  4  1
## 543            3           15  5  2  2  4  2  1  4  2
## 544            3           14  5  1  3  4  2  2  4  1
## 545            1           20  4  1  2  4  2  1  4  1
## 546            6           11  5  2  2  4  3  2  4  2
## 547            4           14  4  1  2  4  2  1  4  2
## 548            3           22  4  2  2  4  2  2  4  2
## 549            1           11  4  1  2  4  2  1  4  1
## 550            2           13  4  1  3  4  3  2  4  1
## 551            1           10  5  1  3  4  3  1  4  2
## 552            1           20  4  2  2  4  3  1  4  2
## 553            6           12  5  2  3  4  3  2  4  2
## 554            3           13  4  2  2  4  2  1  4  1
## 555            7            7  4  1  3  4  3  1  4  2
## 556            1           22  5  2  3  4  2  2  4  1
## 557            2           13  5  2  2  4  2  2  4  2
## 558            1           13  4  1  2  4  2  1  4  1
## 559            3            9  5  1  3  4  3  1  4  2
## 560            3           12  4  1  3  4  3  1  4  2
## 561            5           19  4  2  3  4  2  2  4  2
## 562            2           18  5  2  2  4  3  1  4  1
## 563            4           22  5  1  2  4  3  2  4  2
## 564            3           15  5  1  2  4  3  1  4  2
## 565            7           12  5  1  3  4  3  2  4  1
## 566            2           12  5  2  3  4  3  1  4  2
## 567            3           15  5  1  3  4  2  1  4  2
## 568            2           18  4  1  3  4  2  1  4  2
## 569            2           20  4  2  3  4  2  1  4  1
## 570            4           17  5  2  3  4  2  1  4  2
## 571            3           15  4  1  3  4  3  2  4  1
## 572            2           12  4  2  2  4  3  1  4  1
## 573            1           17  4  1  2  4  2  2  4  2
## 574            4           21  5  1  3  4  3  1  4  1
## 575            3           10  4  2  2  4  2  1  4  2
## 576            2           24  5  1  3  4  2  1  4  1
## 577            2           16  4  2  2  4  3  1  4  2
## 578            3           15  5  1  2  4  3  1  4  1
## 579            1           13  5  1  3  4  2  2  4  2
## 580            7           18  5  1  3  4  2  1  4  2
## 581            4           11  5  2  3  4  2  1  4  1
## 582            2           15  5  1  3  4  2  1  4  2
## 583            2           18  5  2  2  4  3  2  4  2
## 584            3           28  5  1  3  4  3  1  4  2
## 585            4           13  5  2  2  4  3  1  4  2
## 586            5           20  5  2  2  4  2  1  4  2
## 587            5           13  5  2  3  4  2  1  4  1
## 588            4           15  5  2  3  4  3  1  4  2
## 589            1            7  5  1  3  4  2  2  4  2
## 590            3           13  5  2  2  4  3  2  4  1
## 591            1           15  4  2  3  4  2  2  4  1
## 592            3           16  5  1  3  4  2  2  4  2
## 593            2            7  4  1  3  4  2  1  4  1
## 594            1           15  4  2  2  4  3  1  4  2
## 595            3           12  5  1  3  4  3  2  4  2
## 596            2           17  4  1  2  4  2  1  4  2
## 597            3           19  4  2  3  4  3  1  4  2
## 598            1           17  5  1  2  4  2  1  4  1
## 599            2           17  4  1  2  4  3  1  4  1
## 600            3           17  4  1  2  4  2  2  4  2
## 601            1           21  4  2  3  4  3  2  4  2
## 602            2           10  4  1  3  4  3  1  4  1
## 603            2           22  5  1  3  4  2  1  4  2
## 604            3           21  5  2  2  4  2  1  4  2
## 605            4           14  5  2  3  4  2  1  4  2
## 606            6           16  4  1  2  4  3  1  4  1
## 607            3           20  4  1  2  4  2  1  4  2
## 608            2           14  4  1  3  4  2  2  4  1
## 609            2           12  5  1  3  4  3  1  4  1
## 610            1           14  5  1  2  4  2  2  4  2
## 611            2           20  4  1  2  4  2  1  4  1
## 612            2           26  4  1  2  4  3  1  4  2
## 613            1           15  5  2  2  4  3  1  4  2
## 614            3           10  4  2  2  4  2  1  4  2
## 615            2           21  5  2  3  4  2  1  4  2
## 616            2           16  5  2  3  4  3  2  4  1
## 617            1           22  4  1  2  4  2  2  4  1
## 618            4           22  5  2  3  4  3  1  4  1
## 619            1            8  4  2  3  4  3  1  4  1
## 620            2            9  4  2  3  4  2  2  4  1
## 621            3           11  5  1  2  4  3  1  4  2
## 622            4            8  4  1  3  4  2  1  4  2
## 623            4           19  4  1  3  4  2  2  4  1
## 624            4           16  5  1  3  4  3  1  4  2
## 625            2           15  4  1  3  4  2  2  4  2
## 626            4           16  4  2  2  5  2  1  4  2
## 627            3           24  5  1  2  4  2  1  4  1
## 628            1           13  5  2  3  5  2  1  4  2
## 629            3           22  5  1  2  4  3  1  4  1
## 630            4           18  5  1  2  4  3  1  4  2
## 631            1           19  4  2  3  4  3  2  4  2
## 632            3           26  4  2  2  4  2  1  4  1
## 633            3           14  4  1  3  4  3  1  3  2
## 634            6           13  5  2  3  4  3  2  4  2
## 635            4           10  5  2  3  4  2  2  4  1
## 636            1           15  5  1  2  4  2  2  4  1
## 637            2           15  4  2  3  4  2  2  4  1
## 638            1            8  4  2  2  4  3  1  4  2
## 639            3           14  4  1  2  4  2  1  4  2
## 640            4           19  5  2  2  4  2  1  4  1
## 641            1           19  4  2  3  4  3  2  4  2
## 642            2           20  4  1  3  4  3  1  4  1
## 643            2           14  5  1  2  4  2  1  5  2
## 644            1           18  4  2  3  4  3  2  4  2
## 645            4           10  4  1  2  4  3  1  4  1
## 646            1           13  5  2  3  4  3  2  4  1
## 647            1           19  5  2  3  4  3  2  4  2
## 648            4           17  5  2  2  4  3  1  4  2
## 649            2           12  5  2  3  4  2  1  4  2
## 650            3           16  5  2  2  4  3  1  4  1
## 651            3           23  3  1  1  2  4  1  5  3
## 652            3           21  2  1  1  1  4  1  4  3
## 653            2           11  2  1  1  1  4  1  5  2
## 654            3           18  3  1  1  1  4  1  4  3
## 655            2           24  2  1  1  1  4  1  5  3
## 656            5           18  3  1  1  1  4  1  5  2
## 657            5           26  3  1  1  2  4  1  5  3
## 658            3           27  3  1  1  2  4  1  5  2
## 659            4           28  3  1  1  1  4  1  4  2
## 660            2           19  3  1  1  2  4  1  5  3
## 661            4           25  3  1  1  2  4  1  5  3
## 662            3           33  2  1  1  1  4  1  5  3
## 663            3           23  3  1  1  2  4  1  5  2
## 664            3           21  3  1  1  1  4  1  4  3
## 665            3           19  3  1  1  1  4  1  5  2
## 666            2           25  3  1  1  1  4  1  4  2
## 667            3           25  2  1  1  2  4  1  4  2
## 668            4           31  2  1  1  2  4  1  4  2
## 669            3           22  3  1  1  2  4  1  4  3
## 670            2           24  2  1  1  2  4  1  4  3
## 671            1           17  2  1  1  2  4  1  4  2
## 672            6           22  3  1  1  2  4  1  5  3
## 673            3           22  2  1  1  2  4  1  5  2
## 674            2           16  2  1  1  2  4  1  5  3
## 675            5           15  2  1  1  2  4  1  5  3
## 676            5           13  3  1  1  1  4  1  4  3
## 677            2           19  2  1  1  2  4  1  4  2
## 678            1           24  2  1  1  2  4  1  5  2
## 679            1           17  3  1  1  1  4  1  4  3
## 680            2           20  3  1  1  1  4  1  4  3
## 681            4           23  3  1  1  2  4  1  4  2
## 682            2           26  3  1  1  1  4  1  5  2
## 683            3           25  2  1  1  2  4  1  4  2
## 684            1           18  3  1  1  2  4  1  5  2
## 685            1           18  2  1  1  2  4  1  5  2
## 686            5           20  3  1  1  2  4  1  4  3
## 687            4           19  3  1  1  2  4  1  4  3
## 688            1           22  3  1  1  1  3  1  5  2
## 689            4           12  2  1  1  2  4  1  5  2
## 690            2           18  3  1  1  2  4  1  4  3
## 691            2           21  2  1  1  2  4  1  5  2
## 692            3           29  3  1  1  2  4  1  4  3
## 693            5           22  3  1  1  2  4  1  4  2
## 694            3           22  2  1  1  2  4  1  5  2
## 695            3           21  3  1  1  1  4  1  5  3
## 696            1           22  2  1  1  2  4  1  4  2
## 697            5           21  3  1  1  1  4  1  4  3
## 698            1           26  2  1  1  2  4  1  5  2
## 699            2           20  3  1  1  1  4  1  4  2
## 700            4           16  3  1  1  2  4  1  5  2
## 701            4           19  2  1  1  1  4  1  5  3
## 702            2           24  2  1  1  2  4  1  4  3
## 703            4           23  2  1  1  1  4  1  4  3
## 704            4           17  2  1  1  1  4  1  4  3
## 705            3           14  2  1  1  2  4  1  4  2
## 706            2           21  2  1  1  1  4  1  4  3
## 707            1           21  2  1  1  2  4  1  4  3
## 708            2           22  2  1  1  1  4  1  5  2
## 709            1           13  2  1  1  1  4  1  5  3
## 710            3           27  3  1  1  2  4  1  5  2
## 711            2           19  3  1  1  2  4  1  5  3
## 712            6           19  2  1  1  2  4  1  5  2
## 713            4           24  3  1  1  1  4  1  5  3
## 714            3           19  2  1  1  2  4  1  5  3
## 715            4           20  3  1  1  2  4  1  5  3
## 716            5           19  2  1  1  1  4  1  5  3
## 717            3           17  3  1  1  2  4  1  4  2
## 718            3           21  3  1  1  2  4  1  5  3
## 719            3           25  3  1  1  1  4  1  4  3
## 720            4           17  2  1  1  2  4  1  5  3
## 721            6           16  3  1  1  1  4  1  4  3
## 722            4           21  2  1  1  2  4  1  4  2
## 723            6           17  3  1  1  1  4  1  4  2
## 724            4           21  2  1  1  2  4  1  5  2
## 725            4           22  3  1  1  1  4  1  5  2
## 726            5           18  2  1  1  1  4  1  4  2
## 727            1           18  2  1  1  1  4  1  4  2
## 728            3           18  2  1  1  2  4  1  4  2
## 729            4           20  2  1  1  1  4  1  4  2
## 730            1           24  3  1  1  1  4  1  4  2
## 731            5           21  2  1  1  2  4  1  5  3
## 732            3           17  2  1  1  2  4  1  5  3
## 733            2           22  2  1  1  1  4  1  5  2
## 734            4           18  2  1  1  2  4  1  4  2
## 735            3           23  2  1  1  2  4  1  5  3
## 736            5           24  2  1  1  2  4  1  4  2
## 737            2           20  2  1  1  2  4  1  4  2
## 738            3           24  2  1  1  1  4  1  4  2
## 739            3           24  3  1  1  1  4  1  4  3
## 740            3           25  3  1  1  1  4  1  4  3
## 741            5           18  3  1  1  1  4  1  4  3
## 742            4           22  3  1  1  2  4  1  5  3
## 743            3           17  3  1  1  1  4  1  5  2
## 744            4           16  2  1  1  2  4  1  4  2
## 745            2           24  3  1  1  1  4  1  5  3
## 746            2           22  3  1  1  2  4  1  5  3
## 747            3           19  2  1  1  2  4  1  5  3
## 748            2           22  2  1  1  2  4  1  5  3
## 749            2           21  3  1  1  1  4  1  5  3
## 750            3           17  2  1  1  2  4  1  5  3
## 751            3           23  3  1  1  2  4  1  5  2
## 752            1           23  2  1  1  1  4  1  4  2
## 753            2           22  2  1  1  1  4  1  4  2
## 754            2           19  3  1  1  2  4  1  4  3
## 755            3           22  3  1  1  1  4  1  4  3
## 756            4           17  2  1  1  1  4  1  5  3
## 757            3           15  2  1  1  2  4  1  4  2
## 758            2           22  3  1  1  1  4  1  5  2
## 759            2           15  3  1  1  1  4  1  5  2
## 760            1           20  2  1  1  1  4  1  5  2
## 761            5           15  3  1  1  1  4  1  4  2
## 762            5           21  3  1  1  1  4  1  4  2
## 763            5           20  2  1  1  2  4  1  5  3
## 764            2           20  2  1  1  1  4  1  5  2
## 765            2           16  3  1  1  2  3  1  4  2
## 766            2           27  3  1  1  2  4  1  5  3
## 767            1           24  2  1  1  1  4  1  4  2
## 768            3           28  3  1  1  2  4  1  5  3
## 769            2           20  3  1  1  2  4  1  5  3
## 770            4           21  2  1  1  2  4  1  4  3
## 771            1           23  3  1  1  2  4  1  5  2
## 772            3           26  2  1  1  1  4  1  5  2
## 773            5           17  2  1  1  1  4  1  5  2
## 774            7           13  3  1  1  2  4  1  5  2
## 775            6           29  2  1  1  1  4  1  4  3
## 776            2           15  2  1  1  2  3  1  4  2
## 777            3           23  2  1  1  1  4  1  5  3
## 778            3           25  3  1  1  1  4  1  4  3
## 779            2           27  2  1  1  2  4  1  5  2
## 780            4           20  2  1  1  1  4  1  4  3
## 781            3           25  3  1  1  2  4  1  4  2
## 782            2           21  3  1  1  2  4  1  4  2
## 783            2           20  2  1  1  1  4  1  5  2
## 784            4           18  2  1  1  1  4  1  4  3
## 785            1           23  3  1  1  1  4  1  4  2
## 786            1           36  2  1  1  2  4  1  5  3
## 787            3           15  2  1  1  1  4  1  5  3
## 788            2           20  2  1  1  1  4  1  5  2
## 789            3           11  2  1  1  1  4  1  4  3
## 790            3           13  3  1  1  1  4  1  4  2
## 791            1           25  2  1  1  1  4  1  5  3
## 792            4           23  3  1  1  1  4  1  5  2
## 793            4           17  2  1  1  1  4  1  4  3
## 794            2           19  3  1  1  1  4  1  4  2
## 795            4           17  2  1  1  1  4  1  4  3
## 796            4           12  2  1  1  2  4  1  5  3
## 797            5           23  3  1  1  1  4  1  5  3
## 798            4           26  3  1  1  1  4  1  4  3
## 799            5           19  2  1  1  2  4  1  5  2
## 800            6           18  2  1  1  2  4  1  4  2
## 801            3           26  3  1  1  1  4  1  5  3
## 802            2           20  3  1  1  1  4  1  5  2
## 803            2           23  2  1  1  1  4  1  4  3
## 804            2           21  3  1  1  2  3  1  5  3
## 805            3           16  3  1  1  2  4  1  5  3
## 806            4           20  2  1  1  1  4  1  5  2
## 807            2           27  3  1  1  1  4  1  4  2
## 808            1           19  3  1  1  2  4  1  5  2
## 809            2           18  3  1  1  2  4  1  5  3
## 810            3           19  3  1  1  1  4  1  5  2
## 811            2           12  2  1  1  2  4  1  4  3
## 812            3           19  2  1  1  1  4  1  5  2
## 813            2           22  3  1  1  1  4  1  5  3
## 814            4           20  2  1  1  2  4  1  4  3
## 815            3           20  3  1  1  1  4  1  4  2
## 816            1           25  3  1  1  1  4  1  4  2
## 817            1           20  2  1  1  2  4  1  4  2
## 818            5           23  2  1  1  1  4  1  5  3
## 819            1           22  2  1  1  2  4  1  5  3
## 820            3           28  2  1  1  1  4  1  4  2
## 821            3           24  3  1  1  2  4  1  5  3
## 822            5           22  3  1  1  1  4  1  4  2
## 823            1           23  3  1  1  1  4  1  4  2
## 824            4           22  2  1  1  2  4  1  4  2
## 825            2           25  3  1  1  1  4  1  4  2
## 826            2           16  3  1  1  2  4  1  4  3
## 827            2           30  3  1  1  1  4  1  5  2
## 828            2           17  3  1  1  2  4  1  5  3
## 829            1           29  2  1  1  2  4  1  5  2
## 830            3           22  3  1  1  2  4  1  4  2
## 831            6           25  2  1  1  1  4  1  5  3
## 832            1           23  3  1  1  2  4  1  5  3
## 833            2           18  3  1  1  2  4  1  4  3
## 834            2           34  3  1  1  2  4  1  5  2
## 835            7           21  2  1  1  1  4  1  4  2
## 836            3           25  2  1  1  1  4  1  4  3
## 837            2           22  3  1  1  2  4  1  5  2
## 838            5           19  2  1  1  1  4  1  5  3
## 839            2           21  3  1  1  1  4  1  5  3
## 840            3           23  3  1  1  1  4  1  4  2
## 841            4           21  3  1  1  1  4  1  4  2
## 842            2           20  2  1  1  1  4  1  4  2
## 843            2           22  2  1  1  1  4  1  4  2
## 844            2           22  3  1  1  2  4  1  5  2
## 845            9           21  3  1  1  2  4  1  5  2
## 846            4           24  2  1  1  2  4  1  5  2
## 847            3           25  2  1  1  2  4  1  5  2
## 848            2           27  3  1  1  2  4  1  5  3
## 849            6           19  2  1  1  2  4  1  4  2
## 850            2           24  2  1  1  2  4  1  5  3
## 851            2           22  2  1  1  1  4  1  4  3
## 852            2           22  2  1  1  1  4  1  5  3
## 853            3           22  2  1  1  1  4  1  5  2
## 854            2           13  2  1  1  2  4  1  4  3
## 855            4           20  2  1  1  2  4  1  5  2
## 856            6           19  2  1  1  1  4  1  5  2
## 857            6           19  2  1  1  1  4  1  4  2
## 858            4           25  3  1  1  2  4  1  4  3
## 859            1           21  2  1  1  1  4  1  4  3
## 860            5           28  2  1  1  2  4  1  5  2
## 861            5           14  3  1  1  1  4  1  5  3
## 862            3           34  3  1  1  1  4  1  4  3
## 863            4           20  3  1  1  2  4  1  4  2
## 864            3           21  3  1  1  1  4  1  4  2
## 865            2           19  2  1  1  2  4  1  5  2
## 866            4           18  2  1  1  2  4  1  4  3
## 867            4           22  2  1  1  2  4  1  5  2
## 868            1           16  2  1  1  1  4  1  4  2
## 869            2           19  2  1  1  1  4  1  4  3
## 870            3           24  2  1  1  1  4  1  4  3
## 871            2           27  3  1  1  1  4  1  5  2
## 872            2           22  3  1  1  1  4  1  5  2
## 873            2           19  3  1  1  2  4  1  4  3
## 874            4           19  3  1  1  2  4  1  5  2
## 875            2           23  3  1  1  2  4  1  5  2
## 876            3           24  2  1  1  1  4  1  5  3
## 877            4           17  2  1  1  1  4  1  4  2
## 878            1           25  3  1  1  2  4  1  4  3
## 879            5           20  2  1  1  2  4  1  4  3
## 880            2           20  3  1  1  2  4  1  5  3
## 881            2           14  3  1  1  2  4  1  4  3
## 882            3           23  2  1  1  2  4  1  5  3
## 883            2           23  3  1  1  2  4  1  5  3
## 884            4           23  2  1  1  2  4  1  4  2
## 885            2           22  2  1  1  1  4  1  5  2
## 886            3           16  3  1  1  2  4  1  4  2
## 887            1           30  2  1  1  2  4  1  4  2
## 888            3           20  3  1  1  1  4  1  4  2
## 889            4           24  2  1  1  1  4  1  4  2
## 890            3           18  3  1  1  1  4  1  4  2
## 891            5           21  3  1  1  1  4  1  5  2
## 892            4           17  2  1  1  2  4  1  5  3
## 893            3           22  3  1  1  2  4  1  5  3
## 894            4           18  3  1  1  2  4  1  4  2
## 895            3           22  3  1  1  1  4  1  4  3
## 896            2           20  3  1  1  1  4  1  4  2
## 897            3           23  2  1  1  1  4  1  5  3
## 898            2           19  2  1  1  1  4  1  4  2
## 899            4           25  2  1  1  1  4  1  5  3
## 900            4           17  3  1  1  2  4  1  5  2
## 901            1           34  2  1  1  1  4  1  4  3
## 902            3           19  2  1  1  2  4  1  5  3
## 903            4           24  2  1  1  2  4  1  4  2
## 904            2           24  3  1  1  2  4  1  5  2
## 905            4           28  3  1  1  1  4  1  5  3
## 906            3           19  3  1  1  2  4  1  5  2
## 907            4           23  2  1  1  2  4  1  4  2
## 908            1           13  2  1  1  2  4  1  5  3
## 909            5           23  2  1  1  1  4  1  4  3
## 910            5           26  2  1  1  2  4  1  5  3
## 911            3           18  3  1  1  1  4  1  5  3
## 912            4           25  2  1  1  1  4  1  5  2
## 913            7           19  2  1  1  2  4  1  5  3
## 914            3           18  2  1  1  2  4  1  4  2
## 915            3           21  3  1  1  2  4  1  4  3
## 916            6           17  3  1  1  1  4  1  5  3
## 917            1           22  2  1  1  2  4  1  4  3
## 918            1           18  3  1  1  2  4  1  4  3
## 919            4           18  3  1  1  2  4  1  4  2
## 920            2           20  2  1  1  1  4  1  5  3
## 921            2           21  2  1  1  2  4  1  4  2
## 922            2           33  2  1  1  1  4  1  5  3
## 923            6           13  3  1  1  2  4  1  5  3
## 924            4           19  2  1  1  1  4  1  5  3
## 925            4           27  2  1  1  2  4  1  5  2
## 926            1           20  2  1  1  2  4  1  4  3
## 927            5           24  3  1  1  2  4  1  4  3
## 928            2           14  2  1  1  2  4  1  5  3
## 929            1           17  2  1  1  1  4  1  5  2
## 930            3           21  2  1  1  2  4  1  5  2
## 931            1           24  3  1  1  1  4  1  4  3
## 932            3           20  3  1  1  1  4  1  4  2
## 933            3           31  3  1  1  2  4  1  5  2
## 934            3           17  3  1  1  2  4  1  4  3
## 935            1           22  2  1  1  2  4  1  5  2
## 936            3           21  3  1  1  1  4  1  5  2
## 937            5           21  3  1  1  1  4  1  4  3
## 938            5           19  2  1  1  2  4  1  5  3
## 939            2           18  2  1  1  1  4  1  5  3
## 940            1           20  2  1  1  2  4  1  5  2
## 941            3           16  3  1  1  2  4  1  5  3
## 942            2           22  3  1  1  2  4  1  5  3
## 943            3           22  2  1  1  1  4  1  5  2
## 944            3           25  3  1  1  2  4  1  5  3
## 945            1           25  2  1  1  1  4  1  5  2
## 946            3           22  3  1  1  1  4  1  5  3
## 947            2           29  2  1  1  2  4  1  5  2
## 948            1           32  3  1  1  1  4  1  4  2
## 949            4           16  3  1  1  1  4  1  5  3
## 950            6           11  2  1  1  1  4  1  5  3
## 951            6           21  3  1  1  2  4  1  5  3
## 952            3           20  3  1  1  1  4  1  4  3
## 953            3           22  3  1  1  1  4  1  5  3
## 954            2           25  3  1  1  1  4  1  4  3
## 955            4           13  2  1  1  1  4  1  4  3
## 956            3           14  3  1  1  2  4  1  5  3
## 957            5           21  3  1  1  1  4  1  4  3
## 958            2           14  3  1  1  2  4  1  4  3
## 959            6           30  2  1  1  2  4  1  4  3
## 960            4           20  2  1  1  1  4  1  5  3
## 961            4           28  2  1  1  2  4  1  4  3
## 962            4           24  3  1  1  1  4  1  4  3
## 963            2           20  2  1  1  1  4  1  4  3
## 964            7           18  3  1  1  2  4  1  5  3
## 965            2           26  3  1  1  1  4  1  5  2
## 966            1           26  2  1  1  1  4  1  5  2
## 967            3           19  3  1  1  2  4  1  5  3
## 968            6           28  2  1  1  2  4  1  4  3
## 969            4           21  3  1  1  2  4  1  5  3
## 970            3           13  2  1  1  2  4  1  4  2
## 971            3           22  2  1  1  1  4  1  5  3
## 972            3           19  2  1  1  2  4  1  5  3
## 973            3           27  3  1  1  1  4  1  4  3
## 974            3           27  3  1  1  1  4  1  4  3
## 975            3           23  2  1  1  1  4  1  4  2
## 976            3           21  3  1  1  1  4  1  4  2
## 977            3           17  2  1  1  2  4  1  5  2
## 978            1           25  2  1  1  2  4  1  5  2
## 979            3           21  3  1  1  2  4  1  5  2
## 980            2           18  3  1  1  1  4  1  5  3
## 981            3           22  3  1  1  1  4  1  4  3
## 982            3           17  3  1  1  1  4  1  4  2
## 983            3           30  3  1  1  1  4  1  5  3
## 984            1           16  3  1  1  2  4  1  5  3
## 985            1           25  2  1  1  2  4  1  4  3
## 986            2           15  2  1  1  2  4  1  4  3
## 987            3           17  2  1  1  1  4  1  4  3
## 988            3           22  2  1  1  2  4  1  5  2
## 989            2           18  3  1  1  2  5  1  4  3
## 990            6           21  3  1  1  2  4  1  4  3
## 991            3           23  2  1  1  1  4  1  5  2
## 992            1           24  3  1  1  1  4  1  4  2
## 993            3           15  2  1  1  1  4  1  4  3
## 994            4           19  2  1  1  2  4  1  4  2
## 995            4           20  3  1  1  2  4  1  4  3
## 996            2           19  3  1  1  2  4  1  4  2
## 997            2           29  3  1  1  1  4  1  5  3
## 998            2           24  3  1  1  2  4  1  5  2
## 999            1           23  2  1  1  2  4  1  5  3
## 1000           3           16  2  1  1  1  4  1  5  3
##      Q9 Q10     segment total_exp
## 1     2   4       Price     832.6
## 2     1   4       Price     587.5
## 3     1   4       Price     770.1
## 4     2   4       Price     489.5
## 5     2   4       Price     491.9
## 6     1   4       Price     534.0
## 7     1   4       Price     767.1
## 8     2   4       Price     476.0
## 9     1   4       Price     750.8
## 10    1   4       Price     633.8
## 11    2   4       Price     536.8
## 12    1   4       Price     985.1
## 13    2   4       Price     774.8
## 14    1   4       Price     961.6
## 15    1   4       Price     644.3
## 16    1   4       Price     853.2
## 17    1   4       Price     692.1
## 18    2   4       Price     740.1
## 19    2   4       Price     794.9
## 20    2   4       Price     695.4
## 21    1   4       Price     669.2
## 22    2   4       Price     741.0
## 23    1   4       Price     638.5
## 24    2   4       Price     707.4
## 25    1   4       Price     534.9
## 26    2   4       Price     620.7
## 27    2   4       Price     661.9
## 28    2   4       Price     868.7
## 29    1   4       Price     714.5
## 30    2   4       Price     922.8
## 31    1   4       Price     748.5
## 32    1   4       Price     709.0
## 33    2   4       Price     859.5
## 34    2   4       Price     843.3
## 35    1   4       Price     640.6
## 36    1   4       Price     713.9
## 37    1   4       Price     666.0
## 38    1   4       Price     659.8
## 39    2   4       Price     735.4
## 40    2   4       Price     752.6
## 41    1   4       Price     734.8
## 42    2   4       Price     806.9
## 43    2   4       Price     660.5
## 44    1   4       Price     765.2
## 45    1   4       Price     771.0
## 46    1   4       Price     779.5
## 47    2   4       Price     829.0
## 48    1   4       Price     708.7
## 49    2   4       Price     743.7
## 50    2   4       Price     735.3
## 51    1   4       Price     611.4
## 52    2   4       Price     662.1
## 53    2   4       Price     722.8
## 54    1   4       Price     763.3
## 55    1   4       Price     806.8
## 56    2   4       Price     524.5
## 57    2   4       Price     785.7
## 58    2   4       Price     736.8
## 59    2   4       Price     618.6
## 60    1   4       Price     609.0
## 61    2   4       Price     696.8
## 62    2   4       Price     866.9
## 63    1   4       Price     720.3
## 64    1   4       Price     502.0
## 65    1   4       Price     885.3
## 66    2   5       Price     928.7
## 67    1   4       Price     714.9
## 68    1   4       Price     547.9
## 69    2   4       Price     546.6
## 70    1   4       Price     634.3
## 71    1   4       Price     716.2
## 72    1   4       Price     882.2
## 73    2   4       Price     832.4
## 74    1   4       Price     831.2
## 75    1   4       Price     782.0
## 76    1   4       Price     929.9
## 77    2   4       Price     552.1
## 78    1   4       Price     636.5
## 79    1   4       Price     903.6
## 80    2   4       Price     867.7
## 81    2   4       Price     673.6
## 82    1   4       Price     814.8
## 83    2   4       Price     751.1
## 84    2   4       Price     554.5
## 85    2   4       Price     678.6
## 86    1   4       Price     836.5
## 87    1   4       Price     730.4
## 88    1   4       Price     838.1
## 89    2   4       Price     814.9
## 90    2   4       Price     608.1
## 91    2   4       Price     712.7
## 92    2   4       Price     779.0
## 93    1   4       Price     713.4
## 94    1   4       Price     709.2
## 95    2   4       Price     822.6
## 96    2   4       Price     629.5
## 97    2   4       Price     676.9
## 98    1   4       Price     674.6
## 99    1   4       Price     720.2
## 100   1   4       Price     778.1
## 101   2   4       Price     652.0
## 102   1   4       Price     706.1
## 103   1   4       Price     542.6
## 104   1   4       Price     415.0
## 105   2   4       Price     678.9
## 106   2   4       Price     697.7
## 107   1   4       Price     658.0
## 108   2   4       Price     697.6
## 109   2   4       Price     682.5
## 110   1   4       Price     500.7
## 111   1   4       Price     480.2
## 112   2   4       Price     608.5
## 113   1   4       Price     484.7
## 114   1   4       Price     803.4
## 115   2   4       Price     596.3
## 116   2   4       Price     805.9
## 117   2   4       Price     566.2
## 118   2   4       Price     767.8
## 119   2   4       Price     617.3
## 120   2   4       Price     405.3
## 121   2   4       Price     711.8
## 122   1   4       Price     567.2
## 123   1   4       Price     595.7
## 124   1   4       Price     638.7
## 125   2   4       Price     748.3
## 126   2   4       Price     752.6
## 127   2   4       Price     674.8
## 128   1   4       Price     709.3
## 129   2   4       Price     704.5
## 130   1   4       Price     566.7
## 131   1   4       Price     726.7
## 132   2   4       Price     723.5
## 133   2   4       Price     607.9
## 134   1   4       Price     721.1
## 135   2   4       Price     479.0
## 136   1   4       Price     845.8
## 137   2   4       Price     770.5
## 138   2   4       Price     554.6
## 139   2   4       Price     696.9
## 140   2   4       Price     694.6
## 141   1   5       Price     626.9
## 142   1   4       Price     521.9
## 143   2   4       Price     506.0
## 144   2   4       Price     791.2
## 145   1   4       Price     508.2
## 146   2   4       Price     820.5
## 147   2   4       Price     727.8
## 148   2   4       Price     742.6
## 149   1   4       Price     702.0
## 150   1   4       Price     707.8
## 151   1   4       Price     725.5
## 152   1   4       Price     467.3
## 153   2   4       Price     920.2
## 154   2   4       Price     916.6
## 155   1   4       Price     351.9
## 156   1   4       Price     508.6
## 157   1   4       Price     732.0
## 158   2   4       Price     669.3
## 159   1   4       Price     819.7
## 160   2   4       Price     690.0
## 161   1   4       Price     906.1
## 162   2   4       Price     671.9
## 163   1   4       Price     877.5
## 164   1   4       Price     459.7
## 165   2   4       Price     618.6
## 166   2   4       Price     611.9
## 167   2   4       Price     684.4
## 168   2   4       Price     524.7
## 169   1   4       Price     820.2
## 170   2   4       Price     638.6
## 171   2   4       Price     956.2
## 172   1   4       Price     910.0
## 173   1   4       Price     763.0
## 174   1   4       Price     751.0
## 175   2   4       Price     538.0
## 176   2   4       Price     714.4
## 177   2   4       Price     793.5
## 178   2   4       Price     727.0
## 179   2   4       Price     615.5
## 180   1   4       Price     504.9
## 181   2   4       Price     674.2
## 182   2   4       Price     715.4
## 183   2   4       Price     526.7
## 184   2   4       Price     791.2
## 185   1   4       Price     729.9
## 186   2   4       Price     948.7
## 187   2   4       Price     725.5
## 188   1   4       Price     769.7
## 189   2   4       Price     743.0
## 190   1   4       Price     498.9
## 191   2   4       Price     874.5
## 192   2   4       Price     663.0
## 193   1   3       Price     612.0
## 194   1   4       Price     905.1
## 195   2   4       Price     699.1
## 196   2   4       Price     500.2
## 197   1   4       Price     581.3
## 198   2   4       Price     668.7
## 199   2   4       Price     802.0
## 200   2   4       Price     814.4
## 201   2   4       Price     797.0
## 202   1   4       Price     617.9
## 203   2   4       Price     781.3
## 204   2   4       Price     849.7
## 205   1   4       Price     811.5
## 206   2   4       Price     857.6
## 207   2   4       Price     513.3
## 208   2   4       Price     772.7
## 209   1   4       Price     710.1
## 210   1   4       Price     549.1
## 211   2   4       Price     666.7
## 212   2   4       Price     855.7
## 213   2   4       Price     738.1
## 214   1   4       Price     708.5
## 215   1   4       Price     643.8
## 216   1   4       Price     674.1
## 217   1   4       Price     598.5
## 218   2   4       Price     711.4
## 219   2   4       Price     692.4
## 220   1   4       Price     686.7
## 221   2   4       Price     828.4
## 222   2   4       Price     834.2
## 223   2   4       Price     653.0
## 224   2   4       Price     854.5
## 225   1   4       Price     786.8
## 226   2   4       Price     711.3
## 227   2   4       Price     890.5
## 228   1   4       Price     665.2
## 229   1   4       Price     794.7
## 230   1   4       Price     778.4
## 231   1   4       Price     664.7
## 232   2   4       Price     813.7
## 233   1   4       Price     679.5
## 234   2   4       Price     808.0
## 235   1   4       Price     821.9
## 236   1   4       Price     797.6
## 237   1   4       Price     710.4
## 238   1   4       Price     553.2
## 239   2   4       Price     468.5
## 240   1   4       Price     753.3
## 241   1   4       Price     762.6
## 242   1   4       Price     644.7
## 243   1   4       Price     836.3
## 244   2   4       Price     625.3
## 245   2   4       Price     595.3
## 246   1   4       Price     740.4
## 247   1   4       Price     785.9
## 248   2   4       Price     841.7
## 249   1   4       Price     751.7
## 250   1   4       Price     677.4
## 251   4   1 Conspicuous   10826.7
## 252   4   1 Conspicuous    8632.7
## 253   4   1 Conspicuous    9557.7
## 254   4   1 Conspicuous    7292.7
## 255   4   1 Conspicuous    8201.7
## 256   4   1 Conspicuous    7594.8
## 257   4   2 Conspicuous    7341.6
## 258   4   1 Conspicuous    7964.9
## 259   4   1 Conspicuous   11503.6
## 260   4   2 Conspicuous   10251.6
## 261   4   2 Conspicuous    8767.7
## 262   4   1 Conspicuous   11234.7
## 263   4   1 Conspicuous    8458.7
## 264   4   2 Conspicuous    8518.4
## 265   4   1 Conspicuous   12912.2
## 266   4   2 Conspicuous    8438.9
## 267   4   1 Conspicuous    9679.1
## 268   4   1 Conspicuous    9279.2
## 269   4   1 Conspicuous    9291.2
## 270   4   1 Conspicuous   11087.6
## 271   4   2 Conspicuous    8662.0
## 272   4   2 Conspicuous    8493.6
## 273   4   1 Conspicuous    8402.8
## 274   4   1 Conspicuous   11387.6
## 275   4   2 Conspicuous    8674.3
## 276   4   1 Conspicuous    9187.5
## 277   4   2 Conspicuous    9401.3
## 278   4   1 Conspicuous   10910.2
## 279   4   1 Conspicuous   10084.3
## 280   4   2 Conspicuous   11397.0
## 281   3   2 Conspicuous    6175.0
## 282   4   1 Conspicuous    6174.2
## 283   4   1 Conspicuous   13262.5
## 284   4   1 Conspicuous    8039.2
## 285   4   1 Conspicuous   11070.5
## 286   4   2 Conspicuous    9353.2
## 287   4   1 Conspicuous   11697.0
## 288   4   2 Conspicuous   11130.3
## 289   4   1 Conspicuous    9681.1
## 290   4   2 Conspicuous    9054.8
## 291   4   2 Conspicuous   13183.0
## 292   4   2 Conspicuous   11420.2
## 293   4   1 Conspicuous    7747.7
## 294   4   2 Conspicuous    9517.8
## 295   4   1 Conspicuous   11567.1
## 296   4   2 Conspicuous    9979.5
## 297   4   1 Conspicuous   10787.6
## 298   4   2 Conspicuous   10394.2
## 299   4   1 Conspicuous   11359.8
## 300   4   1 Conspicuous   10793.9
## 301   4   2 Conspicuous    8588.7
## 302   4   1 Conspicuous   10443.3
## 303   4   1 Conspicuous   10520.9
## 304   4   1 Conspicuous   10058.0
## 305   4   2 Conspicuous    9183.7
## 306   4   2 Conspicuous    9584.0
## 307   4   2 Conspicuous   11664.7
## 308   4   2 Conspicuous    9897.1
## 309   4   1 Conspicuous    8143.0
## 310   4   1 Conspicuous    7216.1
## 311   4   2 Conspicuous    8770.2
## 312   4   2 Conspicuous   11058.8
## 313   4   1 Conspicuous   13091.3
## 314   4   2 Conspicuous    8405.8
## 315   4   1 Conspicuous   11370.7
## 316   4   2 Conspicuous    8926.1
## 317   4   1 Conspicuous    9782.1
## 318   4   1 Conspicuous    9400.1
## 319   4   2 Conspicuous   10125.1
## 320   4   2 Conspicuous    6673.0
## 321   4   1 Conspicuous   10101.0
## 322   4   2 Conspicuous   11716.2
## 323   4   2 Conspicuous   11390.7
## 324   4   1 Conspicuous    8442.0
## 325   4   1 Conspicuous    9851.0
## 326   4   2 Conspicuous    5676.3
## 327   4   2 Conspicuous   11948.9
## 328   4   2 Conspicuous    9890.9
## 329   4   1 Conspicuous   12296.8
## 330   4   1 Conspicuous    8965.1
## 331   4   1 Conspicuous   10489.3
## 332   4   2 Conspicuous    9445.0
## 333   4   2 Conspicuous    9611.9
## 334   4   1 Conspicuous    9423.0
## 335   4   1 Conspicuous    9391.5
## 336   4   1 Conspicuous   10404.8
## 337   4   1 Conspicuous    9342.3
## 338   4   1 Conspicuous   10667.4
## 339   4   2 Conspicuous    8606.4
## 340   4   1 Conspicuous    8413.4
## 341   4   2 Conspicuous   14046.7
## 342   4   1 Conspicuous    9451.4
## 343   4   1 Conspicuous   10224.4
## 344   4   2 Conspicuous    9606.8
## 345   4   1 Conspicuous    9251.9
## 346   4   1 Conspicuous   12329.9
## 347   4   2 Conspicuous   16503.1
## 348   4   1 Conspicuous   10863.0
## 349   4   1 Conspicuous    7873.9
## 350   4   2 Conspicuous   10022.5
## 351   4   2 Conspicuous    6424.5
## 352   5   2 Conspicuous   11512.6
## 353   4   1 Conspicuous    9029.4
## 354   4   1 Conspicuous   10473.6
## 355   4   1 Conspicuous    8904.1
## 356   4   1 Conspicuous    7776.3
## 357   4   1 Conspicuous   10179.2
## 358   4   2 Conspicuous   10701.1
## 359   4   2 Conspicuous   10475.8
## 360   4   2 Conspicuous    8083.9
## 361   4   2 Conspicuous   10983.0
## 362   4   1 Conspicuous   12464.7
## 363   4   2 Conspicuous    8330.0
## 364   4   1 Conspicuous   13784.4
## 365   4   2 Conspicuous    9598.3
## 366   4   2 Conspicuous   11669.6
## 367   4   1 Conspicuous    8687.9
## 368   4   1 Conspicuous   10046.4
## 369   4   1 Conspicuous   12746.5
## 370   4   2 Conspicuous    8131.9
## 371   4   1 Conspicuous   11026.4
## 372   4   1 Conspicuous    9650.7
## 373   4   2 Conspicuous    9944.9
## 374   4   1 Conspicuous    9603.8
## 375   4   2 Conspicuous    9804.4
## 376   4   1 Conspicuous    9913.6
## 377   4   2 Conspicuous   11239.7
## 378   4   2 Conspicuous   12570.5
## 379   4   1 Conspicuous   10581.6
## 380   4   1 Conspicuous   11072.1
## 381   4   1 Conspicuous   10045.7
## 382   4   2 Conspicuous    9962.7
## 383   4   2 Conspicuous    9721.4
## 384   4   1 Conspicuous    9508.7
## 385   4   1 Conspicuous   10269.2
## 386   4   1 Conspicuous   12390.8
## 387   4   2 Conspicuous   10262.2
## 388   4   2 Conspicuous    9856.0
## 389   4   1 Conspicuous    9799.8
## 390   4   1 Conspicuous   10096.2
## 391   4   1 Conspicuous   11176.4
## 392   4   2 Conspicuous   13157.6
## 393   4   2 Conspicuous    9676.8
## 394   4   1 Conspicuous   10178.3
## 395   4   2 Conspicuous    9174.1
## 396   4   2 Conspicuous    9786.9
## 397   4   2 Conspicuous    6487.5
## 398   4   2 Conspicuous    9088.2
## 399   4   1 Conspicuous   11666.3
## 400   4   1 Conspicuous    8897.2
## 401   4   1 Conspicuous    9754.4
## 402   4   1 Conspicuous   11500.2
## 403   4   2 Conspicuous    8258.4
## 404   4   2 Conspicuous   12206.8
## 405   4   2 Conspicuous   10454.4
## 406   4   1 Conspicuous    6949.6
## 407   4   2 Conspicuous    7209.5
## 408   4   1 Conspicuous    9622.5
## 409   4   1 Conspicuous   53172.3
## 410   4   1 Conspicuous    8438.8
## 411   4   2 Conspicuous   11376.7
## 412   4   1 Conspicuous    8697.1
## 413   4   1 Conspicuous    9684.2
## 414   4   1 Conspicuous   11699.2
## 415   4   2 Conspicuous   12425.0
## 416   4   2 Conspicuous   10767.0
## 417   4   1 Conspicuous    5423.1
## 418   4   1 Conspicuous   11523.8
## 419   4   1 Conspicuous    8997.7
## 420   4   2 Conspicuous    9537.1
## 421   4   2 Conspicuous   10473.2
## 422   4   2 Conspicuous    9150.8
## 423   4   2 Conspicuous    9853.2
## 424   4   2 Conspicuous   10512.2
## 425   4   2 Conspicuous   11197.8
## 426   4   1 Conspicuous    8779.9
## 427   4   1 Conspicuous    9636.3
## 428   4   2 Conspicuous    9852.7
## 429   4   2 Conspicuous   10756.0
## 430   4   1 Conspicuous    4743.7
## 431   4   1 Conspicuous    7992.0
## 432   4   2 Conspicuous    9423.4
## 433   4   1 Conspicuous   11940.0
## 434   4   1 Conspicuous    9490.1
## 435   4   1 Conspicuous   12357.2
## 436   4   1 Conspicuous    8641.8
## 437   4   2 Conspicuous    6774.2
## 438   4   2 Conspicuous   10073.8
## 439   4   1 Conspicuous    9597.9
## 440   4   1 Conspicuous    8408.0
## 441   4   1 Conspicuous   10414.8
## 442   4   1 Conspicuous    7985.6
## 443   4   2 Conspicuous   10833.0
## 444   4   2 Conspicuous   11064.9
## 445   4   2 Conspicuous   11832.7
## 446   4   1 Conspicuous   10451.4
## 447   4   2 Conspicuous   11295.6
## 448   4   2 Conspicuous   13528.3
## 449   4   2 Conspicuous    8236.4
## 450   4   1 Conspicuous    7894.9
## 451   2   2     Quality    2076.0
## 452   3   2     Quality    2220.3
## 453   3   3     Quality    2125.3
## 454   3   3     Quality    1886.6
## 455   3   2     Quality    2531.7
## 456   3   3     Quality    2471.2
## 457   3   2     Quality    2570.2
## 458   2   2     Quality    2457.2
## 459   3   2     Quality    2332.0
## 460   2   3     Quality    2194.8
## 461   3   3     Quality    2185.7
## 462   3   3     Quality    2620.3
## 463   2   2     Quality    2039.1
## 464   2   3     Quality    2174.5
## 465   2   3     Quality    2676.4
## 466   2   3     Quality    2028.6
## 467   2   2     Quality    2135.6
## 468   3   2     Quality    1994.5
## 469   2   3     Quality    2280.6
## 470   3   2     Quality    2158.2
## 471   3   2     Quality    2507.5
## 472   3   3     Quality    2377.1
## 473   2   2     Quality    2405.8
## 474   3   3     Quality    2369.9
## 475   3   3     Quality    2338.8
## 476   2   3     Quality    2358.1
## 477   2   2     Quality    2043.8
## 478   2   2     Quality    2063.2
## 479   3   2     Quality    2088.4
## 480   3   2     Quality    2295.6
## 481   2   2     Quality    2146.8
## 482   3   3     Quality    2275.9
## 483   3   3     Quality    2158.7
## 484   3   3     Quality    2195.7
## 485   3   2     Quality    2373.9
## 486   2   3     Quality    2323.8
## 487   3   2     Quality    2604.3
## 488   2   3     Quality    2005.2
## 489   3   2     Quality    2666.8
## 490   2   3     Quality    2639.3
## 491   2   3     Quality    2188.4
## 492   2   3     Quality    2188.5
## 493   2   3     Quality    2236.0
## 494   3   2     Quality    2113.8
## 495   3   3     Quality    2069.4
## 496   3   2     Quality    2522.1
## 497   2   3     Quality    2071.4
## 498   3   3     Quality    2403.2
## 499   2   3     Quality    2572.3
## 500   2   3     Quality    2709.7
## 501   3   3     Quality    1915.7
## 502   2   2     Quality    2133.4
## 503   2   2     Quality    2475.1
## 504   3   2     Quality    2466.2
## 505   2   3     Quality    2143.6
## 506   2   3     Quality    2457.5
## 507   3   3     Quality    2459.9
## 508   3   2     Quality    2121.3
## 509   3   3     Quality    2558.9
## 510   2   3     Quality    2176.2
## 511   3   3     Quality    2444.7
## 512   2   2     Quality    2282.9
## 513   2   3     Quality    2183.1
## 514   3   3     Quality    3118.4
## 515   3   3     Quality    1943.5
## 516   3   3     Quality    1992.4
## 517   2   2     Quality    2213.8
## 518   2   3     Quality    2307.7
## 519   3   3     Quality    2246.8
## 520   3   2     Quality    2398.2
## 521   2   2     Quality    2264.5
## 522   2   2     Quality    2519.4
## 523   3   3     Quality    2318.2
## 524   2   2     Quality    2267.8
## 525   2   3     Quality    2431.0
## 526   3   3     Quality    2549.5
## 527   3   2     Quality    2194.6
## 528   3   2     Quality    2380.2
## 529   2   2     Quality    2437.6
## 530   3   2     Quality    2513.3
## 531   3   2     Quality    2008.5
## 532   3   3     Quality    1932.5
## 533   3   3     Quality    2590.8
## 534   2   3     Quality    2061.6
## 535   3   3     Quality    2347.2
## 536   3   3     Quality    2377.2
## 537   2   3     Quality    2444.7
## 538   2   2     Quality    2353.5
## 539   2   2     Quality    2411.0
## 540   2   3     Quality    2521.7
## 541   2   3     Quality    2423.1
## 542   2   2     Quality    2448.8
## 543   2   2     Quality    2475.3
## 544   2   2     Quality    2319.6
## 545   2   3     Quality    1858.2
## 546   2   2     Quality    2148.9
## 547   3   2     Quality    2477.5
## 548   3   3     Quality    2223.4
## 549   3   2     Quality    1817.4
## 550   2   2     Quality    2324.6
## 551   2   2     Quality    2751.4
## 552   3   3     Quality    2428.0
## 553   2   2     Quality    2386.3
## 554   3   3     Quality    2138.0
## 555   3   3     Quality    2512.7
## 556   3   3     Quality    2098.1
## 557   3   3     Quality    2062.3
## 558   3   3     Quality    2292.4
## 559   3   3     Quality    2580.4
## 560   2   3     Quality    2464.0
## 561   3   3     Quality    2366.8
## 562   3   3     Quality    2366.1
## 563   2   2     Quality    2354.6
## 564   2   3     Quality    2626.3
## 565   2   2     Quality    2524.3
## 566   2   3     Quality    2487.7
## 567   2   2     Quality    2289.6
## 568   3   2     Quality    2318.4
## 569   2   2     Quality    2130.1
## 570   2   2     Quality    2390.5
## 571   3   2     Quality    2348.4
## 572   2   3     Quality    2290.8
## 573   2   3     Quality    2134.9
## 574   3   2     Quality    2186.2
## 575   3   2     Quality    2230.4
## 576   2   2     Quality    2511.0
## 577   3   3     Quality    2126.3
## 578   3   3     Quality    2204.4
## 579   3   2     Quality    2285.4
## 580   3   2     Quality    2389.3
## 581   2   2     Quality    2447.9
## 582   2   2     Quality    2333.6
## 583   3   3     Quality    2035.9
## 584   2   3     Quality    2206.9
## 585   3   3     Quality    2280.7
## 586   3   2     Quality    2129.8
## 587   3   3     Quality    2172.1
## 588   3   3     Quality    2617.3
## 589   3   2     Quality    2114.5
## 590   2   3     Quality    2256.4
## 591   2   3     Quality    2201.6
## 592   2   3     Quality    2318.3
## 593   2   3     Quality    1927.8
## 594   2   3     Quality    2960.9
## 595   3   3     Quality    2489.1
## 596   3   2     Quality    2441.9
## 597   2   3     Quality    2061.0
## 598   3   2     Quality    2301.2
## 599   3   2     Quality    2288.7
## 600   2   2     Quality    2357.8
## 601   3   3     Quality    1887.0
## 602   3   3     Quality    2585.4
## 603   3   3     Quality    2487.8
## 604   3   2     Quality    2485.2
## 605   2   3     Quality    2231.6
## 606   2   3     Quality    2458.0
## 607   2   3     Quality    2219.4
## 608   3   2     Quality    2181.7
## 609   3   3     Quality    1917.0
## 610   2   3     Quality    2251.6
## 611   3   3     Quality    2174.5
## 612   3   2     Quality    2541.3
## 613   3   2     Quality    2332.3
## 614   2   3     Quality    2496.9
## 615   3   3     Quality    2153.2
## 616   3   2     Quality    2225.9
## 617   2   2     Quality    2577.6
## 618   3   2     Quality    2216.8
## 619   3   2     Quality    2481.8
## 620   3   2     Quality    2110.8
## 621   2   2     Quality    2819.3
## 622   3   3     Quality    2294.6
## 623   2   3     Quality    2214.4
## 624   3   2     Quality    2157.4
## 625   3   3     Quality    2505.2
## 626   3   2     Quality    2410.3
## 627   2   2     Quality    2377.9
## 628   3   2     Quality    2575.2
## 629   2   2     Quality    2371.9
## 630   2   2     Quality    1999.8
## 631   2   3     Quality    2440.2
## 632   2   2     Quality    2213.7
## 633   3   2     Quality    2587.7
## 634   2   3     Quality    2353.0
## 635   3   2     Quality    2271.8
## 636   3   2     Quality    2176.1
## 637   2   2     Quality    2064.9
## 638   3   2     Quality    2328.2
## 639   2   2     Quality    2675.8
## 640   3   3     Quality    2309.4
## 641   2   2     Quality    2350.5
## 642   3   2     Quality    2476.3
## 643   2   2     Quality    2116.9
## 644   2   2     Quality    2404.3
## 645   3   2     Quality    2271.2
## 646   2   3     Quality    2350.0
## 647   3   2     Quality    2462.2
## 648   2   2     Quality    2598.6
## 649   2   3     Quality    2142.2
## 650   2   3     Quality    2335.0
## 651   4   2       Style    2711.8
## 652   4   1       Style    2282.1
## 653   4   1       Style    1444.5
## 654   4   2       Style    1465.3
## 655   4   1       Style    1237.0
## 656   4   1       Style    2560.1
## 657   4   2       Style    2010.2
## 658   4   2       Style    2491.2
## 659   4   1       Style    1829.5
## 660   4   2       Style    2124.5
## 661   4   2       Style    1959.3
## 662   4   2       Style    1409.8
## 663   4   2       Style    2349.3
## 664   4   1       Style    1706.8
## 665   4   2       Style    2222.5
## 666   4   1       Style    3083.6
## 667   4   2       Style    2499.1
## 668   4   1       Style    2011.2
## 669   4   2       Style    2600.3
## 670   4   1       Style    2077.5
## 671   4   1       Style    1850.6
## 672   4   1       Style    2795.9
## 673   4   1       Style    3135.9
## 674   4   2       Style    1707.6
## 675   4   1       Style    2739.5
## 676   4   1       Style    2127.8
## 677   4   1       Style    2367.0
## 678   4   1       Style    1087.3
## 679   4   2       Style    1859.5
## 680   4   2       Style    2186.3
## 681   4   1       Style    2121.0
## 682   4   1       Style    2000.1
## 683   4   2       Style    1707.1
## 684   4   2       Style    2273.5
## 685   4   2       Style    1937.6
## 686   4   2       Style    1186.0
## 687   4   1       Style    2738.0
## 688   4   1       Style    2780.2
## 689   4   2       Style    2446.9
## 690   4   2       Style    3002.1
## 691   4   2       Style    2096.5
## 692   4   1       Style    2112.8
## 693   4   2       Style    2298.8
## 694   4   1       Style    2441.9
## 695   4   2       Style    2693.5
## 696   4   1       Style    1718.4
## 697   4   2       Style    1740.1
## 698   4   2       Style    2629.4
## 699   4   1       Style    3270.5
## 700   4   2       Style    2036.5
## 701   4   2       Style    2318.9
## 702   4   1       Style    1439.6
## 703   4   2       Style    2081.1
## 704   4   1       Style    2582.7
## 705   4   1       Style    1621.4
## 706   4   2       Style    2588.7
## 707   4   1       Style    1742.6
## 708   4   1       Style    2229.4
## 709   4   2       Style    1369.9
## 710   4   1       Style    2574.4
## 711   4   2       Style    2397.0
## 712   4   1       Style    2687.8
## 713   4   2       Style    2586.7
## 714   4   2       Style    2525.4
## 715   4   1       Style    1832.1
## 716   4   2       Style    2121.2
## 717   4   1       Style    1297.2
## 718   4   1       Style    1692.2
## 719   4   1       Style    2344.9
## 720   4   1       Style    1935.2
## 721   4   2       Style    2742.4
## 722   4   2       Style    2165.9
## 723   4   2       Style    2659.0
## 724   4   1       Style    2637.1
## 725   4   1       Style    1588.7
## 726   4   1       Style    2437.8
## 727   4   2       Style    2272.0
## 728   4   2       Style    1941.0
## 729   4   1       Style    2880.3
## 730   4   2       Style    1343.9
## 731   4   1       Style    2646.5
## 732   4   2       Style    1972.1
## 733   4   1       Style    1727.1
## 734   4   2       Style    2290.0
## 735   4   1       Style    2809.1
## 736   4   2       Style    1573.3
## 737   4   1       Style    3234.1
## 738   4   2       Style    1246.6
## 739   4   2       Style    2154.1
## 740   4   1       Style    2352.0
## 741   4   1       Style    2016.1
## 742   4   1       Style    2228.5
## 743   4   2       Style    2275.9
## 744   4   2       Style    2408.5
## 745   4   2       Style    2565.5
## 746   4   2       Style    1777.4
## 747   4   2       Style    2841.3
## 748   4   2       Style    1636.6
## 749   4   2       Style    1578.1
## 750   4   2       Style    1996.7
## 751   4   2       Style    2107.0
## 752   4   1       Style    1626.2
## 753   4   1       Style    3267.7
## 754   4   2       Style    1800.7
## 755   4   2       Style    2382.1
## 756   4   1       Style    2212.5
## 757   4   1       Style    2092.4
## 758   4   2       Style    1984.5
## 759   4   2       Style    2564.6
## 760   4   1       Style    2255.1
## 761   4   1       Style    2299.0
## 762   4   2       Style    1735.0
## 763   4   1       Style    2405.0
## 764   4   1       Style    2395.4
## 765   4   2       Style    1761.3
## 766   4   1       Style    2383.8
## 767   4   2       Style    1486.7
## 768   4   2       Style    1784.7
## 769   4   2       Style    2214.9
## 770   4   1       Style    1857.2
## 771   4   1       Style    2650.3
## 772   4   1       Style    1972.7
## 773   4   2       Style    2536.9
## 774   4   1       Style    2348.8
## 775   4   1       Style    2479.4
## 776   4   2       Style    1838.3
## 777   4   1       Style    2195.1
## 778   4   2       Style    1958.7
## 779   4   1       Style    1793.9
## 780   4   1       Style    2188.3
## 781   4   2       Style    2405.6
## 782   4   2       Style    2336.8
## 783   4   1       Style    1495.5
## 784   4   2       Style    2390.0
## 785   4   2       Style    1717.6
## 786   4   2       Style    2404.9
## 787   4   2       Style    2764.2
## 788   4   1       Style    1397.6
## 789   4   1       Style    2927.4
## 790   4   1       Style    2878.6
## 791   4   1       Style    2349.0
## 792   4   2       Style    2282.1
## 793   4   2       Style    2117.0
## 794   4   1       Style    2387.2
## 795   4   1       Style    2380.7
## 796   4   2       Style    2022.5
## 797   4   2       Style    1830.7
## 798   4   1       Style    1868.9
## 799   4   1       Style    2707.1
## 800   4   2       Style    1611.3
## 801   4   2       Style    2823.1
## 802   4   1       Style    2089.0
## 803   4   1       Style    2950.0
## 804   4   2       Style    2511.2
## 805   4   2       Style    2033.0
## 806   4   1       Style    2119.7
## 807   4   2       Style    2387.3
## 808   4   1       Style    1884.6
## 809   4   1       Style    2008.0
## 810   4   1       Style    2229.5
## 811   4   2       Style    1823.9
## 812   4   1       Style    1928.4
## 813   4   1       Style    2015.2
## 814   4   1       Style    2155.3
## 815   4   1       Style    2993.8
## 816   4   1       Style    1723.5
## 817   4   2       Style    2364.0
## 818   4   2       Style    2447.1
## 819   4   2       Style    1730.1
## 820   4   1       Style    2136.0
## 821   4   1       Style    2506.0
## 822   4   1       Style    2212.3
## 823   4   1       Style    2142.6
## 824   4   2       Style    2039.8
## 825   4   2       Style    2285.0
## 826   4   1       Style    1855.4
## 827   4   1       Style    2389.3
## 828   4   2       Style    1472.8
## 829   4   2       Style    2333.4
## 830   4   2       Style    1728.8
## 831   4   2       Style    1879.8
## 832   4   1       Style    2496.9
## 833   4   1       Style    1228.3
## 834   4   1       Style    2296.6
## 835   4   1       Style    2169.4
## 836   4   2       Style    1857.2
## 837   4   1       Style    1704.2
## 838   4   2       Style    3196.5
## 839   4   1       Style    1964.7
## 840   4   1       Style     945.5
## 841   4   2       Style    1385.6
## 842   4   1       Style    2863.9
## 843   4   1       Style    2410.6
## 844   4   2       Style    1376.4
## 845   4   1       Style    2594.3
## 846   4   1       Style    2122.3
## 847   4   2       Style    2042.7
## 848   4   1       Style    2184.8
## 849   4   2       Style    1596.9
## 850   4   1       Style    2235.5
## 851   4   2       Style    2294.3
## 852   4   2       Style    1321.5
## 853   4   1       Style    1449.5
## 854   4   1       Style    2359.9
## 855   4   2       Style    1818.2
## 856   4   1       Style    2640.9
## 857   4   1       Style    1903.0
## 858   4   2       Style    2476.1
## 859   4   1       Style    2317.0
## 860   4   1       Style    1793.9
## 861   4   1       Style    1456.0
## 862   4   2       Style    1669.2
## 863   4   1       Style    2144.1
## 864   4   2       Style    1932.7
## 865   4   1       Style    2324.5
## 866   4   1       Style    2623.9
## 867   4   2       Style    1877.9
## 868   4   2       Style    2722.2
## 869   4   2       Style    1932.8
## 870   4   1       Style    2214.5
## 871   4   2       Style    1596.0
## 872   4   2       Style    2236.8
## 873   4   2       Style    2333.6
## 874   4   1       Style    1520.8
## 875   4   1       Style    1767.9
## 876   4   2       Style    2222.5
## 877   4   2       Style    2203.3
## 878   4   1       Style    1179.1
## 879   4   1       Style    1553.7
## 880   4   1       Style    1417.6
## 881   4   1       Style   31678.9
## 882   4   1       Style    2822.7
## 883   4   1       Style    2464.7
## 884   4   2       Style    2432.3
## 885   4   2       Style    1356.7
## 886   4   1       Style    1880.6
## 887   4   2       Style    2200.1
## 888   4   1       Style    2580.2
## 889   4   2       Style    2124.2
## 890   4   1       Style    2934.3
## 891   4   2       Style    1936.1
## 892   4   1       Style    1270.8
## 893   4   2       Style    1484.3
## 894   4   2       Style    2080.9
## 895   4   1       Style    1811.9
## 896   4   1       Style    2238.3
## 897   4   2       Style    2621.9
## 898   4   1       Style    2069.9
## 899   4   1       Style    1911.2
## 900   4   1       Style    3525.9
## 901   4   1       Style    1784.0
## 902   4   2       Style    2148.4
## 903   4   1       Style    2272.7
## 904   4   1       Style    1968.3
## 905   4   2       Style    1965.6
## 906   4   1       Style    2296.5
## 907   4   2       Style    1758.8
## 908   4   1       Style    2029.7
## 909   4   1       Style    2410.1
## 910   4   1       Style    2220.9
## 911   4   2       Style    2677.1
## 912   4   2       Style    2618.0
## 913   4   1       Style    3076.2
## 914   4   1       Style    2241.2
## 915   4   1       Style    2541.9
## 916   4   1       Style    2305.8
## 917   4   1       Style    1444.8
## 918   4   2       Style    1288.5
## 919   4   2       Style    2857.2
## 920   4   2       Style    1987.1
## 921   4   2       Style    2003.9
## 922   4   2       Style    3126.3
## 923   4   1       Style    2069.3
## 924   4   2       Style    2092.6
## 925   4   1       Style    1978.2
## 926   4   1       Style    1729.5
## 927   4   2       Style    2366.6
## 928   4   2       Style    3127.7
## 929   4   2       Style    1574.8
## 930   4   2       Style    2670.4
## 931   4   1       Style    1970.0
## 932   4   1       Style    2926.1
## 933   4   2       Style    2117.9
## 934   4   2       Style    2013.9
## 935   4   1       Style    1859.2
## 936   4   2       Style    1595.6
## 937   4   2       Style   33298.9
## 938   4   2       Style    2022.6
## 939   4   2       Style    2367.0
## 940   4   1       Style    2464.7
## 941   4   1       Style    2560.9
## 942   4   2       Style    2124.8
## 943   4   2       Style    1562.5
## 944   4   1       Style    2270.1
## 945   4   2       Style    1697.8
## 946   4   2       Style    2223.0
## 947   4   1       Style    1808.0
## 948   4   1       Style    2391.2
## 949   4   2       Style    1321.8
## 950   4   1       Style    3069.9
## 951   4   2       Style    2626.5
## 952   4   2       Style    2758.5
## 953   4   2       Style    2343.2
## 954   4   1       Style    2221.2
## 955   4   2       Style    2887.2
## 956   4   2       Style    2378.1
## 957   4   2       Style    1387.5
## 958   4   2       Style    2930.2
## 959   4   1       Style    2883.9
## 960   4   1       Style     658.3
## 961   4   2       Style    1633.6
## 962   4   2       Style    2059.4
## 963   4   2       Style    1517.0
## 964   4   2       Style    3234.0
## 965   4   1       Style    2555.0
## 966   4   1       Style    1639.2
## 967   4   2       Style    1992.8
## 968   4   1       Style    2395.6
## 969   4   2       Style    1995.6
## 970   4   2       Style    1300.0
## 971   4   1       Style    1631.4
## 972   4   2       Style    3060.5
## 973   4   2       Style    2521.6
## 974   4   2       Style    1618.6
## 975   4   1       Style    1661.3
## 976   4   1       Style    2022.1
## 977   4   2       Style    3411.3
## 978   4   2       Style    2295.8
## 979   4   1       Style    2235.4
## 980   4   2       Style    2317.6
## 981   4   1       Style    2003.4
## 982   4   2       Style    1728.2
## 983   4   2       Style    1204.8
## 984   4   1       Style    2402.6
## 985   4   1       Style    2334.6
## 986   4   1       Style    2832.8
## 987   4   2       Style    2295.3
## 988   4   2       Style    1942.9
## 989   4   1       Style    1736.0
## 990   4   2       Style    1322.5
## 991   4   2       Style    2877.6
## 992   4   2       Style    2817.3
## 993   4   1       Style    1887.3
## 994   4   2       Style    2416.1
## 995   4   2       Style    2681.8
## 996   4   2       Style    2091.1
## 997   4   1       Style    2636.3
## 998   4   1       Style    2555.4
## 999   4   2       Style    1914.7
## 1000  4   2       Style    2682.3
\end{verbatim}

The above code sums up two columns and appends the result
(\texttt{total\_exp}) to \texttt{sim.dat}. Another similar function is
\texttt{transmute()}. The difference is that \texttt{transmute()} will
delete the original columns and only keep the new ones.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dplyr::}\KeywordTok{transmute}\NormalTok{(sim.dat, }\DataTypeTok{total_exp =} \NormalTok{store_exp +}\StringTok{ }\NormalTok{online_exp) }
\end{Highlighting}
\end{Shaded}

\textbf{Merge}

Similar to SQL, there are different joins in \texttt{dplyr}. We create
two baby data sets to show how the functions work.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(x<-}\KeywordTok{data.frame}\NormalTok{(}\KeywordTok{cbind}\NormalTok{(}\DataTypeTok{ID=}\KeywordTok{c}\NormalTok{(}\StringTok{"A"}\NormalTok{,}\StringTok{"B"}\NormalTok{,}\StringTok{"C"}\NormalTok{),}\DataTypeTok{x1=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{))))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   ID x1
## 1  A  1
## 2  B  2
## 3  C  3
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(y<-}\KeywordTok{data.frame}\NormalTok{(}\KeywordTok{cbind}\NormalTok{(}\DataTypeTok{ID=}\KeywordTok{c}\NormalTok{(}\StringTok{"B"}\NormalTok{,}\StringTok{"C"}\NormalTok{,}\StringTok{"D"}\NormalTok{),}\DataTypeTok{y1=}\KeywordTok{c}\NormalTok{(T,T,F))))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   ID    y1
## 1  B  TRUE
## 2  C  TRUE
## 3  D FALSE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# join to the left}
\CommentTok{# keep all rows in x}
\KeywordTok{left_join}\NormalTok{(x,y,}\DataTypeTok{by=}\StringTok{"ID"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   ID x1   y1
## 1  A  1 <NA>
## 2  B  2 TRUE
## 3  C  3 TRUE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# get rows matched in both data sets}
\KeywordTok{inner_join}\NormalTok{(x,y,}\DataTypeTok{by=}\StringTok{"ID"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   ID x1   y1
## 1  B  2 TRUE
## 2  C  3 TRUE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# get rows in either data set}
\KeywordTok{full_join}\NormalTok{(x,y,}\DataTypeTok{by=}\StringTok{"ID"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   ID   x1    y1
## 1  A    1  <NA>
## 2  B    2  TRUE
## 3  C    3  TRUE
## 4  D <NA> FALSE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# filter out rows in x that can be matched in y }
\CommentTok{# it doesn't bring in any values from y }
\KeywordTok{semi_join}\NormalTok{(x,y,}\DataTypeTok{by=}\StringTok{"ID"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   ID x1
## 1  B  2
## 2  C  3
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# the opposite of  semi_join()}
\CommentTok{# it gets rows in x that cannot be matched in y}
\CommentTok{# it doesn't bring in any values from y}
\KeywordTok{anti_join}\NormalTok{(x,y,}\DataTypeTok{by=}\StringTok{"ID"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   ID x1
## 1  A  1
\end{verbatim}

There are other functions(\texttt{intersect()}, \texttt{union()} and
\texttt{setdiff()}). Also the data frame version of \texttt{rbind} and
\texttt{cbind} which are \texttt{bind\_rows()} and \texttt{bind\_col()}.
We are not going to go through them all. You can try them yourself. If
you understand the functions we introduced so far. It should be easy for
you to figure out the rest.

\section{Tidy and Reshape Data}\label{tidy-and-reshape-data}

``Tidy data'' represent the information from a dataset as data frames
where each row is an observation, and each column contains the values of
a variable (i.e., an attribute of what we are observing). Depending on
the situation, the requirements on what to present as rows and columns
may change. To make data easy to work with for the problem at hand, in
practice, we often need to convert data between the ``wide'' and the
``long'' format. The process feels like kneading the dough.

There are two commonly used packages for this kind of manipulations:
\texttt{tidyr} and \texttt{reshape2}. We will show how to tidy and
reshape data using the two packages. By comparing the functions to show
how they overlap and where they differ.

\subsection{\texorpdfstring{\texttt{reshape2}
package}{reshape2 package}}\label{reshape2-package}

It is a reboot of the previous package \texttt{reshape}. Take a baby
subset of our exemplary clothes consumers data to illustrate:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(sdat<-sim.dat[}\DecValTok{1}\NormalTok{:}\DecValTok{5}\NormalTok{,}\DecValTok{1}\NormalTok{:}\DecValTok{6}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   age gender income house store_exp online_exp
## 1  57 Female 120963   Yes     529.1      303.5
## 2  63 Female 122008   Yes     478.0      109.5
## 3  59   Male 114202   Yes     490.8      279.2
## 4  60   Male 113616   Yes     347.8      141.7
## 5  51   Male 124253   Yes     379.6      112.2
\end{verbatim}

For the above data \texttt{sdat}, what if we want to have a variable
indicating the purchasing channel (i.e.~online or in-store) and another
column with the corresponding expense amount? Assume we want to keep the
rest of the columns the same. It is a task to change data from ``wide''
to ``long''. There are two general ways to shape data:

\begin{itemize}
\tightlist
\item
  Use \texttt{melt()} to convert an object into a molten data frame,
  i.e., from wide to long
\item
  Use \texttt{dcast()} to cast a molten data frame into the shape you
  want, i.e., from long to wide
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(reshape2)}
\NormalTok{(mdat <-}\StringTok{ }\KeywordTok{melt}\NormalTok{(sdat, }\DataTypeTok{measure.vars=}\KeywordTok{c}\NormalTok{(}\StringTok{"store_exp"}\NormalTok{,}\StringTok{"online_exp"}\NormalTok{),}
              \DataTypeTok{variable.name =} \StringTok{"Channel"}\NormalTok{,}
              \DataTypeTok{value.name =} \StringTok{"Expense"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    age gender income house    Channel Expense
## 1   57 Female 120963   Yes  store_exp   529.1
## 2   63 Female 122008   Yes  store_exp   478.0
## 3   59   Male 114202   Yes  store_exp   490.8
## 4   60   Male 113616   Yes  store_exp   347.8
## 5   51   Male 124253   Yes  store_exp   379.6
## 6   57 Female 120963   Yes online_exp   303.5
## 7   63 Female 122008   Yes online_exp   109.5
## 8   59   Male 114202   Yes online_exp   279.2
## 9   60   Male 113616   Yes online_exp   141.7
## 10  51   Male 124253   Yes online_exp   112.2
\end{verbatim}

You melted the data frame \texttt{sdat} by two variables:
\texttt{store\_exp} and \texttt{online\_exp}
(\texttt{measure.vars=c("store\_exp","online\_exp")}). The new variable
name is \texttt{Channel} set by command
\texttt{variable.name\ =\ "Channel"}. The value name is \texttt{Expense}
set by command \texttt{value.name\ =\ "Expense"}.

You can run a regression to study the effect of purchasing channel as
follows:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Here we use all observations from sim.dat}
\CommentTok{# Don't show result here}
\NormalTok{mdat<-}\KeywordTok{melt}\NormalTok{(sim.dat[,}\DecValTok{1}\NormalTok{:}\DecValTok{6}\NormalTok{], }\DataTypeTok{measure.vars=}\KeywordTok{c}\NormalTok{(}\StringTok{"store_exp"}\NormalTok{,}\StringTok{"online_exp"}\NormalTok{),}
            \DataTypeTok{variable.name =} \StringTok{"Channel"}\NormalTok{,}
              \DataTypeTok{value.name =} \StringTok{"Expense"}\NormalTok{)}
\NormalTok{fit<-}\KeywordTok{lm}\NormalTok{(Expense~gender+house+income+Channel+age,}\DataTypeTok{data=}\NormalTok{mdat)}
\KeywordTok{summary}\NormalTok{(fit)}
\end{Highlighting}
\end{Shaded}

You can \texttt{melt()} list, matrix, table too. The syntax is similar,
and we won't go through every situation. Sometimes we want to convert
the data from ``long'' to ``wide''. For example, \textbf{you want to
compare the online and in-store expense between male and female based on
the house ownership. }

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{dcast}\NormalTok{(mdat, house +}\StringTok{ }\NormalTok{gender ~}\StringTok{ }\NormalTok{Channel, sum)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Using 'Expense' as value column. Use 'value.var' to override
\end{verbatim}

\begin{verbatim}
##   house gender store_exp online_exp
## 1   Yes Female      1007      413.0
## 2   Yes   Male      1218      533.2
\end{verbatim}

In the above code, what is the left side of \texttt{\textasciitilde{}}
are variables that you want to group by. The right side is the variable
you want to spread as columns. It will use the column indicating value
from \texttt{melt()} before. Here is ``\texttt{Expense}'' .

\subsection{\texorpdfstring{\texttt{tidyr}
package}{tidyr package}}\label{tidyr-package}

The other package that will do similar manipulations is \texttt{tidyr}.
Let's get a subset to illustrate the usage.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(dplyr)}
\CommentTok{# practice functions we learnt before}
\NormalTok{sdat<-sim.dat[}\DecValTok{1}\NormalTok{:}\DecValTok{5}\NormalTok{,]%>%}
\StringTok{  }\NormalTok{dplyr::}\KeywordTok{select}\NormalTok{(age,gender,store_exp,store_trans)}
\NormalTok{sdat %>%}\StringTok{ }\KeywordTok{tbl_df}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 5 x 4
##     age gender store_exp store_trans
## * <int> <fctr>     <dbl>       <int>
## 1    57 Female     529.1           2
## 2    63 Female     478.0           4
## 3    59   Male     490.8           7
## 4    60   Male     347.8          10
## 5    51   Male     379.6           4
\end{verbatim}

\texttt{gather()} function in \texttt{tidyr} is analogous to
\texttt{melt()} in \texttt{reshape2}. The following code will do the
same thing as we did before using \texttt{melt()}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyr)}
\NormalTok{msdat<-tidyr::}\KeywordTok{gather}\NormalTok{(sdat,}\StringTok{"variable"}\NormalTok{,}\StringTok{"value"}\NormalTok{,store_exp,store_trans)}
\NormalTok{msdat %>%}\StringTok{ }\KeywordTok{tbl_df}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 10 x 4
##      age gender    variable value
##    <int> <fctr>       <chr> <dbl>
##  1    57 Female   store_exp 529.1
##  2    63 Female   store_exp 478.0
##  3    59   Male   store_exp 490.8
##  4    60   Male   store_exp 347.8
##  5    51   Male   store_exp 379.6
##  6    57 Female store_trans   2.0
##  7    63 Female store_trans   4.0
##  8    59   Male store_trans   7.0
##  9    60   Male store_trans  10.0
## 10    51   Male store_trans   4.0
\end{verbatim}

Or if we use the pipe operation, we can write the above code as:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sdat%>%}\KeywordTok{gather}\NormalTok{(}\StringTok{"variable"}\NormalTok{,}\StringTok{"value"}\NormalTok{,store_exp,store_trans)}
\end{Highlighting}
\end{Shaded}

It is identical with the following code using \texttt{melt()}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{melt}\NormalTok{(sdat, }\DataTypeTok{measure.vars=}\KeywordTok{c}\NormalTok{(}\StringTok{"store_exp"}\NormalTok{,}\StringTok{"store_trans"}\NormalTok{),}
            \DataTypeTok{variable.name =} \StringTok{"variable"}\NormalTok{,}
              \DataTypeTok{value.name =} \StringTok{"value"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The opposite operation to \texttt{gather()} is \texttt{spread()}. The
previous one stacks columns and the latter one spread the columns.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{msdat %>%}\StringTok{ }\KeywordTok{spread}\NormalTok{(variable,value)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   age gender store_exp store_trans
## 1  51   Male     379.6           4
## 2  57 Female     529.1           2
## 3  59   Male     490.8           7
## 4  60   Male     347.8          10
## 5  63 Female     478.0           4
\end{verbatim}

Another pair of functions that do opposite manipulations are
\texttt{separate()} and \texttt{unite()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sepdat<-}\StringTok{ }\NormalTok{msdat %>%}\StringTok{ }
\StringTok{  }\KeywordTok{separate}\NormalTok{(variable,}\KeywordTok{c}\NormalTok{(}\StringTok{"Source"}\NormalTok{,}\StringTok{"Type"}\NormalTok{))}
\NormalTok{sepdat %>%}\StringTok{ }\KeywordTok{tbl_df}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 10 x 5
##      age gender Source  Type value
##  * <int> <fctr>  <chr> <chr> <dbl>
##  1    57 Female  store   exp 529.1
##  2    63 Female  store   exp 478.0
##  3    59   Male  store   exp 490.8
##  4    60   Male  store   exp 347.8
##  5    51   Male  store   exp 379.6
##  6    57 Female  store trans   2.0
##  7    63 Female  store trans   4.0
##  8    59   Male  store trans   7.0
##  9    60   Male  store trans  10.0
## 10    51   Male  store trans   4.0
\end{verbatim}

You can see that the function separates the original column
``\texttt{variable}'' to two new columns ``\texttt{Source}'' and
``\texttt{Type}''. You can use \texttt{sep=} to set the string or
regular expression to separate the column. By default, it is
``\texttt{\_}''.

The \texttt{unite()} function will do the opposite: combining two
columns. It is the generalization of \texttt{paste()} to a data frame.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sepdat %>%}\StringTok{ }
\StringTok{  }\KeywordTok{unite}\NormalTok{(}\StringTok{"variable"}\NormalTok{,Source,Type,}\DataTypeTok{sep=}\StringTok{"_"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    age gender    variable value
## 1   57 Female   store_exp 529.1
## 2   63 Female   store_exp 478.0
## 3   59   Male   store_exp 490.8
## 4   60   Male   store_exp 347.8
## 5   51   Male   store_exp 379.6
## 6   57 Female store_trans   2.0
## 7   63 Female store_trans   4.0
## 8   59   Male store_trans   7.0
## 9   60   Male store_trans  10.0
## 10  51   Male store_trans   4.0
\end{verbatim}

The reshaping manipulations may be the trickiest part. You have to
practice a lot to get familiar with those functions. Unfortunately,
there is no shortcut.

\chapter{Model Tuning Strategy}\label{model-tuning-strategy}

When training a machine learning model, there are many decisions to
make. For example, when training a random forest, you need to decide the
number of trees and the number of variables at each node. For lasso
method, you need to determine the penalty parameter. There may be
standard settings for some of the parameters, but it's unlikely to guess
the right values for all of these correctly. Other than that, making
good choices on how you split the data into training and testing sets
can make a huge difference in helping you find a high-performance model
efficiently.

This chapter will illustrate the practical aspects of model tuning. We
will talk about different types of model error, sources of model error,
hyperparameter tuning, how to set up your data and how to make sure your
model implementation is correct. In practice applying machine learning
is a highly iterative process.

\section{Systematic Error and Random
Error}\label{systematic-error-and-random-error}

Assume \(\mathbf{X}\) is \(n \times p\) observation matrix and
\(\mathbf{y}\) is response variable, we have:

\[\mathbf{y}=f(\mathbf{X})+\mathbf{\epsilon}\]

where \(\mathbf{\epsilon}\) is the random error with a mean of zero. The
function \(f(\cdot)\) is our modeling target, which represents the
information in the response variable that predictors can explain. The
main goal of estimating \(f(\cdot)\) is inference or prediction, or
sometimes both. In general, there is a trade-off between flexibility and
interpretability of the model. So data scientists need to comprehend the
delicate balance between these two.

Depending on the modeling purposes, the requirement for interpretability
varies. If the prediction is the only goal, then as long as the
prediction is accurate enough, the interpretability is not under
consideration. In this case, people can use ``black box'' model, such as
random forest, boosting tree, neural network and so on. These models are
very flexible but nearly impossible to explain. Their accuracy is
usually higher on the training set, but not necessary when it predicts.
It is not surprising since those models have a huge number of parameters
and high flexibility that they can ``memorize'' the entire training
data. A paper by Chiyuan Zhang et al. in 2017 pointed out that ``Deep
neural networks (even just two-layer net) easily fit random labels''
\citep{rethinkDL}. The traditional forms of regularization, such as
weight decay, dropout, and data augmentation, fail to control
generalization error. It poses a conceptual challenge to statistical
theory and also calls our attention when we use such black-box models.

There are two kinds of application problems: complete information
problem and incomplete information problem. The complete information
problem has all the information you need to know the correct response.
Take the famous cat recognition, for example, all the information you
need to identify a cat is in the picture. In this situation, the
algorithm that penetrates the data the most wins. There are some other
similar problems such as the self-driving car, chess game, facial
recognition and speech recognition. But in most of the data science
applications, the information is incomplete. If you want to know whether
a customer is going to purchase again or not, it is unlikely to have
360-degree of the customer's information. You may have their historical
purchasing record, discounts and service received. But you don't know if
the customer sees your advertisement, or has a friend recommends
competitor's product, or encounters some unhappy purchasing experience
somewhere. There could be a myriad of factors that will influence the
customer's purchase decision while what you have as data is only a small
part. To make things worse, in many cases, you don't even know what you
don't know. Deep learning doesn't have any advantage in solving those
problems. Instead, some parametric models often work better in this
situation. You will comprehend this more after learning the different
types of model error. Assume we have \(\hat{f}\) which is an estimator
of \(f\). Then we can further get
\(\mathbf{\hat{y}}=\hat{f}(\mathbf{X})\). The predicted error is divided
into two parts, systematic error, and random error:

\[E(\mathbf{y}-\hat{\mathbf{y}})^{2}=E[f(\mathbf{X})+\mathbf{\epsilon}-\hat{f}(\mathbf{X})]^{2}=\underset{\text{(1)}}{\underbrace{E[f(\mathbf{X})-\hat{f}(\mathbf{X})]^{2}}}+\underset{\text{(2)}}{\underbrace{Var(\mathbf{\epsilon})}}
\label{eq:error}\]

It is also called Mean Square Error (MSE) where (1) is the systematic
error. It exists because \(\hat{f}\) usually does not entirely describe
the ``systematic relation'' between X and y which refers to the stable
relationship that exists across different samples or time. Model
improvement can help reduce this kind of error; (2) is the random error
which represents the part of y that cannot be explained by X. A more
complex model does not reduce the error. There are three reasons for
random error:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  the current sample is not representative, so the pattern in one sample
  set does not generalize to a broader scale.
\item
  The information is incomplete. In other words, you don't have all
  variables needed to explain the response.
\item
  Measurement error in the variables.
\end{enumerate}

Deep learning has significant success solving problems with complete
information and usually low measurement error. As mentioned before, in a
task like image recognition, all you need are the pixels in the
pictures. So in deep learning applications, increasing the sample size
can improve the model performance significantly. But it may not perform
well in problems with incomplete information. The biggest problem with
the black-box model is that it fits random error, i.e., over-fitting.
The notable feature of random error is that it varies over different
samples. So one way to determine whether overfitting happens is to
reserve a part of the data as the test set and then check the
performance of the trained model on the test data. Note that overfitting
is a general problem from which any model could suffer. However, since
black-box models usually have a large number of parameters, it is much
more suspectable to over-fitting.

\begin{figure}[htbp]
\centering
\includegraphics{images/ModelError.png}
\caption{Types of Model Error}
\end{figure}

The systematic error can be further decomposed as:

\[
\begin{array}{ccc}
E[f(\mathbf{X})-\hat{f}(\mathbf{X})]^{2} & = & E\left(f(\mathbf{X})-E[\hat{f}(\mathbf{X})]+E[\hat{f}(\mathbf{X})]-\hat{f}(\mathbf{X})\right)^{2}\\
 & = & E\left(E[\hat{f}(\mathbf{X})]-f(\mathbf{X})\right)^{2}+E\left(\hat{f}(\mathbf{X})-E[\hat{f}(\mathbf{X})]\right)^{2}\\
 & = & [Bias(\hat{f}(\mathbf{X}))]^{2}+Var(\hat{f}(\mathbf{X}))
\end{array}
\]

The systematic error consists of two parts,
\(Bias(\hat{f}(\mathbf{X}))\) and \(Var (\hat{f}(\mathbf{X}))\). To
minimize the systematic error, we need to minimize both. The bias
represents the error caused by the model's approximation of the reality,
i.e., systematic relation, which may be very complex. For example,
linear regression assumes a linear relationship between the predictors
and the response, but rarely is there a perfect linear relationship in
real life. So linear regression is more likely to have a high bias.

To explore bias and variance, let's begin with a simple simulation. We
will simulate a data with a non-linear relationship and fit different
models on it. An intuitive way to show these is to compare the plots of
various models.

The code below simulates one predictor (\texttt{x}) and one response
variable (\texttt{fx}). The relationship between \texttt{x} and
\texttt{fx} is non-linear.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{source}\NormalTok{(}\KeywordTok{ids_url}\NormalTok{(}\StringTok{'R/multiplot.r'}\NormalTok{))}
\CommentTok{# randomly simulate some non-linear samples}
\NormalTok{x =}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{10}\NormalTok{, }\FloatTok{0.01}\NormalTok{) *}\StringTok{ }\NormalTok{pi}
\NormalTok{e =}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(}\KeywordTok{length}\NormalTok{(x), }\DataTypeTok{mean =} \DecValTok{0}\NormalTok{, }\DataTypeTok{sd =} \FloatTok{0.2}\NormalTok{)}
\NormalTok{fx <-}\StringTok{ }\KeywordTok{sin}\NormalTok{(x) +}\StringTok{ }\NormalTok{e +}\StringTok{ }\KeywordTok{sqrt}\NormalTok{(x)}
\NormalTok{dat =}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(x, fx)}
\end{Highlighting}
\end{Shaded}

Then fit a simple linear regression on these data:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# plot fitting result}
\KeywordTok{library}\NormalTok{(ggplot2)}
\KeywordTok{ggplot}\NormalTok{(dat, }\KeywordTok{aes}\NormalTok{(x, fx)) +}\StringTok{ }\KeywordTok{geom_point}\NormalTok{() +}\StringTok{ }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{method =} \StringTok{"lm"}\NormalTok{, }\DataTypeTok{se =} \OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{IDS_files/figure-latex/linearbias-1} 

}

\caption{High bias model}\label{fig:linearbias}
\end{figure}

Despite a large sample size, trained linear regression cannot describe
the relationship very well. In other words, in this case, the model has
a high bias (Fig. \ref{fig:linearbias}). People also call it
underfitting.

Since the estimated parameters will be somewhat different for the
different samples, there is the variance of estimates. Intuitively, it
gives you some sense that if we fit the same model with different
samples (presumably, they are from the same population), how much will
the estimates change. Ideally, the change is trivial. For high variance
models, small changes in the training data result in very different
estimates. In general, a model with high flexibility also has high
variance., such as the CART tree, and the initial boosting method. To
overcome that problem, the Random Forest and Gradient Boosting Model aim
to reduce the variance by summarizing the results obtained from
different samples.

Let's fit the above data using a smoothing method which is highly
flexible and can fit the current data tightly:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(dat, }\KeywordTok{aes}\NormalTok{(x, fx)) +}\StringTok{ }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{span =} \FloatTok{0.03}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{IDS_files/figure-latex/linearvar-1} 

}

\caption{High variance model}\label{fig:linearvar}
\end{figure}

The resulting plot (Fig. \ref{fig:linearvar}) indicates the smoothing
method fit the data much better so it has a much smaller bias. However,
this method has a high variance. If we simulate different subsets of the
sample, the result curve will change significantly:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# set random seed}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{2016}\NormalTok{)}
\CommentTok{# sample part of the data to fit model sample 1}
\NormalTok{idx1 =}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\DecValTok{1}\NormalTok{:}\KeywordTok{length}\NormalTok{(x), }\DecValTok{100}\NormalTok{)}
\NormalTok{dat1 =}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{x1 =} \NormalTok{x[idx1], }\DataTypeTok{fx1 =} \NormalTok{fx[idx1])}
\NormalTok{p1 =}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(dat1, }\KeywordTok{aes}\NormalTok{(x1, fx1)) +}\StringTok{ }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{span =} \FloatTok{0.03}\NormalTok{)}
\CommentTok{# sample 2}
\NormalTok{idx2 =}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\DecValTok{1}\NormalTok{:}\KeywordTok{length}\NormalTok{(x), }\DecValTok{100}\NormalTok{)}
\NormalTok{dat2 =}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{x2 =} \NormalTok{x[idx2], }\DataTypeTok{fx2 =} \NormalTok{fx[idx2])}
\NormalTok{p2 =}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(dat2, }\KeywordTok{aes}\NormalTok{(x2, fx2)) +}\StringTok{ }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{span =} \FloatTok{0.03}\NormalTok{)}
\CommentTok{# sample 3}
\NormalTok{idx3 =}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\DecValTok{1}\NormalTok{:}\KeywordTok{length}\NormalTok{(x), }\DecValTok{100}\NormalTok{)}
\NormalTok{dat3 =}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{x3 =} \NormalTok{x[idx3], }\DataTypeTok{fx3 =} \NormalTok{fx[idx3])}
\NormalTok{p3 =}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(dat3, }\KeywordTok{aes}\NormalTok{(x3, fx3)) +}\StringTok{ }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{span =} \FloatTok{0.03}\NormalTok{)}
\CommentTok{# sample 4}
\NormalTok{idx4 =}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\DecValTok{1}\NormalTok{:}\KeywordTok{length}\NormalTok{(x), }\DecValTok{100}\NormalTok{)}
\NormalTok{dat4 =}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{x4 =} \NormalTok{x[idx4], }\DataTypeTok{fx4 =} \NormalTok{fx[idx4])}
\NormalTok{p4 =}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(dat4, }\KeywordTok{aes}\NormalTok{(x4, fx4)) +}\StringTok{ }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{span =} \FloatTok{0.03}\NormalTok{)}
\KeywordTok{multiplot}\NormalTok{(p1, p2, p3, p4, }\DataTypeTok{cols =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{IDS_files/figure-latex/unnamed-chunk-89-1.pdf}

The fitted lines (blue) change over different samples which means it has
high variance. People also call it overfitting. Fitting the linear model
using the same four subsets, the result barely changes:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p1 =}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(dat1, }\KeywordTok{aes}\NormalTok{(x1, fx1)) +}\StringTok{ }\KeywordTok{geom_point}\NormalTok{() +}\StringTok{ }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{method =} \StringTok{"lm"}\NormalTok{, }
    \DataTypeTok{se =} \OtherTok{FALSE}\NormalTok{)}
\NormalTok{p2 =}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(dat2, }\KeywordTok{aes}\NormalTok{(x2, fx2)) +}\StringTok{ }\KeywordTok{geom_point}\NormalTok{() +}\StringTok{ }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{method =} \StringTok{"lm"}\NormalTok{, }
    \DataTypeTok{se =} \OtherTok{FALSE}\NormalTok{)}
\NormalTok{p3 =}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(dat3, }\KeywordTok{aes}\NormalTok{(x3, fx3)) +}\StringTok{ }\KeywordTok{geom_point}\NormalTok{() +}\StringTok{ }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{method =} \StringTok{"lm"}\NormalTok{, }
    \DataTypeTok{se =} \OtherTok{FALSE}\NormalTok{)}
\NormalTok{p4 =}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(dat4, }\KeywordTok{aes}\NormalTok{(x4, fx4)) +}\StringTok{ }\KeywordTok{geom_point}\NormalTok{() +}\StringTok{ }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{method =} \StringTok{"lm"}\NormalTok{, }
    \DataTypeTok{se =} \OtherTok{FALSE}\NormalTok{)}
\KeywordTok{multiplot}\NormalTok{(p1, p2, p3, p4, }\DataTypeTok{cols =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{IDS_files/figure-latex/unnamed-chunk-90-1.pdf}

In general, the variance (\(Var(\hat{f}(\mathbf{X}))\))
\textbf{increases} and the bias (\(Bias(\hat{f}(\mathbf{X}))\))
\textbf{decreases} as the model flexibility increases. Variance and bias
together determine the systematic error. As we increase the flexibility
of the model, at first the rate at which \(Bias(\hat{f}(\mathbf{X}))\)
decreases is faster than \(Var (\hat{f} (\mathbf{X}))\), so the MSE
decreases. However, to some degree, higher flexibility has little effect
on \(Bias(\hat{f}(\mathbf{X}))\) but \(Var(\hat{f} (\mathbf{X}))\)
increases significantly, so the MSE increases.

\subsection{Measurement Error in the
Response}\label{measurement-error-in-the-response}

The measurement error in the response contributes to the random error
(\(\mathbf{\epsilon}\)). This part of the error is irreducible if you
change the data collection mechanism, and so it makes the root mean
square error (RMSE) and \(R^2\) have the corresponding upper and lower
limits. RMSE and \(R^2\) are commonly used performance measures for the
regression model which we will talk in more detail later. Therefore, the
random error term not only represents the part of fluctuations the model
cannot explain but also contains measurement error in the response
variables. Section 20.2 of Applied Predictive Modeling \citep{APM} has
an example that shows the effect of the measurement error in the
response variable on the model performance (RMSE and \(R^2\)).

The authors increased the error in the response proportional to a base
level error which was gotten using the original data without introducing
extra noise. Then fit a set of models repeatedly using the
``contaminated'' data sets to study the change of \(RMSE\) and \(R^2\)
as the level of noise. Here we use clothing consumer data for a similar
illustration. Suppose many people do not want to disclose their income
and so we need to use other variables to establish a model to predict
income. We set up the following model:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# load data}
\NormalTok{sim.dat <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"https://raw.githubusercontent.com/happyrabbit/DataScientistR/master/Data/SegData.csv "}\NormalTok{)}
\NormalTok{ymad <-}\StringTok{ }\KeywordTok{mad}\NormalTok{(}\KeywordTok{na.omit}\NormalTok{(sim.dat$income))}
\CommentTok{# calculate z-score}
\NormalTok{zs <-}\StringTok{ }\NormalTok{(sim.dat$income -}\StringTok{ }\KeywordTok{mean}\NormalTok{(}\KeywordTok{na.omit}\NormalTok{(sim.dat$income)))/ymad}
\CommentTok{# which(na.omit(zs>3.5)): identify outliers which(is.na(zs)):}
\CommentTok{# identify missing values}
\NormalTok{idex <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\KeywordTok{which}\NormalTok{(}\KeywordTok{na.omit}\NormalTok{(zs >}\StringTok{ }\FloatTok{3.5}\NormalTok{)), }\KeywordTok{which}\NormalTok{(}\KeywordTok{is.na}\NormalTok{(zs)))}
\CommentTok{# delete rows with outliers and missing values}
\NormalTok{sim.dat <-}\StringTok{ }\NormalTok{sim.dat[-idex, ]}
\NormalTok{fit <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(income ~}\StringTok{ }\NormalTok{store_exp +}\StringTok{ }\NormalTok{online_exp +}\StringTok{ }\NormalTok{store_trans +}\StringTok{ }\NormalTok{online_trans, }
    \DataTypeTok{data =} \NormalTok{sim.dat)}
\end{Highlighting}
\end{Shaded}

The output shows that without additional noise, the root mean square
error (RMSE) of the model is 29567, \(R^2\) is 0.6.

Let's add various degrees of noise (0 to 3 times the RMSE) to the
variable \texttt{income}:

\[ RMSE \times (0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0) \]

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{noise <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\KeywordTok{rep}\NormalTok{(}\OtherTok{NA}\NormalTok{, }\DecValTok{7} \NormalTok{*}\StringTok{ }\KeywordTok{nrow}\NormalTok{(sim.dat)), }\DataTypeTok{nrow =} \KeywordTok{nrow}\NormalTok{(sim.dat), }
    \DataTypeTok{ncol =} \DecValTok{7}\NormalTok{)}
\NormalTok{for (i in }\DecValTok{1}\NormalTok{:}\KeywordTok{nrow}\NormalTok{(sim.dat)) \{}
    \NormalTok{noise[i, ] <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(}\DecValTok{7}\NormalTok{, }\KeywordTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{7}\NormalTok{), }\KeywordTok{summary}\NormalTok{(fit)$sigma *}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }
        \DecValTok{3}\NormalTok{, }\DataTypeTok{by =} \FloatTok{0.5}\NormalTok{))}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

We then examine the effect of noise intensity on \(R^2\) for models with
different complexity. The models with complexity from low to high are:
ordinary linear regression, partial least square regression(PLS),
multivariate adaptive regression spline (MARS), support vector machine
(SVM, the kernel function is radial basis function), and random forest.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# fit ordinary linear regression}
\NormalTok{rsq_linear <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, }\KeywordTok{ncol}\NormalTok{(noise))}
\NormalTok{for (i in }\DecValTok{1}\NormalTok{:}\DecValTok{7}\NormalTok{) \{}
    \NormalTok{withnoise <-}\StringTok{ }\NormalTok{sim.dat$income +}\StringTok{ }\NormalTok{noise[, i]}
    \NormalTok{fit0 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(withnoise ~}\StringTok{ }\NormalTok{store_exp +}\StringTok{ }\NormalTok{online_exp +}\StringTok{ }\NormalTok{store_trans +}\StringTok{ }
\StringTok{        }\NormalTok{online_trans, }\DataTypeTok{data =} \NormalTok{sim.dat)}
    \NormalTok{rsq_linear[i] <-}\StringTok{ }\KeywordTok{summary}\NormalTok{(fit0)$adj.r.squared}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

PLS is a method of linearizing nonlinear relationships through hidden
layers. It is similar to the principal component regression (PCR),
except that PCR does not take into account the information of the
dependent variable when selecting the components, and its purpose is to
find the linear combinations (i.e., unsupervised) that capture the most
variance of the independent variables. When the independent variables
and response variables are related, PCR can well identify the systematic
relationship between them. However, when there exist independent
variables not associated with response variable, it will undermine PCR's
performance. And PLS maximizes the linear combination of dependencies
with the response variable. In the current case, the more complicated
PLS does not perform better than simple linear regression.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# pls: conduct PLS and PCR}
\KeywordTok{library}\NormalTok{(pls)}
\NormalTok{rsq_pls <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, }\KeywordTok{ncol}\NormalTok{(noise))}
\CommentTok{# fit PLS}
\NormalTok{for (i in }\DecValTok{1}\NormalTok{:}\DecValTok{7}\NormalTok{) \{}
    \NormalTok{withnoise <-}\StringTok{ }\NormalTok{sim.dat$income +}\StringTok{ }\NormalTok{noise[, i]}
    \NormalTok{fit0 <-}\StringTok{ }\KeywordTok{plsr}\NormalTok{(withnoise ~}\StringTok{ }\NormalTok{store_exp +}\StringTok{ }\NormalTok{online_exp +}\StringTok{ }\NormalTok{store_trans +}\StringTok{ }
\StringTok{        }\NormalTok{online_trans, }\DataTypeTok{data =} \NormalTok{sim.dat)}
    \NormalTok{rsq_pls[i] <-}\StringTok{ }\KeywordTok{max}\NormalTok{(}\KeywordTok{drop}\NormalTok{(}\KeywordTok{R2}\NormalTok{(fit0, }\DataTypeTok{estimate =} \StringTok{"train"}\NormalTok{, }\DataTypeTok{intercept =} \OtherTok{FALSE}\NormalTok{)$val))}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# earth: fit mars}
\KeywordTok{library}\NormalTok{(earth)}
\NormalTok{rsq_mars <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, }\KeywordTok{ncol}\NormalTok{(noise))}
\NormalTok{for (i in }\DecValTok{1}\NormalTok{:}\DecValTok{7}\NormalTok{) \{}
    \NormalTok{withnoise <-}\StringTok{ }\NormalTok{sim.dat$income +}\StringTok{ }\NormalTok{noise[, i]}
    \NormalTok{fit0 <-}\StringTok{ }\KeywordTok{earth}\NormalTok{(withnoise ~}\StringTok{ }\NormalTok{store_exp +}\StringTok{ }\NormalTok{online_exp +}\StringTok{ }\NormalTok{store_trans +}\StringTok{ }
\StringTok{        }\NormalTok{online_trans, }\DataTypeTok{data =} \NormalTok{sim.dat)}
    \NormalTok{rsq_mars[i] <-}\StringTok{ }\NormalTok{fit0$rsq}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# caret: awesome package for tuning predictive model}
\KeywordTok{library}\NormalTok{(caret)}
\NormalTok{rsq_svm <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, }\KeywordTok{ncol}\NormalTok{(noise))}
\CommentTok{# Need some time to run}
\NormalTok{for (i in }\DecValTok{1}\NormalTok{:}\DecValTok{7}\NormalTok{) \{}
    \NormalTok{idex <-}\StringTok{ }\KeywordTok{which}\NormalTok{(}\KeywordTok{is.na}\NormalTok{(sim.dat$income))}
    \NormalTok{withnoise <-}\StringTok{ }\NormalTok{sim.dat$income +}\StringTok{ }\NormalTok{noise[, i]}
    \NormalTok{trainX <-}\StringTok{ }\NormalTok{sim.dat[, }\KeywordTok{c}\NormalTok{(}\StringTok{"store_exp"}\NormalTok{, }\StringTok{"online_exp"}\NormalTok{, }\StringTok{"store_trans"}\NormalTok{, }
        \StringTok{"online_trans"}\NormalTok{)]}
    \NormalTok{trainY <-}\StringTok{ }\NormalTok{withnoise}
    \NormalTok{fit0 <-}\StringTok{ }\KeywordTok{train}\NormalTok{(trainX, trainY, }\DataTypeTok{method =} \StringTok{"svmRadial"}\NormalTok{, }\DataTypeTok{tuneLength =} \DecValTok{15}\NormalTok{, }
        \DataTypeTok{trControl =} \KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method =} \StringTok{"cv"}\NormalTok{))}
    \NormalTok{rsq_svm[i] <-}\StringTok{ }\KeywordTok{max}\NormalTok{(fit0$results$Rsquared)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# randomForest: random forest model}
\KeywordTok{library}\NormalTok{(randomForest)}
\NormalTok{rsq_rf <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, }\KeywordTok{ncol}\NormalTok{(noise))}
\CommentTok{# ntree=500 number of trees na.action = na.omit ignore}
\CommentTok{# missing value}
\NormalTok{for (i in }\DecValTok{1}\NormalTok{:}\DecValTok{7}\NormalTok{) \{}
    \NormalTok{withnoise <-}\StringTok{ }\NormalTok{sim.dat$income +}\StringTok{ }\NormalTok{noise[, i]}
    \NormalTok{fit0 <-}\StringTok{ }\KeywordTok{randomForest}\NormalTok{(withnoise ~}\StringTok{ }\NormalTok{store_exp +}\StringTok{ }\NormalTok{online_exp +}\StringTok{ }
\StringTok{        }\NormalTok{store_trans +}\StringTok{ }\NormalTok{online_trans, }\DataTypeTok{data =} \NormalTok{sim.dat, }\DataTypeTok{ntree =} \DecValTok{500}\NormalTok{, }
        \DataTypeTok{na.action =} \NormalTok{na.omit)}
    \NormalTok{rsq_rf[i] <-}\StringTok{ }\KeywordTok{tail}\NormalTok{(fit0$rsq, }\DecValTok{1}\NormalTok{)}
\NormalTok{\}}
\KeywordTok{library}\NormalTok{(reshape2)}
\NormalTok{rsq <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\KeywordTok{cbind}\NormalTok{(}\DataTypeTok{Noise =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\DecValTok{1}\NormalTok{, }\FloatTok{1.5}\NormalTok{, }\DecValTok{2}\NormalTok{, }\FloatTok{2.5}\NormalTok{, }\DecValTok{3}\NormalTok{), }
    \NormalTok{rsq_linear, rsq_pls, rsq_mars, rsq_svm, rsq_rf))}
\NormalTok{rsq <-}\StringTok{ }\KeywordTok{melt}\NormalTok{(rsq, }\DataTypeTok{id.vars =} \StringTok{"Noise"}\NormalTok{, }\DataTypeTok{measure.vars =} \KeywordTok{c}\NormalTok{(}\StringTok{"rsq_linear"}\NormalTok{, }
    \StringTok{"rsq_pls"}\NormalTok{, }\StringTok{"rsq_mars"}\NormalTok{, }\StringTok{"rsq_svm"}\NormalTok{, }\StringTok{"rsq_rf"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}







\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(ggplot2)}
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =} \NormalTok{rsq, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \NormalTok{Noise, }\DataTypeTok{y =} \NormalTok{value, }\DataTypeTok{group =} \NormalTok{variable, }
    \DataTypeTok{colour =} \NormalTok{variable)) +}\StringTok{ }\KeywordTok{geom_line}\NormalTok{() +}\StringTok{ }\KeywordTok{geom_point}\NormalTok{() +}\StringTok{ }\KeywordTok{ylab}\NormalTok{(}\StringTok{"R2"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{IDS_files/figure-latex/error-1} 

}

\caption{Test set \(R^2\) profiles for income models when
measurement system noise increases. \texttt{rsq\_linear}: linear
regression, \texttt{rsq\_pls}: Partial Least Square, \texttt{rsq\_mars}:
Multiple Adaptive Regression Spline Regression, \texttt{rsq\_svm}:
Support Vector Machine，\texttt{rsq\_rf}: Random Forest}\label{fig:error}
\end{figure}

Fig. \ref{fig:error} shows that:

All model performance decreases sharply with increasing noise intensity.
To better anticipate model performance, it helps to understand the way
variable is measured. It is something need to make clear at the
beginning of an analytical project. A data scientist should be aware of
the quality of the data in the database. For data from the clients, it
is an important to understand the quality of the data by communication.

More complex model is not necessarily better. The best model in this
situation is MARS, not random forests or SVM. Simple linear regression
and PLS perform the worst when noise is low. MARS is more complicated
than the linear regression and PLS, but it is simpler and easier to
explain than random forest and SVM.

When noise increases to a certain extent, the potential structure
becomes vaguer, and complex random forest model starts to fail. When the
systematic measurement error is significant, a more straightforward but
not naive model may be a better choice. It is always a good practice to
try different models, and select the simplest model in the case of
similar performance. Model evaluation and selection represent the career
``maturity'' of a data scientist.

\subsection{Measurement Error in the Independent
Variables}\label{measurement-error-in-the-independent-variables}

The traditional statistical model usually assumes that the measurement
of the independent variable has no error which is not possible in
practice. Considering the error in the independent variables is
necessary. The impact of the error depends on the following factors: (1)
the magnitude of the randomness; (2) the importance of the corresponding
variable in the model, and (3) the type of model used. Use variable
\texttt{online\_exp} as an example. The approach is similar to the
previous section. Add varying degrees of noise and see its impact on the
model performance. We add the following different levels of noise (0 to
3 times the standard deviation) to\texttt{online\_exp}:

\[\sigma_{0} \times (0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0)\]

where \(\sigma_{0}\) is the standard error of \texttt{online\_exp}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{noise<-}\KeywordTok{matrix}\NormalTok{(}\KeywordTok{rep}\NormalTok{(}\OtherTok{NA}\NormalTok{,}\DecValTok{7}\NormalTok{*}\KeywordTok{nrow}\NormalTok{(sim.dat)),}\DataTypeTok{nrow=}\KeywordTok{nrow}\NormalTok{(sim.dat),}\DataTypeTok{ncol=}\DecValTok{7}\NormalTok{)}
\NormalTok{for (i in }\DecValTok{1}\NormalTok{:}\KeywordTok{nrow}\NormalTok{(sim.dat))\{}
\NormalTok{noise[i,]<-}\KeywordTok{rnorm}\NormalTok{(}\DecValTok{7}\NormalTok{,}\KeywordTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{7}\NormalTok{),}\KeywordTok{sd}\NormalTok{(sim.dat$online_exp)*}\KeywordTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DataTypeTok{by=}\FloatTok{0.5}\NormalTok{))}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Likewise, we examine the effect of noise intensity on different models
(\(R^2\)). The models with complexity from low to high are: ordinary
linear regression, partial least square regression(PLS), multivariate
adaptive regression spline (MARS), support vector machine (SVM, the
Kernel function is radial basis function), and random forest. The code
is similar as before so not shown here.








\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{IDS_files/figure-latex/errorvariable-1} 

}

\caption{Test set \(R^2\) profiles for income models when
noise in \texttt{online\_exp} increases. \texttt{rsq\_linear} : linear
regression, \texttt{rsq\_pls} : Partial Least Square,
\texttt{rsq\_mars}: Multiple Adaptive Regression Spline Regression,
\texttt{rsq\_svm}: Support Vector Machine，\texttt{rsq\_rf}: Random
Forest}\label{fig:errorvariable}
\end{figure}

Comparing Fig. \ref{fig:errorvariable} and Fig. \ref{fig:error}, the
influence of the two types of error is very different. The error in
response cannot be overcome for any model, but it is not the case for
the independent variables. Imagine an extreme case, if
\texttt{online\_exp} is completely random, that is, no information in
it, the impact on the performance of random forest and support vector
machine is marginal. Linear regression and PLS still perform similarly.
With the increase of noise, the performance starts to decline faster. To
a certain extent, it becomes steady. In general, if an independent
variable contains error, other variables associated with it can
compensate to some extent.

\section{Data Splitting and
Resampling}\label{data-splitting-and-resampling}

Those highly adaptable models can model complex relationships. However,
they tend to overfit which leads to the poor prediction by learning too
much from the data. It means that the model is susceptible to the
specific sample used to fit it. When future data is not exactly like the
past data, the model prediction may have big mistakes. A simple model
like ordinary linear regression tends instead to underfit which leads to
a bad prediction by learning too little from the data. It systematically
over-predicts or under-predicts the data regardless of how well future
data resemble past data. Without evaluating models, the modeler will not
know about the problem before the future samples. Data splitting and
resampling are fundamental techniques to build sound models for
prediction.

\subsection{Data Splitting}\label{data-splitting}

\emph{Data splitting} is to put part of the data aside as testing set
(or Hold-outs, out of bag samples) and use the rest for model training.
Training samples are also called in-sample. Model performance metrics
evaluated using in-sample are retrodictive, not predictive.

The traditional business intelligence usually handles data description.
Answer simple questions by querying and summarizing the data, such as:

\begin{itemize}
\tightlist
\item
  What is the monthly sales of a product in 2015?
\item
  What is the number of visits to our site in the past month?\\
\item
  What is the sales difference in 2015 for two different product
  designs?
\end{itemize}

There is no need to go through the tedious process of splitting the
data, tuning and testing model to answer questions of this kind.
Instead, people usually use as complete data as possible and then sum or
average the parts of interest.

Many models have parameters which cannot be directly estimated from the
data, such as \(\lambda\) in the lasso (penalty parameter), the number
of trees in the random forest. This type of model parameter is called
tuning parameter, and there is no analytical formula available to
calculate the optimized value. Tuning parameters often control the
complexity of the model. A poor choice can result in over-fitting or
under-fitting. A standard approach to estimate tuning parameters is
through cross-validation which is a data resampling approach.

To get a reasonable precision of the performance based on a single test
set, the size of the test set may need to be large. So a conventional
approach is to use a subset of samples to fit the model and use the rest
to evaluate model performance. This process will repeat multiple times
to get a performance profile. In that sense, resampling is based on
splitting. The general steps are:

\begin{itemize}
\tightlist
\item
  Define a set of candidate values for tuning parameter(s)

  \begin{itemize}
  \tightlist
  \item
    For each candidate value in the set

    \begin{itemize}
    \tightlist
    \item
      Resample data
    \item
      Fit model
    \item
      Predict hold-out
    \item
      Calculate performance
    \end{itemize}
  \end{itemize}
\item
  Aggregate the results
\item
  Determine the final tuning parameter
\item
  Refit the model with the entire data set
\end{itemize}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.80000\textwidth]{images/ParameterTuningProcess.png}
\caption{Parameter Tuning Process}
\end{figure}

The above is an outline of the general procedure to tune parameters. Now
let's focus on the critical part of the process: data splitting.
Ideally, we should evaluate model using samples that were not used to
build or fine-tune the model. So it provides an unbiased sense of model
effectiveness. When the sample size is large, it is a good practice to
set aside part of the samples to evaluate the final model. People use
``training'' data to indicate samples used to fit or fine-tune the model
and ``test'' or ``validation'' data set is used to validate performance.

The first decision to make for data splitting is to decide the
proportion of data in the test set. There are two factors to consider
here: (1) sample size; (2) computation intensity. If the sample size is
large enough which is the most common situation according to my
experience, you can try to use 20\%, 30\% and 40\% of the data as the
test set, and see which one works the best. If the model is
computationally intense, then you may consider starting from a smaller
sample of data to train the model hence will have a higher portion of
data in the test set. Depending on how it performs, you may need to
increase the training set. If the sample size is small, you can use
cross-validation or bootstrap which is the topic in the next section.

The next decision is to decide which samples are in the test set. There
is a desire to make the training and test sets as similar as possible. A
simple way is to split data by random sampling which, however, does not
control for any of the data attributes, such as the percentage of the
retained customer in the data. So it is possible that the distribution
of outcomes is substantially different between the training and test
sets. There are three main ways to split the data that account for the
similarity of resulted data sets. We will describe the three approaches
using the clothing company customer data as examples.

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\tightlist
\item
  Split data according to the outcome variable
\end{enumerate}

Assume the outcome variable is customer segment (column
\texttt{segment}) and we decide to use 80\% as training and 20\% test.
The goal is to make the proportions of the categories in the two sets as
similar as possible. The \texttt{createDataPartition()} function in
\texttt{caret} will return a balanced splitting based on assigned
variable.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# load data}
\NormalTok{sim.dat <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"https://raw.githubusercontent.com/happyrabbit/DataScientistR/master/Data/SegData.csv"}\NormalTok{)}
\KeywordTok{library}\NormalTok{(caret)}
\CommentTok{# set random seed to make sure reproducibility}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{3456}\NormalTok{)}
\NormalTok{trainIndex <-}\StringTok{ }\KeywordTok{createDataPartition}\NormalTok{(sim.dat$segment, }\DataTypeTok{p =} \FloatTok{0.8}\NormalTok{, }\DataTypeTok{list =} \OtherTok{FALSE}\NormalTok{, }
    \DataTypeTok{times =} \DecValTok{1}\NormalTok{)}
\KeywordTok{head}\NormalTok{(trainIndex)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      Resample1
## [1,]         1
## [2,]         2
## [3,]         3
## [4,]         4
## [5,]         6
## [6,]         7
\end{verbatim}

The \texttt{list\ =\ FALSE} in the call to \texttt{createDataPartition}
is to return a data frame. The \texttt{times\ =\ 1} tells R how many
times you want to split the data. Here we only do it once, but you can
repeat the splitting multiple times. In that case, the function will
return multiple vectors indicating the rows to training/test. You can
set \texttt{times＝2} and rerun the above code to see the result. Then
we can use the returned indicator vector \texttt{trainIndex} to get
training and test sets:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# get training set}
\NormalTok{datTrain <-}\StringTok{ }\NormalTok{sim.dat[trainIndex, ]}
\CommentTok{# get test set}
\NormalTok{datTest <-}\StringTok{ }\NormalTok{sim.dat[-trainIndex, ]}
\end{Highlighting}
\end{Shaded}

According to the setting, there are 800 samples in the training set and
200 in test set. Let's check the distribution of the two sets:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(plyr)}
\KeywordTok{ddply}\NormalTok{(datTrain, }\StringTok{"segment"}\NormalTok{, summarise, }\DataTypeTok{count =} \KeywordTok{length}\NormalTok{(segment), }
    \DataTypeTok{percentage =} \KeywordTok{round}\NormalTok{(}\KeywordTok{length}\NormalTok{(segment)/}\KeywordTok{nrow}\NormalTok{(datTrain), }\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       segment count percentage
## 1 Conspicuous   160       0.20
## 2       Price   200       0.25
## 3     Quality   160       0.20
## 4       Style   280       0.35
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ddply}\NormalTok{(datTest, }\StringTok{"segment"}\NormalTok{, summarise, }\DataTypeTok{count =} \KeywordTok{length}\NormalTok{(segment), }
    \DataTypeTok{percentage =} \KeywordTok{round}\NormalTok{(}\KeywordTok{length}\NormalTok{(segment)/}\KeywordTok{nrow}\NormalTok{(datTest), }\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       segment count percentage
## 1 Conspicuous    40       0.20
## 2       Price    50       0.25
## 3     Quality    40       0.20
## 4       Style    70       0.35
\end{verbatim}

The percentages are the same for these two sets. In practice, it is
possible that the distributions are not exactly identical but should be
close.

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  Divide data according to predictors
\end{enumerate}

An alternative way is to split data based on the predictors. The goal is
to get a diverse subset from a dataset so that the sample is
representative. In other words, we need an algorithm to identify the
\(n\) most diverse samples from a dataset with size \(N\). However, the
task is generally infeasible for non-trivial values of \(n\) and \(N\)
\citep{willett}. And hence practicable approaches to dissimilarity-based
selection involve approximate methods that are sub-optimal. A major
class of algorithms split the data on \emph{maximum dissimilarity
sampling}. The process starts from:

\begin{itemize}
\tightlist
\item
  Initialize a single sample as starting test set
\item
  Calculate the dissimilarity between this initial sample and each
  remaining samples in the dataset
\item
  Add the most dissimilar unallocated sample to the test set
\end{itemize}

To move forward, we need to define the dissimilarity between groups.
Each definition results in a different version of the algorithm and
hence a different subset. It is the same problem as in hierarchical
clustering where you need to define a way to measure the distance
between clusters. The possible approaches are to use minimum, maximum,
sum of all distances, the average of all distances, etc. Unfortunately,
there is not a single best choice, and you may have to try multiple
methods and check the resulted sample sets. R users can implement the
algorithm using \texttt{maxDissim()} function from \texttt{caret}
package. The \texttt{obj} argument is to set the definition of
dissimilarity. Refer to the help documentation for more details
(\texttt{?maxDissim}).

Let's use two variables (\texttt{age} and \texttt{income}) from the
customer data as an example to illustrate how it works in R and compare
maximum dissimilarity sampling with random sampling.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(lattice)}
\CommentTok{# select variables}
\NormalTok{testing <-}\StringTok{ }\KeywordTok{subset}\NormalTok{(sim.dat, }\DataTypeTok{select =} \KeywordTok{c}\NormalTok{(}\StringTok{"age"}\NormalTok{, }\StringTok{"income"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Random select 5 samples as initial subset (\texttt{start}) , the rest
will be in \texttt{samplePool}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{5}\NormalTok{)}
\CommentTok{# select 5 random samples}
\NormalTok{startSet <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\DecValTok{1}\NormalTok{:}\KeywordTok{dim}\NormalTok{(testing)[}\DecValTok{1}\NormalTok{], }\DecValTok{5}\NormalTok{)}
\NormalTok{start <-}\StringTok{ }\NormalTok{testing[startSet, ]}
\CommentTok{# save the rest in data frame 'samplePool'}
\NormalTok{samplePool <-}\StringTok{ }\NormalTok{testing[-startSet, ]}
\end{Highlighting}
\end{Shaded}

Use \texttt{maxDissim()} to select another 5 samples from
\texttt{samplePool} that are as different as possible with the initical
set \texttt{start}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{selectId <-}\StringTok{ }\KeywordTok{maxDissim}\NormalTok{(start, samplePool, }\DataTypeTok{obj =} \NormalTok{minDiss, }\DataTypeTok{n =} \DecValTok{5}\NormalTok{)}
\NormalTok{minDissSet <-}\StringTok{ }\NormalTok{samplePool[selectId, ]}
\end{Highlighting}
\end{Shaded}

The \texttt{obj\ =\ minDiss} in the above code tells R to use minimum
dissimilarity to define the distance between groups. Next, random select
5 samples from \texttt{samplePool} in data frame \texttt{RandomSet}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{selectId <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\DecValTok{1}\NormalTok{:}\KeywordTok{dim}\NormalTok{(samplePool)[}\DecValTok{1}\NormalTok{], }\DecValTok{5}\NormalTok{)}
\NormalTok{RandomSet <-}\StringTok{ }\NormalTok{samplePool[selectId, ]}
\end{Highlighting}
\end{Shaded}

Plot the resulted set to compare different sampling methods:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{start$group <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\StringTok{"Initial Set"}\NormalTok{, }\KeywordTok{nrow}\NormalTok{(start))}
\NormalTok{minDissSet$group <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\StringTok{"Maximum Dissimilarity Sampling"}\NormalTok{, }\KeywordTok{nrow}\NormalTok{(minDissSet))}
\NormalTok{RandomSet$group <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\StringTok{"Random Sampling"}\NormalTok{, }\KeywordTok{nrow}\NormalTok{(RandomSet))}
\KeywordTok{xyplot}\NormalTok{(age ~}\StringTok{ }\NormalTok{income, }\DataTypeTok{data =} \KeywordTok{rbind}\NormalTok{(start, minDissSet, RandomSet), }\DataTypeTok{grid =} \OtherTok{TRUE}\NormalTok{, }
    \DataTypeTok{group =} \NormalTok{group, }\DataTypeTok{auto.key =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{IDS_files/figure-latex/maxdis-1} 

}

\caption{Compare Maximum Dissimilarity Sampling with  Random Sampling}\label{fig:maxdis}
\end{figure}

The points from maximum dissimilarity sampling are far away from the
initial samples ( Fig. \ref{fig:maxdis}, while the random samples are
much closer to the initial ones. Why do we need a diverse subset?
Because we hope the test set to be representative. If all test set
samples are from respondents younger than 30, model performance on the
test set has a high risk to fail to tell you how the model will perform
on more general population.

\begin{itemize}
\tightlist
\item
  Divide data according to time
\end{itemize}

For time series data, random sampling is usually not the best way. There
is an approach to divide data according to time-series. Since time
series is beyond the scope of this book, there is not much discussion
here. For more detail of this method, see \citep{Hyndman}. We will use a
simulated first-order autoregressive model {[}AR (1){]} time-series data
with 100 observations to show how to implement using the function
\texttt{createTimeSlices\ ()} in the \texttt{caret} package.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# simulte AR(1) time series samples}
\NormalTok{timedata =}\StringTok{ }\KeywordTok{arima.sim}\NormalTok{(}\KeywordTok{list}\NormalTok{(}\DataTypeTok{order=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{), }\DataTypeTok{ar=}\NormalTok{-.}\DecValTok{9}\NormalTok{), }\DataTypeTok{n=}\DecValTok{100}\NormalTok{)}
\CommentTok{# plot time series}
\KeywordTok{plot}\NormalTok{(timedata, }\DataTypeTok{main=}\NormalTok{(}\KeywordTok{expression}\NormalTok{(}\KeywordTok{AR}\NormalTok{(}\DecValTok{1}\NormalTok{)~}\ErrorTok{~~}\NormalTok{phi==-.}\DecValTok{9}\NormalTok{)))     }
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{IDS_files/figure-latex/times-1} 

}

\caption{Divide data according to time}\label{fig:times}
\end{figure}

Fig. \ref{fig:times} shows 100 simulated time series observation. The
goal is to make sure both training and test set to cover the whole
period.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{timeSlices <-}\StringTok{ }\KeywordTok{createTimeSlices}\NormalTok{(}\DecValTok{1}\NormalTok{:}\KeywordTok{length}\NormalTok{(timedata), }
                   \DataTypeTok{initialWindow =} \DecValTok{36}\NormalTok{, }\DataTypeTok{horizon =} \DecValTok{12}\NormalTok{, }\DataTypeTok{fixedWindow =} \NormalTok{T)}
\KeywordTok{str}\NormalTok{(timeSlices,}\DataTypeTok{max.level =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## List of 2
##  $ train:List of 53
##  $ test :List of 53
\end{verbatim}

There are three arguments in the above \texttt{createTimeSlices()}.

\begin{itemize}
\tightlist
\item
  \texttt{initialWindow}: The initial number of consecutive values in
  each training set sample
\item
  \texttt{horizon}: the number of consecutive values in test set sample
\item
  \texttt{fixedWindow}: if FALSE, all training samples start at 1
\end{itemize}

The function returns two lists, one for the training set, the other for
the test set. Let's look at the first training sample:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# get result for the 1st training set}
\NormalTok{trainSlices <-}\StringTok{ }\NormalTok{timeSlices[[}\DecValTok{1}\NormalTok{]]}
\CommentTok{# get result for the 1st test set}
\NormalTok{testSlices <-}\StringTok{ }\NormalTok{timeSlices[[}\DecValTok{2}\NormalTok{]]}
\CommentTok{# check the index for the 1st training and test set}
\NormalTok{trainSlices[[}\DecValTok{1}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17
## [18] 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34
## [35] 35 36
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{testSlices[[}\DecValTok{1}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] 37 38 39 40 41 42 43 44 45 46 47 48
\end{verbatim}

The first training set is consist of sample 1-36 in the dataset
(\texttt{initialWindow\ =\ 36}). Then sample 37-48 are in the first test
set ( \texttt{horizon\ =\ 12}). Type \texttt{head(trainSlices)} or
\texttt{head(testSlices)} to check the later samples. If you are not
clear about the argument \texttt{fixedWindow}, try to change the setting
to be \texttt{F} and check the change in \texttt{trainSlices} and
\texttt{testSlices}.

Understand and implement data splitting is not difficult. But there are
two things to note:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The randomness in the splitting process will lead to uncertainty in
  performance measurement.
\item
  When the dataset is small, it can be too expensive to leave out test
  set. In this situation, if collecting more data is just not possible,
  the best shot is to use leave-one-out cross-validation which is in the
  next section.
\end{enumerate}

\subsection{Resampling}\label{resampling}

You can consider resampling as repeated splitting. The basic idea is:
use part of the data to fit model and then use the rest of data to
calculate model performance. Repeat the process multiple times and
aggregate the results. The differences in resampling techniques usually
center around the ways to choose subsamples. There are two main reasons
that we may need resampling:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Estimate tuning parameters through resampling. Some examples of models
  with such parameters are Support Vector Machine (SVM), models
  including the penalty (LASSO) and random forest.
\item
  For models without tuning parameter, such as ordinary linear
  regression and partial least square regression, the model fitting
  doesn't require resampling. But you can study the model stability
  through resampling.
\end{enumerate}

We will introduce three most common resampling techniques: k-fold
cross-validation, repeated training/test splitting, and bootstrap.

\subsubsection{k-fold cross-validation}\label{k-fold-cross-validation}

k-fold cross-validation is to partition the original sample into \(k\)
equal size subsamples (folds). Use one of the \(k\) folds to validate
the model and the rest \(k-1\) to train model. Then repeat the process
\(k\) times with each of the \(k\) folds as the test set. Aggregate the
results into a performance profile.

Denote by \(\hat{f}^{-\kappa}(X)\) the fitted function, computed with
the \(\kappa^{th}\) fold removed and \(x_i^\kappa\) the predictors for
samples in left-out fold. The process of k-fold cross-validation is as
follows:

\begin{quote}
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Partition the original sample into \(k\) equal size folds
\item
  for \(\kappa=1…k\)
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Use data other than fold \(\kappa\) to train the model
  \(\hat{f}^{-\kappa}(X)\)
\item
  Apply \(\hat{f}^{-\kappa}(X)\) to predict fold \(\kappa\) to get
  \(\hat{f}^{-\kappa}(x_i^\kappa)\)
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Aggregate the results
  \[\hat{Error} = \frac{1}{N}\Sigma_{\kappa=1}^k\Sigma_{x_i^{\kappa}}L(y_i^{\kappa},\hat{f}^{-\kappa}(x_i^\kappa))\]
\end{enumerate}
\end{quote}

It is a standard way to find the value of tuning parameter that gives
you the best performance. It is also a way to study the variability of
model performance.

The following figure represents a 5-fold cross-validation example.

\begin{figure}[htbp]
\centering
\includegraphics{images/cv5fold.png}
\caption{5-fold cross-validation}
\end{figure}

A special case of k-fold cross-validation is Leave One Out Cross
Validation (LOOCV) where \(k=1\). When sample size is small, it is
desired to use as many data to train the model. Most of the functions
have default setting \(k=10\). The choice is usually 5-10 in practice,
but there is no standard rule. The more folds to use, the more samples
are used to fit model, and then the performance estimate is closer to
the theoretical performance. Meanwhile, the variance of the performance
is larger since the samples to fit model in different iterations are
more similar. However, LOOCV has high computational cost since the
number of interactions is the same as the sample size and each model fit
uses a subset that is nearly the same size of the training set. On the
other hand, when k is small (such as 2 or 3), the computation is more
efficient, but the bias will increase. When the sample size is large,
the impact of \(k\) becomes marginal.

Chapter 7 of \citep{Hastie2008} presents a more in-depth and more
detailed discussion about the bias-variance trade-off in k-fold
cross-validation.

You can implement k-fold cross-validation using \texttt{createFolds()}
in \texttt{caret}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(caret)}
\NormalTok{class<-sim.dat$segment}
\CommentTok{# creat k-folds}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{cv<-}\KeywordTok{createFolds}\NormalTok{(class,}\DataTypeTok{k=}\DecValTok{10}\NormalTok{,}\DataTypeTok{returnTrain=}\NormalTok{T)}
\KeywordTok{str}\NormalTok{(cv)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## List of 10
##  $ Fold01: int [1:900] 1 2 3 4 5 6 7 8 9 10 ...
##  $ Fold02: int [1:900] 1 2 3 4 5 6 7 9 10 11 ...
##  $ Fold03: int [1:900] 1 2 3 4 5 6 7 8 10 11 ...
##  $ Fold04: int [1:900] 1 2 3 4 5 6 7 8 9 11 ...
##  $ Fold05: int [1:900] 1 3 4 6 7 8 9 10 11 12 ...
##  $ Fold06: int [1:900] 1 2 3 4 5 6 7 8 9 10 ...
##  $ Fold07: int [1:900] 2 3 4 5 6 7 8 9 10 11 ...
##  $ Fold08: int [1:900] 1 2 3 4 5 8 9 10 11 12 ...
##  $ Fold09: int [1:900] 1 2 4 5 6 7 8 9 10 11 ...
##  $ Fold10: int [1:900] 1 2 3 5 6 7 8 9 10 11 ...
\end{verbatim}

The above code creates ten folds (\texttt{k=10}) according to the
customer segments (we set \texttt{class} to be the categorical variable
\texttt{segment}). The function returns a list of 10 with the index of
rows in training set.

\subsubsection{Repeated Training/Test
Splits}\label{repeated-trainingtest-splits}

In fact, this method is nothing but repeating the training/test set
division on the original data. Fit the model with the training set, and
evaluate the model with the test set. Unlike k-fold cross-validation,
the test set generated by this procedure may have duplicate samples. A
sample usually shows up in more than one test sets. There is no standard
rule for split ratio and number of repetitions. The most common choice
in practice is to use 75\% to 80\% of the total sample for training. The
remaining samples are for validation. The more sample in the training
set, the less biased the model performance estimate is. Increasing the
repetitions can reduce the uncertainty in the performance estimates. Of
course, it is at the cost of computational time when the model is
complex. The number of repetitions is also related to the sample size of
the test set. If the size is small, the performance estimate is more
volatile. In this case, the number of repetitions needs to be higher to
deal with the uncertainty of the evaluation results.

We can use the same function (\texttt{createDataPartition\ ()}) as
before. If you look back, you will see \texttt{times\ =\ 1}. The only
thing to change is to set it to the number of repetitions.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{trainIndex <-}\StringTok{ }\KeywordTok{createDataPartition}\NormalTok{(sim.dat$segment, }\DataTypeTok{p =} \NormalTok{.}\DecValTok{8}\NormalTok{, }\DataTypeTok{list =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{times =} \DecValTok{5}\NormalTok{)}
\NormalTok{dplyr::}\KeywordTok{glimpse}\NormalTok{(trainIndex)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  int [1:800, 1:5] 1 3 4 5 6 7 8 9 10 11 ...
##  - attr(*, "dimnames")=List of 2
##   ..$ : NULL
##   ..$ : chr [1:5] "Resample1" "Resample2" "Resample3" "Resample4" ...
\end{verbatim}

Once know how to split the data, the repetition comes naturally.

\subsubsection{Bootstrap Methods}\label{bootstrap-methods}

Bootstrap is a powerful statistical tool (a little magic too). It can be
used to analyze the uncertainty of parameter estimates
\citep{bootstrap1986} quantitatively. For example, estimate the standard
deviation of linear regression coefficients. The power of this method is
that the concept is so simple that it can be easily applied to any model
as long as the computation allows. However, you can hardly obtain the
standard deviation for some models by using the traditional statistical
inference.

Since it is with replacement, a sample can be selected multiple times,
and the bootstrap sample size is the same as the original data. So for
every bootstrap set, there are some left-out samples, which is also
called ``out-of-bag samples.'' The out-of-bag sample is used to evaluate
the model. Efron points out that under normal circumstances
\citep{efron1983}, bootstrap estimates the error rate of the model with
more certainty.The probability of an observation \(i\) in bootstrap
sample B is:

\(\begin{array}{ccc} Pr{i\in B} & = & 1-\left(1-\frac{1}{N}\right)^{N}\\  & \approx & 1-e^{-1}\\  & = & 0.632 \end{array}\)

On average, 63.2\% of the observations appeared at least once in a
bootstrap sample, so the estimation bias is similar to 2-fold
cross-validation. As mentioned earlier, the smaller the number of folds,
the larger the bias. Increasing the sample size will ease the problem.
In general, bootstrap has larger bias and smaller uncertainty than
cross-validation. Efron came up the following ``.632 estimator'' to
alleviate this bias:

\[(0.632 × original\ bootstrap\ estimate) + (0.368 × apparent\ error\ rate)\]

The apparent error rate is the error rate when the data is used twice,
both to fit the model and to check its accuracy and it is apparently
over-optimistic. The modified bootstrap estimate reduces the bias but
can be unstable with small samples size. This estimate can also be
unduly optimistic when the model severely over-fits since the apparent
error rate will be close to zero. Efron and Tibshirani \citep{b632plus}
discuss another technique, called the ``632+ method,'' for adjusting the
bootstrap estimates.

\chapter{Measuring Performance}\label{measuring-performance}

\bibliography{bibliography.bib}

\backmatter
\printindex

\end{document}
