\PassOptionsToPackage{unicode=true}{hyperref} % options for packages loaded elsewhere
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames*,x11names*}{xcolor}
%
\documentclass[12pt,]{krantz}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provides euro and other symbols
\else % if luatex or xelatex
  \usepackage{unicode-math}
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
    \setmonofont[Mapping=tex-ansi,Scale=0.7]{Source Code Pro}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage[]{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\usepackage{xcolor}
\usepackage{hyperref}
\hypersetup{
            pdftitle={Introduction to Data Science},
            pdfauthor={Hui Lin and Ming Li},
            colorlinks=true,
            linkcolor=Maroon,
            filecolor=Maroon,
            citecolor=Blue,
            urlcolor=Blue,
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.33,0.33,0.33}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.61,0.61,0.61}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.06,0.06,0.06}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.5,0.5,0.5}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0,0,0}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.27,0.27,0.27}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.27,0.27,0.27}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.06,0.06,0.06}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.14,0.14,0.14}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.06,0.06,0.06}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0,0,0}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.27,0.27,0.27}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.43,0.43,0.43}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0,0,0}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.5,0.5,0.5}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.5,0.5,0.5}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0,0,0}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.5,0.5,0.5}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs}
% Fix footnotes in tables (requires footnote package)
\IfFileExists{footnote.sty}{\usepackage{footnote}\makesavenoteenv{longtable}}{}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

% set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\usepackage{booktabs}
\usepackage{longtable}
\usepackage[bf,singlelinecheck=off]{caption}

\usepackage{framed,color}
\definecolor{shadecolor}{RGB}{248,248,248}

\renewcommand{\textfraction}{0.05}
\renewcommand{\topfraction}{0.8}
\renewcommand{\bottomfraction}{0.8}
\renewcommand{\floatpagefraction}{0.75}

\renewenvironment{quote}{\begin{VF}}{\end{VF}}
\let\oldhref\href
\renewcommand{\href}[2]{#2\footnote{\url{#1}}}

\makeatletter
\newenvironment{kframe}{%
\medskip{}
\setlength{\fboxsep}{.8em}
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\renewenvironment{Shaded}{\begin{kframe}}{\end{kframe}}

\usepackage{makeidx}
\makeindex

\urlstyle{tt}

\usepackage{amsthm}
\makeatletter
\def\thm@space@setup{%
  \thm@preskip=8pt plus 2pt minus 4pt
  \thm@postskip=\thm@preskip
}
\makeatother

\frontmatter
\usepackage[]{natbib}
\bibliographystyle{apalike}

\title{Introduction to Data Science}
\author{Hui Lin and Ming Li}
\date{2019-12-20}

\begin{document}
\maketitle

%\cleardoublepage\newpage\thispagestyle{empty}\null
%\cleardoublepage\newpage\thispagestyle{empty}\null
%\cleardoublepage\newpage
\thispagestyle{empty}
\begin{center}
%\includegraphics{images/dedication.pdf}
\end{center}

\setlength{\abovedisplayskip}{-5pt}
\setlength{\abovedisplayshortskip}{-5pt}

{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}
\listoftables
\listoffigures
\hypertarget{preface}{%
\chapter*{Preface}\label{preface}}


During the first couple years of our career as data scientists, we were bewildered by all kinds of data science hype. There is a lack of definition of many basic terminologies such as ``Big Data'' and ``Data Science.'' How big is big? If someone ran into you asked what data science was all about, what would you tell them? What is the difference between the sexy role ``Data Scientist'' and the traditional ``Data Analyst''? How suddenly came all kinds of machine algorithms? All those struck us as confusing and vague as real-world data scientists! But we always felt that there was something real there. After applying data science for many years, we explored it more and had a much better idea about data science. And this book is our endeavor to make data science to a more legitimate field.

\hypertarget{goal-of-the-book}{%
\section*{Goal of the Book}\label{goal-of-the-book}}


This is an introductory book to data science with a specific focus on the application. Data Science is a cross-disciplinary subject involving hands-on experience and business problem-solving exposures. The majority of existing introduction books on data science are about the modeling techniques and the implementation of models using R or Python. However, they fail to introduce data science in a context of the industrial environment. Moreover, a crucial part, the art of data science in practice, is often missing. This book intends to fill the gap.

Some key features of this book are as follows:

\begin{itemize}
\item
  It is comprehensive. It covers not only technical skills but also soft skills and big data environment in the industry.
\item
  It is hands-on. We provide the data and repeatable R and Python code. You can repeat the analysis in the book using the data and code provided. We also suggest you perform the analyses with your data whenever possible. You can only learn data science by doing it!
\item
  It is based on context. We put methods in the context of industrial data science questions.
\item
  Where appropriate, we point you to more advanced materials on models to dive deeper
\end{itemize}

\hypertarget{who-this-book-is-for}{%
\section*{Who This Book Is For}\label{who-this-book-is-for}}


Non-mathematical readers will appreciate the emphasis on problem-solving with real data across a wide variety of applications and the reproducibility of the companion R and python code.

Readers should know basic statistical ideas, such as correlation and linear regression analysis. While the text is biased against complex equations, a mathematical background is needed for advanced topics.

\hypertarget{what-this-book-covers}{%
\section*{What This Book Covers}\label{what-this-book-covers}}


Based on industry experience, this book outlines the real world scenario and points out pitfalls data science practitioners should avoid. It also covers big data cloud platform and the art of data science such as soft skills. We use R as the main tool and provide code for both R and Python.

\hypertarget{conventions}{%
\section*{Conventions}\label{conventions}}


\hypertarget{acknowledgements}{%
\section*{Acknowledgements}\label{acknowledgements}}


\hypertarget{about-the-authors}{%
\chapter*{About the Authors}\label{about-the-authors}}


\textbf{Hui Lin} is leading and building data science department at Netlify. Before Netlify, she was a Data Scientist at DuPont. She was a leader in the company of applying advanced data science to enhance Marketing and Sales Effectiveness. She provided data science leadership for a broad range of predictive analytics and market research analysis from 2013 to 2018. She is the co-founder of Central Iowa R User Group, blogger of scientistcafe.com and 2018 Program Chair of ASA Statistics in Marketing Section. She enjoys making analytics accessible to a broad audience and teaches tutorials and workshops for practitioners on data science (\url{https://course2019.netlify.com/}). She holds MS and Ph.D in statistics from Iowa State University.

\textbf{Ming Li} is currently a Senior Data Scientist at Amazon and an Adjunct Faculty of Department of Marketing and Business Analytics in Texas A\&M University - Commerce. He is the Chair of Quality \& Productivity Section of ASA for 2017. He was a Data Scientist at Walmart and a Statistical Leader at General Electric Global Research Center. He obtained his Ph.D.~in Statistics from Iowa State University at 2010. With deep statistics background and a few years' experience in data science, he has trained and mentored numerous junior data scientist with different background such as statistician, programmer, software developer, database administrator and business analyst. He is also an Instructor of Amazon's internal Machine Learning University and was one of the key founding member of Walmart's Analytics Rotational Program which bridges the skill gaps between new hires and productive data scientists.

\mainmatter

\hypertarget{introduction}{%
\chapter{Introduction}\label{introduction}}

Interest in data science is at an all-time high and has exploded in popularity in the last couple of years. Data scientists today are from various backgrounds. If someone ran into you ask what data science is all about, what would you tell them? It is not an easy question to answer. Data science is one of the areas that everyone is talking about, but no one can define.

Media has been hyping about ``Data Science'' ``Big Data'' and ``Artificial Intelligence'' over the past few years. I like this amusing statement from the internet:

\begin{quote}
``When you're fundraising, it's AI. When you're hiring, it's ML. When you're implementing, it's logistic regression.''
\end{quote}

For outsiders, data science is whatever magic that can get useful information out of data. Everyone should have heard about big data. Data science trainees now need the skills to cope with such big data sets. What are those skills? You may hear about: Hadoop, a system using Map/Reduce to process large data sets distributed across a cluster of computers or about Spark, a system build atop Hadoop for speeding up the same by loading huge datasets into shared memory(RAM) across clusters. The new skills are for dealing with organizational artifacts of large-scale cluster computing but not for better solving the real problem. A lot of data means more tinkering with computers. After all, it isn't the size of the data that's important. It's what you do with it. Your first reaction to all of this might be some combination of skepticism and confusion. We want to address this up front that: we had that exact reaction.

To declutter, let's start from a brief history of data science. If you hit up the Google Trends website which shows search keyword information over time and check the term ``data science,'' you will find the history of data science goes back a little further than 2004. From the way media describes it, you may feel machine learning algorithms were just invented last month, and there was never ``big'' data before Google. That is not true. There are new and exciting developments of data science, but many of the techniques we are using are based on decades of work by statisticians, computer scientists, mathematicians and scientists of all types.

In the early 19th century when Legendre and Gauss came up the least squares method for linear regression, only physicists would use it to fit linear regression. Now, non-technical people can fit linear regressions using excel. In 1936 Fisher came up with linear discriminant analysis. In the 1940s, we had another widely used model -- logistic regression. In the 1970s, Nelder and Wedderburn formulated ``generalized linear model (GLM)'' which:

\begin{quote}
``generalized linear regression by allowing the linear model to be related to the response variable via a link function and by allowing the magnitude of the variance of each measurement to be a function of its predicted value.'' {[}from Wikipedia{]}
\end{quote}

By the end of the 1970s, there was a range of analytical models and most of them were linear because computers were not powerful enough to fit non-linear model until the 1980s.

In 1984 Breiman et al\citep{Breiman1984}. introduced the classification and regression tree (CART) which is one of the oldest and most utilized classification and regression techniques. After that Ross Quinlan came up with more tree algorithms such as ID3, C4.5, and C5.0. In the 1990s, ensemble techniques (methods that combine many models' predictions) began to appear. Bagging is a general approach that uses bootstrapping in conjunction with regression or classification model to construct an ensemble. Based on the ensemble idea, Breiman came up with random forest in 2001\citep{Breiman2001}. In the same year, Leo Breiman published a paper ``\href{http://www2.math.uu.se/~thulin/mm/breiman.pdf}{Statistical Modeling: The Two Cultures}'' \citep{Breiman2001TwoCulture} where he pointed out two cultures in the use of statistical modeling to get information from data:

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\tightlist
\item
  Data is from a given stochastic data model\\
\item
  Data mechanism is unknown and people approach the data using algorithmic model
\end{enumerate}

Most of the classic statistical models are the first type. Black box models, such as random forest, GMB, and today's buzz work deep learning are algorithmic modeling. As Breiman pointed out, those models can be used both on large complex data as a more accurate and informative alternative to data modeling on smaller data sets. Those algorithms have developed rapidly, however, in fields outside statistics. That is one of the most important reasons that statisticians are not the mainstream of today's data science, both in theory and practice. Hence Python is catching up R as the most commonly used language in data science. It is due to the data scientists background rather than the language itself. Since 2000, the approaches to get information out of data have been shifting from traditional statistical models to a more diverse toolbox named machine learning.

What is the driving force behind the shifting trend? John Tukey identified four forces driving data analysis (there was no ``data science'' back to 1962):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The formal theories of math and statistics
\item
  Acceleration of developments in computers and display devices
\item
  The challenge, in many fields, of more and ever larger bodies of data
\item
  The emphasis on quantification in an ever wider variety of disciplines
\end{enumerate}

Tukey's 1962 list is surprisingly modern. Let's inspect those points in today's context. People usually develop theories way before they find the applications. In the past 50 years, statisticians, mathematician, and computer scientists have been laying the theoretical groundwork for constructing ``data science'' today. The development of computers enables us to apply the algorithmic models (they can be very computationally expensive) and deliver results in a friendly and intuitive way. The striking transition to the internet of things generates vast amounts of commercial data. Industries have also sensed the value of exploiting that data. Data science seems certain to be a major preoccupation of commercial life in coming decades. All the four forces John identified exist today and have been driving data science.

Benefiting from the increasing availability of digitized information, and the possibility to distribute that through the internet, the toolbox and application have been expanding fast. Today, people apply data science in a plethora of areas including business, health, biology, social science, politics, etc.
Now data science is everywhere. But what is today's data science?

\hypertarget{blind-men-and-an-elephant}{%
\section{Blind men and an elephant}\label{blind-men-and-an-elephant}}

There is a widely diffused Chinese parable (depending on where you are from, you may think it is a Japanese parable) which is about a group of blind men conceptualizing what the elephant is like by touching it:

\begin{quote}
``\ldots{}In the case of the first person, whose hand landed on the trunk, said: `This being is like a thick snake'. For another one whose hand reached its ear, it seemed like a kind of fan. As for another person, whose hand was upon its leg, said, the elephant is a pillar like a tree-trunk. The blind man who placed his hand upon its side said, `elephant is a wall'. Another who felt its tail described it as a rope. The last felt its tusk, stating the elephant is that which is hard, smooth and like a spear.'' \href{https://en.wikipedia.org/wiki/Blind_men_and_an_elephant}{wikipedia}
\end{quote}

Data science is the elephant. With the data science hype picking up stream, many professionals changed their titles to be ``Data Scientist'' without any of the necessary qualifications. Today's data scientists have vastly different backgrounds, yet each one conceptualizes what the elephant is based on his/her own professional training and application area. And to make matters worse, most of us are not even fully aware of our own conceptualizations, much less the uniqueness of the experience from which they are derived. Here is a list of somewhat whimsical definitions for a ``data scientist'':

\begin{itemize}
\tightlist
\item
  ``A data scientist is a data analyst who lives in California''
\item
  ``A data scientist is someone who is better at statistics than any software engineer and better at software engineering than any statistician.''
\item
  ``A data scientist is a statistician who lives in San Francisco.''
\item
  ``Data Science is statistics on a Mac.''
\end{itemize}

\begin{quote}
``We don't see things as they are, we see them as we are. {[}by Anais Nin{]}''
\end{quote}

It is annoying but true. So the answer to the question ``what is data science?'' depends on who you are talking to. Who you may be talking to then? Data science has three main skill tracks: engineering, analysis, and modeling (and yes, the order matters!).

Here are some representative skills in each track. Different tracks and combinations of tracks will define different roles in data science. \footnote{``The tanh function is almost always strictly superior.'' ---- by Andrew Ng from his coursera course ``Neural Networks and Deep Learning''}

\hypertarget{data-science-roleskill-tracks}{%
\subsection{Data science role/skill tracks}\label{data-science-roleskill-tracks}}

When people talk about all the machine learning and AI algorithms, they often over look the critical data engineering part that makes everything else possible. Data engineering is the unseen iceberg under the water surface. Think your company need a data scientist? You are wrong if you haven't hired a data engineer yet. You need to have the ability to get data before making sense of it. If you only deal with small datasets, you may be able to get by with entering some numbers into a spreadsheet. As the data increasing in size, data engineering becomes a sophisticated discipline in its own right.

\begin{itemize}
\tightlist
\item
  \textbf{Engineering: the process of making everything else possible}
\end{itemize}

Data engineering mainly involves in building the data pipeline infrastructure. In the (not that) old day, when data is stored on local servers, computers or other devices, building the data infrastructure can be a humongous IT project which involves not only the software but also the hardware that used to store the data and perform ETL process. As the development of cloud service, data storage and computing on the cloud becomes the new norm. Data engineering today at its core is software engineering. Ensuring maintainability through modular, well-commented code and version control is fundamental.

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\tightlist
\item
  Data environment
\end{enumerate}

Design and set up the environment to support data science workflow. It may include setting up data storage in the cloud, Kafka platform, Hadoop and Spark cluster etc. Each company has its unique data condition and needs. So the environment will be different depending on size of the data, update frequency, complexity of analytics, compatibility with the backend infrastructure and (of course) budget.

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  Data management
\end{enumerate}

Automated data collection is a common task which includes parsing the logs (depending on the stage of the company and the type of industry you are in), web scraping, API queries, and interrogating data streams. Determine and construct data schema to support analytical and modeling need. Use tools, processes, guidelines to ensure data is correct, standardized and documented.

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\setcounter{enumi}{2}
\tightlist
\item
  Production
\end{enumerate}

If you want to integrate the model or analysis into the production system, then you have to automate all data handling steps. It involves the whole pipeline from data access to preprocessing, modeling and final deployment. It is necessary to make the system work smoothly with all existing stacks. So it requires to monitor the system through some sort of robust measures, such as rigorous error handling, fault tolerance, and graceful degradation to make sure the system is running smoothly and the users are happy.

\begin{itemize}
\tightlist
\item
  \textbf{Analysis -- the process of turning raw information into insights in a fast way}
\end{itemize}

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\tightlist
\item
  Domain knowledge
\end{enumerate}

Domain knowledge is the understanding of the organization or industry where you apply data science. You can't make sense of data without context, such as what are the important metric for this kind of business, what are the business questions, what type of data they have and what the data represents, how to translate a business need to a data problem, what has been tried before and with what results, what are the accuracy-cost-time trade-offs, how can things fail, what other factors are not accounted, what are the reasonable assumptions and what are faulty. In the end, domain knowledge helps you to deliver the results in an audience-friendly way.

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  Exploratory analysis
\end{enumerate}

This type of analysis is about exploration and discovery. Rigor conclusion is not a concern which means the goal is to get insights driven by correlation not causation. The later one requires statistical skills and hence more expensive. Instead this role will help your team look at as much data as possible so that the decision-makers can get a sense of what's worth further pursuing. It often involves different ways to slice and aggregate data. An important thing to note here is that you should be careful not to get conclusion beyond the data. You don't need to write gorgeous, robust code to perform well in this role.

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\setcounter{enumi}{2}
\tightlist
\item
  Story telling
\end{enumerate}

Storytelling with data is key to deliver the insights and drive better decision making. It is the art of telling people what the numbers actually signify. It usually requires data summarization, aggregation and visualization. It is important to answer the following questions before you begin down the path of creating a data story:
* Who are your audience?
* What do you want your audience to know or do?
* How can you use data to help make your point?

\begin{itemize}
\tightlist
\item
  \textbf{Modeling -- the process of diving deeper into the data to discover the pattern we don't easily see}
\end{itemize}

Even fancy machine learning model is the first thing comes to mind when people think about data science, unfortunately, in industry, it occupies the smallest part of data scientist's time. Nevertheless, it is a powerful set of tools.

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\tightlist
\item
  Supervised learning
\end{enumerate}

In supervised learning, each observation of the predictor measurement(s) corresponds to a response measurement. There are two flavors of supervised learning: regression and classification. In regression, the response is a real number such as the total net sales in 2017, or the yield of corn next year. The goal is to approximate the response measurement as much as possible. In classification, the response is a class label, such as dichotomous response such as yes/no. The response can also have more than two categories, such as four segments of customers. A supervised learning model is a function that maps some input variables with corresponding parameters to a response y. Modeling tuning is to adjust the value of parameters to make the mapping fit the given response. In other words, it is to minimize the discrepancy between given responses and the model output. When the response y is a real value, it is intuitive to define discrepancy as the squared difference between model output and given the response. When y is categorical, there are other ways to measure the difference, such as AUC or information gain.

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  Unsupervised learning
\end{enumerate}

In unsupervised learning, there is no response variable. For a long time, the machine learning community overlooked unsupervised learning except for one called clustering. Moreover, many researchers thought that clustering was the only form of unsupervised learning. One reason is that it is hard to define the goal of unsupervised learning explicitly. Unsupervised learning can be used to do the following:
* Identify a good internal representation or pattern of the input that is useful for subsequent supervised or reinforcement learning, such as finding clusters.
* It is a dimension reduction tool that is to provide compact, low dimensional representations of the input, such as factor analysis.
* Provide a reduced number of uncorrelated learned features from original variables, such as principal component regression.

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\setcounter{enumi}{2}
\tightlist
\item
  Customized model development
\end{enumerate}

In most of the cases, you just need to use the out of the box algorithms to solve the problem. But in some situations, there isn't enough data to use machine learning model, or the question doesn't fit neatly in the specifications of existing tools, or the model needs to incorporate some prior domain knowledge . You may need to develop new models to accommodate the subtleties of the problem at hand. For example, people use bayesian models to include domain knowledge as prior distribution.

\textbf{What others?}

There are some common skills to have regardless the role people have in data science.

\begin{itemize}
\tightlist
\item
  \textbf{Data Preprocessing: the process nobody wants to go through yet nobody can avoid}
\end{itemize}

No matter what role you hold in data science team, you will have to do some data cleaning which tend not to be the favorite part of anyone's job. Data preprocessing is the process of converting raw data into clean data that is proper for use.

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\tightlist
\item
  Data preprocessing for data engineer
\end{enumerate}

Getting data together from different sources and dumping them to a Data Lake, a dumping ground of amorphous data, is far from the data schema analyst and scientist would use. A data lake is a storage repository that stores a vast amount of raw data in its native format, including XML, JSON, CSV, Parquet, etc. It is a data cesspool rather than data lake. It is data engineer's job to get a clean schema out of the data lake by transforming and formatting the data. Some common problems to resolve are:

\begin{itemize}
\tightlist
\item
  Enforce new tables' schema to be the desired one
\item
  Repair broken records in newly inserted data
\item
  Aggregate the data to form the tables with a proper granularity
\end{itemize}

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  Data preprocessing for data analyst and scientist
\end{enumerate}

Not just for data engineer, it also occupies a large fraction of data analyst and scientist's working hours too. A facility and a willingness to do these tasks are a prerequisite for a strong data scientist. If you are lucky as a data scientist, you may end up spending 50\% of your time doing this. If you are like most of us, you will spend over 80\% of your working hours wrangling data.

The data you get can still be very rough even it is from a nice and clean database that engineers set up. Dates and times are notorious for having many representations and time zone ambiguity. You may also get market survey responds from your clients in an excel file where the table title could be multi-line, or the format does not meet the requirements, such as using 50\% to represent the percentage rather than 0.5. So in many cases, you need to set the data to be the right format before moving on to analysis.

Even the data is in the right format, there are other issues to solve before or during analysis. For example, variables can have missing values. Knowledge about the data collection process and what it will be used for is necessary to decide a way to handle the missing. Also, different models have different requirements on the data. For example, some model may require the variables are of consistent scale; some may be susceptible to outliers or collinearity, some may not be able to handle categorical variables and so on. The modeler has to preprocess the data to make it proper for the specific model.

Most of the people in data science today focus on one of the tracks. A small number of people are experts of two tracks. People that are proficient in all three? They are unicorns!

\hypertarget{what-should-data-science-do}{%
\section{What should data science do?}\label{what-should-data-science-do}}

\hypertarget{lets-dream-big}{%
\subsection{Let's dream big}\label{lets-dream-big}}

Here is my two points for the question:

\begin{itemize}
\tightlist
\item
  Make human better human by alleviating bounded rationality and minimize politics/emotion (rather than make machine more like human)
\item
  Strive for the ``democratization'' of data as legally possible: empower everyone in the organization to acquire, process, and leverage data in a timely and efficient fashion
\end{itemize}

I know it is vague. Behold, I am going to explain more.

It's easy to pretend that you are data driven. But if you get into the mindset to collect and measure everything you can, and think about what the data you've collected means, you'll be ahead of most of the organizations that claim to be data driven. If you know the difference between ``data driven'' and ``data confirmed'', you'll be sailing at the right direction. What on earth is the difference?

Imagine that you are buying something online and you need to decide whether or not to trust the product without seeing it physically. You see the average rating is 4.1 out of 5.0. Is this a good score? It depends on your subconscious decision. If you really need the thing, you may happily cheer ``It is more than 4.0!''. If you are still not sure whether you need it, you can't help to check the few low rating reviews and tell yourself ``look at those 1-star reviews''. Sounds familiar? Psychologists call it confirmation bias.

\begin{quote}
Confirmation bias is the tendency to search for, interpret, favor, and recall information in a way that confirms one's preexisting beliefs or hypotheses {[}Wikipedia{]}
\end{quote}

So if you use data to feel better (confirm) decisions/assumptions that are already made before you analyze the data, that is ``data confirmed''. A clear sign of confirmation bias is when you go back to tinker the definition of your metic because the current result is not impressive. However, this bias is not always easy to see. It is not only misleading but also expensive. Because it could take data science team days of toil to boil everything down to that magic number and put the result on the report. Data scientists are not totally immune to the bias either. Good news is that there is antidote to confirmation bias.

Antidote 1: Do the brainstorming of data definition and set the goal in advance and resist temptation to move them later. In other words, the decision makers have to set decision criteria and the boundary up front in your data science project.

Antidote 2: Data democratization. Keep in mind that data isn't just for the professionals or a small group of people in the organization that are ``key decision makers''. Everyone should be able to get access to and look at the data (as much as legally possible). In that way, there will be more eyes on the decision.

The way data science can help is to provide a sound data framework and necessary training for the organization to access data with least amount of pain. Also be clear about the data definition and documentation. Data science holds the responsibility for data stewardship in the organization with high integrity. (there is data science for social good which is data science's responsibility for outside the organization but we are not going to discuss that here)

That is still very abstract, I hear you. Now, Let's be more specific\ldots{}

\hypertarget{what-kind-of-questions-can-data-science-solve}{%
\subsection{What kind of questions can data science solve?}\label{what-kind-of-questions-can-data-science-solve}}

\hypertarget{prerequisites}{%
\subsubsection{Prerequisites}\label{prerequisites}}

Data science is not a panacea and there are problems data science can't help. It is best to make a judgment as early in the analytical cycle as possible. Tell your clients honestly and clearly when you think data analytics can't give the answer they want. What kind of questions can data science solve?

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Your question needs to be specific enough
\end{enumerate}

Look at two examples:

\begin{itemize}
\tightlist
\item
  Question 1: How can I increase product sales?
\item
  Question 2: Is the new promotional tool introduced at the beginning of this year boosting the annual sales of P1197 in Iowa and Wisconsin? (P1197 is an impressive corn seed product from DuPont Pioneer)
\end{itemize}

It is easy to see the difference between the two questions. Question 1 is a grammatically correct question, but it is proper for data analysis to answer. Why? It is too general. What is the response variable here? Product sales? Which product? Is it annual sales or monthly sales? What are the candidate predictors? You nearly can't get any useful information from the questions. In contrast, question 2 is much more specific. From the analysis point of view, the response variable is clearly ``annual sales of P1197 in Iowa and Wisconsin''. Even we don't know all the predictors, but the variable of interest is ``the new promotional tool introduced early this year.'' We want to study the impact of the promotion of sales. You can start from there and move on to figure out other variables need to include in the model by further communication.

As a data scientist, you may start with something general and unspecific like question 1 and eventually get to question 2. Effective communication and in-depth domain knowledge about the business problem are essential to convert a general business question into a solvable analytical problem. Domain knowledge helps data scientist communicate with the language the other people can understand and obtain the required information.

However, defining the question and variables involved won't guarantee that you can answer it. For example, I encountered this situation with a well-defined supply chain problem. My client asked me to estimate the stock needed for a product in a particular area. Why can't this question be answered? I tried fitting a Multivariate Adaptive Regression Spline (MARS) model and thought I found a reasonable solution. But it turned out later that the data my client gave me was inaccurate. In this case, only estimates rather than actual values of past supply figures were available and there was no way to get accurate data. The lesson lends itself to the next point.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  You need to have sound and relevant data
\end{enumerate}

One cannot make a silk purse out of a sow's ear. Data scientists need data, sound and relevant data. The supply problem is a case in point. There was relevant data, but not sound. All the later analytics based on that data was a building on sand. Of course, data nearly almost have noise, but it has to be in a certain range. Generally speaking, the accuracy requirement for the independent variables of interest and response variable is higher than others. In question 2, it is data related to the ``new promotion'' and ``sales of P1197''.

The data has to be helpful for the question. If you want to predict which product consumers are most likely to buy in the next three months, you need to have historical purchasing data: the last buying time, the amount of invoice, coupons and so on. Information about customers' credit card number, ID number, the email address is not going to help.

Often the quality of the data is more important than the quantity, but the quantity cannot be overlooked. In the premise of guaranteeing quality, usually the more data, the better. If you have a specific and reasonable question, also sound and relevant data, then congratulations, you can start playing data science!

\hypertarget{problem-type}{%
\subsubsection{Problem type}\label{problem-type}}

Many of the data science books classify the various models from a technical point of view. Such as supervised vs.~unsupervised models, linear vs.~nonlinear models, parametric models vs.~non-parametric models, and so on. Here we will continue on ``problem-oriented'' track. We first introduce different groups of real problems and then present which models can be used to answer the corresponding category of questions.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Description
\end{enumerate}

The basic analytic problem is to summarize and explore a data set with descriptive statistics (mean, standard deviation, and so forth) and visualization methods. It is the simplest problem and yet the most crucial and common one. You will need to describe and explore the dataset before moving on to more complex analysis. In the problem such as customer segmentation, after you cluster the sample, the next step is to figure out the profile of each class by comparing the descriptive statistics of the various variables. Questions of this kind are:

\begin{itemize}
\tightlist
\item
  How does the annual income distribute?
\item
  Are there outliers?
\item
  What are the mean active days of different accounts?
\end{itemize}

Data description is often used to check data, find the appropriate data preprocessing method, and demonstrate the model results.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Comparison
\end{enumerate}

The first common problem is to compare different groups. Such as: Is A better in some way than B? Or more comparisons: Is there any difference among A, B, and C in a particular aspect? Here are some examples:

\begin{itemize}
\tightlist
\item
  Are males more inclined to buy our products than females?
\item
  Are there any differences in customer satisfaction in different business districts?
\item
  Do soybean carrying a particular gene have higher oil content?
\end{itemize}

For those problems, it is usually to start exploring from the summary statistics and visualization by groups. After a preliminary visualization, you can test the differences between treatment and control group statistically. The commonly used statistical tests are chi-square test, t-test, and ANOVA. There are also methods using Bayesian methods. In biology industry, such as new drug development, crop breeding, mixed effect models are the dominant technique.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Clustering
\end{enumerate}

Clustering is a widespread problem, which is usually related to classification. Clustering answers questions like:

\begin{itemize}
\tightlist
\item
  Which customers have similar product preference?
\item
  Which printer performs a similar pattern to the broken ones?
\item
  How many different themes are there in the corpus?
\end{itemize}

Note that clustering is unsupervised learning. The most common clustering algorithms include K-Means and Hierarchical Clustering.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Classification
\end{enumerate}

Usually, a labeled sample set is used as a training set to train the classifier. Then the classifier is used to predict the category of a future sample. Here are some example questions:

\begin{itemize}
\tightlist
\item
  Who is more likely to buy our product?
\item
  Is the borrower going to pay back?
\item
  Is it spam?
\end{itemize}

There are hundreds of classifiers. In practice, we do not need to try all the models but several models that perform well generally.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  Regression
\end{enumerate}

In general, regression deals with the problem of ``how much is it?'' and return a numerical answer. In some cases, it is necessary to coerce the model results to be 0, or round the result to the nearest integer. It is the most common problem.

\begin{itemize}
\tightlist
\item
  What will be the temperature tomorrow?
\item
  What is the projected net income for the next season?
\item
  How much inventory should we have?
\end{itemize}

\hypertarget{structure-data-science-team}{%
\section{Structure data science team}\label{structure-data-science-team}}

During the past decade, a huge amount of data has become available and readily accessible for analysis in many companies across different business sectors. The size, complexity, and speed of increment of data suddenly beyond the traditional scope of statistical analysis or BI reporting. To leverage the big data, do you need an internal data science team to be a core competency, or can you outsource it? The answer depends on the problems you want to solve using data. If the problems are critical to the business, you can't afford to outsource it. Also, each company has its own business context and hence needs new kinds of data or or use the results in novel ways. Being a data driven organization requires cross organization commitments to identify what data each department needs to collect, establish the infrastructure and process for collecting and maintaining that data, and the way to deliver analytical results. Unfortunately, it is unlikely that an off-the-shelf solution will be flexible enough to adapt to the specific business context. So most of the companies establish their own data science team.

Where should data science team fit? Much has been written about different ways data science function fit in the organization. In general, data science team is organized in three ways.

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\tightlist
\item
  A standalone team
\end{enumerate}

Data science is an autonomous unit that is parallel to the other organizations (such as engineering, product etc.) and the head of data science reports directly to senior leadership, ideally to the CEO or at least to someone who understands data strategy and is willing to invest to give it what it needs. The advantages of this type of data organization are:

\begin{itemize}
\tightlist
\item
  Data science team has autonomy and is well positioned to tackle whatever problems it deems important to the whole company.
\item
  It is advantageous for people in data science team to share knowledge and grow professionally.
\item
  It provides a clear career path for data science professionals and shows the company treats data as a first-class asset. So it tends to attract and retain top talent people.
\end{itemize}

The biggest concern of this type of organization is the risk of marginalization. Data science only has value if data drives action which requires collaboration among data scientists, engineers, product managers and other business stakeholders across the organization. If you have a standalone data science team, it is critical to choose a data science leader who is knowledgable about the applications of data science in different areas and also has strong inter-discipline communication skills. The head of data science needs to build strong collaboration with other departments.

Also, as companies grow, each department prefers to be self-sufficient and tries to hire data own analytical personal under different titles even when they can get support from the data science team. This is why it is unlikely for an already mature company to have a standalone data science team. If you start your data science team in the early stage as a startup, it is important that the CEO sets a clear vision from the beginning and sends out strong message to the whole company about accessing data support.

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  An embedded model
\end{enumerate}

There is still a head of data science but his/her role is mostly a hiring manager and coach and he/she may report to a senior manager in IT department. The data science team brings in talented people and farms them out to the rest of the company. In other words, it gives up autonomy to ensure utility. The advantages are:

\begin{itemize}
\tightlist
\item
  Data science is closer to its applications.
\item
  There is still a data science group so it is easy to share knowledge.
\item
  It has high flexibility to allocate data science resource to the rest of the company.
\end{itemize}

However, there are also concerns.

\begin{itemize}
\tightlist
\item
  It brings difficulty to the management since the lead of the designated team is not responsible for data science professionals' growth and happiness while the data science managers are not directly vested in their work.
\item
  Data scientists are second-class citizens everywhere and it is hard to attract and retain top talent.
\end{itemize}

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\setcounter{enumi}{2}
\tightlist
\item
  Integrated team
\end{enumerate}

There is no data science team. Each team hires its own data science people. For example, there may be a marketing analytics group consisting of data engineer, data analyst and data scientists. The team leader is a marketing manager who has an analytical mind and deep business knowledge. The advantages are obvious.

\begin{itemize}
\tightlist
\item
  Data science resource aligns with the organization very well
\item
  Data science professionals are first-class members and valued in their own team. The manager is responsible for data science professionals' growth and happiness.
\item
  The insights from data are easily put into actions.
\end{itemize}

It works well in the short term for both the company and the data science hires. However, there are also many concerns.

\begin{itemize}
\tightlist
\item
  It sacrifices the professional growth of data science hires since they work in silos and specialize in specific application. It is also difficult to share knowledge across different applied areas.
\item
  It is harder to move people around since they are highly associated with a specific function in the organization.
\item
  There is no career path for data science people and so it is difficult to retain talent.
\end{itemize}

There is not an universal answer for the best way to organize data science team. It depends on the answer of many other questions. How important do you think the data science team is for your company? What is the stage of your company when you start to build data science team? Are you a startup or a relatively mature company? Data science has its own skillset, workflow, tooling, integration process and culture. If it is critical to your organization, it is the best not to bury it under any part of the organization. Otherwise data science will inevitably only serve the need for specific branch of the organization and it also impedes data democratization across the organization. How valuable it is to use data to tell the truth, how dangerous it is to use data to affirm existing opinions. No matter which way you choose, be aware of both sides of the coin. If you are looking for a data science position, it is important to know where the data science team fits in the company.

\hypertarget{soft-skills-for-data-scientists}{%
\chapter{Soft Skills for Data Scientists}\label{soft-skills-for-data-scientists}}

\hypertarget{comparison-between-statistician-and-data-scientist}{%
\section{Comparison between Statistician and Data Scientist}\label{comparison-between-statistician-and-data-scientist}}

Statistics as a scientific area can be traced back to 1749 and statistician as a career has been around for hundreds of years with well-established theory and application. Data Scientist becomes an attractive career for only a few years along with the fact that data size and variety beyond the traditional statistician's toolbox and the fast-growing of computation power. Statistician and data scientist have a lot of in common, but there are also significant differences.

\includegraphics{images/softskill1.png}

Both statistician and data scientist work closely with data. For the traditional statistician, the data is usually well-formatted text files with numbers and labels. The size of the data usually can be fitted in a PC's memory. Comparing to statisticians, data scientists need to deal with more varieties of data:

\begin{itemize}
\tightlist
\item
  well-formatted data stored in a database system with size much larger than a PC's memory or hard-disk;
\item
  huge amount of verbatim text, voice, image, and video;
\item
  real-time streaming data and other types of records.
\end{itemize}

One unique power of statistics is to make statistical inference based on a small set of data. Statisticians spend most of their time developing models and don't need to put too much effort on data cleaning. Today, data is relatively abundant, and modeling is only part of the overall effort, often a small part. Due to the active development of some open source communities, fitting models is not too far from button pushing. Data scientists instead spend lot of time preprocessing and wrangling the data before feeding them to the model.

Different from statisticians, data scientists often focus on delivering actionable results and sometimes need to fit model on the cloud. The data can be too large to read in laptop. From the entire problem-solving cycle, statisticians are usually not well integrated with the production system where data is obtained in real time; while data scientists are more embedded in the production system and closer to the data generation procedures.

\hypertarget{beyond-data-and-analytics}{%
\section{Beyond Data and Analytics}\label{beyond-data-and-analytics}}

Data scientists usually have a good sense of data and analytics, but data scientist project is more than that. A data science project may involve people with different roles, especially in a big company:

\begin{itemize}
\tightlist
\item
  a business owner or leader to identify business value;
\item
  program manager to ensure the data science project fits into the overall technical program development and coordinate all parties to set periodical tasks so that the project meets the preset milestones and results;
\item
  data owner and computation resource and infrastructure owner from the IT department;
\item
  dedicated team to make sure the data and model are under model governance and privacy guidelines;
\item
  a team to implement, maintain and refresh the model;
\item
  multiple rounds of discussion of resource allocation among groups (i.e., who pay for the data science project).
\end{itemize}

Effective communication and in-depth domain knowledge about the business problem are essential requirements for a successful data scientist. A data scientist may interact with people at various levels from senior leaders who set the corporate strategies to front-line employees who do the daily work. A data scientist needs to have the capability to view the problem from 10,000 feet above the ground, as well as down to the detail to the very bottom. To convert a business question into a data problem, a data scientist needs to communicate using the language the other people can understand and obtain the required information.

In the entire process of data science project defining, planning, executing and implementing, every step involves the data scientist to ensure people correctly define the business problem and reasonably evaluate the business value and success. Corporates are investing heavily in data science and machine learning with a very high expectation of return.

However, it is easy to set unrealistic goal and wrongly estimate the business impact. The data scientist lead should navigate the discussions to make sure the goal can be backed by data and analytics. Many data science projects over promise and are too optimistic on the timeline. These projects eventually fail by not delivering the preset business impact within the timeline. As data scientists, we need to identify these issues early in the stage and communicate with the entire team to make sure the project has a realistic deliverable and timeline. The data scientist team also need to work closely with data owners to identify relevant internal and external data source and evaluate the quality of the data; as well as working closely with the infrastructure team to understand the computation resources (i.e.~hardware and software) available for the data science project.

\hypertarget{three-pillars-of-knowledge}{%
\section{Three Pillars of Knowledge}\label{three-pillars-of-knowledge}}

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\tightlist
\item
  Analytics knowledge and tool sets
\end{enumerate}

A successful data scientist needs to have a strong technical background in data mining, statistics and machine learning. The in-depth understanding of modeling with the insight about data enable a data scientist to convert a business problem to a data science problem.

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  Domain knowledge and collaboration
\end{enumerate}

A successful data scientist needs some domain knowledge to understand the business problem. For any data science project, the data scientist need to collaborate with other team members and effective communication and leadership skills are critical, especially when you are the only data person in the room and you need to decide with uncertainty.

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\setcounter{enumi}{2}
\tightlist
\item
  (Big) data management and (new) IT skills
\end{enumerate}

The last pillar is about computation environment and model implementation in a big data platform. This used to be the most difficult one for a data scientist with statistics background (i.e.~lack computer science or programming skills). The good news is that with the rise of cloud computation big data platform, it is easier for a statistician to overcome this barrier.

\includegraphics{images/softskill2.png}

\hypertarget{data-science-project-cycle}{%
\section{Data Science Project Cycle}\label{data-science-project-cycle}}

A data science project has various stages. Many textbooks and blogs focus on one or two specific stages and it is rare to see the end-to-end life cycle of data science projects. In fact, to get a good grasp of the end-to-end cycle requires many years of experience of doing real-world data science. We will share our opinions on that in this section. Seeing a holistic picture of the whole cycle helps you to better prepare for real-world applications.

\hypertarget{types-of-data-science-projects}{%
\subsection{Types of Data Science Projects}\label{types-of-data-science-projects}}

People often use data science project to describe any project that uses data to solve a business problem, including traditional business analytics or data visualization. Here we limit our discussion of data science projects that involve data and some statistical or machine learning models. The business problem itself gives us the flavor of the project, data is the raw ingredient to start with, and the model makes the dish. Different types of data science projects can be determined by the types of data used and the final model development and implementation.

\hypertarget{offline-and-online-data}{%
\subsubsection{Offline and online Data}\label{offline-and-online-data}}

There are offline and online data. Offline data are historical archived data stored in databases or data warehouses. With the development of data storage, the cost to store a large amount of data is cheap and offline data are very rich in general (for example website may track and store each individual user's mouse position, click and typing information while the user is visiting the website). Offline data is usually stored in a distributed system and it can be extracted in batch as raw materials to create features that can be used in model training. Online data are real-time information that can be feed to models to make automatic actions. Real-time information can changes frequently such as the keywords a customer is searching for. Capturing and using real-time online data requires the integration of machine learning to the production infrastructure. It used to be a steep learning curve for data scientists, but the cloud infrastructure makes it much easier.

\hypertarget{offline-training-and-offline-application}{%
\subsubsection{Offline training and offline application}\label{offline-training-and-offline-application}}

This type of data science project is for a specific business problem which needs to be solved once or multiple times. But the dynamic nature of the business problem requires substantial work every time. One example of such a project is ``whether a new workflow is going to improve efficiency.'' In this situation, we often use offline internal and external data, build models, and deliver the final results as a report to answer the specific business question. It is similar to the traditional business intelligence project but with more focus on data and model. Sometimes the data size and model complexity are beyond the capacity of a single computer. So you need to use distributed storage and computation. Since the model is based on the historical data and the output is a report, there is no need for real-time execution. Usually, there is no run-time constraint on the machine learning model unless the model is running beyond a reasonable time frame such as a few hours or a few days. We can call this type of data science project ``offline~training, offline application'' project.

\hypertarget{offline-training-and-online-application}{%
\subsubsection{Offline training and online application}\label{offline-training-and-online-application}}

Another type of data science project is to use offline data for training and apply the trained model to real-time online data in the production environment. One example of such a project is ``using historical data to train a personalized advertisement model, and then provides real-time ad recommendation when customers visit the website.'' The model is trained based on offline data, and then use a customer's online real-time data as features to run the model in real time to provide an automatic action. The model training is very similar to the ``offline~training, offline application'' project, but as the trained model will be put to production, there are specific requirements such as features used in the offline training have to be available online in real time, and the online run-time of the model has to be short enough without impacting user experience. In most cases, data science projects in this category create continuous and scalable business value. We will use this type of data science project to describe the project cycle.

\hypertarget{online-training-and-online-application}{%
\subsubsection{Online training and online application}\label{online-training-and-online-application}}

For some business problems, it is so dynamic that even yesterday's data is out of date. For such cases, we can use online data to train the model and then applying it in real time. We call this type of data science project ``online~training, online application.'' This type of data science project requires high automation and low latency.

\hypertarget{at-the-planning-stage}{%
\subsection{At the Planning Stage}\label{at-the-planning-stage}}

To ensure a successful data science project, a data-driven and fact-based planning stage is essential. With the recent big data and data science hype, there is a high demand for data science projects to create business value across different business sectors. Often times, these data science project proposals are initiated by the leaders of an organization. This top-down style data science projects usually have high visibility with certain human and computation resources pre-allocated. However, it is crucial to understand the business problem first and align the goal across different teams including

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\item
  the business team which may include members from the business operation team, business analyst, insight and reporting team;
\item
  technology team which may include members from database and data warehouse team, data engineering team, infrastructure team, core machine learning team, and software development team; (3) project management team which may include program management team and product management team depending on the scope of the data science project.
\end{enumerate}

To start the conversation, we can ask the following questions to everyone in the team:

\begin{itemize}
\tightlist
\item
  What are the pain points in current business operation?
\item
  What data are available and how is the quality and quantity of the data?
\item
  What might be the most significant impacts of a data science project?
\item
  Are there any negative impact to other teams?
\item
  What computation resources are available for model training and model execution?
\item
  Can we define key metrics to compare and quantity business value?
\item
  Are there any data security, privacy and legal concerns?
\item
  What are the desired milestones, check points and timeline?
\item
  Is the final application online or offline?
\item
  Are the data online or offline?
\end{itemize}

It is likely to have a series of intense meetings and heated discussions to frame the project to a reasonable scope. After the planning stage, we should be able to define a set of key metrics related to the project, identify some offline and online data sources, request needed computation resources, draft tentative timeline and milestones, and form a team of data scientist, data engineer, software developer, project manager and members from business operation. Data scientists should play a major role in these discussions. If data scientist is not leading the data science project formulation, it is very likely the entire project will not reach the timeline and milestones.

\hypertarget{at-the-modeling-stage}{%
\subsection{At the Modeling Stage}\label{at-the-modeling-stage}}

Even though at the planning stage we already set some strategy, milestone, and timeline, data science projects are dynamic in nature and there could be uncertainties along the road. As a data scientist, clearly communicate any newly encountered difficulties during the modeling stage to the entire team is essential to keep the data science project progress. With the available data source identified at the planning stage, data clearing, data wrangling, and exploratory data analysis are great starting points toward modeling. Meanwhile, abstracting the business problem to be a set of statistical and machine learning problems is an iterative process. It is rare that business problems can be solved by using just one statistical or machine learning model. The ability to use a sequence of methods to decompose the business problem is one of the key responsibility for a senior data scientist. The process requires iterative rounds of discussions with the business team and data engineering team based on the new learning from each iteration. Each iteration includes both data related and model related part.

\hypertarget{data-related}{%
\subsubsection{Data related}\label{data-related}}

Data cleaning, data preprocessing and feature engineering are closely related procedures. The goal of these data-related procedures is to create usable variables or features for statistical and machine learning models. One important aspect of data related procedures is to make sure the data source we are using is a good representation of the situation where the final trained model will be applied. The exact same representation is rarely possible, and reasonable approximation is totally fine to start with. A data scientist has to be clear on the assumptions and communicate with the entire team the limitations of biased data and quantify its impact on the application. In data related part, sometimes the available data is not that relevant to the business problem we want to solve, and we have to collect more data.

\hypertarget{model-related}{%
\subsubsection{Model related}\label{model-related}}

There are different types of statistical and machine learning models, such as supervised learning, unsupervised learning, and causal inference. For each type, there are various algorithms, libraries, or packages readily available. To solve a business problem, you sometimes need to piece together a few methods at the model exploring and developing stage. This stage also includes model training, validation, and testing to make sure the model works well in the production environment; i.e., it is not overfitting and can be generalized.~The model selection follows Occam's razor, which is to choose the simplest among a set of compatible models. Before you try complicated models, it is a good practice to get some benchmark by additional business rules, common sense decision, or standard models (such as random forest for classification).

\hypertarget{at-the-production-stage}{%
\subsection{At the Production Stage}\label{at-the-production-stage}}

For offline application data science projects, the end product is often a detailed report with model result and output. However, for online application projects, a trained model is just halfway from the finish line. The offline data is stored and processed in a totally different environment from the online production environment. Building the online data pipeline and implementing machine learning models in a production environment requires lots of additional work. Even though recent advance in cloud infrastructure lowers the barrier dramatically, it still takes effort to implement an offline model in the online production system. Before you promote the model to production, there are two more steps to go:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  shadow mode
\item
  A/B testing
\end{enumerate}

A \textbf{shadow mode} is like an observation period when the data pipeline and machine learning models run as it is fully functional, but we only record the model output without any actions. Some people call it proof of concept (POC). During POC, people frequently check the data pipeline and model and detect bugs such as a timeout or missing features, version conflict (for example python 2 v.s. python 3), data type mismatch, etc.

Once the online model passes the shadow mode, \textbf{A/B testing} is the next stage. During A/B testing, all the incoming observations are randomly separated into two groups: control and treatment. The control group is going to skip the machine learning model, while the treatment group is going through the machine learning model. After that, people monitor a list of pre-defined key metrics during a specific time period to compare the control and treatment groups. The differences in these key metrics determine whether the machine learning model provides business value or not. Real applications can be complicated. For example, there can be multiple treatment groups, or hundreds, even thousands of A/B testing running by different teams at any given time.

Once the A/B testing shows that the model provides significant business value, then you can put it into full production. It is ideal that the model runs as expected and continues to provide scalable values. However, the business can change and a machine learning model works now can break tomorrow, and features available now may not be available tomorrow. You need a monitoring system to automatically notify us when one or multiple features change. When the model performance degrades below a pre-defined a level, you need to fine-tune the parameters and thresholds, re-train the model with more recent data, add or remove features to improve model performance. Eventually, any model will fail or retire at some time.

\hypertarget{summary}{%
\subsection{Summary}\label{summary}}

Data science end-to-end project cycle is a complicated process which requires close collaboration among many teams. Data scientist, maybe the only scientist in the team, has to lead the planning discussion and model development based on data available and clearly communicate key assumptions and uncertainties with the entire team. A data science project may fail at any stage, and a clear end-to-end cycle view of the project helps avoid some mistakes.

\hypertarget{common-mistakes-in-data-science}{%
\section{Common Mistakes in Data Science}\label{common-mistakes-in-data-science}}

Data science project can go wrong at different stages in many ways. Most textbooks and online blogs~focus on technical mistakes about machine learning model, algorithm or theory, such as including outliers and overfitting. It is important to avoid these technical mistakes. However, there are common systematic mistakes across data science projects that are rarely discussed in textbooks. To summarize these mistakes, people need to do real-world data science projects. In this section, we describe these common mistakes in detail so that readers can proactively identify and avoid these systematic mistakes in their own data science projects.

\hypertarget{problem-formulation-stage}{%
\subsection{Problem Formulation Stage}\label{problem-formulation-stage}}

The most challenging part of a data science project is problem formulation. Data science project stems from pain points of the business. The draft version of the goal of the project is relatively vague without much quantification or is the gut feeling of the leadership team. Often there are multiple teams involved in the initial project formulation stage and they have different views. It is easy to have malalignment across different teams such as resource allocation, milestone deliverable, and timeline. At the problem formulation stage, data science team members with technical background sometimes are not even invited to the initial discussion. It sounds ridiculous, but sadly true that a lot of resources are spent on \textbf{solving the wrong problem,} the number one systematic common mistake in data science. Formulating a business problem into the right data science project requires an in-depth understanding of the business context, data availability and quality, computation infrastructure, and methodology to leverage the data to quantify business value.

We have seen people over promise about business value all the time, another common mistake that is going to fail the project at the beginning. With the hype of big data and machine learning, leaders across industries often have unrealistic high expectation on data science. It is especially true during enterprise transformation when there is a strong push to adopt new technology to get value out of the data. The unrealistic expectations are based on assumptions that are way off chart without checking~the data availability, data quality, computation resource, and current best practices in the field.~Even there is some exploratory analysis by the data science team at the problem formulation stage,~project leaders sometimes ignore their data-driven voice.

These two systematic mistakes undermine the organization's data science strategy.~The higher the expectation, the bigger the disappointment when the project cannot deliver business value. Data and business context are essential to formulate the business problem and set reachable business value. It helps to avoid the mistakes by having a strong data science leader with a broad technical background and let data scientist coordinate and drive the problem formulation and set realistic goals based data and business context.

\hypertarget{problem-planning-stage}{%
\subsection{Problem Planning Stage}\label{problem-planning-stage}}

Now suppose the data science project is formulated correctly with a reasonable expectation on the business value. The next step is to plan the project such as allocating resources, setting up milestones and timeline, and defining deliverable. In most cases, there are project managers to coordinate different teams that are involved in the project and use agile project management tools similar to those in software development. Unfortunately,~the project management team may not have experience with data science projects and hence fail to account for the uncertainties at the planning stage. The fundamental difference between data science projects and other projects lead to another common mistake: \textbf{too optimistic about the timeline}. For example, data exploratory and preparation may take 60\% to 80\% of the total time for a given data science project, but people often don't realize that.

When there are lots of data already collected across the organization, people assume you have enough data for everything. It leads to the mistake: t\textbf{oo optimistic about data availability and quality}. What you need is not ``big data'', but data that can help you solve the problem. The data available may be low quality and you need to put substantial efforts to clean the data before you can use it. There are ``unexpected'' efforts to bring the right and relevant data for a specific data science project. To ensure smooth delivery of data science projects, you need to account for the ``unexpected'' work at the planning stage. We all know data pre-processing and feature engineering is usually the most time-consuming part of a data science project. However, people outside data science are not aware of it and we need to educate other team members and the leadership team.

\hypertarget{modeling-stage}{%
\subsection{Modeling Stage}\label{modeling-stage}}

Finally, you start to look at the data and fit some models. One common mistake at this stage is unrepresentative data. The model trained using historical data may not generalize to the future. There is always a problem of biased or unrepresentative data. As a data scientist, we need to use data that are closer to the situation where the model is going to apply and quantify the impact of model output in production. Another mistake at this stage is overfitting and obsession for complicated models. Now we can easily get hundreds or even thousands of features and the machine learning models are getting more complicated. People can use open source libraries to try all kinds of models. People are sometimes obsessed with complicated models instead of using the simplest among a set of compatible models.

The data used to build the models is always biased or unrepresentative to some extent, simpler models are better to generalize and it has a higher chance to provide consistent business value once the model passes the test and is finally implemented in the production environment. It is possible that you can not use the existing data and methods to solve the business problem. You can try to collect more data, do feature engineering, or create your own models. However, if there is a fundamental gap between data and the business problem, the data scientist has to make the tough decision to unplug the project. On the other hand, data science projects usually have high visibility and may be initiated by senior leadership. Even the data science team provide enough evidence that they can't deliver the expected business value,~people may not want to stop the project which leads another common mistake at modeling stage: \textbf{take too long to fail}. The earlier we can stop a failing project, the better. Because we can put valuable resources to other promising projects. It damages the data science strategy and everyone will be hurt by a long data science project that is doomed to fail.

\hypertarget{production-stage}{%
\subsection{Production Stage}\label{production-stage}}

Now suppose you have found a model that works great for the training and testing data. If it is an online application, you are halfway. The next is to put the model in production, which sounds like alien work for a data scientist. It is true that the data engineering team can help with model production. However, as a data scientist, you need to know the potential mistakes at this stage. One big mistake is \textbf{missing A/B testing} and assuming that the model performance at model training/testing stays the same in the production environment. Unfortunately, the model trained and evaluated using historical data nearly never performs the same in the production environment. The data used in the offline training maybe significant different from online data and the business context may have changed. If possible, machine learning models in production should always go through A/B testing to evaluate performance.

In the model training stage, people usually focus on model performance, such as accuracy without paying too much attention to the model execution time. When a model runs online in real time, the total run time for each instance (i.e., model latency) should not impact the customer's user experience. Nobody wants to wait for even one second to see the results after click the ``Search'' button. In the production stage, feature availability is crucial to run a real-time model.~Engineering resources are essential for model production. However, in traditional companies, it is common that a data science project \textbf{fail to scale in real time applications} due to lack of computation capacity, engineering resources, or non-tech culture and environment.

As the business problem evolve rapidly, the data and model in the production environment need to change accordingly or the performance of the model deteriorates over time. The online production environment is more complicated than model training and testing, for example, you pull online features from different resources, and some features may be missing at a specific time; the model may run into time out zone, and there are tons of different software and data exceptions that may happen. We need regular checkup during the entire life of the model cycle from implementation to retirement. Unfortunately, people often don't set the monitoring system for data science projects, and it is another common mistake: \textbf{missing necessary online checkup}. It is essential to set a monitoring dashboard and automatic alarms, create model tuning, re-training, and retirement plans.

\hypertarget{summary-1}{%
\subsection{Summary}\label{summary-1}}

The data science project is a combination of art and engineering. A data science project may fail in different ways. However, if we put data and business context at the center of the project, get familiar with the data science project cycle and proactively identify and avoid these potential mistakes, the data science project can provide significant business value. Here is the summary of the mistakes:

\begin{itemize}
\tightlist
\item
  Solving the wrong problem
\item
  Over promise on business value
\item
  Too optimistic about the timeline
\item
  Too optimistic about data availability and quality
\item
  Unrepresentative data
\item
  Overfitting and obsession for complicated models
\item
  Take too long to fail
\item
  Missing A/B testing
\item
  Fail to scale in real-time applications
\item
  Missing necessary online checkup
\end{itemize}

\hypertarget{introduction-to-the-data}{%
\chapter{Introduction to The Data}\label{introduction-to-the-data}}

Before tackling analytics problem, we start by introducing data to be analyzed in later chapters.

\hypertarget{customer-data-for-a-clothing-company}{%
\section{Customer Data for A Clothing Company}\label{customer-data-for-a-clothing-company}}

Our first data set represents customers of a clothing company who sells products in physical stores and online. This data is typical of what one might get from a company's marketing data base (the data base will have more data than the one we show here). This data includes 1000 customers:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Demography

  \begin{itemize}
  \tightlist
  \item
    \texttt{age}: age of the respondent
  \item
    \texttt{gender}: male/female
  \item
    \texttt{house}: 0/1 variable indicating if the customer owns a house or not
  \end{itemize}
\item
  Sales in the past year

  \begin{itemize}
  \tightlist
  \item
    \texttt{store\_exp}: expense in store
  \item
    \texttt{online\_exp}: expense online
  \item
    \texttt{store\_trans}: times of store purchase
  \item
    \texttt{online\_trans}: times of online purchase
  \end{itemize}
\item
  Survey on product preference
\end{enumerate}

It is common for companies to survey their customers and draw insights to guide future marketing activities. The survey is as below:

How strongly do you agree or disagree with the following statements:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Strong disagree
\item
  Disagree
\item
  Neither agree nor disagree
\item
  Agree
\item
  Strongly agree
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Q1. I like to buy clothes from different brands
\item
  Q2. I buy almost all my clothes from some of my favorite brands
\item
  Q3. I like to buy premium brands
\item
  Q4. Quality is the most important factor in my purchasing decision
\item
  Q5. Style is the most important factor in my purchasing decision
\item
  Q6. I prefer to buy clothes in store
\item
  Q7. I prefer to buy clothes online
\item
  Q8. Price is important
\item
  Q9. I like to try different styles
\item
  Q10. I like to make decision myself and don't need too much of others' suggestions
\end{itemize}

There are 4 segments of customers:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Price
\item
  Conspicuous
\item
  Quality
\item
  Style
\end{enumerate}

Let's check it:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{str}\NormalTok{(sim.dat,}\DataTypeTok{vec.len=}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 'data.frame':    1000 obs. of  19 variables:
##  $ age         : int  57 63 59 60 51 59 57 57 ...
##  $ gender      : Factor w/ 2 levels "Female","Male": 1 1 2 2 2 2 2 2 ...
##  $ income      : num  120963 122008 114202 113616 ...
##  $ house       : Factor w/ 2 levels "No","Yes": 2 2 2 2 2 2 2 2 ...
##  $ store_exp   : num  529 478 491 348 ...
##  $ online_exp  : num  304 110 279 142 ...
##  $ store_trans : int  2 4 7 10 4 4 5 11 ...
##  $ online_trans: int  2 2 2 2 4 5 3 5 ...
##  $ Q1          : int  4 4 5 5 4 4 4 5 ...
##  $ Q2          : int  2 1 2 2 1 2 1 2 ...
##  $ Q3          : int  1 1 1 1 1 1 1 1 ...
##  $ Q4          : int  2 2 2 3 3 2 2 3 ...
##  $ Q5          : int  1 1 1 1 1 1 1 1 ...
##  $ Q6          : int  4 4 4 4 4 4 4 4 ...
##  $ Q7          : int  1 1 1 1 1 1 1 1 ...
##  $ Q8          : int  4 4 4 4 4 4 4 4 ...
##  $ Q9          : int  2 1 1 2 2 1 1 2 ...
##  $ Q10         : int  4 4 4 4 4 4 4 4 ...
##  $ segment     : Factor w/ 4 levels "Conspicuous",..: 2 2 2 2 2 2 2 2 ...
\end{verbatim}

Refer to Appendix for the simulation code.

\hypertarget{customer-satisfaction-survey-data-from-airline-company}{%
\section{Customer Satisfaction Survey Data from Airline Company}\label{customer-satisfaction-survey-data-from-airline-company}}

This data set is from a customer satisfaction survey for three airline companies. There are \texttt{N=1000} respondents and 15 questions. The market researcher asked respondents to recall the experience with different airline companies and assign a score (1-9) to each airline company for all the 15 questions. The higher the score, the more satisfied the customer to the specific item. The 15 questions are of 4 types (the variable names are in the parentheses):

\begin{itemize}
\tightlist
\item
  How satisfied are you with our\_\_\_\_\_\_?
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Ticketing

  \begin{itemize}
  \tightlist
  \item
    Ease of making reservationEasy\_Reservation
  \item
    Availability of preferred seatsPreferred\_Seats
  \item
    Variety of flight optionsFlight\_Options
  \item
    Ticket pricesTicket\_Prices
  \end{itemize}
\item
  Aircraft

  \begin{itemize}
  \tightlist
  \item
    Seat comfortSeat\_Comfort
  \item
    Roominess of seat areaSeat\_Roominess
  \item
    Availability of OverheadOverhead\_Storage
  \item
    Cleanliness of aircraftClean\_Aircraft
  \end{itemize}
\item
  Service

  \begin{itemize}
  \tightlist
  \item
    Courtesy of flight attendantCourtesy
  \item
    FriendlinessFriendliness
  \item
    HelpfulnessHelpfulness
  \item
    Food and drinksService
  \end{itemize}
\item
  General

  \begin{itemize}
  \tightlist
  \item
    Overall satisfactionSatisfaction
  \item
    Purchase againFly\_Again
  \item
    Willingness to recommendRecommend
  \end{itemize}
\end{enumerate}

Now check the data frame we have:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{str}\NormalTok{(rating,}\DataTypeTok{vec.len=}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 'data.frame':    3000 obs. of  17 variables:
##  $ Easy_Reservation: int  6 5 6 5 4 5 6 4 ...
##  $ Preferred_Seats : int  5 7 6 6 5 6 6 6 ...
##  $ Flight_Options  : int  4 7 5 5 3 4 6 3 ...
##  $ Ticket_Prices   : int  5 6 6 5 6 5 5 5 ...
##  $ Seat_Comfort    : int  5 6 7 7 6 6 6 4 ...
##  $ Seat_Roominess  : int  7 8 6 8 7 8 6 5 ...
##  $ Overhead_Storage: int  5 5 7 6 5 4 4 4 ...
##  $ Clean_Aircraft  : int  7 6 7 7 7 7 6 4 ...
##  $ Courtesy        : int  5 6 6 4 2 5 5 4 ...
##  $ Friendliness    : int  4 6 6 6 3 4 5 5 ...
##  $ Helpfulness     : int  6 5 6 4 4 5 5 4 ...
##  $ Service         : int  6 5 6 5 3 5 5 5 ...
##  $ Satisfaction    : int  6 7 7 5 4 6 5 5 ...
##  $ Fly_Again       : int  6 6 6 7 4 5 3 4 ...
##  $ Recommend       : int  3 6 5 5 4 5 6 5 ...
##  $ ID              : int  1 2 3 4 5 6 7 8 ...
##  $ Airline         : Factor w/ 3 levels "AirlineCo.1",..: 1 1 1 1 1 1 1 1 ...
\end{verbatim}

Refer to Appendix for the simulation code.

\hypertarget{swinediseasedata}{%
\section{Swine Disease Breakout Data}\label{swinediseasedata}}

The swine disease data includes simulated 120 survey questions from 800 farms. There are three choices for each question. The outbreak status for the \(i^{th}\) farm is generated from a \(Bernoulli(1, p_i)\) distribution with \(p_i\) being a function of the question answers:

\[ln(\frac{p_i}{1-p_i})=\beta_0 + \Sigma_{g=1}^G\mathbf{x_{i,g}^T\beta_{g}}\]

where \(\beta_0\) is the intercept, \(\mathbf{x_{i,g}}\) is a three-dimensional indication vector for question answer and \(\mathbf(\beta_g)\) is the parameter vector corresponding to the \(g^{th}\) predictor. Three types of questions are considered regarding their effects on the outcome. The first forty survey questions are important questions such that the coefficients of the three answers to these
questions are all different:

\[\mathbf{\beta_g}=(1,0,-1)\times \gamma,\ g=1,\dots,40\]

The second forty survey questions are also important questions but only one answer has a coefficient that is different from the other two answers:

\[\mathbf{\beta_g}=(1,0,0)\times \gamma,\ g=41,\dots,80\]

The last forty survey questions are also unimportant questions such that all three answers have the same coefficients:

\[\mathbf{\beta_g}=(0,0,0)\times \gamma,\ g=81,\dots,120\]

The baseline coefficient \(\beta_0\) is set to be \(-\frac{40}{3}\gamma\) so that on average a farm have 50\% of chance to have an outbreak. The parameter \(\gamma\) in the above simulation is set to control the strength of the questions' effect on the outcome. In this simulation study, we consider the situations where \(\gamma = 0.1, 0.25, 0.5, 1, 2\). So the parameter settings are:

\[\mathbf{\beta^{T}}=\left(\underset{question\ 1}{\frac{40}{3},\underbrace{1,0,-1}},...,\underset{question\ 40}{\underbrace{1,0,-1}},\underset{question\ 41}{\underbrace{1,0,0}},...,\underset{question\ 80}{\underbrace{1,0,0}},\underset{question\ 81}{\underbrace{0,0,0}},...,\underset{question\ 120}{\underbrace{0,0,0}}\right)*\gamma\]

For each value of \(\gamma\), 20 data sets are simulated. The bigger \(\gamma\) is, the larger the corresponding parameter. We provided the data sets with \(\gamma = 2\). Let's check the data:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{disease_dat <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"http://bit.ly/2KXb1Qi"}\NormalTok{)}
\CommentTok{# only show the last 7 columns here}
\KeywordTok{head}\NormalTok{(}\KeywordTok{subset}\NormalTok{(disease_dat,}\DataTypeTok{select=}\KeywordTok{c}\NormalTok{(}\StringTok{"Q118.A"}\NormalTok{,}\StringTok{"Q118.B"}\NormalTok{,}\StringTok{"Q119.A"}\NormalTok{,}
                                 \StringTok{"Q119.B"}\NormalTok{,}\StringTok{"Q120.A"}\NormalTok{,}\StringTok{"Q120.B"}\NormalTok{,}\StringTok{"y"}\NormalTok{))) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   Q118.A Q118.B Q119.A Q119.B Q120.A Q120.B y
## 1      1      0      0      0      0      1 1
## 2      0      1      0      1      0      0 1
## 3      1      0      0      0      1      0 1
## 4      1      0      0      0      0      1 1
## 5      1      0      0      0      1      0 0
## 6      1      0      0      1      1      0 1
\end{verbatim}

Here \texttt{y} indicates the outbreak situation of the farms. \texttt{y=1} means there is an outbreak in 5 years after the survey. The rest columns indicate survey responses. For example \texttt{Q120.A\ =\ 1} means the respondent chose \texttt{A} in Q120. We consider \texttt{C} as the baseline.

Refer to Appendix for the simulation code.

\hypertarget{mnist-dataset}{%
\section{MNIST Dataset}\label{mnist-dataset}}

The MNIST dataset is a popular dataset for image classification machine learning model tutorials. It is conveniently included in the Keras library and ready to be loaded with build-in functions for analysis. The WIKI page of MNIST provides a detailed description of the dataset: \url{https://en.wikipedia.org/wiki/MNIST_database}. It contains 70,000 images of handwritten digits from American Census Bureau employees and American high school students. There are 60,000 training images and 10,000 testing images. Each image has a resolution of 28x28, and the numerical pixel values are in greyscale. Each image is represented by a 28x28 matrix with each element of the matrix an integer between 0 and 255. The label of each image is the intended digit of the handwritten image between 0 and 9. We cover the detailed steps to explore the MNIST dataset in the R and Python notebooks. A sample of the dataset is illustrated in the figure below:

\begin{figure}
\centering
\includegraphics[width=0.7\textwidth,height=\textheight]{images/MnistExamples.png}
\caption{Sample of MNIST dataset (\url{https://en.wikipedia.org/wiki/File:MnistExamples.png})}
\end{figure}

\hypertarget{imdb-dataset}{%
\section{IMDB Dataset}\label{imdb-dataset}}

The IMDB dataset (\url{http://ai.stanford.edu/~amaas/data/sentiment/}) is a popular dataset for text and language-related machine learning tutorials. It is also conveniently included in the Keras library, and there are a few build-in functions in Keras for data loading and pre-processing. It contains 50,000 movie reviews (25,000 in training and 25,000 in testing) from IMDB, as well as each movie review's binary sentiment: positive or negative. The raw data contains the text of each movie review, and it has to be pre-processed before being fitted with any machine learning models. By using Keras's built-in functions, we can easily get the processed dataset (i.e., a numerical data frame) for machine learning algorithms. Keras' build-in functions perform the following tasks to convert the raw review text into a data frame:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Convert text data into numerical data. Machine learning models cannot work with raw text data directly, and we have to convert text into numbers. There are many different ways for the conversion and Keras' build-in function uses each word's rank of frequency in the entire training dataset to replace the raw text in both the training and testing dataset. For example, the 10th most frequent word is replaced by integer 10. There are a few additional setups for this process, including:

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Skip top frequent words. We usually skip a few top frequent words as they are mainly stopwords like ``the'' ``and'' or ``a,'' which usually do not provide much information. There is a parameter in the build-in function to specify how many top words to skip.\\
  \item
    Set the maximum number of unique words. The entire vocabulary of the unique words in the training dataset may be large, and many of them have very low frequencies such as just appearing once in the entire training dataset.~To keep the size of the vocabulary, we can also set up the maximum number of the unique words using Keras' built-in function such that any words with least frequencies will be replaced with a special index such as ``2''.
  \end{enumerate}
\item
  Padding or truncation to keep all the reviews to be the same length. For most machine learning models, the algorithms expect to see the same number of features (i.e., same number of input columns in the data frame). There is a parameter in the Keras build-in function to set the maximum number of words in each review (i.e., max\_length). For reviews that have less than max\_legth words, we pad them with ``0''. For reviews that have more than max\_length words, we truncate them.
\end{enumerate}

After the above pre-processing, each review is represented by one row in the data frame. There is one column for the binary positive/negative sentiment, and max\_length columns input features converted from the raw review text. In the corresponding R and Python notebooks, we will go over the details of the data pre-processing using Keras' built-in functions.

\hypertarget{big-data-cloud-platform}{%
\chapter{Big Data Cloud Platform}\label{big-data-cloud-platform}}

Data has been statisticians and analysts' friend for hundreds of years. Tabulated data are the most common format that we use daily. People used to store data on papers, tapes, diskettes, or hard drives. Only recently, with the development of computer hardware and software, the volume, variety, and speed of the data exceed the capacity of a traditional statistician or analyst. So using data becomes a science that focuses on the question: how can we store, access, process, analyze the massive amount of data and provide actionable insights? In the past few years, by utilizing commodity hardware and open-source software, people created a big data ecosystem for data storage, data retrieval, and parallel computation. Hadoop and Spark have become a popular platform that enables data scientists, statisticians, and analysts to access the data and to build models. Programming skills in the big data platform have been an obstacle for a traditional statistician or analyst to become a successful data scientist. However, cloud computing reduces the difficulty significantly. The user interface of the data platform is much more friendly today, and people push mush of the technical details to the background. Today's cloud systems also enable quick implementation of the production environment. Now data science emphasizes more on the data itself, models and algorithms on top of the data, rather than the platform, infrastructure and low-level programming such as Java.

\hypertarget{power-of-cluster-of-computers}{%
\section{Power of Cluster of Computers}\label{power-of-cluster-of-computers}}

We are familiar with our laptop/desktop computers which have three main components to do data computation: (1) Hard disk, (2) Memory, and (3) CPU.

The data and codes stored in the hard disk have specific features such as slow to read and write, and large capacity of around a few TB in today's market. Memory is fast to read and write but with small capacity in the order of a few dozens of GB in today's market. CPU is where all the computation happens.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{images/cluster} 

}

\caption{Single computer (left) and a cluster of computers (right)}\label{fig:sparkcluster}
\end{figure}

For statistical software such as R, the amount of data it can process is limited by the computer's memory. The memory of computers before 2000 is less than 1 GB. The memory capacity grows way slower than the amount of the data. Now it is common that we need to analyze data far beyond the capacity of a single computer's memory, especially in an enterprise environment. Meanwhile, as the data size increases, to solve the same problem (such as regressions), the computation time is growing faster than linear. Using a cluster of computers become a common way to solve a big data problem. In figure \ref{fig:sparkcluster} (right), a cluster of computers can be viewed as one powerful machine with memory, hard disk and CPU equivalent to the sum of individual computers. It is common to have hundreds or even thousands of nodes for a cluster.

In the past, users need to write code (such as MPI) to distribute data and do parallel computing. Fortunately, with the recent new development, the cloud environment for big data analysis is more user-friendly. As data is often beyond the size of the hard disk, the dataset itself is stored across different nodes (i.e., the Hadoop system). When doing analysis, the data is distributed across different nodes, and algorithms are parallel to leverage corresponding nodes' CPUs to compute (i.e., the Spark system).

\hypertarget{evolution-of-cluster-computing}{%
\section{Evolution of Cluster Computing}\label{evolution-of-cluster-computing}}

Using computer clusters to solve general-purpose data and analytics problems needs a lot of effort if we have to specifically control every element and steps such as data storage, memory allocation, and parallel computation. Fortunately, high tech companies and open source communities have developed the entire ecosystem based on Hadoop and Spark. Users need only to know high-level scripting languages such as Python and R to leverage computer clusters' distributed storage, memory and parallel computation power.

\hypertarget{hadoop}{%
\subsection{Hadoop}\label{hadoop}}

The very first problem internet companies face is that a lot of data has been collected and how to better store these data for future analysis. Google developed its own file system to provide efficient, reliable access to data using large clusters of commodity hardware. The open-source version is known as Hadoop Distributed File System (HDFS). Both systems use Map-Reduce to allocate computation across computation nodes on top of the file system. Hadoop is written in Java and writing map-reduce job using Java is a direct way to interact with Hadoop which is not familiar to many in the data and analytics community. To help better use the Hadoop system, an SQL-like data warehouse system called Hive, and a scripting language for analytics interface called Pig were introduced for people with analytics background to interact with Hadoop system. Within Hive, we can create user-defined functions through R or Python to leverage the distributed and parallel computing infrastructure. Map-reduce on top of HDFS is the main concept of the Hadoop ecosystem. Each map-reduce operation requires retrieving data from hard disk, then performing the computation, and storing the result onto the disk again. So, jobs on top of Hadoop require a lot of disk operation which may slow down the entire computation process.

\hypertarget{spark}{%
\subsection{Spark}\label{spark}}

Spark works on top of a distributed file system including HDFS with better data and analytics efficiency by leveraging in-memory operations. Spark is more tailored for data processing and analytics and the need to interact with Hadoop directly is greatly reduced. The spark system includes an SQL-like framework called Spark SQL and a parallel machine learning library called MLib. Fortunately for many in the analytics community, Spark also supports R and Python. We can interact with data stored in a distributed file system using parallel computing across nodes easily with R and Python through the Spark API and do not need to worry about lower-level details of distributed computing. We will introduce how to use an R notebook to drive Spark computations.

\hypertarget{CloudEnvironment}{%
\section{Introduction of Cloud Environment}\label{CloudEnvironment}}

Even though Spark provides a solution for big data analytics, the maintenance of the computing cluster and Spark system requires a dedicated team. Historically for each organization the IT departments own the hardware and the regular maintenance. It usually takes months for a new environment to be built and the cost is high. Luckily, the time to deployment and cost are dramatically down due to the cloud computation trend. Now we can create a Spark computing cluster in the cloud in a few minutes with the desired configuration and the user only pay when the cluster is up. Cloud computing environments enable smaller organizations to adopt big data analytics.

There are many cloud computing environments such as Amazon's AWS and Microsoft Azure which provide a complete list of functions for heavy-duty enterprise applications. For example, Netflix runs its business entirely on AWS without owning any data centers. For beginners, however, Databricks provides an easy to use cloud system for learning purposes. Databricks is a company founded by the creators of Apache Spark and it provides a user-friendly web-based notebook environment that can create a Spark cluster on the fly to run R/Python/Scala/SQL scripts. We will use Databricks' free community edition to run demos in this book. Please note, to help readers to get familiar with the Databricks cloud system, the content of this section is partially adopted from the following web pages:

\begin{itemize}
\tightlist
\item
  \url{https://docs.databricks.com/user-guide/faq/sparklyr.html}
\item
  \url{http://spark.rstudio.com/index.html}
\end{itemize}

\hypertarget{open-account-and-create-a-cluster}{%
\subsection{Open Account and Create a Cluster}\label{open-account-and-create-a-cluster}}

Anyone can apply for a free Databrick account through \url{https://databricks.com/try-databricks} and please make sure to choose the ``\textbf{COMMUNITY EDITION}'' which does not require credit card information and will always be free. Once the community edition account is open and activated. Users can create a cluster computation environment with Spark. The computing cluster associated with community edition account is relatively small, but it is good enough for running all the examples in this book. The main user interface to the computing environment is notebook: a collection of cells that contains formatted text or codes. When a new notebook is created, user will need to choose the default programming language type (i.e.~Python, R, Scala, or SQL) and every cells in the notebook will assume the default programming language. However, user can easily override the default selection of programming language by adding \texttt{\%sql}, \texttt{\%python}, \texttt{\%r} or \texttt{\%scala} at the first line of each cell to indicate the programming language in that cell. Allowing running different cells with different programming language in the same notebook enable user to have the flexibility to choose the best tools for each task. User can also define a cell to be markdown cell by adding \texttt{\%md} at the first line of the cell. A markdown cell does not performance computation and it is just a cell to show formatted text. Well separated cells with computation, graph and formatted text enable user to create easy to maintain reproducible reports. The link to a video showing how to open Databricks account, how to create a cluster, and how to create notebooks is included in the book's website.

\hypertarget{r-notebook}{%
\subsection{R Notebook}\label{r-notebook}}

For this book, we will use R notebook for examples and demos and the corresponding Python notebook will be available online too. For an R notebook, it contains multiple cells, and, by default, the content within each cell are R scripts. Usually, each cell is a well-managed segment of a few lines of codes that accomplish a specific task. For example, Figure \ref{fig:rnotebook} shows the default cell for an R notebook. We can type in R scripts and comments same as we are using R console. By default, only the result from the last line will be shown following the cell. However, you can use \texttt{print()} function to output results for any lines. If we move the mouse to the middle of the lower edge of the cell below the results, a ``\texttt{+}'' symbol will show up and click on the symbol will insert a new cell below. When we click any area within a cell, it will make it editable and you will see a few icons on the top right corn of the cell where we can run the cell, as well as add a cell below or above, copy the cell, cut the cell etc. One quick way to run the cell is \texttt{Shift+Enter} when the cell is chosen. User will become familiar with the notebook environment quickly.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{images/dbxrnotebook} 

}

\caption{Example of R Notebook}\label{fig:rnotebook}
\end{figure}

\hypertarget{markdown-cells}{%
\subsection{Markdown Cells}\label{markdown-cells}}

For an R notebook, every cell by default will contain R scripts. But if we put \texttt{\%md}, \texttt{\%sql} or \texttt{\%python} at the first line of a cell, that cell becomes Markdown cell, SQL script cell, and Python script cell accordingly. For example, Figure \ref{fig:dbxmarkdown} shows a markdown cell with scripts and the actual appearance when exits editing mode. Markdown cell provides a straightforward way to descript what each cell is doing as well as what the entire notebook is about. It is a better way than a simple comment within the code.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{images/dbxmarkdown} 

}

\caption{Example of Markdown Cell}\label{fig:dbxmarkdown}
\end{figure}

\hypertarget{leverage-spark-using-r-notebook}{%
\section{Leverage Spark Using R Notebook}\label{leverage-spark-using-r-notebook}}

R is a powerful tool for data analysis given the data can be fit into memory. Because of the memory bounded dataset limit, R itself cannot be used directly for big data analysis where the data is likely stored in Hadoop and Spark system. By leverage the \texttt{sparklyr} package created by RStudio, we can use Databricks' R notebook to analyze data stored in the Spark system. As the data are stored across different nodes, Spark enables parallel computation using the collection of memory and CPU across all nodes. The fundamental data element in the Spark system is called Spark DataFrames (SDF). In this section, we will illustrate how to use Databricks' R notebook for big data analysis on top of the Spark environment through \texttt{sparklyr} package.

\textbf{Install pacakge}

First, we need to install \texttt{sparklyr} package which enables the connection between master or local node to Spark cluster environments. As it will install more than 10 dependencies, it may take more than 10 minutes to finish. Be patient while it is installing! Once the installation finishes, load the \texttt{sparklyr} package as illustrated by the following code:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Install sparklyr}
\ControlFlowTok{if}\NormalTok{ (}\OperatorTok{!}\KeywordTok{require}\NormalTok{(}\StringTok{"sparklyr"}\NormalTok{)) \{}
\KeywordTok{install.packages}\NormalTok{(}\StringTok{"sparklyr"}\NormalTok{)}
\NormalTok{\}}
\CommentTok{# Load sparklyr package}
\KeywordTok{library}\NormalTok{(sparklyr)}
\end{Highlighting}
\end{Shaded}

\textbf{Create a Spark Connection}

Once the library is loaded, we need to create a Spark Connection to link the computing node (i.e.~local node) running the R notebook to the Spark environment. Here we use the \texttt{"databricks"} option for parameter \texttt{method} which is specific for databricks' cloud system. In other enterprise environments, please consult your administrator for details. The Spark Connection (i.e. \texttt{sc}) is the pipe to connect R notebook in the local node with the Spark Cluster. We can think of the R notebook is running on a local node that has its memory and CPU; the Spark system has a cluster of connected computation nodes, and the Spark Connection creates a mechanism to connect both systems. The Spark Connection can be established with:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# create a sparklyr connection}
\NormalTok{sc <-}\StringTok{ }\KeywordTok{spark_connect}\NormalTok{(}\DataTypeTok{method =} \StringTok{"databricks"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

To simplify the learning process, let us use a very familiar small dataset: the iris dataset. It is part of the \texttt{dplyr} library and we can load that library to use the \texttt{iris} data frame. Now the \texttt{iris} dataset is still on the local node where the R notebook is running on. And we can check the first a few lines of the \texttt{iris} dataset using the code below:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(dplyr)}
\KeywordTok{head}\NormalTok{(iris)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   Sepal.Length Sepal.Width Petal.Length Petal.Width
## 1          5.1         3.5          1.4         0.2
## 2          4.9         3.0          1.4         0.2
## 3          4.7         3.2          1.3         0.2
## 4          4.6         3.1          1.5         0.2
## 5          5.0         3.6          1.4         0.2
## 6          5.4         3.9          1.7         0.4
##   Species
## 1  setosa
## 2  setosa
## 3  setosa
## 4  setosa
## 5  setosa
## 6  setosa
\end{verbatim}

\textbf{IMPORTANT - Copy Data to Spark Environment}

In real applications, the data set may be massive and cannot fit in a single hard disk and most likely such data are already stored in the Spark system. If the data is already in Hadoop/Spark ecosystem in the form of SDF, we can create a local R object to link to the SDF by the \texttt{tbl()} function where \texttt{my\_sdf} is the SDF in the Spark system, and \texttt{my\_sdf\_tbl} is the R local object that referring to \texttt{my\_sdf}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my_sdf_tbl <-}\StringTok{ }\KeywordTok{tbl}\NormalTok{(}\DataTypeTok{sc =}\NormalTok{ sc, my_sdf)}
\end{Highlighting}
\end{Shaded}

As we just created a brand new Spark computing environment, there is no SDF in the system yet. We will need to copy a local dataset to the Spark environment. As we have already created the Spark Connection \texttt{sc}, it is easy to copy data to spark system using \texttt{sdf\_copy\_to()} function as below:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{iris_tbl <-}\StringTok{ }\KeywordTok{sdf_copy_to}\NormalTok{(}\DataTypeTok{sc =}\NormalTok{ sc, }\DataTypeTok{x =}\NormalTok{ iris, }\DataTypeTok{overwrite =}\NormalTok{ T)}
\end{Highlighting}
\end{Shaded}

The above one-line code copies iris dataset from the local node to Spark cluster environment. ``\texttt{sc}'' is the Spark Connection we just created; ``\textbf{x}'' is the data frame that we want to copy; ``\texttt{overwrite}'' is the option whether we want to overwrite the target object if the same name SDF exists in the Spark environment. Finally, \texttt{sdf\_copy\_to()} function will return an R object representing the copied SDF (i.e.~creating a ``pointer'' to the SDF such that we can refer \texttt{iris\_tbl} in the R notebook to operate \texttt{iris} SDF). Now \texttt{irir\_tbl} in the local R environment can be used to refer to the \texttt{iris} SDF in the Spark system.

To check whether the \texttt{iris} data was copied to the Spark environment successfully or not, we can use \texttt{src\_tbls()} function to the Spark Connection (\texttt{sc}):

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## code to return all the data frames associated with sc}
\KeywordTok{src_tbls}\NormalTok{(sc) }
\end{Highlighting}
\end{Shaded}

\textbf{Analyzing the Data}

Now we have successfully copied the \texttt{iris} dataset to the Spark environment as a SDF. This means that \texttt{iris\_tbl} is an R object representing the \texttt{iris} SDF and we can use \texttt{iris\_tbl} in R to refer the \texttt{iris} dataset in the Spark system (i.e.~the \texttt{iris} SDF). With the \texttt{sparklyr} packages, we can nearly all the functions in \texttt{dplyr} to Spark DataFrame directly through \texttt{iris\_tbl}, same as we are applying \texttt{dplyr} functions to a local R data frame in our laptop. For example, we can use the \texttt{\%\textgreater{}\%} operator to pass \texttt{iris\_tbl} to the \texttt{count()} function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{iris_tbl }\OperatorTok{%>%}\StringTok{ }\NormalTok{count}
\end{Highlighting}
\end{Shaded}

or using the \texttt{head()} function to return the first few rows in \texttt{iris\_tbl}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{head}\NormalTok{(iris_tbl)}
\end{Highlighting}
\end{Shaded}

or applying more advanced data manipulation directly to \texttt{iris\_tbl}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{iris_tbl }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{Sepal_Width =} \KeywordTok{round}\NormalTok{(Sepal_Width }\OperatorTok{*}\StringTok{ }\DecValTok{2}\NormalTok{) }\OperatorTok{/}\StringTok{ }\DecValTok{2}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{group_by}\NormalTok{(Species, Sepal_Width) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{summarize}\NormalTok{(}\DataTypeTok{count =} \KeywordTok{n}\NormalTok{(), }\DataTypeTok{Sepal_Length =} \KeywordTok{mean}\NormalTok{(Sepal_Length), }
            \DataTypeTok{stdev =} \KeywordTok{sd}\NormalTok{(Sepal_Length))}
\end{Highlighting}
\end{Shaded}

\textbf{Collect Results Back to Master Node}

Even though we can run nearly all of the \texttt{dplyr} functions on SDF, we cannot apply functions from other packages directly to SDF (such as \texttt{ggplot()}). For functions that can only work on local R data frames, we must copy the SDF back to the local node as an R data frame. To copy SDF back to the local node, we use the \texttt{collect()} function. The following code using \texttt{collect()} will collect the results of a few operations and assign the collected data to \texttt{iris\_summary}, a local R data frame:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{iris_summary <-}\StringTok{ }\NormalTok{iris_tbl }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{Sepal_Width =} \KeywordTok{round}\NormalTok{(Sepal_Width }\OperatorTok{*}\StringTok{ }\DecValTok{2}\NormalTok{) }\OperatorTok{/}\StringTok{ }\DecValTok{2}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{group_by}\NormalTok{(Species, Sepal_Width) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{summarize}\NormalTok{(}\DataTypeTok{count =} \KeywordTok{n}\NormalTok{(), }
              \DataTypeTok{Sepal_Length =} \KeywordTok{mean}\NormalTok{(Sepal_Length), }
              \DataTypeTok{stdev =} \KeywordTok{sd}\NormalTok{(Sepal_Length)) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{collect}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

Now, \texttt{iris\_summary} is a local R object to the R notebook and we can use any R packages and functions to it. In the following code, we will apply \texttt{ggplot()} to it, exactly the same as a stand along R console:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(ggplot2)}
\KeywordTok{ggplot}\NormalTok{(iris_summary, }\KeywordTok{aes}\NormalTok{(Sepal_Width, Sepal_Length, }\DataTypeTok{color =}\NormalTok{ Species)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_line}\NormalTok{(}\DataTypeTok{size =} \FloatTok{1.2}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_errorbar}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{ymin =}\NormalTok{ Sepal_Length }\OperatorTok{-}\StringTok{ }\NormalTok{stdev, }
                    \DataTypeTok{ymax =}\NormalTok{ Sepal_Length }\OperatorTok{+}\StringTok{ }\NormalTok{stdev), }\DataTypeTok{width =} \FloatTok{0.05}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_text}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{label =}\NormalTok{ count), }\DataTypeTok{vjust =} \FloatTok{-0.2}\NormalTok{, }\DataTypeTok{hjust =} \FloatTok{1.2}\NormalTok{, }\DataTypeTok{color =} \StringTok{"black"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{legend.position=}\StringTok{"top"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

In most cases, the heavy-duty data preprocessing and aggregation is done in Spark using functions in dplyr. Once the data is aggregated, the size is usually dramatically reduced and such reduced data can be collected to an R local object for downstream analysis.

\textbf{Fit Regression to SDF}

One of the advantages of the Spark system is the parallel machine learning algorithm. There are many statistical and machine learning algorithms developed to run in parallel across many CPUs with data across many memory units for SDF. In this example, we have already uploaded the \texttt{iris} data to the Spark system, and the data in the SDF can be referred through \texttt{iris\_tbl} as in the last section. The linear regression algorithm implemented in the Spark system can be called through \texttt{ml\_linear\_regression()} function. The syntax to call the function is to define the local R object that representing the SDF (i.e. \texttt{iris\_tbl} (local R object) for \texttt{iris} (SDF)), response variable (i.e.~the y variable in linear regression in the SDF) and features (i.e.~the x variables in linear regression in the SDF). Now, we can easily fit a linear regression for large dataset far beyond the memory limit of one single computer, and it is truly scalable and only constrained by the resource of the Spark cluster. Below is an illustration of how to fit a linear regression to SDF using R notebook:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit1 <-}\StringTok{ }\KeywordTok{ml_linear_regression}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ iris_tbl, }\DataTypeTok{response =} \StringTok{"Sepal_Length"}\NormalTok{,}
        \DataTypeTok{features =} \KeywordTok{c}\NormalTok{(}\StringTok{"Sepal_Width"}\NormalTok{, }\StringTok{"Petal_Length"}\NormalTok{, }\StringTok{"Petal_Width"}\NormalTok{))}
\KeywordTok{summary}\NormalTok{(fit1)}
\end{Highlighting}
\end{Shaded}

In the above code, \texttt{x} is the R object pointing to the SDF; \texttt{response} is y-variable, \texttt{features} are the collection of explanatory variables. For this function, both the data and computation are in the Spark cluster which leverages multiple CPUs, distributed memories and parallel computing.

\textbf{Fit a K-means Cluster}

Through the \texttt{sparklyr} package, we can use an R notebook to access many Spark Machine Learning Library (MLlib) algorithms such as Linear Regression, Logistic Regression, Survival Regression, Generalized Linear Regression, Decision Trees, Random Forests, Gradient-Boosted Trees, Principal Components Analysis, Naive-Bayes, K-Means Clustering, and a few other methods. Below codes fit a k-means cluster algorithm:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## Now fit a k-means clustering using iris_tbl data}
\CommentTok{## with only two out of four features in iris_tbl}
\NormalTok{fit2 <-}\StringTok{ }\KeywordTok{ml_kmeans}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ iris_tbl, }\DataTypeTok{centers =} \DecValTok{3}\NormalTok{,}
                    \DataTypeTok{features =} \KeywordTok{c}\NormalTok{(}\StringTok{"Petal_Length"}\NormalTok{, }\StringTok{"Petal_Width"}\NormalTok{))}
\CommentTok{# print our model fit}
\KeywordTok{print}\NormalTok{(fit2)}
\end{Highlighting}
\end{Shaded}

\textless{}\textless{}\textless{}\textless{}\textless{}\textless{}\textless{} HEAD
After fitting the k-means model, we can apply the model to predict other datasets through \texttt{sdf\_predict()} function. Following code applies the model to \texttt{iris\_tbl} again to predict the cluster and collect the results as a local R object (i.e. \texttt{prediction}) using \texttt{collect()} function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{prediction =}\StringTok{ }\KeywordTok{collect}\NormalTok{(}\KeywordTok{sdf_predict}\NormalTok{(fit2, iris_tbl))}
\end{Highlighting}
\end{Shaded}

As \texttt{prediction} is a local R object, we can apply any R functions from any libraries to it. For example:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{prediction }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(Petal_Length, Petal_Width)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\KeywordTok{aes}\NormalTok{(Petal_Width, Petal_Length, }
                 \DataTypeTok{col =} \KeywordTok{factor}\NormalTok{(prediction }\OperatorTok{+}\StringTok{ }\DecValTok{1}\NormalTok{)),}
             \DataTypeTok{size =} \DecValTok{2}\NormalTok{, }\DataTypeTok{alpha =} \FloatTok{0.5}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ fit2}\OperatorTok{$}\NormalTok{centers, }\KeywordTok{aes}\NormalTok{(Petal_Width, Petal_Length),}
             \DataTypeTok{col =}\NormalTok{ scales}\OperatorTok{::}\KeywordTok{muted}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"red"}\NormalTok{, }\StringTok{"green"}\NormalTok{, }\StringTok{"blue"}\NormalTok{)),}
             \DataTypeTok{pch =} \StringTok{'x'}\NormalTok{, }\DataTypeTok{size =} \DecValTok{12}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_color_discrete}\NormalTok{(}\DataTypeTok{name =} \StringTok{"Predicted Cluster"}\NormalTok{,}
                       \DataTypeTok{labels =} \KeywordTok{paste}\NormalTok{(}\StringTok{"Cluster"}\NormalTok{, }\DecValTok{1}\OperatorTok{:}\DecValTok{3}\NormalTok{)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{"Petal Length"}\NormalTok{, }
       \DataTypeTok{y =} \StringTok{"Petal Width"}\NormalTok{, }
       \DataTypeTok{title =} \StringTok{"K-Means Clustering"}\NormalTok{,}
       \DataTypeTok{subtitle =} \StringTok{"Use Spark ML to predict cluster }
\StringTok{       membership with the iris dataset"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

So far, we have illustrated

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  the relationship between a local node (i.e.~where R notebook is running) and Spark Clusters (i..e where data are stored and computation are done);
\item
  how to copy a local data frame to a Spark DataFrames (please note if your data is already in Spark environment, there is no need to copy and we only need to build the connection. This is likely to be the case for enterprise environment);
\item
  how to manipulate Spark DataFrames for data cleaning and preprocessing through \texttt{dplyr} functions with the installation of \texttt{sparklyr} package;
\item
  how to fit statistical and machine learning models to Spark DataFrame in a truly parallel manner;
\item
  how to collect information from Spark DataFrames back to a local R object (i.e.~local R data frame) for future analysis.
\end{enumerate}

These procedures cover the basics of big data analysis that a data scientist needs to know as a beginner. We have an R notebook on the book website that contains the contents of this chapter. We also have a Python notebook on the book website.

\hypertarget{databases-and-sql}{%
\section{Databases and SQL}\label{databases-and-sql}}

\hypertarget{history}{%
\subsection{History}\label{history}}

Databases have been around for many years to efficiently organize, store, retrieve, and update data systematically. In the past, statisticians and analysts usually dealt with small datasets stored in text or Excel files and often did not interact with database systems. Students from the traditional statistics department usually lack the necessary database knowledge. However, as data grow bigger, database knowledge becomes essential and required for statisticians, analysts and data scientists in an enterprise environment where data are stored in some form of database systems. Databases often contain a collection of tables and the relationship among these tables (i.e.~schema). The table is the fundamental structure for databases that contain rows and columns similar to data frames in R or Python. Database management systems (DBMS) ensure data integration and security in real time operations. There are many different DBMS such as Oracle, SQL Server, MySQL, Teradata, Hive, Redshift and Hana. The majority of database operations are very similar among different DBMS, and Structured Query Language (SQL) is the standard language to use these systems.

SQL became a standard of the American National Standards Institute (ANSI) in 1986, and of the International Organization for Standardization (ISO) in 1987. The most recent version is published in December 2016. For typical users, fundamental knowledge is nearly the same across all database systems. In addition to the standard features, each DBMS providers include their own specific functions and features. So, for the same query, there may be slightly different implementations (i.e.~SQL script) for different systems. In this section, we use the Databricks' SQL implementation (i.e.~all the SQL scripts can run in Databricks SQL notebook).

More recent data is stored in a distributed system such as Hive for disk storage or Hana for in-memory storage. Most relational databases are row-based (i.e.~data for each row are stored closely), whereas analytics workflows often favor column-based systems (i.e.~data for each column are stored closely). Fortunately, as a database user, we only need to learn how to write SQL scripts to retrieve and manipulate data. Even though there are different implantations of SQL across different DBMS, SQL is nearly universal across relational databases including Hive and Spark, which means once we know SQL, our knowledge can be transferred among different database systems. SQL is easy to learn even if you do not have previous experience. In this session, we will go over the key concepts in the database and SQL.

\hypertarget{database-table-and-view}{%
\subsection{Database, Table and View}\label{database-table-and-view}}

A database is a collection of tables that are related to each other. A database has its own database name and each table has its name as well. We can think a database is a ``folder'' where tables within a database are ``files'' within the folder. A table has rows and columns exactly as an R or Python pandas data frame. Each row (also called record) represents a unique instance of the subject and each column (also called field or attribute) represents a characteristic of the subject on the table. For each table, there is a special column called the primary key which uniquely identifies each of its records.

Tables within a specific database contain related information and the schema of a database illustrates all fields in every table as well as how these tables and fields relate to each other (i.e.~the structure of a database). Tables can be filtered, joined and aggregated to return specific information. The view is a virtual table composed of fields from one or more base tables. The view does not store data and only store table structure. The view is also referred to as a saved query. The view is typically used to protect the data stored in the table and users can only query information from a view and cannot change or update its contents.

We will use two simple tables to illustrate basic SQL operations. These two tables are from an R dataset which contains the 50 states' population and income (\url{https://stat.ethz.ch/R-manual/R-patched/library/datasets/html/state.html}). The first table is called \texttt{divisions} which has two columns: \texttt{state} and \texttt{division} and the first few rows are shown in the following table:

\begin{longtable}[]{@{}cc@{}}
\toprule
state & division\tabularnewline
\midrule
\endhead
Alabama & East South Central\tabularnewline
Alaska & Pacific\tabularnewline
Arizona & Mountain\tabularnewline
Arkansas & West South Central\tabularnewline
California & Pacific\tabularnewline
\bottomrule
\end{longtable}

The second table is called \texttt{metrics} which contains three columns: \texttt{state}, \texttt{population} and \texttt{income} and first few rows of the table are shown below:

\begin{longtable}[]{@{}ccc@{}}
\toprule
state & population & income\tabularnewline
\midrule
\endhead
Alabama & 3615 & 3624\tabularnewline
Alaska & 365 & 6315\tabularnewline
Arizona & 2212 & 4530\tabularnewline
Arkansas & 2110 & 3378\tabularnewline
California & 21198 & 5114\tabularnewline
\bottomrule
\end{longtable}

To illustrate missing information, three more rows are added at the end of the original division table with state Alberta, Ontario, and Quebec with their corresponding division NULL. We first creat these two tables and save them as csv files, and then we upload these two files as Databricks tables.

\hypertarget{basic-sql-statement}{%
\subsection{Basic SQL Statement}\label{basic-sql-statement}}

After logging into Databricks and creating two tables, we can now create a notebook and choose the default language of the notebook to be SQL. There are a few very easy SQL statements to help us understand the database and table structure:

\begin{itemize}
\tightlist
\item
  \texttt{show\ database}: show current databases in the system
\item
  \texttt{create\ database\ db\_name}: create a new database with name \texttt{db\_name}
\item
  \texttt{drop\ database\ db\_name}: delete database \texttt{db\_name} (be careful when using it!)
\item
  \texttt{use\ db\_name}: set up the current database to be used
\item
  \texttt{show\ tables}: show all the tables within the currently used database
\item
  \texttt{describe\ tbl\_name}: show the structure of table with name \texttt{tbl\_name} (i.e.~list of column name and data type)
\item
  \texttt{drop\ tbl\_name}: delete a table with name \texttt{tbl\_name} (be careful when using it!)
\item
  \texttt{select\ *\ from\ metrics\ limit\ 10}: show the first 10 rows of a table
\end{itemize}

If you are familiar with a procedural programming language such as C and FORTRAN or scripting languages such as R and Python, you may find SQL code a little bit strange. We should view SQL code by each specific chunk where it defines a specific task. SQL codes descript a specific task and DBMS will run and finish the task. SQL does not follow typical procedure program rules and we can think SQL is ``descriptive'' (i.e.~we describe what we want using SQL and DBMS figures out how to do it).

\hypertarget{select-statement}{%
\subsubsection{\texorpdfstring{\texttt{SELECT} Statement}{SELECT Statement}}\label{select-statement}}

\texttt{SELECT} is the most used statement in SQL, especially for database users and business analysts. It is used to extract specific information (i.e.~column or columns) \texttt{FROM} one or multiple tables. It can be used to combine multiple tables. \texttt{WHERE} can be used in the \texttt{SELECT} statement to selected rows with specific conditions (i.e.~filters). \texttt{ORDER\ BY} can be used in the \texttt{SELECT} statement to order the results in descending or ascending order of one or multiple columns. We can use \texttt{*} after \texttt{SELECT} to represent all columns in the table, or specifically write the column names separated by a comma. Below is the basic structure of a \texttt{SELECT} statement:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{SELECT}\NormalTok{ Col_Name1, Col_Name2}
\KeywordTok{FROM}\NormalTok{ Table_Name}
\KeywordTok{WHERE}\NormalTok{ Specific_Condition}
\KeywordTok{ORDER} \KeywordTok{BY}\NormalTok{ Col_Name1, Col_Name2;}
\end{Highlighting}
\end{Shaded}

Here \texttt{Specific\_Condition} is the typical logical conditions and only columns with \texttt{TRUE} for this condition will be chosen. For example, if we want to choose states and its total income where the population larger than 10000 and individual income less than 5000 with the result order by state name, we can use the following query:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{select}\NormalTok{ state, income*population }\KeywordTok{as}\NormalTok{ total_income}
\KeywordTok{from}\NormalTok{ metrics}
\KeywordTok{where}\NormalTok{ population > }\DecValTok{10000} \KeywordTok{and}\NormalTok{ income < }\DecValTok{5000}
\KeywordTok{order} \KeywordTok{by}\NormalTok{ state}
\end{Highlighting}
\end{Shaded}

The \texttt{SELECT} statement is used to slicing and dicing the dataset as well as create new columns of interest (such as \texttt{total\_income}) using basic computation functions.

\hypertarget{aggregation-functions-and-group-by}{%
\subsubsection{\texorpdfstring{Aggregation Functions and \texttt{GROUP\ BY}}{Aggregation Functions and GROUP BY}}\label{aggregation-functions-and-group-by}}

We can also use aggregation functions in the \texttt{SELECT} statement to summarize the data. For example, \texttt{count(col\_name)} function will return the total number of not \texttt{NULL} rows for a specific column. Other aggregation function on numerical values include \texttt{min(col\_name)}, \texttt{max(col\_name)}, \texttt{avg(col\_name)}. Let's use the \texttt{metrics} table again to illustrate aggregation functions. For aggregation function, it takes all the rows that match WHERE condition (if any) and return one number. The following statement will calculate the maximum, minimum, and average population for all states starts with letter A to E.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{select} \FunctionTok{sum}\NormalTok{(population) }\KeywordTok{as}\NormalTok{ sum_pop, }\FunctionTok{max}\NormalTok{(population) }\KeywordTok{as}
\NormalTok{max_pop, }\FunctionTok{min}\NormalTok{(population) }\KeywordTok{as}\NormalTok{ min_pop, }\FunctionTok{avg}\NormalTok{(population)}
\KeywordTok{as}\NormalTok{ avg_pop, }\FunctionTok{count}\NormalTok{(population) }\KeywordTok{as}\NormalTok{ count_pop}
\KeywordTok{from}\NormalTok{ metrics}
\KeywordTok{where}\NormalTok{ substring(state, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{) }\KeywordTok{in}\NormalTok{ (}\StringTok{'A'}\NormalTok{, }\StringTok{'B'}\NormalTok{, }\StringTok{'C'}\NormalTok{, }\StringTok{'D'}\NormalTok{, }\StringTok{'E'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The results from the above query only return one row as expected. Sometimes we want to find the aggregated value based on groups that can be defined by one or more columns. Instead of writing multiple SQL to calculate the aggregated value for each group, we can easily use the GROUP BY to calculate the aggregated value for each group in the \texttt{SELECT} statement. For example, if we want to find how many states in each division, we can use the following:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{select}\NormalTok{ division, }\FunctionTok{count}\NormalTok{(state) }\KeywordTok{as}\NormalTok{ number_of_states}
\KeywordTok{from}\NormalTok{ divisions}
\KeywordTok{group} \KeywordTok{by}\NormalTok{ division}
\end{Highlighting}
\end{Shaded}

Another special aggregation function is to return distinct values for one column or a combination of multiple columns. Simple use \texttt{SELECT\ DISTINCT\ col\_name1,\ col\_name2} in the first line of the \texttt{SELECT} statement.

\hypertarget{join-multiple-tables}{%
\subsubsection{Join Multiple Tables}\label{join-multiple-tables}}

The database system is usually designed such that each table contains a piece of specific information and oftentimes we need to JOIN multiple tables to achieve a specific task. There are few types typically JOINs: inner join (keep only rows that match the join condition from both tables), left outer join (rows from inner join + unmatched rows from the first table), right outer join (rows from inner join + unmatched rows from the second table) and full outer join (rows from inner join + unmatched rows from both tables). The typical \texttt{JOIN} statement is illustrated below:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{SELECT}\NormalTok{ a.col_name1 }\KeywordTok{as}\NormalTok{ var1, b.col_name2 }\KeywordTok{as}\NormalTok{ var2}
\KeywordTok{FROM}\NormalTok{ tbl_one }\KeywordTok{as}\NormalTok{ a}
\KeywordTok{LEFT} \KeywordTok{JOIN}\NormalTok{ tabl_two }\KeywordTok{as}\NormalTok{ b}
\KeywordTok{ON}\NormalTok{ a.col_to_match = b.col_to_match}
\end{Highlighting}
\end{Shaded}

For example, let us join the division table and metrics table to find what is the average population and income for each division, and the results order by division names:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{select}\NormalTok{ a.division, }\FunctionTok{avg}\NormalTok{(b.population) }\KeywordTok{as}\NormalTok{ avg_pop,}
\FunctionTok{avg}\NormalTok{(b.income) }\KeywordTok{as}\NormalTok{ avg_inc}
\KeywordTok{from}\NormalTok{ divisions }\KeywordTok{as}\NormalTok{ a}
\KeywordTok{inner} \KeywordTok{join}\NormalTok{ metrics }\KeywordTok{as}\NormalTok{ b}
\KeywordTok{on}\NormalTok{ a.state = b.state}
\KeywordTok{group} \KeywordTok{by}\NormalTok{ division}
\KeywordTok{order} \KeywordTok{by}\NormalTok{ division}
\end{Highlighting}
\end{Shaded}

\hypertarget{add-more-content-into-a-table}{%
\subsubsection{Add More Content into a Table}\label{add-more-content-into-a-table}}

We can use the \texttt{INSERT} statement to add additional rows into a particular table, for example, we can add one more row to the metrics table by using the following query:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{insert} \KeywordTok{into}\NormalTok{ metrics}
\KeywordTok{values}\NormalTok{ (}\StringTok{'Alberta'}\NormalTok{, }\DecValTok{4146}\NormalTok{, }\DecValTok{7370}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{advanced-topics-in-database}{%
\subsection{Advanced Topics in Database}\label{advanced-topics-in-database}}

There are many advanced topics such as how to efficiently query data using index; how to take care of data integrity when multiple users are using the same table; algorithm behind data storage (i.e.~column-wise or row-wise data storage); how to design the database schema. Users can learn these advanced topics gradually. We hope the basic knowledge covered in this section will kick off the initial momentum to learn SQL. As you can see, it is easy to write SQL statement to retrieve, join, slice, dice and aggregate data. The SQL notebook that contains all the above operations is included in the book's website.

\hypertarget{data-pre-processing}{%
\chapter{Data Pre-processing}\label{data-pre-processing}}

Many data analysis related books focus on models, algorithms and statistical inferences. However, in practice, raw data is usually not directly used for modeling. Data preprocessing is the process of converting raw data into clean data that is proper for modeling. A model fails for various reasons. One is that the modeler doesn't correctly preprocess data before modeling. Data preprocessing can significantly impact model results, such as imputing missing value and handling with outliers. So data preprocessing is a very critical part.

\begin{figure}
\centering
\includegraphics[width=0.9\textwidth,height=\textheight]{images/DataPre-processing.png}
\caption{Data Pre-processing Outline}
\end{figure}

In real life, depending on the stage of data cleanup, data has the following types:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Raw data
\item
  Technically correct data
\item
  Data that is proper for the model
\item
  Summarized data
\item
  Data with fixed format
\end{enumerate}

The raw data is the first-hand data that analysts pull from the database, market survey responds from your clients, the experimental results collected by the R \& D department, and so on. These data may be very rough, and R sometimes can't read them directly. The table title could be multi-line, or the format does not meet the requirements:

\begin{itemize}
\tightlist
\item
  Use 50\% to represent the percentage rather than 0.5, so R will read it as a character;
\item
  The missing value of the sales is represented by ``-'' instead of space so that R will treat the variable as character or factor type;
\item
  The data is in a slideshow document, or the spreadsheet is not ``.csv'' but ``.xlsx''
\item
  \ldots{}
\end{itemize}

Most of the time, you need to clean the data so that R can import them. Some data format requires a specific package. Technically correct data is the data, after preliminary cleaning or format conversion, that R (or another tool you use) can successfully import it.

Assume we have loaded the data into R with reasonable column names, variable format and so on. That does not mean the data is entirely correct. There may be some observations that do not make sense, such as age is negative, the discount percentage is greater than 1, or data is missing. Depending on the situation, there may be a variety of problems with the data. It is necessary to clean the data before modeling. Moreover, different models have different requirements on the data. For example, some model may require the variables are of consistent scale; some may be susceptible to outliers or collinearity, some may not be able to handle categorical variables and so on. The modeler has to preprocess the data to make it proper for the specific model.

Sometimes we need to aggregate the data. For example, add up the daily sales to get annual sales of a product at different locations. In customer segmentation, it is common practice to build a profile for each segment. It requires calculating some statistics such as average age, average income, age standard deviation, etc. Data aggregation is also necessary for presentation, or for data visualization.

The final table results for clients need to be in a nicer format than what used in the analysis. Usually, data analysts will take the results from data scientists and adjust the format, such as labels, cell color, highlight. It is important for a data scientist to make sure the results look consistent which makes the next step easier for data analysts.

It is highly recommended to store each step of the data and the R code, making the whole process as repeatable as possible. The R markdown reproducible report will be extremely helpful for that. If the data changes, it is easy to rerun the process. In the remainder of this chapter, we will show the most common data preprocessing methods.

Load the R packages first:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# install packages from CRAN}
\NormalTok{p_needed <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{'imputeMissings'}\NormalTok{,}\StringTok{'caret'}\NormalTok{,}\StringTok{'e1071'}\NormalTok{,}\StringTok{'psych'}\NormalTok{,}\StringTok{'car'}\NormalTok{,}\StringTok{'corrplot'}\NormalTok{)}
\NormalTok{packages <-}\StringTok{ }\KeywordTok{rownames}\NormalTok{(}\KeywordTok{installed.packages}\NormalTok{())}
\NormalTok{p_to_install <-}\StringTok{ }\NormalTok{p_needed[}\OperatorTok{!}\NormalTok{(p_needed }\OperatorTok{%in%}\StringTok{ }\NormalTok{packages)]}
\ControlFlowTok{if}\NormalTok{ (}\KeywordTok{length}\NormalTok{(p_to_install) }\OperatorTok{>}\StringTok{ }\DecValTok{0}\NormalTok{) \{}
    \KeywordTok{install.packages}\NormalTok{(p_to_install)}
\NormalTok{\}}

\KeywordTok{lapply}\NormalTok{(p_needed, require, }\DataTypeTok{character.only =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{data-cleaning}{%
\section{Data Cleaning}\label{data-cleaning}}

After you load the data, the first thing is to check how many variables are there, the type of variables, the distributions, and data errors. Let's read and check the data:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sim.dat <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"http://bit.ly/2P5gTw4"}\NormalTok{)}
\KeywordTok{summary}\NormalTok{(sim.dat)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
      age           gender        income       house       store_exp    
 Min.   : 16.0   Female:554   Min.   : 41776   No :432   Min.   : -500  
 1st Qu.: 25.0   Male  :446   1st Qu.: 85832   Yes:568   1st Qu.:  205  
 Median : 36.0                Median : 93869             Median :  329  
 Mean   : 38.8                Mean   :113543             Mean   : 1357  
 3rd Qu.: 53.0                3rd Qu.:124572             3rd Qu.:  597  
 Max.   :300.0                Max.   :319704             Max.   :50000  
                              NA's   :184                               
   online_exp    store_trans     online_trans        Q1            Q2      
 Min.   :  69   Min.   : 1.00   Min.   : 1.0   Min.   :1.0   Min.   :1.00  
 1st Qu.: 420   1st Qu.: 3.00   1st Qu.: 6.0   1st Qu.:2.0   1st Qu.:1.00  
 Median :1942   Median : 4.00   Median :14.0   Median :3.0   Median :1.00  
 Mean   :2120   Mean   : 5.35   Mean   :13.6   Mean   :3.1   Mean   :1.82  
 3rd Qu.:2441   3rd Qu.: 7.00   3rd Qu.:20.0   3rd Qu.:4.0   3rd Qu.:2.00  
 Max.   :9479   Max.   :20.00   Max.   :36.0   Max.   :5.0   Max.   :5.00  
                                                                           
       Q3             Q4             Q5             Q6             Q7      
 Min.   :1.00   Min.   :1.00   Min.   :1.00   Min.   :1.00   Min.   :1.00  
 1st Qu.:1.00   1st Qu.:2.00   1st Qu.:1.75   1st Qu.:1.00   1st Qu.:2.50  
 Median :1.00   Median :3.00   Median :4.00   Median :2.00   Median :4.00  
 Mean   :1.99   Mean   :2.76   Mean   :2.94   Mean   :2.45   Mean   :3.43  
 3rd Qu.:3.00   3rd Qu.:4.00   3rd Qu.:4.00   3rd Qu.:4.00   3rd Qu.:4.00  
 Max.   :5.00   Max.   :5.00   Max.   :5.00   Max.   :5.00   Max.   :5.00  
                                                                           
       Q8            Q9            Q10              segment   
 Min.   :1.0   Min.   :1.00   Min.   :1.00   Conspicuous:200  
 1st Qu.:1.0   1st Qu.:2.00   1st Qu.:1.00   Price      :250  
 Median :2.0   Median :4.00   Median :2.00   Quality    :200  
 Mean   :2.4   Mean   :3.08   Mean   :2.32   Style      :350  
 3rd Qu.:3.0   3rd Qu.:4.00   3rd Qu.:3.00                    
 Max.   :5.0   Max.   :5.00   Max.   :5.00                    
\end{verbatim}

Are there any problems? Questionnaire response Q1-Q10 seem reasonable, the minimum is 1 and maximum is 5. Recall that the questionnaire score is 1-5. The number of store transactions (\texttt{store\_trans}) and online transactions (\texttt{online\_trans}) make sense too. Things to pay attention are:

\begin{itemize}
\tightlist
\item
  There are some missing values.
\item
  There are outliers for store expenses (\texttt{store\_exp}). The maximum value is 50000. Who would spend \$50000 a year buying clothes? Is it an imputation error?
\item
  There is a negative value ( -500) in \texttt{store\_exp} which is not logical.
\item
  Someone is 300 years old.
\end{itemize}

How to deal with that? Depending on the real situation, if the sample size is large enough, it does not hurt to delete those problematic samples. Here we have 1000 observations. Since marketing survey is usually expensive, it is better to set these values as missing and impute them instead of deleting the rows.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# set problematic values as missings}
\NormalTok{sim.dat}\OperatorTok{$}\NormalTok{age[}\KeywordTok{which}\NormalTok{(sim.dat}\OperatorTok{$}\NormalTok{age }\OperatorTok{>}\StringTok{ }\DecValTok{100}\NormalTok{)] <-}\StringTok{ }\OtherTok{NA}
\NormalTok{sim.dat}\OperatorTok{$}\NormalTok{store_exp[}\KeywordTok{which}\NormalTok{(sim.dat}\OperatorTok{$}\NormalTok{store_exp }\OperatorTok{<}\StringTok{ }\DecValTok{0}\NormalTok{)] <-}\StringTok{ }\OtherTok{NA}
\CommentTok{# see the results}
\KeywordTok{summary}\NormalTok{(}\KeywordTok{subset}\NormalTok{(sim.dat, }\DataTypeTok{select =} \KeywordTok{c}\NormalTok{(}\StringTok{"age"}\NormalTok{, }\StringTok{"store_exp"}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
      age          store_exp      
 Min.   :16.00   Min.   :  155.8  
 1st Qu.:25.00   1st Qu.:  205.1  
 Median :36.00   Median :  329.8  
 Mean   :38.58   Mean   : 1358.7  
 3rd Qu.:53.00   3rd Qu.:  597.4  
 Max.   :69.00   Max.   :50000.0  
 NA's   :1       NA's   :1        
\end{verbatim}

Now let's deal with the missing values in the data.

\hypertarget{missing-values}{%
\section{Missing Values}\label{missing-values}}

You can write a whole book about missing value. This section will only show some of the most commonly used methods without getting too deep into the topic. Chapter 7 of the book by De Waal, Pannekoek and Scholtus \citep{Ton2011} makes a concise overview of some of the existing imputation methods. The choice of specific method depends on the actual situation. There is no best way.

One question to ask before imputation: Is there any auxiliary information? Being aware of any auxiliary information is critical. For example, if the system set customer who did not purchase as missing, then the real purchasing amount should be 0. Is missing a random occurrence? If so, it may be reasonable to impute with mean or median. If not, is there a potential mechanism for the missing data? For example, older people are more reluctant to disclose their ages in the questionnaire, so that the absence of age is not completely random. In this case, the missing values need to be estimated using the relationship between age and other independent variables. For example, use variables such as whether they have children, income, and other survey questions to build a model to predict age.

Also, the purpose of modeling is important for selecting imputation methods. If the goal is to interpret the parameter estimate or statistical inference, then it is important to study the missing mechanism carefully and to estimate the missing values using non-missing information as much as possible. If the goal is to predict, people usually will not study the absence mechanism rigorously (but sometimes the mechanism is obvious). If the absence mechanism is not clear, treat it as missing at random and use mean, median, or k-nearest neighbor to impute. Since statistical inference is sensitive to missing values, researchers from survey statistics have conducted in-depth studies of various imputation schemes which focus on valid statistical inference. The problem of missing values in the prediction model is different from that in the traditional survey. Therefore, there are not many papers on missing value imputation in the prediction model. Those who want to study further can refer to Saar-Tsechansky and Provost's comparison of different imputation methods \citep{missing1} and De Waal, Pannekoek and Scholtus' book \citep{Ton2011}.

\hypertarget{impute-missing-values-with-medianmode}{%
\subsection{Impute missing values with median/mode}\label{impute-missing-values-with-medianmode}}

In the case of missing at random, a common method is to impute with the mean (continuous variable) or median (categorical variables). You can use \texttt{impute()} function in \texttt{imputeMissings} package.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# save the result as another object}
\NormalTok{demo_imp <-}\StringTok{ }\KeywordTok{impute}\NormalTok{(sim.dat, }\DataTypeTok{method =} \StringTok{"median/mode"}\NormalTok{)}
\CommentTok{# check the first 5 columns, there is no missing values in other columns}
\KeywordTok{summary}\NormalTok{(demo_imp[, }\DecValTok{1}\OperatorTok{:}\DecValTok{5}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
      age           gender        income       house       store_exp      
 Min.   :16.00   Female:554   Min.   : 41776   No :432   Min.   :  155.8  
 1st Qu.:25.00   Male  :446   1st Qu.: 87896   Yes:568   1st Qu.:  205.1  
 Median :36.00                Median : 93869             Median :  329.8  
 Mean   :38.58                Mean   :109923             Mean   : 1357.7  
 3rd Qu.:53.00                3rd Qu.:119456             3rd Qu.:  597.3  
 Max.   :69.00                Max.   :319704             Max.   :50000.0
\end{verbatim}

After imputation, \texttt{demo\_imp} has no missing value. This method is straightforward and widely used. The disadvantage is that it does not take into account the relationship between the variables. When there is a significant proportion of missing, it will distort the data. In this case, it is better to consider the relationship between variables and study the missing mechanism. In the example here, the missing variables are numeric. If the missing variable is a categorical/factor variable, the \texttt{impute()} function will impute with the mode.

You can also use \texttt{preProcess()} in package \texttt{caret}, but it is only for numeric variables, and can not impute categorical variables. Since missing values here are numeric, we can use the \texttt{preProcess()} function. The result is the same as the \texttt{impute()} function. \texttt{PreProcess()} is a powerful function that can link to a variety of data preprocessing methods. We will use the function later for other data preprocessing.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{imp <-}\StringTok{ }\KeywordTok{preProcess}\NormalTok{(sim.dat, }\DataTypeTok{method =} \StringTok{"medianImpute"}\NormalTok{)}
\NormalTok{demo_imp2 <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(imp, sim.dat)}
\KeywordTok{summary}\NormalTok{(demo_imp2[, }\DecValTok{1}\OperatorTok{:}\DecValTok{5}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
      age           gender        income       house       store_exp      
 Min.   :16.00   Female:554   Min.   : 41776   No :432   Min.   :  155.8  
 1st Qu.:25.00   Male  :446   1st Qu.: 87896   Yes:568   1st Qu.:  205.1  
 Median :36.00                Median : 93869             Median :  329.8  
 Mean   :38.58                Mean   :109923             Mean   : 1357.7  
 3rd Qu.:53.00                3rd Qu.:119456             3rd Qu.:  597.3  
 Max.   :69.00                Max.   :319704             Max.   :50000.0  
\end{verbatim}

\hypertarget{k-nearest-neighbors}{%
\subsection{K-nearest neighbors}\label{k-nearest-neighbors}}

K-nearest neighbor (KNN) will find the k closest samples (Euclidian distance) in the training set and impute the mean of those ``neighbors.''

Use \texttt{preProcess()} to conduct KNN:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{imp <-}\StringTok{ }\KeywordTok{preProcess}\NormalTok{(sim.dat, }\DataTypeTok{method =} \StringTok{"knnImpute"}\NormalTok{, }\DataTypeTok{k =} \DecValTok{5}\NormalTok{)}
\CommentTok{# need to use predict() to get KNN result}
\NormalTok{demo_imp <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(imp, sim.dat)}
\CommentTok{# only show the first three elements}
\KeywordTok{lapply}\NormalTok{(sim.dat, class)[}\DecValTok{1}\OperatorTok{:}\DecValTok{3}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
      age                gender        income        
 Min.   :-1.5910972   Female:554   Min.   :-1.43989  
 1st Qu.:-0.9568733   Male  :446   1st Qu.:-0.53732  
 Median :-0.1817107                Median :-0.37606  
 Mean   : 0.0000156                Mean   : 0.02389  
 3rd Qu.: 1.0162678                3rd Qu.: 0.21540  
 Max.   : 2.1437770                Max.   : 4.13627 
\end{verbatim}

The \texttt{preProcess()} in the first line will automatically ignore non-numeric columns.

Comparing the KNN result with the previous median imputation, the two are very different. This is because when you tell the \texttt{preProcess()} function to use KNN (the option \texttt{method\ ="\ knnImpute"}), it will automatically standardize the data.
Another way is to use Bagging tree (in the next section). Note that KNN can not impute samples with the entire row missing. The reason is straightforward. Since the algorithm uses the average of its neighbors if none of them has a value, what does it apply to calculate the mean?

Let's append a new row with all values missing to the original data frame to get a new object called \texttt{temp}. Then apply KNN to \texttt{temp} and see what happens:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{temp <-}\StringTok{ }\KeywordTok{rbind}\NormalTok{(sim.dat, }\KeywordTok{rep}\NormalTok{(}\OtherTok{NA}\NormalTok{, }\KeywordTok{ncol}\NormalTok{(sim.dat)))}
\NormalTok{imp <-}\StringTok{ }\KeywordTok{preProcess}\NormalTok{(sim.dat, }\DataTypeTok{method =} \StringTok{"knnImpute"}\NormalTok{, }\DataTypeTok{k =} \DecValTok{5}\NormalTok{)}
\NormalTok{demo_imp <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(imp, temp)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Error in FUN(newX[, i], ...) : 
  cannot impute when all predictors are missing in the new data point
\end{verbatim}

There is an error saying ``\texttt{cannot\ impute\ when\ all\ predictors\ are\ missing\ in\ the\ new\ data\ point}''. It is easy to fix by finding and removing the problematic row(s):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{idx <-}\StringTok{ }\KeywordTok{apply}\NormalTok{(temp, }\DecValTok{1}\NormalTok{, }\ControlFlowTok{function}\NormalTok{(x) }\KeywordTok{sum}\NormalTok{(}\KeywordTok{is.na}\NormalTok{(x)))}
\KeywordTok{as.vector}\NormalTok{(}\KeywordTok{which}\NormalTok{(idx }\OperatorTok{==}\StringTok{ }\KeywordTok{ncol}\NormalTok{(temp)))}
\end{Highlighting}
\end{Shaded}

It shows that row 1001 is problematic. You can go ahead to delete it.

\hypertarget{bagging-tree}{%
\subsection{Bagging Tree}\label{bagging-tree}}

Bagging (Bootstrap aggregating) was originally proposed by Leo Breiman. It is one of the earliest ensemble methods \citep{bag1}. When used in missing value imputation, it will use the remaining variables as predictors to train a bagging tree and then use the tree to predict the missing values. Although theoretically, the method is powerful, the computation is much more intense than KNN. In practice, there is a trade-off between computation time and the effect. If a median or mean meet the modeling needs, even bagging tree may improve the accuracy a little, but the upgrade is so marginal that it does not deserve the extra time. The bagging tree itself is a model for regression and classification. Here we use \texttt{preProcess()} to impute \texttt{sim.dat}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{imp <-}\StringTok{ }\KeywordTok{preProcess}\NormalTok{(sim.dat, }\DataTypeTok{method =} \StringTok{"bagImpute"}\NormalTok{)}
\NormalTok{demo_imp <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(imp, sim.dat)}
\KeywordTok{summary}\NormalTok{(demo_imp[, }\DecValTok{1}\OperatorTok{:}\DecValTok{5}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
      age           gender        income       house       store_exp      
 Min.   :16.00   Female:554   Min.   : 41776   No :432   Min.   :  155.8  
 1st Qu.:25.00   Male  :446   1st Qu.: 86762   Yes:568   1st Qu.:  205.1  
 Median :36.00                Median : 94739             Median :  329.0  
 Mean   :38.58                Mean   :114665             Mean   : 1357.7  
 3rd Qu.:53.00                3rd Qu.:123726             3rd Qu.:  597.3  
 Max.   :69.00                Max.   :319704             Max.   :50000.0  
\end{verbatim}

\hypertarget{centering-and-scaling}{%
\section{Centering and Scaling}\label{centering-and-scaling}}

It is the most straightforward data transformation. It centers and scales a variable to mean 0 and standard deviation 1. It ensures that the criterion for finding linear combinations of the predictors is based on how much variation they explain and therefore improves the numerical stability. Models involving finding linear combinations of the predictors to explain response/predictors variation need data centering and scaling, such as PCA \citep{pca1}, PLS \citep{PLS1} and EFA \citep{EFA1}. You can quickly write code yourself to conduct this transformation.

Let's standardize the variable \texttt{income} from \texttt{sim.dat}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{income <-}\StringTok{ }\NormalTok{sim.dat}\OperatorTok{$}\NormalTok{income}
\CommentTok{# calculate the mean of income}
\NormalTok{mux <-}\StringTok{ }\KeywordTok{mean}\NormalTok{(income, }\DataTypeTok{na.rm =}\NormalTok{ T)}
\CommentTok{# calculate the standard deviation of income}
\NormalTok{sdx <-}\StringTok{ }\KeywordTok{sd}\NormalTok{(income, }\DataTypeTok{na.rm =}\NormalTok{ T)}
\CommentTok{# centering}
\NormalTok{tr1 <-}\StringTok{ }\NormalTok{income }\OperatorTok{-}\StringTok{ }\NormalTok{mux}
\CommentTok{# scaling}
\NormalTok{tr2 <-}\StringTok{ }\NormalTok{tr1}\OperatorTok{/}\NormalTok{sdx}
\end{Highlighting}
\end{Shaded}

Or the function \texttt{preProcess()} can apply this transformation to a set of predictors.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sdat <-}\StringTok{ }\KeywordTok{subset}\NormalTok{(sim.dat, }\DataTypeTok{select =} \KeywordTok{c}\NormalTok{(}\StringTok{"age"}\NormalTok{, }\StringTok{"income"}\NormalTok{))}
\CommentTok{# set the 'method' option}
\NormalTok{trans <-}\StringTok{ }\KeywordTok{preProcess}\NormalTok{(sdat, }\DataTypeTok{method =} \KeywordTok{c}\NormalTok{(}\StringTok{"center"}\NormalTok{, }\StringTok{"scale"}\NormalTok{))}
\CommentTok{# use predict() function to get the final result}
\NormalTok{transformed <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(trans, sdat)}
\end{Highlighting}
\end{Shaded}

Now the two variables are in the same scale. You can check the result using \texttt{summary(transformed)}. Note that there are missing values.

\hypertarget{resolve-skewness}{%
\section{Resolve Skewness}\label{resolve-skewness}}

\href{https://en.wikipedia.org/wiki/Skewness}{Skewness} is defined to be the third standardized central moment. The formula for the sample skewness statistics is:
\[ skewness=\frac{\sum(x_{i}-\bar{x})^{3}}{(n-1)v^{3/2}}\]
\[v=\frac{\sum(x_{i}-\bar{x})^{2}}{(n-1)}\]
Skewness=0 means that the destribution is symmetric, i.e.~the probability of falling on either side of the distribution's mean is equal.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# need skewness() function from e1071 package}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1000}\NormalTok{)}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{), }\DataTypeTok{oma =} \KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{))}
\CommentTok{# random sample 1000 chi-square distribution with df=2}
\CommentTok{# right skew}
\NormalTok{x1 <-}\StringTok{ }\KeywordTok{rchisq}\NormalTok{(}\DecValTok{1000}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DataTypeTok{ncp =} \DecValTok{0}\NormalTok{)}
\CommentTok{# get left skew variable x2 from x1}
\NormalTok{x2 <-}\StringTok{ }\KeywordTok{max}\NormalTok{(x1) }\OperatorTok{-}\StringTok{ }\NormalTok{x1}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{density}\NormalTok{(x2), }\DataTypeTok{main =} \KeywordTok{paste}\NormalTok{(}\StringTok{"left skew, skewnwss ="}\NormalTok{, }
    \KeywordTok{round}\NormalTok{(}\KeywordTok{skewness}\NormalTok{(x2), }\DecValTok{2}\NormalTok{)), }\DataTypeTok{xlab =} \StringTok{"X2"}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{density}\NormalTok{(x1), }\DataTypeTok{main =} \KeywordTok{paste}\NormalTok{(}\StringTok{"right skew, skewness ="}\NormalTok{, }
    \KeywordTok{round}\NormalTok{(}\KeywordTok{skewness}\NormalTok{(x1), }\DecValTok{2}\NormalTok{)), }\DataTypeTok{xlab =} \StringTok{"X1"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{IDS_files/figure-latex/unnamed-chunk-10-1.pdf}

There are different ways may help to remove skewness such as log, square root or inverse. However, it is often difficult to determine from plots which transformation is most appropriate for correcting skewness. The Box-Cox procedure automatically identified a transformation from the family of power transformations that are indexed by a parameter \(\lambda\)\citep{BOXCOX1}.

\[
x^{*}=\begin{cases}
\begin{array}{c}
\frac{x^{\lambda}-1}{\lambda}\\
log(x)
\end{array} & \begin{array}{c}
if\ \lambda\neq0\\
if\ \lambda=0
\end{array}\end{cases}
\]

It is easy to see that this family includes log transformation (\(\lambda=0\)), square transformation (\(\lambda=2\)), square root (\(\lambda=0.5\)), inverse (\(\lambda=-1\)) and others in-between. We can still use function \texttt{preProcess()} in package \texttt{caret} to apply this transformation by chaning the \texttt{method} argument.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{describe}\NormalTok{(sim.dat)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
             vars    n      mean       sd median   trimmed      mad ...
age             1 1000     38.84    16.42     36     37.69    16.31
gender*         2 1000      1.45     0.50      1      1.43     0.00
income          3  816 113543.07 49842.29  93869 104841.94 28989.47
house*          4 1000      1.57     0.50      2      1.58     0.00
store_exp       5 1000   1356.85  2774.40    329    839.92   196.45
online_exp      6 1000   2120.18  1731.22   1942   1874.51  1015.21
store_trans     7 1000      5.35     3.70      4      4.89     2.97
online_trans    8 1000     13.55     7.96     14     13.42    10.38
...
\end{verbatim}

It is easy to see the skewed variables. If \texttt{mean} and \texttt{trimmed} differ a lot, there is very likely outliers. By default, \texttt{trimmed} reports mean by dropping the top and bottom 10\%. It can be adjusted by setting argument \texttt{trim=}. It is clear that \texttt{store\_exp} has outliers.

As an example, we will apply Box-Cox transformation on \texttt{store\_trans} and \texttt{online\_trans}:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# select the two columns and save them as dat_bc}
\NormalTok{dat_bc <-}\StringTok{ }\KeywordTok{subset}\NormalTok{(sim.dat, }\DataTypeTok{select =} \KeywordTok{c}\NormalTok{(}\StringTok{"store_trans"}\NormalTok{, }\StringTok{"online_trans"}\NormalTok{))}
\NormalTok{(trans <-}\StringTok{ }\KeywordTok{preProcess}\NormalTok{(dat_bc, }\DataTypeTok{method =} \KeywordTok{c}\NormalTok{(}\StringTok{"BoxCox"}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Created from 1000 samples and 2 variables
## 
## Pre-processing:
##   - Box-Cox transformation (2)
##   - ignored (0)
## 
## Lambda estimates for Box-Cox transformation:
## 0.1, 0.7
\end{verbatim}

The last line of the output shows the estimates of \(\lambda\) for each variable. As before, use \texttt{predict()} to get the transformed result:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{transformed <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(trans, dat_bc)}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{), }\DataTypeTok{oma =} \KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{))}
\KeywordTok{hist}\NormalTok{(dat_bc}\OperatorTok{$}\NormalTok{store_trans, }\DataTypeTok{main =} \StringTok{"Before Transformation"}\NormalTok{, }
    \DataTypeTok{xlab =} \StringTok{"store_trans"}\NormalTok{)}
\KeywordTok{hist}\NormalTok{(transformed}\OperatorTok{$}\NormalTok{store_trans, }\DataTypeTok{main =} \StringTok{"After Transformation"}\NormalTok{, }
    \DataTypeTok{xlab =} \StringTok{"store_trans"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{IDS_files/figure-latex/unnamed-chunk-12-1.pdf}

Before the transformation, the \texttt{stroe\_trans} is skewed right.
\texttt{BoxCoxTrans\ ()} can also conduct Box-Cox transform. But note that \texttt{BoxCoxTrans\ ()} can only be applied to a single variable, and it is not possible to transform difference columns in a data frame at the same time.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(trans <-}\StringTok{ }\KeywordTok{BoxCoxTrans}\NormalTok{(dat_bc}\OperatorTok{$}\NormalTok{store_trans))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Box-Cox Transformation
## 
## 1000 data points used to estimate Lambda
## 
## Input data summary:
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##    1.00    3.00    4.00    5.35    7.00   20.00 
## 
## Largest/Smallest: 20 
## Sample Skewness: 1.11 
## 
## Estimated Lambda: 0.1 
## With fudge factor, Lambda = 0 will be used for transformations
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{transformed <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(trans, dat_bc}\OperatorTok{$}\NormalTok{store_trans)}
\KeywordTok{skewness}\NormalTok{(transformed)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] -0.2155
\end{verbatim}

The estimate of \(\lambda\) is the same as before (0.1). The skewness of the original observation is 1.1, and -0.2 after transformation. Although it is not strictly 0, it is greatly improved.

\hypertarget{resolve-outliers}{%
\section{Resolve Outliers}\label{resolve-outliers}}

Even under certain assumptions we can statistically define outliers, it can be hard to define in some situations. Box plot, histogram and some other basic visualizations can be used to initially check whether there are outliers. For example, we can visualize numerical non-survey variables in \texttt{sim.dat}:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# select numerical non-survey data}
\NormalTok{sdat <-}\StringTok{ }\KeywordTok{subset}\NormalTok{(sim.dat, }\DataTypeTok{select =} \KeywordTok{c}\NormalTok{(}\StringTok{"age"}\NormalTok{, }\StringTok{"income"}\NormalTok{, }\StringTok{"store_exp"}\NormalTok{, }
    \StringTok{"online_exp"}\NormalTok{, }\StringTok{"store_trans"}\NormalTok{, }\StringTok{"online_trans"}\NormalTok{))}
\CommentTok{# use scatterplotMatrix() function from car package}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{oma =} \KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{))}
\KeywordTok{scatterplotMatrix}\NormalTok{(sdat, }\DataTypeTok{diagonal =} \StringTok{"boxplot"}\NormalTok{, }\DataTypeTok{smoother =} \OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{IDS_files/figure-latex/unnamed-chunk-15-1.pdf}

It is also easy to observe the pair relationship from the plot. \texttt{age} is negatively correlated with \texttt{online\_trans} but positively correlated with \texttt{store\_trans}. It seems that older people tend to purchase from the local store. The amount of expense is positively correlated with income. Scatterplot matrix like this can reveal lots of information before modeling.

In addition to visualization, there are some statistical methods to define outliers, such as the commonly used Z-score. The Z-score for variable \(\mathbf{Y}\) is defined as:

\[Z_{i}=\frac{Y_{i}-\bar{Y}}{s}\]

where \(\bar{Y}\) and \(s\) are mean and standard deviation for \(Y\). Z-score is a measurement of the distance between each observation and the mean. This method may be misleading, especially when the sample size is small. Iglewicz and Hoaglin proposed to use the modified Z-score to determine the outlier\citep{mad1}

\[M_{i}=\frac{0.6745(Y_{i}-\bar{Y})}{MAD}\]

Where MAD is the median of a series of \(|Y_ {i} - \bar{Y}|\), called the median of the absolute dispersion. Iglewicz and Hoaglin suggest that the points with the Z-score greater than 3.5 corrected above are possible outliers. Let's apply it to \texttt{income}:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# calculate median of the absolute dispersion for income}
\NormalTok{ymad <-}\StringTok{ }\KeywordTok{mad}\NormalTok{(}\KeywordTok{na.omit}\NormalTok{(sdat}\OperatorTok{$}\NormalTok{income))}
\CommentTok{# calculate z-score}
\NormalTok{zs <-}\StringTok{ }\NormalTok{(sdat}\OperatorTok{$}\NormalTok{income }\OperatorTok{-}\StringTok{ }\KeywordTok{mean}\NormalTok{(}\KeywordTok{na.omit}\NormalTok{(sdat}\OperatorTok{$}\NormalTok{income)))}\OperatorTok{/}\NormalTok{ymad}
\CommentTok{# count the number of outliers}
\KeywordTok{sum}\NormalTok{(}\KeywordTok{na.omit}\NormalTok{(zs }\OperatorTok{>}\StringTok{ }\FloatTok{3.5}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 59
\end{verbatim}

According to modified Z-score, variable income has 59 outliers. Refer to \citep{mad1} for other ways of detecting outliers.

The impact of outliers depends on the model. Some models are sensitive to outliers, such as linear regression, logistic regression. Some are pretty robust to outliers, such as tree models, support vector machine. Also, the outlier is not wrong data. It is real observation so cannot be deleted at will. If a model is sensitive to outliers, we can use \emph{spatial sign transformation} \citep{ssp} to minimize the problem. It projects the original sample points to the surface of a sphere by:

\[x_{ij}^{*}=\frac{x_{ij}}{\sqrt{\sum_{j=1}^{p}x_{ij}^{2}}}\]

where \(x_{ij}\) represents the \(i^{th}\) observation and \(j^{th}\) variable. As shown in the equation, every observation for sample \(i\) is divided by its square mode. The denominator is the Euclidean distance to the center of the p-dimensional predictor space. Three things to pay attention here:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  It is important to center and scale the predictor data before using this transformation
\item
  Unlike centering or scaling, this manipulation of the predictors transforms them as a group
\item
  If there are some variables to remove (for example, highly correlated variables), do it before the transformation
\end{enumerate}

Function \texttt{spatialSign()} \texttt{caret} package can conduct the transformation. Take \texttt{income} and \texttt{age} as an example:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# KNN imputation}
\NormalTok{sdat <-}\StringTok{ }\NormalTok{sim.dat[, }\KeywordTok{c}\NormalTok{(}\StringTok{"income"}\NormalTok{, }\StringTok{"age"}\NormalTok{)]}
\NormalTok{imp <-}\StringTok{ }\KeywordTok{preProcess}\NormalTok{(sdat, }\DataTypeTok{method =} \KeywordTok{c}\NormalTok{(}\StringTok{"knnImpute"}\NormalTok{), }\DataTypeTok{k =} \DecValTok{5}\NormalTok{)}
\NormalTok{sdat <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(imp, sdat)}
\NormalTok{transformed <-}\StringTok{ }\KeywordTok{spatialSign}\NormalTok{(sdat)}
\NormalTok{transformed <-}\StringTok{ }\KeywordTok{as.data.frame}\NormalTok{(transformed)}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{), }\DataTypeTok{oma =} \KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{))}
\KeywordTok{plot}\NormalTok{(income }\OperatorTok{~}\StringTok{ }\NormalTok{age, }\DataTypeTok{data =}\NormalTok{ sdat, }\DataTypeTok{col =} \StringTok{"blue"}\NormalTok{, }\DataTypeTok{main =} \StringTok{"Before"}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(income }\OperatorTok{~}\StringTok{ }\NormalTok{age, }\DataTypeTok{data =}\NormalTok{ transformed, }\DataTypeTok{col =} \StringTok{"blue"}\NormalTok{, }\DataTypeTok{main =} \StringTok{"After"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{IDS_files/figure-latex/unnamed-chunk-17-1.pdf}

Some readers may have found that the above code does not seem to standardize the data before transformation. Recall the introduction of KNN, \texttt{preProcess()} with \texttt{method="knnImpute"} by default will standardize data.

\hypertarget{collinearity}{%
\section{Collinearity}\label{collinearity}}

It is probably the technical term known by the most un-technical people. When two predictors are very strongly correlated, including both in a model may lead to confusion or problem with a singular matrix. There is an excellent function in \texttt{corrplot} package with the same name \texttt{corrplot()} that can visualize correlation structure of a set of predictors. The function has the option to reorder the variables in a way that reveals clusters of highly correlated ones.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# select non-survey numerical variables}
\NormalTok{sdat <-}\StringTok{ }\KeywordTok{subset}\NormalTok{(sim.dat, }\DataTypeTok{select =} \KeywordTok{c}\NormalTok{(}\StringTok{"age"}\NormalTok{, }\StringTok{"income"}\NormalTok{, }\StringTok{"store_exp"}\NormalTok{, }
    \StringTok{"online_exp"}\NormalTok{, }\StringTok{"store_trans"}\NormalTok{, }\StringTok{"online_trans"}\NormalTok{))}
\CommentTok{# use bagging imputation here}
\NormalTok{imp <-}\StringTok{ }\KeywordTok{preProcess}\NormalTok{(sdat, }\DataTypeTok{method =} \StringTok{"bagImpute"}\NormalTok{)}
\NormalTok{sdat <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(imp, sdat)}
\CommentTok{# get the correlation matrix}
\NormalTok{correlation <-}\StringTok{ }\KeywordTok{cor}\NormalTok{(sdat)}
\CommentTok{# plot}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{oma =} \KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{))}
\KeywordTok{corrplot.mixed}\NormalTok{(correlation, }\DataTypeTok{order =} \StringTok{"hclust"}\NormalTok{, }\DataTypeTok{tl.pos =} \StringTok{"lt"}\NormalTok{, }
    \DataTypeTok{upper =} \StringTok{"ellipse"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{IDS_files/figure-latex/unnamed-chunk-18-1.pdf}

The closer the correlation is to 0, the lighter the color is and the closer the shape is to a circle. The elliptical means the correlation is not equal to 0 (because we set the \texttt{upper\ =\ "ellipse"}), the greater the correlation, the narrower the ellipse. Blue represents a positive correlation; red represents a negative correlation. The direction of the ellipse also changes with the correlation. The correlation coefficient is shown in the lower triangle of the matrix.

The variables relationship from previous scatter matrix are clear here: the negative correlation between age and online shopping, the positive correlation between income and amount of purchasing. Some correlation is very strong ( such as the correlation between \texttt{online\_trans} and\texttt{age} is -0.7414) which means the two variables contain duplicate information.

Section 3.5 of ``Applied Predictive Modeling'' \citep{APM} presents a heuristic algorithm to remove a minimum number of predictors to ensure all pairwise correlations are below a certain threshold:

\begin{quote}
\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\tightlist
\item
  Calculate the correlation matrix of the predictors.
\item
  Determine the two predictors associated with the largest absolute pairwise correlation (call them predictors A and B).
\item
  Determine the average correlation between A and the other variables. Do the same for predictor B.
\item
  If A has a larger average correlation, remove it; otherwise, remove predictor B.
\item
  Repeat Step 2-4 until no absolute correlations are above the threshold.
\end{enumerate}
\end{quote}

The \texttt{findCorrelation()} function in package \texttt{caret} will apply the above algorithm.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(highCorr <-}\StringTok{ }\KeywordTok{findCorrelation}\NormalTok{(}\KeywordTok{cor}\NormalTok{(sdat), }\DataTypeTok{cutoff =} \FloatTok{0.7}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 2 6
\end{verbatim}

It returns the index of columns need to be deleted. It tells us that we need to remove the first column to make sure the correlations are all below 0.7.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# delete highly correlated columns}
\NormalTok{sdat <-}\StringTok{ }\NormalTok{sdat[}\OperatorTok{-}\NormalTok{highCorr]}
\CommentTok{# check the new correlation matrix}
\NormalTok{(}\KeywordTok{cor}\NormalTok{(sdat))}
\end{Highlighting}
\end{Shaded}

The absolute value of the elements in the correlation matrix after removal are all below 0.7. How strong does a correlation have to get, before you should start worrying about multicollinearity? There is no easy answer to that question. You can treat the threshold as a tuning parameter and pick one that gives you best prediction accuracy.

\hypertarget{sparse-variables}{%
\section{Sparse Variables}\label{sparse-variables}}

Other than the highly related predictors, predictors with degenerate distributions can cause the problem too. Removing those variables can significantly improve some models' performance and stability (such as linear regression and logistic regression but the tree based model is impervious to this type of predictors). One extreme example is a variable with a single value which is called zero-variance variable. Variables with very low frequency of unique values are near-zero variance predictors. In general, detecting those variables follows two rules:

\begin{itemize}
\tightlist
\item
  The fraction of unique values over the sample size
\item
  The ratio of the frequency of the most prevalent value to the frequency of the second most prevalent value.
\end{itemize}

\texttt{nearZeroVar()} function in the \texttt{caret} package can filter near-zero variance predictors according to the above rules. In order to show the useage of the function, let's arbitaryly add some problematic variables to the origional data \texttt{sim.dat}:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# make a copy}
\NormalTok{zero_demo <-}\StringTok{ }\NormalTok{sim.dat}
\CommentTok{# add two sparse variable zero1 only has one unique value zero2 is a}
\CommentTok{# vector with the first element 1 and the rest are 0s}
\NormalTok{zero_demo}\OperatorTok{$}\NormalTok{zero1 <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{, }\KeywordTok{nrow}\NormalTok{(zero_demo))}
\NormalTok{zero_demo}\OperatorTok{$}\NormalTok{zero2 <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\KeywordTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, }\KeywordTok{nrow}\NormalTok{(zero_demo) }\OperatorTok{-}\StringTok{ }\DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

The function will return a vector of integers indicating which columns to remove:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{nearZeroVar}\NormalTok{(zero_demo,}\DataTypeTok{freqCut =} \DecValTok{95}\OperatorTok{/}\DecValTok{5}\NormalTok{, }\DataTypeTok{uniqueCut =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 20 21
\end{verbatim}

As expected, it returns the two columns we generated. You can go ahead to remove them. Note the two arguments in the function \texttt{freqCut\ =} and \texttt{uniqueCut\ =} are corresponding to the previous two rules.

\begin{itemize}
\tightlist
\item
  \texttt{freqCut}: the cutoff for the ratio of the most common value to the second most common value
\item
  \texttt{uniqueCut}: the cutoff for the percentage of distinct values out of the number of total samples
\end{itemize}

\hypertarget{re-encode-dummy-variables}{%
\section{Re-encode Dummy Variables}\label{re-encode-dummy-variables}}

A dummy variable is a binary variable (0/1) to represent subgroups of the sample. Sometimes we need to recode categories to smaller bits of information named ``dummy variables.'' For example, some questionnaires have five options for each question, A, B, C, D, and E. After you get the data, you will usually convert the corresponding categorical variables for each question into five nominal variables, and then use one of the options as the baseline.

Let's encode \texttt{gender} and \texttt{house} from \texttt{sim.dat} to dummy variables. There are two ways to implement this. The first is to use \texttt{class.ind()} from \texttt{nnet} package. However, it only works on one variable at a time.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dumVar <-}\StringTok{ }\NormalTok{nnet}\OperatorTok{::}\KeywordTok{class.ind}\NormalTok{(sim.dat}\OperatorTok{$}\NormalTok{gender)}
\KeywordTok{head}\NormalTok{(dumVar)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      Female Male
## [1,]      1    0
## [2,]      1    0
## [3,]      0    1
## [4,]      0    1
## [5,]      0    1
## [6,]      0    1
\end{verbatim}

Since it is redundant to keep both, we need to remove one of them when modeling. Another more powerful function is \texttt{dummyVars()} from \texttt{caret}:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# use "origional variable name + level" as new name}
\NormalTok{dumMod <-}\StringTok{ }\KeywordTok{dummyVars}\NormalTok{(}\OperatorTok{~}\NormalTok{gender }\OperatorTok{+}\StringTok{ }\NormalTok{house }\OperatorTok{+}\StringTok{ }\NormalTok{income, }
                    \DataTypeTok{data =}\NormalTok{ sim.dat, }
                    \DataTypeTok{levelsOnly =}\NormalTok{ F)}
\KeywordTok{head}\NormalTok{(}\KeywordTok{predict}\NormalTok{(dumMod, sim.dat))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   gender.Female gender.Male house.No house.Yes income
## 1             1           0        0         1 120963
## 2             1           0        0         1 122008
## 3             0           1        0         1 114202
## 4             0           1        0         1 113616
## 5             0           1        0         1 124253
## 6             0           1        0         1 107661
\end{verbatim}

\texttt{dummyVars()} can also use formula format. The variable on the right-hand side can be both categorical and numeric. For a numerical variable, the function will keep the variable unchanged. The advantage is that you can apply the function to a data frame without removing numerical variables. Other than that, the function can create interaction term:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dumMod <-}\StringTok{ }\KeywordTok{dummyVars}\NormalTok{(}\OperatorTok{~}\NormalTok{gender }\OperatorTok{+}\StringTok{ }\NormalTok{house }\OperatorTok{+}\StringTok{ }\NormalTok{income }\OperatorTok{+}\StringTok{ }\NormalTok{income}\OperatorTok{:}\NormalTok{gender, }
                    \DataTypeTok{data =}\NormalTok{ sim.dat, }
                    \DataTypeTok{levelsOnly =}\NormalTok{ F)}
\KeywordTok{head}\NormalTok{(}\KeywordTok{predict}\NormalTok{(dumMod, sim.dat))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   gender.Female gender.Male house.No house.Yes income
## 1             1           0        0         1 120963
## 2             1           0        0         1 122008
## 3             0           1        0         1 114202
## 4             0           1        0         1 113616
## 5             0           1        0         1 124253
## 6             0           1        0         1 107661
##   genderFemale:income genderMale:income
## 1              120963                 0
## 2              122008                 0
## 3                   0            114202
## 4                   0            113616
## 5                   0            124253
## 6                   0            107661
\end{verbatim}

If you think the impact income levels on purchasing behavior is different for male and female, then you may add the interaction term between \texttt{income} and \texttt{gender}. You can do this by adding \texttt{income:\ gender} in the formula.

\hypertarget{data-wrangling}{%
\chapter{Data Wrangling}\label{data-wrangling}}

This chapter focuses on some of the most frequently used data manipulations and shows how to implement them in R and Python. It is critical to explore the data with descriptive statistics (mean, standard deviation, etc.) and data visualization before analysis. Transform data so that the data structure is in line with the requirements of the model. You also need to summarize the results after analysis.

Load the R packages first:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# install packages from CRAN}
\NormalTok{p_needed <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{'readr'}\NormalTok{,}\StringTok{'dplyr'}\NormalTok{,}\StringTok{'data.table'}\NormalTok{,}\StringTok{'reshape2'}\NormalTok{,}\StringTok{'tidyr'}\NormalTok{)}
\NormalTok{packages <-}\StringTok{ }\KeywordTok{rownames}\NormalTok{(}\KeywordTok{installed.packages}\NormalTok{())}
\NormalTok{p_to_install <-}\StringTok{ }\NormalTok{p_needed[}\OperatorTok{!}\NormalTok{(p_needed }\OperatorTok{%in%}\StringTok{ }\NormalTok{packages)]}
\ControlFlowTok{if}\NormalTok{ (}\KeywordTok{length}\NormalTok{(p_to_install) }\OperatorTok{>}\StringTok{ }\DecValTok{0}\NormalTok{) \{}
    \KeywordTok{install.packages}\NormalTok{(p_to_install)}
\NormalTok{\}}

\KeywordTok{lapply}\NormalTok{(p_needed, require, }\DataTypeTok{character.only =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{read-and-write-data}{%
\section{Read and write data}\label{read-and-write-data}}

\hypertarget{readr}{%
\subsection{\texorpdfstring{\texttt{readr}}{readr}}\label{readr}}

You must be familiar with \texttt{read.csv()}, \texttt{read.table()} and \texttt{write.csv()} in base R. Here we will introduce a more efficient package from RStudio in 2015 for reading and writing data: \texttt{readr} package. The corresponding functions are \texttt{read\_csv()}, \texttt{read\_table()} and \texttt{write\_csv()}. The commands look quite similar, but \texttt{readr} is different in the following respects:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  It is 10x faster. The trick is that \texttt{readr} uses C++ to process the data quickly.
\item
  It doesn't change the column names. The names can start with a number and ``\texttt{.}'' will not be substituted to ``\texttt{\_}''. For example:
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{read_csv}\NormalTok{(}\StringTok{"2015,2016,2017}
\StringTok{1,2,3}
\StringTok{4,5,6"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 2 x 3
##   `2015` `2016` `2017`
##    <dbl>  <dbl>  <dbl>
## 1      1      2      3
## 2      4      5      6
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \texttt{readr} functions do not convert strings to factors by default, are able to parse dates and times and can automatically determine the data types in each column.
\item
  The killing character, in my opinion, is that \texttt{readr} provides \textbf{progress bar}. What makes you feel worse than waiting is not knowing how long you have to wait.
\end{enumerate}

\includegraphics{images/prograssbar.png}

The major functions of readr is to turn flat files into data frames:

\begin{itemize}
\tightlist
\item
  \texttt{read\_csv()}: reads comma delimited files
\item
  \texttt{read\_csv2()}: reads semicolon separated files (common in countries where \texttt{,} is used as the decimal place)
\item
  \texttt{read\_tsv()}: reads tab delimited files
\item
  \texttt{read\_delim()}: reads in files with any delimiter
\item
  \texttt{read\_fwf()}: reads fixed width files. You can specify fields either by their widths with \texttt{fwf\_widths()} or their position with \texttt{fwf\_positions()}\\
\item
  \texttt{read\_table()}: reads a common variation of fixed width files where columns are separated by white space
\item
  \texttt{read\_log()}: reads Apache style log files
\end{itemize}

The good thing is that those functions have similar syntax. Once you learn one, the others become easy. Here we will focus on \texttt{read\_csv()}.

The most important information for \texttt{read\_csv()} is the path to your data:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sim.dat <-}\StringTok{ }\KeywordTok{read_csv}\NormalTok{(}\StringTok{"http://bit.ly/2P5gTw4"}\NormalTok{)}
\KeywordTok{head}\NormalTok{(sim.dat)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 19
    age gender income house store_exp online_exp store_trans online_trans    Q1
  <int> <chr>   <dbl> <chr>     <dbl>      <dbl>       <int>        <int> <int>
1    57 Female 1.21e5 Yes        529.       304.           2            2     4
2    63 Female 1.22e5 Yes        478.       110.           4            2     4
3    59 Male   1.14e5 Yes        491.       279.           7            2     5
4    60 Male   1.14e5 Yes        348.       142.          10            2     5
5    51 Male   1.24e5 Yes        380.       112.           4            4     4
6    59 Male   1.08e5 Yes        338.       196.           4            5     4
# ... with 10 more variables: Q2 <int>, Q3 <int>, Q4 <int>, Q5 <int>, Q6 <int>,
#   Q7 <int>, Q8 <int>, Q9 <int>, Q10 <int>, segment <chr>
\end{verbatim}

The function reads the file to R as a \texttt{tibble}. You can consider \texttt{tibble} as next iteration of the data frame. They are different with data frame for the following aspects:

\begin{itemize}
\tightlist
\item
  It never changes an input's type (i.e., no more \texttt{stringsAsFactors\ =\ FALSE}!)
\item
  It never adjusts the names of variables
\item
  It has a refined print method that shows only the first 10 rows and all the columns that fit on the screen. You can also control the default print behavior by setting options.
\end{itemize}

Refer to \url{http://r4ds.had.co.nz/tibbles.html} for more information about `tibble'.

When you run \texttt{read\_csv()} it prints out a column specification that gives the name and type of each column. To better understanding how \texttt{readr} works, it is helpful to type in some baby data set and check the results:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dat <-}\StringTok{ }\KeywordTok{read_csv}\NormalTok{(}\StringTok{"2015,2016,2017}
\StringTok{100,200,300}
\StringTok{canola,soybean,corn"}\NormalTok{)}
\KeywordTok{print}\NormalTok{(dat)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 2 x 3
##   `2015` `2016`  `2017`
##   <chr>  <chr>   <chr> 
## 1 100    200     300   
## 2 canola soybean corn
\end{verbatim}

You can also add comments on the top and tell R to skip those lines:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dat <-}\StringTok{ }\KeywordTok{read_csv}\NormalTok{(}\StringTok{"# I will never let you know that}
\StringTok{          # my favorite food is carrot}
\StringTok{          Date,Food,Mood}
\StringTok{          Monday,carrot,happy}
\StringTok{          Tuesday,carrot,happy}
\StringTok{          Wednesday,carrot,happy}
\StringTok{          Thursday,carrot,happy}
\StringTok{          Friday,carrot,happy}
\StringTok{          Saturday,carrot,extremely happy}
\StringTok{          Sunday,carrot,extremely happy"}\NormalTok{, }
          \DataTypeTok{skip =} \DecValTok{2}\NormalTok{)}
\KeywordTok{print}\NormalTok{(dat)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 7 x 3
##   Date      Food   Mood           
##   <chr>     <chr>  <chr>          
## 1 Monday    carrot happy          
## 2 Tuesday   carrot happy          
## 3 Wednesday carrot happy          
## 4 Thursday  carrot happy          
## 5 Friday    carrot happy          
## 6 Saturday  carrot extremely happy
## 7 Sunday    carrot extremely happy
\end{verbatim}

If you don't have column names, set \texttt{col\_names\ =\ FALSE} then R will assign names ``\texttt{X1}'',``\texttt{X2}''\ldots{} to the columns:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dat <-}\StringTok{ }\KeywordTok{read_csv}\NormalTok{(}\StringTok{"Saturday,carrot,extremely happy}
\StringTok{          Sunday,carrot,extremely happy"}\NormalTok{, }\DataTypeTok{col_names =} \OtherTok{FALSE}\NormalTok{)}
\KeywordTok{print}\NormalTok{(dat)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 2 x 3
##   X1       X2     X3             
##   <chr>    <chr>  <chr>          
## 1 Saturday carrot extremely happy
## 2 Sunday   carrot extremely happy
\end{verbatim}

You can also pass \texttt{col\_names} a character vector which will be used as the column names. Try to replace \texttt{col\_names=FALSE} with \texttt{col\_names=c("Date","Food","Mood")} and see what happen.

As mentioned before, you can use \texttt{read\_csv2()} to read semicolon separated files:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dat <-}\StringTok{ }\KeywordTok{read_csv2}\NormalTok{(}\StringTok{"Saturday; carrot; extremely happy }\CharTok{\textbackslash{}n}\StringTok{ Sunday; carrot; extremely happy"}\NormalTok{, }\DataTypeTok{col_names =} \OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Using ',' as decimal and '.' as grouping mark. Use read_delim() for more control.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{print}\NormalTok{(dat)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 2 x 3
##   X1       X2     X3             
##   <chr>    <chr>  <chr>          
## 1 Saturday carrot extremely happy
## 2 Sunday   carrot extremely happy
\end{verbatim}

Here ``\texttt{\textbackslash{}n}'' is a convenient shortcut for adding a new line.

You can use \texttt{read\_tsv()} to read tab delimited files

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dat <-}\StringTok{ }\KeywordTok{read_tsv}\NormalTok{(}\StringTok{"every}\CharTok{\textbackslash{}t}\StringTok{man}\CharTok{\textbackslash{}t}\StringTok{is}\CharTok{\textbackslash{}t}\StringTok{a}\CharTok{\textbackslash{}t}\StringTok{poet}\CharTok{\textbackslash{}t}\StringTok{when}\CharTok{\textbackslash{}t}\StringTok{he}\CharTok{\textbackslash{}t}\StringTok{is}\CharTok{\textbackslash{}t}\StringTok{in}\CharTok{\textbackslash{}t}\StringTok{love}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{, }
    \DataTypeTok{col_names =} \OtherTok{FALSE}\NormalTok{)}
\KeywordTok{print}\NormalTok{(dat)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 10
##   X1    X2    X3    X4    X5    X6    X7    X8    X9   
##   <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr> <chr>
## 1 every man   is    a     poet  when  he    is    in   
## # ... with 1 more variable: X10 <chr>
\end{verbatim}

Or more generally, you can use \texttt{read\_delim()} and assign separating character

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dat <-}\StringTok{ }\KeywordTok{read_delim}\NormalTok{(}\StringTok{"THE|UNBEARABLE|RANDOMNESS|OF|LIFE}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{, }
    \DataTypeTok{delim =} \StringTok{"|"}\NormalTok{, }\DataTypeTok{col_names =} \OtherTok{FALSE}\NormalTok{)}
\KeywordTok{print}\NormalTok{(dat)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 5
##   X1    X2         X3         X4    X5   
##   <chr> <chr>      <chr>      <chr> <chr>
## 1 THE   UNBEARABLE RANDOMNESS OF    LIFE
\end{verbatim}

Another situation you will often run into is the missing value. In marketing survey, people like to use ``99'' to represent missing. You can tell R to set all observation with value ``99'' as missing when you read the data:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dat <-}\StringTok{ }\KeywordTok{read_csv}\NormalTok{(}\StringTok{"Q1,Q2,Q3}
\StringTok{               5, 4,99"}\NormalTok{, }
               \DataTypeTok{na =} \StringTok{"99"}\NormalTok{)}
\KeywordTok{print}\NormalTok{(dat)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 3
##      Q1    Q2 Q3   
##   <dbl> <dbl> <lgl>
## 1     5     4 NA
\end{verbatim}

For writing data back to disk, you can use \texttt{write\_csv()} and \texttt{write\_tsv()}. The following two characters of the two functions increase the chances of the output file being read back in correctly:

\begin{itemize}
\tightlist
\item
  Encode strings in UTF-8
\item
  Save dates and date-times in ISO8601 format so they are easily parsed elsewhere
\end{itemize}

For example:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{write_csv}\NormalTok{(sim.dat, }\StringTok{"sim_dat.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

For other data types, you can use the following packages:

\begin{itemize}
\tightlist
\item
  \texttt{Haven}: SPSS, Stata and SAS data
\item
  \texttt{Readxl} and \texttt{xlsx}: excel data(.xls and .xlsx)
\item
  \texttt{DBI}: given data base, such as RMySQL, RSQLite and RPostgreSQL, read data directly from the database using SQL
\end{itemize}

Some other useful materials:

\begin{itemize}
\tightlist
\item
  For getting data from the internet, you can refer to the book ``XML and Web Technologies for Data Sciences with R''.\\
\item
  \href{https://cran.r-project.org/doc/manuals/r-release/R-data.html\#Acknowledgements}{R data import/export manual}
\item
  \texttt{rio} package\url{https://github.com/leeper/rio}
\end{itemize}

\hypertarget{data.table-enhanced-data.frame}{%
\subsection{\texorpdfstring{\texttt{data.table}--- enhanced \texttt{data.frame}}{data.table--- enhanced data.frame}}\label{data.table-enhanced-data.frame}}

What is \texttt{data.table}? It is an R package that provides an enhanced version of \texttt{data.frame}. The most used object in R is \texttt{data\ frame}. Before we move on, let's briefly review some basic characters and manipulations of data.frame:

\begin{itemize}
\tightlist
\item
  It is a set of rows and columns.
\item
  Each row is of the same length and data type
\item
  Every column is of the same length but can be of differing data types
\item
  It has characteristics of both a matrix and a list
\item
  It uses \texttt{{[}{]}} to subset data
\end{itemize}

We will use the clothes customer data to illustrate. There are two dimensions in \texttt{{[}{]}}. The first one indicates the row and second one indicates column. It uses a comma to separate them.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# read data}
\NormalTok{sim.dat <-}\StringTok{ }\NormalTok{readr}\OperatorTok{::}\KeywordTok{read_csv}\NormalTok{(}\StringTok{"http://bit.ly/2P5gTw4"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Parsed with column specification:
## cols(
##   age = col_double(),
##   gender = col_character(),
##   income = col_double(),
##   house = col_character(),
##   store_exp = col_double(),
##   online_exp = col_double(),
##   store_trans = col_double(),
##   online_trans = col_double(),
##   Q1 = col_double(),
##   Q2 = col_double(),
##   Q3 = col_double(),
##   Q4 = col_double(),
##   Q5 = col_double(),
##   Q6 = col_double(),
##   Q7 = col_double(),
##   Q8 = col_double(),
##   Q9 = col_double(),
##   Q10 = col_double(),
##   segment = col_character()
## )
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# subset the first two rows}
\NormalTok{sim.dat[}\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{, ]}
\CommentTok{# subset the first two rows and column 3 and 5}
\NormalTok{sim.dat[}\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{, }\KeywordTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{5}\NormalTok{)]}
\CommentTok{# get all rows with age>70}
\NormalTok{sim.dat[sim.dat}\OperatorTok{$}\NormalTok{age }\OperatorTok{>}\StringTok{ }\DecValTok{70}\NormalTok{, ]}
\CommentTok{# get rows with age> 60 and gender is Male select column 3 and 4}
\NormalTok{sim.dat[sim.dat}\OperatorTok{$}\NormalTok{age }\OperatorTok{>}\StringTok{ }\DecValTok{68} \OperatorTok{&}\StringTok{ }\NormalTok{sim.dat}\OperatorTok{$}\NormalTok{gender }\OperatorTok{==}\StringTok{ "Male"}\NormalTok{, }\DecValTok{3}\OperatorTok{:}\DecValTok{4}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

Remember that there are usually different ways to conduct the same manipulation. For example, the following code presents three ways to calculate an average number of online transactions for male and female:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{tapply}\NormalTok{(sim.dat}\OperatorTok{$}\NormalTok{online_trans, sim.dat}\OperatorTok{$}\NormalTok{gender, mean)}

\KeywordTok{aggregate}\NormalTok{(online_trans }\OperatorTok{~}\StringTok{ }\NormalTok{gender, }\DataTypeTok{data =}\NormalTok{ sim.dat, mean)}

\NormalTok{sim.dat }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(gender) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{Avg_online_trans =} \KeywordTok{mean}\NormalTok{(online_trans))}
\end{Highlighting}
\end{Shaded}

There is no gold standard to choose a specific function to manipulate data. The goal is to solve the real problem, not the tool itself. So just use whatever tool that is convenient for you.

The way to use \texttt{{[}{]}} is straightforward. But the manipulations are limited. If you need more complicated data reshaping or aggregation, there are other packages to use such as \texttt{dplyr}, \texttt{reshape2}, \texttt{tidyr} etc. But the usage of those packages are not as straightforward as \texttt{{[}{]}}. You often need to change functions. Keeping related operations together, such as subset, group, update, join etc, will allow for:

\begin{itemize}
\tightlist
\item
  concise, consistent and readable syntax irrespective of the set of operations you would like to perform to achieve your end goal
\item
  performing data manipulation fluidly without the cognitive burden of having to change among different functions
\item
  by knowing precisely the data required for each operation, you can automatically optimize operations effectively
\end{itemize}

\texttt{data.table} is the package for that. If you are not familiar with other data manipulating packages and are interested in reducing programming time tremendously, then this package is for you.

Other than extending the function of \texttt{{[}{]}}, \texttt{data.table} has the following advantages:

\begin{itemize}
\tightlist
\item
  Offers fast import, subset, grouping, update, and joins for large data files
\item
  It is easy to turn data frame to data table
\item
  Can behave just like a data frame
\end{itemize}

You need to install and load the package:

Use \texttt{data.table()} to convert the existing data frame \texttt{sim.dat} to data table:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dt <-}\StringTok{ }\KeywordTok{data.table}\NormalTok{(sim.dat)}
\KeywordTok{class}\NormalTok{(dt)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "data.table" "data.frame"
\end{verbatim}

Calculate mean for counts of online transactions:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dt[, }\KeywordTok{mean}\NormalTok{(online_trans)]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 13.55
\end{verbatim}

You can't do the same thing using data frame:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sim.dat[,}\KeywordTok{mean}\NormalTok{(online_trans)]}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Error in mean(online_trans) : object 'online_trans' not found}
\end{Highlighting}
\end{Shaded}

If you want to calculate mean by group as before, set ``\texttt{by\ =}'' argument:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dt[ , }\KeywordTok{mean}\NormalTok{(online_trans), by =}\StringTok{ }\NormalTok{gender]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    gender    V1
## 1: Female 15.38
## 2:   Male 11.26
\end{verbatim}

You can group by more than one variables. For example, group by ``\texttt{gender}'' and ``\texttt{house}'':

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dt[ , }\KeywordTok{mean}\NormalTok{(online_trans), by =}\StringTok{ }\NormalTok{.(gender, house)]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    gender house     V1
## 1: Female   Yes 11.312
## 2:   Male   Yes  8.772
## 3: Female    No 19.146
## 4:   Male    No 16.486
\end{verbatim}

Assign column names for aggregated variables:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dt[ , .(}\DataTypeTok{avg =} \KeywordTok{mean}\NormalTok{(online_trans)), by =}\StringTok{ }\NormalTok{.(gender, house)]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    gender house    avg
## 1: Female   Yes 11.312
## 2:   Male   Yes  8.772
## 3: Female    No 19.146
## 4:   Male    No 16.486
\end{verbatim}

\texttt{data.table} can accomplish all operations that \texttt{aggregate()} and \texttt{tapply()}can do for data frame.

\begin{itemize}
\tightlist
\item
  General setting of \texttt{data.table}
\end{itemize}

Different from data frame, there are three arguments for data table:

\includegraphics{images/datable1.png}

It is analogous to SQL. You don't have to know SQL to learn data table. But experience with SQL will help you understand data table. In SQL, you select column \texttt{j} (use command \texttt{SELECT}) for row \texttt{i} (using command \texttt{WHERE}). \texttt{GROUP\ BY} in SQL will assign the variable to group the observations.

\includegraphics{images/rSQL.png}

Let's review our previous code:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dt[ , }\KeywordTok{mean}\NormalTok{(online_trans), by =}\StringTok{ }\NormalTok{gender]}
\end{Highlighting}
\end{Shaded}

The code above is equal to the following SQL

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{SELECT}\NormalTok{  gender, }\FunctionTok{avg}\NormalTok{(online_trans) }\KeywordTok{FROM}\NormalTok{ sim.dat }\KeywordTok{GROUP} \KeywordTok{BY}\NormalTok{ gender}
\end{Highlighting}
\end{Shaded}

R code:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dt[ , .(}\DataTypeTok{avg =} \KeywordTok{mean}\NormalTok{(online_trans)), by =}\StringTok{ }\NormalTok{.(gender, house)]}
\end{Highlighting}
\end{Shaded}

is equal to SQL

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{SELECT}\NormalTok{ gender, house, }\FunctionTok{avg}\NormalTok{(online_trans) }\KeywordTok{AS} \FunctionTok{avg} \KeywordTok{FROM}\NormalTok{ sim.dat }\KeywordTok{GROUP} \KeywordTok{BY}\NormalTok{ gender, house}
\end{Highlighting}
\end{Shaded}

R code

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dt[ age }\OperatorTok{<}\StringTok{ }\DecValTok{40}\NormalTok{, .(}\DataTypeTok{avg =} \KeywordTok{mean}\NormalTok{(online_trans)), by =}\StringTok{ }\NormalTok{.(gender, house)]}
\end{Highlighting}
\end{Shaded}

is equal to SQL

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{SELECT}\NormalTok{ gender, house, }\FunctionTok{avg}\NormalTok{(online_trans) }\KeywordTok{AS} \FunctionTok{avg} \KeywordTok{FROM}\NormalTok{ sim.dat }\KeywordTok{WHERE}\NormalTok{ age < }\DecValTok{40} \KeywordTok{GROUP} \KeywordTok{BY}\NormalTok{ gender, house}
\end{Highlighting}
\end{Shaded}

You can see the analogy between \texttt{data.table} and \texttt{SQL}. Now let's focus on operations in data table.

\begin{itemize}
\tightlist
\item
  select row
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# select rows with age<20 and income > 80000}
\NormalTok{dt[age }\OperatorTok{<}\StringTok{ }\DecValTok{20} \OperatorTok{&}\StringTok{ }\NormalTok{income }\OperatorTok{>}\StringTok{ }\DecValTok{80000}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    age gender income house store_exp online_exp
## 1:  19 Female  83535    No     227.7       1491
## 2:  18 Female  89416   Yes     209.5       1926
## 3:  19 Female  92813    No     186.7       1042
##    store_trans online_trans Q1 Q2 Q3 Q4 Q5 Q6 Q7 Q8 Q9
## 1:           1           22  2  1  1  2  4  1  4  2  4
## 2:           3           28  2  1  1  1  4  1  4  2  4
## 3:           2           18  3  1  1  2  4  1  4  3  4
##    Q10 segment
## 1:   1   Style
## 2:   1   Style
## 3:   1   Style
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# select the first two rows}
\NormalTok{dt[}\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    age gender income house store_exp online_exp
## 1:  57 Female 120963   Yes     529.1      303.5
## 2:  63 Female 122008   Yes     478.0      109.5
##    store_trans online_trans Q1 Q2 Q3 Q4 Q5 Q6 Q7 Q8 Q9
## 1:           2            2  4  2  1  2  1  4  1  4  2
## 2:           4            2  4  1  1  2  1  4  1  4  1
##    Q10 segment
## 1:   4   Price
## 2:   4   Price
\end{verbatim}

\begin{itemize}
\tightlist
\item
  select column
\end{itemize}

Selecting columns in \texttt{data.table} don't need \texttt{\$}:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# select column age but return it as a vector}
\CommentTok{# the argument for row is empty so the result will return all observations}
\NormalTok{ans <-}\StringTok{ }\NormalTok{dt[, age]}
\KeywordTok{head}\NormalTok{(ans)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 57 63 59 60 51 59
\end{verbatim}

To return \texttt{data.table} object, put column names in \texttt{list()}:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Select age and online_exp columns and return as a data.table instead}
\NormalTok{ans <-}\StringTok{ }\NormalTok{dt[, }\KeywordTok{list}\NormalTok{(age, online_exp)]}
\KeywordTok{head}\NormalTok{(ans)}
\end{Highlighting}
\end{Shaded}

Or you can also put column names in \texttt{.()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ans <-}\StringTok{ }\NormalTok{dt[, .(age, online_exp)]}
\CommentTok{# head(ans)}
\end{Highlighting}
\end{Shaded}

To select all columns from ``\texttt{age}'' to ``\texttt{income}'':

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ans <-}\StringTok{ }\NormalTok{dt[, age}\OperatorTok{:}\NormalTok{income, with =}\StringTok{ }\OtherTok{FALSE}\NormalTok{]}
\KeywordTok{head}\NormalTok{(ans,}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    age gender income
## 1:  57 Female 120963
## 2:  63 Female 122008
\end{verbatim}

Delete columns using \texttt{-} or \texttt{!}:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# delete columns from  age to online_exp}
\NormalTok{ans <-}\StringTok{ }\NormalTok{dt[, }\OperatorTok{-}\NormalTok{(age}\OperatorTok{:}\NormalTok{online_exp), with =}\StringTok{ }\OtherTok{FALSE}\NormalTok{]}
\NormalTok{ans <-}\StringTok{ }\NormalTok{dt[, }\OperatorTok{!}\NormalTok{(age}\OperatorTok{:}\NormalTok{online_exp), with =}\StringTok{ }\OtherTok{FALSE}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  tabulation
\end{itemize}

In data table. \texttt{.N} means to count

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# row count}
\NormalTok{dt[, .N] }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1000
\end{verbatim}

If you assign the group variable, then it will count by groups:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# counts by gender}
\NormalTok{dt[, .N, by=}\StringTok{ }\NormalTok{gender]  }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    gender   N
## 1: Female 554
## 2:   Male 446
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# for those younger than 30, count by gender}
\NormalTok{ dt[age }\OperatorTok{<}\StringTok{ }\DecValTok{30}\NormalTok{, .(}\DataTypeTok{count=}\NormalTok{.N), by=}\StringTok{ }\NormalTok{gender] }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    gender count
## 1: Female   292
## 2:   Male    86
\end{verbatim}

Order table:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# get records with the highest 5 online expense:}
\KeywordTok{head}\NormalTok{(dt[}\KeywordTok{order}\NormalTok{(}\OperatorTok{-}\NormalTok{online_exp)],}\DecValTok{5}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
   age gender   income house store_exp online_exp store_trans ...
1:  40 Female 217599.7    No  7023.684   9479.442          10
2:  41 Female       NA   Yes  3786.740   8638.239          14
3:  36   Male 228550.1   Yes  3279.621   8220.555           8
4:  31 Female 159508.1   Yes  5177.081   8005.932          11
5:  43 Female 190407.4   Yes  4694.922   7875.562           6
...
\end{verbatim}

Since data table keep some characters of data frame, they share some operations:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dt[}\KeywordTok{order}\NormalTok{(}\OperatorTok{-}\NormalTok{online_exp)][}\DecValTok{1}\OperatorTok{:}\DecValTok{5}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

You can also order the table by more than one variable. The following code will order the table by \texttt{gender}, then order within \texttt{gender} by \texttt{online\_exp}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dt[}\KeywordTok{order}\NormalTok{(gender, }\OperatorTok{-}\NormalTok{online_exp)][}\DecValTok{1}\OperatorTok{:}\DecValTok{5}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  Use \texttt{fread()} to import dat
\end{itemize}

Other than \texttt{read.csv} in base R, we have introduced `read\_csv' in `readr'. \texttt{read\_csv} is much faster and will provide progress bar which makes user feel much better (at least make me feel better). \texttt{fread()} in \texttt{data.table} further increase the efficiency of reading data. The following are three examples of reading the same data file \texttt{topic.csv}. The file includes text data scraped from an agriculture forum with 209670 rows and 6 columns:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{system.time}\NormalTok{(topic<-}\KeywordTok{read.csv}\NormalTok{(}\StringTok{"http://bit.ly/2zam5ny"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{   user  system elapsed }
\NormalTok{  3.561   0.051   4.888 }
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{system.time}\NormalTok{(topic<-readr}\OperatorTok{::}\KeywordTok{read_csv}\NormalTok{(}\StringTok{"http://bit.ly/2zam5ny"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{   user  system elapsed }
\NormalTok{  0.409   0.032   2.233 }
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{system.time}\NormalTok{(topic<-data.table}\OperatorTok{::}\KeywordTok{fread}\NormalTok{(}\StringTok{"http://bit.ly/2zam5ny"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{   user  system elapsed }
\NormalTok{  0.276   0.096   1.117 }
\end{Highlighting}
\end{Shaded}

It is clear that \texttt{read\_csv()} is much faster than \texttt{read.csv()}. \texttt{fread()} is a little faster than \texttt{read\_csv()}. As the size increasing, the difference will become for significant. Note that \texttt{fread()} will read file as \texttt{data.table} by default.

\hypertarget{summarize-data}{%
\section{Summarize data}\label{summarize-data}}

\hypertarget{apply-lapply-and-sapply-in-base-r}{%
\subsection{\texorpdfstring{\texttt{apply()}, \texttt{lapply()} and \texttt{sapply()} in base R}{apply(), lapply() and sapply() in base R}}\label{apply-lapply-and-sapply-in-base-r}}

There are some powerful functions to summarize data in base R, such as \texttt{apply()}, \texttt{lapply()} and \texttt{sapply()}. They do the same basic things and are all from ``apply'' family: apply functions over parts of data. They differ in two important respects:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  the type of object they apply to
\item
  the type of result they will return
\end{enumerate}

When do we use \texttt{apply()}? When we want to apply a function to margins of an array or matrix. That means our data need to be structured. The operations can be very flexible. It returns a vector or array or list of values obtained by applying a function to margins of an array or matrix.

For example you can compute row and column sums for a matrix:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## simulate a matrix}
\NormalTok{x <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(}\DataTypeTok{x1 =}\DecValTok{1}\OperatorTok{:}\DecValTok{8}\NormalTok{, }\DataTypeTok{x2 =} \KeywordTok{c}\NormalTok{(}\DecValTok{4}\OperatorTok{:}\DecValTok{1}\NormalTok{, }\DecValTok{2}\OperatorTok{:}\DecValTok{5}\NormalTok{))}
\KeywordTok{dimnames}\NormalTok{(x)[[}\DecValTok{1}\NormalTok{]] <-}\StringTok{ }\NormalTok{letters[}\DecValTok{1}\OperatorTok{:}\DecValTok{8}\NormalTok{]}
\KeywordTok{apply}\NormalTok{(x, }\DecValTok{2}\NormalTok{, mean)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  x1  x2 
## 4.5 3.0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{col.sums <-}\StringTok{ }\KeywordTok{apply}\NormalTok{(x, }\DecValTok{2}\NormalTok{, sum)}
\NormalTok{row.sums <-}\StringTok{ }\KeywordTok{apply}\NormalTok{(x, }\DecValTok{1}\NormalTok{, sum)}
\end{Highlighting}
\end{Shaded}

You can also apply other functions:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ma <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{4}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{6}\OperatorTok{:}\DecValTok{8}\NormalTok{), }\DataTypeTok{nrow =} \DecValTok{2}\NormalTok{)}
\NormalTok{ma}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1] [,2] [,3] [,4]
## [1,]    1    3    1    7
## [2,]    2    4    6    8
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{apply}\NormalTok{(ma, }\DecValTok{1}\NormalTok{, table)  }\CommentTok{#--> a list of length 2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [[1]]
## 
## 1 3 7 
## 2 1 1 
## 
## [[2]]
## 
## 2 4 6 8 
## 1 1 1 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{apply}\NormalTok{(ma, }\DecValTok{1}\NormalTok{, stats}\OperatorTok{::}\NormalTok{quantile) }\CommentTok{# 5 x n matrix with rownames}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1] [,2]
## 0%      1  2.0
## 25%     1  3.5
## 50%     2  5.0
## 75%     4  6.5
## 100%    7  8.0
\end{verbatim}

Results can have different lengths for each call. This is a trickier example. What will you get?

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{## Example with different lengths for each call}
\NormalTok{z <-}\StringTok{ }\KeywordTok{array}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{24}\NormalTok{, }\DataTypeTok{dim =} \DecValTok{2}\OperatorTok{:}\DecValTok{4}\NormalTok{)}
\NormalTok{zseq <-}\StringTok{ }\KeywordTok{apply}\NormalTok{(z, }\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{, }\ControlFlowTok{function}\NormalTok{(x) }\KeywordTok{seq_len}\NormalTok{(}\KeywordTok{max}\NormalTok{(x)))}
\NormalTok{zseq         }\CommentTok{## a 2 x 3 matrix}
\KeywordTok{typeof}\NormalTok{(zseq) }\CommentTok{## list}
\KeywordTok{dim}\NormalTok{(zseq) }\CommentTok{## 2 3}
\NormalTok{zseq[}\DecValTok{1}\NormalTok{,]}
\KeywordTok{apply}\NormalTok{(z, }\DecValTok{3}\NormalTok{, }\ControlFlowTok{function}\NormalTok{(x) }\KeywordTok{seq_len}\NormalTok{(}\KeywordTok{max}\NormalTok{(x)))}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  \texttt{lapply()} applies a function over a list, data.frame or vector and returns a list of the same length.
\item
  \texttt{sapply()} is a user-friendly version and wrapper of \texttt{lapply()}. By default it returns a vector, matrix or if \texttt{simplify\ =\ "array"}, an array if appropriate. \texttt{apply(x,\ f,\ simplify\ =\ FALSE,\ USE.NAMES\ =\ FALSE)} is the same as \texttt{lapply(x,\ f)}. If \texttt{simplify=TRUE}, then it will return a \texttt{data.frame} instead of \texttt{list}.
\end{itemize}

Let's use some data with context to help you better understand the functions.

\begin{itemize}
\tightlist
\item
  Get the mean and standard deviation of all numerical variables in the dataset.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Read data}
\NormalTok{sim.dat <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"http://bit.ly/2P5gTw4"}\NormalTok{)}
\CommentTok{# Get numerical variables}
\NormalTok{sdat <-}\StringTok{ }\NormalTok{sim.dat[, }\OperatorTok{!}\KeywordTok{lapply}\NormalTok{(sim.dat, class) }\OperatorTok{==}\StringTok{ "factor"}\NormalTok{]}
\CommentTok{## Try the following code with apply() function apply(sim.dat,2,class)}
\CommentTok{## What is the problem?}
\end{Highlighting}
\end{Shaded}

The data frame \texttt{sdat} only includes numeric columns. Now we can go head and use \texttt{apply()} to get mean and standard deviation for each column:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{apply}\NormalTok{(sdat, }\DataTypeTok{MARGIN=}\DecValTok{2}\NormalTok{,}\ControlFlowTok{function}\NormalTok{(x) }\KeywordTok{mean}\NormalTok{(}\KeywordTok{na.omit}\NormalTok{(x)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          age       income    store_exp   online_exp 
##    3.884e+01    1.135e+05    1.357e+03    2.120e+03 
##  store_trans online_trans           Q1           Q2 
##    5.350e+00    1.355e+01    3.101e+00    1.823e+00 
##           Q3           Q4           Q5           Q6 
##    1.992e+00    2.763e+00    2.945e+00    2.448e+00 
##           Q7           Q8           Q9          Q10 
##    3.434e+00    2.396e+00    3.085e+00    2.320e+00
\end{verbatim}

Here we defined a function using \texttt{function(x)\ mean(na.omit(x))}. It is a very simple function. It tells R to ignore the missing value when calculating the mean. \texttt{MARGIN=2} tells R to apply the function to each column. It is not hard to guess what \texttt{MARGIN=1} mean. The result show that the average online expense is much higher than store expense. You can also compare the average scores across different questions. The command to calculate standard deviation is very similar. The only difference is to change \texttt{mean()} to \texttt{sd()}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{apply}\NormalTok{(sdat, }\DataTypeTok{MARGIN=}\DecValTok{2}\NormalTok{,}\ControlFlowTok{function}\NormalTok{(x) }\KeywordTok{sd}\NormalTok{(}\KeywordTok{na.omit}\NormalTok{(x)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          age       income    store_exp   online_exp 
##       16.417    49842.287     2774.400     1731.224 
##  store_trans online_trans           Q1           Q2 
##        3.696        7.957        1.450        1.168 
##           Q3           Q4           Q5           Q6 
##        1.402        1.155        1.284        1.439 
##           Q7           Q8           Q9          Q10 
##        1.456        1.154        1.118        1.136
\end{verbatim}

Even the average online expense is higher than store expense, the standard deviation for store expense is much higher than online expense which indicates there is very likely some big/small purchase in store. We can check it quickly:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(sdat}\OperatorTok{$}\NormalTok{store_exp)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##    -500     205     329    1357     597   50000
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(sdat}\OperatorTok{$}\NormalTok{online_exp)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##      69     420    1942    2120    2441    9479
\end{verbatim}

There are some odd values in store expense. The minimum value is -500 which indicates that you should preprocess data before analyzing it. Checking those simple statistics will help you better understand your data. It then gives you some idea how to preprocess and analyze them. How about using \texttt{lapply()} and \texttt{sapply()}?

Run the following code and compare the results:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{lapply}\NormalTok{(sdat, }\ControlFlowTok{function}\NormalTok{(x) }\KeywordTok{sd}\NormalTok{(}\KeywordTok{na.omit}\NormalTok{(x)))}
\KeywordTok{sapply}\NormalTok{(sdat, }\ControlFlowTok{function}\NormalTok{(x) }\KeywordTok{sd}\NormalTok{(}\KeywordTok{na.omit}\NormalTok{(x)))}
\KeywordTok{sapply}\NormalTok{(sdat, }\ControlFlowTok{function}\NormalTok{(x) }\KeywordTok{sd}\NormalTok{(}\KeywordTok{na.omit}\NormalTok{(x)), }\DataTypeTok{simplify =} \OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{dplyr-package}{%
\subsection{\texorpdfstring{\texttt{dplyr} package}{dplyr package}}\label{dplyr-package}}

\texttt{dplyr} provides a flexible grammar of data manipulation focusing on tools for working with data frames (hence the \texttt{d} in the name). It is faster and more friendly:

\begin{itemize}
\tightlist
\item
  It identifies the most important data manipulations and make they easy to use from R
\item
  It performs faster for in-memory data by writing key pieces in C++ using \texttt{Rcpp}
\item
  The interface is the same for data frame, data table or database.
\end{itemize}

We will illustrate the following functions in order:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Display
\item
  Subset
\item
  Summarize
\item
  Create new variable
\item
  Merge
\end{enumerate}

\textbf{Display}

\begin{itemize}
\tightlist
\item
  \texttt{tbl\_df()}: Convert the data to \texttt{tibble} which offers better checking and printing capabilities than traditional data frames. It will adjust output width according to fit the current window.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{tbl_df}\NormalTok{(sim.dat)}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  \texttt{glimpse()}: This is like a transposed version of \texttt{tbl\_df()}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{glimpse}\NormalTok{(sim.dat)}
\end{Highlighting}
\end{Shaded}

\textbf{Subset}

Get rows with \texttt{income} more than 300000:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{filter}\NormalTok{(sim.dat, income }\OperatorTok{>}\DecValTok{300000}\NormalTok{) }\OperatorTok{%>%}
\StringTok{  }\KeywordTok{tbl_df}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

Here we meet a new operator \texttt{\%\textgreater{}\%}. It is called ``Pipe operator'' which pipes a value forward into an expression or function call. What you get in the left operation will be the first argument or the only argument in the right operation.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OperatorTok{%>%}\StringTok{ }\KeywordTok{f}\NormalTok{(y) =}\StringTok{ }\KeywordTok{f}\NormalTok{(x, y)}
\NormalTok{y }\OperatorTok{%>%}\StringTok{ }\KeywordTok{f}\NormalTok{(x, ., z) =}\StringTok{ }\KeywordTok{f}\NormalTok{(x, y, z )}
\end{Highlighting}
\end{Shaded}

It is an operator from \texttt{magrittr} which can be really beneficial. Look at the following code. Can you tell me what it does?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ave_exp <-}\StringTok{ }\KeywordTok{filter}\NormalTok{( }
  \KeywordTok{summarise}\NormalTok{(}
    \KeywordTok{group_by}\NormalTok{( }
      \KeywordTok{filter}\NormalTok{(}
\NormalTok{        sim.dat, }
        \OperatorTok{!}\KeywordTok{is.na}\NormalTok{(income)}
\NormalTok{      ), }
\NormalTok{      segment}
\NormalTok{    ), }
    \DataTypeTok{ave_online_exp =} \KeywordTok{mean}\NormalTok{(online_exp), }
    \DataTypeTok{n =} \KeywordTok{n}\NormalTok{()}
\NormalTok{  ), }
\NormalTok{  n }\OperatorTok{>}\StringTok{ }\DecValTok{200}
\NormalTok{) }
\end{Highlighting}
\end{Shaded}

Now look at the identical code using ``\texttt{\%\textgreater{}\%}'':

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ave_exp <-}\StringTok{ }\NormalTok{sim.dat }\OperatorTok{%>%}\StringTok{ }
\StringTok{ }\KeywordTok{filter}\NormalTok{(}\OperatorTok{!}\KeywordTok{is.na}\NormalTok{(income)) }\OperatorTok{%>%}\StringTok{ }
\StringTok{ }\KeywordTok{group_by}\NormalTok{(segment) }\OperatorTok{%>%}\StringTok{ }
\StringTok{ }\KeywordTok{summarise}\NormalTok{( }
   \DataTypeTok{ave_online_exp =} \KeywordTok{mean}\NormalTok{(online_exp), }
   \DataTypeTok{n =} \KeywordTok{n}\NormalTok{() ) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(n }\OperatorTok{>}\StringTok{ }\DecValTok{200}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Isn't it much more straightforward now? Let's read it:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Delete observations from \texttt{sim.dat} with missing income values
\item
  Group the data from step 1 by variable \texttt{segment}
\item
  Calculate mean of online expense for each segment and save the result as a new variable named \texttt{ave\_online\_exp}
\item
  Calculate the size of each segment and saved it as a new variable named \texttt{n}
\item
  Get segments with size larger than 200
\end{enumerate}

You can use \texttt{distinct()} to delete duplicated rows.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{distinct}\NormalTok{(sim.dat)}
\end{Highlighting}
\end{Shaded}

\texttt{sample\_frac()} will randomly select some rows with a specified percentage. \texttt{sample\_n()} can randomly select rows with a specified number.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{sample_frac}\NormalTok{(sim.dat, }\FloatTok{0.5}\NormalTok{, }\DataTypeTok{replace =} \OtherTok{TRUE}\NormalTok{) }
\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{sample_n}\NormalTok{(sim.dat, }\DecValTok{10}\NormalTok{, }\DataTypeTok{replace =} \OtherTok{TRUE}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\texttt{slice()} will select rows by position:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{slice}\NormalTok{(sim.dat, }\DecValTok{10}\OperatorTok{:}\DecValTok{15}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

It is equivalent to \texttt{sim.dat{[}10:15,{]}}.

\texttt{top\_n()} will select the order top n entries:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{top_n}\NormalTok{(sim.dat,}\DecValTok{2}\NormalTok{,income)}
\end{Highlighting}
\end{Shaded}

If you want to select columns instead of rows, you can use \texttt{select()}. The following are some sample codes:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# select by column name}
\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{select}\NormalTok{(sim.dat,income,age,store_exp)}

\CommentTok{# select columns whose name contains a character string}
\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{select}\NormalTok{(sim.dat, }\KeywordTok{contains}\NormalTok{(}\StringTok{"_"}\NormalTok{))}

\CommentTok{# select columns whose name ends with a character string}
\CommentTok{# similar there is "starts_with"}
\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{select}\NormalTok{(sim.dat, }\KeywordTok{ends_with}\NormalTok{(}\StringTok{"e"}\NormalTok{))}

\CommentTok{# select columns Q1,Q2,Q3,Q4 and Q5}
\KeywordTok{select}\NormalTok{(sim.dat, }\KeywordTok{num_range}\NormalTok{(}\StringTok{"Q"}\NormalTok{, }\DecValTok{1}\OperatorTok{:}\DecValTok{5}\NormalTok{)) }

\CommentTok{# select columns whose names are in a group of names}
\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{select}\NormalTok{(sim.dat, }\KeywordTok{one_of}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"age"}\NormalTok{, }\StringTok{"income"}\NormalTok{)))}

\CommentTok{# select columns between age and online_exp}
\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{select}\NormalTok{(sim.dat, age}\OperatorTok{:}\NormalTok{online_exp)}

\CommentTok{# select all columns except for age}
\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{select}\NormalTok{(sim.dat, }\OperatorTok{-}\NormalTok{age)}
\end{Highlighting}
\end{Shaded}

\textbf{Summarize}

A standard marketing problem is customer segmentation. It usually starts with designing survey and collecting data. Then run a cluster analysis on the data to get customer segments. Once we have different segments, the next is to understand how each group of customer look like by summarizing some key metrics. For example, we can do the following data aggregation for different segments of clothes customers.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sim.dat }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(segment) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{Age =} \KeywordTok{round}\NormalTok{(}\KeywordTok{mean}\NormalTok{(}\KeywordTok{na.omit}\NormalTok{(age)), }\DecValTok{0}\NormalTok{), }
            \DataTypeTok{FemalePct =} \KeywordTok{round}\NormalTok{(}\KeywordTok{mean}\NormalTok{(gender }\OperatorTok{==}\StringTok{ "Female"}\NormalTok{), }\DecValTok{2}\NormalTok{), }
            \DataTypeTok{HouseYes =} \KeywordTok{round}\NormalTok{(}\KeywordTok{mean}\NormalTok{(house }\OperatorTok{==}\StringTok{ "Yes"}\NormalTok{), }\DecValTok{2}\NormalTok{), }
            \DataTypeTok{store_exp =} \KeywordTok{round}\NormalTok{(}\KeywordTok{mean}\NormalTok{(}\KeywordTok{na.omit}\NormalTok{(store_exp), }\DataTypeTok{trim =} \FloatTok{0.1}\NormalTok{), }\DecValTok{0}\NormalTok{),}
            \DataTypeTok{online_exp =} \KeywordTok{round}\NormalTok{(}\KeywordTok{mean}\NormalTok{(online_exp), }\DecValTok{0}\NormalTok{), }
            \DataTypeTok{store_trans =} \KeywordTok{round}\NormalTok{(}\KeywordTok{mean}\NormalTok{(store_trans), }\DecValTok{1}\NormalTok{), }
            \DataTypeTok{online_trans =} \KeywordTok{round}\NormalTok{(}\KeywordTok{mean}\NormalTok{(online_trans), }\DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   Age FemalePct HouseYes store_exp online_exp
## 1  39      0.55     0.57       840       2120
##   store_trans online_trans
## 1         5.3         13.5
\end{verbatim}

Now, let's peel the onion.

The first line \texttt{sim.dat} is easy. It is the data you want to work on. The second line \texttt{group\_by(segment)} tells R that in the following steps you want to summarise by variable \texttt{segment}. Here we only summarize data by one categorical variable, but you can group by multiple variables, such as \texttt{group\_by(segment,\ house)}. The third argument \texttt{summarise} tells R the manipulation(s) to do. Then list the exact actions inside \texttt{summarise()} . For example, \texttt{Age=round(mean(na.omit(age)),0)} tell R the following things:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Calculate the mean of column \texttt{age} ignoring missing value for each customer segment
\item
  Round the result to the specified number of decimal places
\item
  Store the result in a new variable named \texttt{Age}
\end{enumerate}

The rest of the command above is similar. In the end, we calculate the following for each segment:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{Age}: average age for each segment
\item
  \texttt{FemalePct}: percentage for each segment
\item
  \texttt{HouseYes}: percentage of people who own a house
\item
  \texttt{stroe\_exp}: average expense in store
\item
  \texttt{online\_exp}: average expense online
\item
  \texttt{store\_trans}: average times of transactions in the store
\item
  \texttt{online\_trans}: average times of online transactions
\end{enumerate}

There is a lot of information you can extract from those simple averages.

\begin{itemize}
\item
  Conspicuous: average age is about 40. It is a group of middle-age wealthy people. 1/3 of them are female, and 2/3 are male. They buy regardless the price. Almost all of them own house (0.86). It makes us wonder what is wrong with the rest 14\%?
\item
  Price: They are older people with average age 60. Nearly all of them own a house(0.94). They are less likely to purchase online (store\_trans=6 while online\_trans=3). It is the only group that is less likely to buy online.
\item
  Quality: The average age is 35. They are not way different with Conspicuous regarding age. But they spend much less. The percentages of male and female are similar. They prefer online shopping. More than half of them don't own a house (0.66).
\item
  Style: They are young people with average age 24. The majority of them are female (0.81). Most of them don't own a house (0.73). They are very likely to be digital natives and prefer online shopping.
\end{itemize}

You may notice that Style group purchase more frequently online (\texttt{online\_trans}) but the expense (\texttt{online\_exp}) is not higher. It makes us wonder what is the average expense each time, so you have a better idea about the price range of the group.

The analytical process is aggregated instead of independent steps. The current step will shed new light on what to do next. Sometimes you need to go back to fix something in the previous steps. Let's check average one-time online and instore purchase amounts:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sim.dat }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{group_by}\NormalTok{(segment) }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{avg_online =} \KeywordTok{round}\NormalTok{(}\KeywordTok{sum}\NormalTok{(online_exp)}\OperatorTok{/}\KeywordTok{sum}\NormalTok{(online_trans), }\DecValTok{2}\NormalTok{),}
            \DataTypeTok{avg_store =} \KeywordTok{round}\NormalTok{(}\KeywordTok{sum}\NormalTok{(store_exp)}\OperatorTok{/}\KeywordTok{sum}\NormalTok{(store_trans), }\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   avg_online avg_store
## 1      156.5     253.6
\end{verbatim}

Price group has the lowest averaged one-time purchase. The Conspicuous group will pay the highest price. When we build customer profile in real life, we will also need to look at the survey summarization. You may be surprised how much information simple data manipulations can provide.

Another comman task is to check which column has missing values. It requires the program to look at each column in the data. In this case you can use \texttt{summarise\_all}:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# apply function anyNA() to each column}
\CommentTok{# you can also assign a function vector such as: c("anyNA","is.factor")}
\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{summarise_all}\NormalTok{(sim.dat, }\KeywordTok{funs_}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\StringTok{"anyNA"}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     age gender income house store_exp online_exp
## 1 FALSE  FALSE   TRUE FALSE     FALSE      FALSE
##   store_trans online_trans    Q1    Q2    Q3    Q4
## 1       FALSE        FALSE FALSE FALSE FALSE FALSE
##      Q5    Q6    Q7    Q8    Q9   Q10 segment
## 1 FALSE FALSE FALSE FALSE FALSE FALSE   FALSE
\end{verbatim}

The above code returns a vector indicating if there is any value missing in each column.

\textbf{Create new variable}

There are often situations where you need to create new variables. For example, adding online and store expense to get total expense. In this case, you will apply \textbf{window function} to the columns and return a column with the same length. \texttt{mutate()} can do it for you and append one or more new columns:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{mutate}\NormalTok{(sim.dat, }\DataTypeTok{total_exp =}\NormalTok{ store_exp }\OperatorTok{+}\StringTok{ }\NormalTok{online_exp)}
\end{Highlighting}
\end{Shaded}

The above code sums up two columns and appends the result (\texttt{total\_exp}) to \texttt{sim.dat}. Another similar function is \texttt{transmute()}. The difference is that \texttt{transmute()} will delete the original columns and only keep the new ones.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{transmute}\NormalTok{(sim.dat, }\DataTypeTok{total_exp =}\NormalTok{ store_exp }\OperatorTok{+}\StringTok{ }\NormalTok{online_exp)}
\end{Highlighting}
\end{Shaded}

\textbf{Merge}

Similar to SQL, there are different joins in \texttt{dplyr}. We create two baby data sets to show how the functions work.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(x <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\KeywordTok{cbind}\NormalTok{(}\DataTypeTok{ID =} \KeywordTok{c}\NormalTok{(}\StringTok{"A"}\NormalTok{, }\StringTok{"B"}\NormalTok{, }\StringTok{"C"}\NormalTok{), }\DataTypeTok{x1 =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{))))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   ID x1
## 1  A  1
## 2  B  2
## 3  C  3
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(y <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\KeywordTok{cbind}\NormalTok{(}\DataTypeTok{ID =} \KeywordTok{c}\NormalTok{(}\StringTok{"B"}\NormalTok{, }\StringTok{"C"}\NormalTok{, }\StringTok{"D"}\NormalTok{), }\DataTypeTok{y1 =} \KeywordTok{c}\NormalTok{(T, T, F))))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   ID    y1
## 1  B  TRUE
## 2  C  TRUE
## 3  D FALSE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# join to the left}
\CommentTok{# keep all rows in x}
\KeywordTok{left_join}\NormalTok{(x, y, }\DataTypeTok{by =} \StringTok{"ID"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: Column `ID` joining factors with different
## levels, coercing to character vector
\end{verbatim}

\begin{verbatim}
##   ID x1   y1
## 1  A  1 <NA>
## 2  B  2 TRUE
## 3  C  3 TRUE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# get rows matched in both data sets}
\KeywordTok{inner_join}\NormalTok{(x, y, }\DataTypeTok{by =} \StringTok{"ID"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: Column `ID` joining factors with different
## levels, coercing to character vector
\end{verbatim}

\begin{verbatim}
##   ID x1   y1
## 1  B  2 TRUE
## 2  C  3 TRUE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# get rows in either data set}
\KeywordTok{full_join}\NormalTok{(x, y, }\DataTypeTok{by =} \StringTok{"ID"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: Column `ID` joining factors with different
## levels, coercing to character vector
\end{verbatim}

\begin{verbatim}
##   ID   x1    y1
## 1  A    1  <NA>
## 2  B    2  TRUE
## 3  C    3  TRUE
## 4  D <NA> FALSE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# filter out rows in x that can be matched in y }
\CommentTok{# it doesn't bring in any values from y }
\KeywordTok{semi_join}\NormalTok{(x, y, }\DataTypeTok{by =} \StringTok{"ID"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# the opposite of  semi_join()}
\CommentTok{# it gets rows in x that cannot be matched in y}
\CommentTok{# it doesn't bring in any values from y}
\KeywordTok{anti_join}\NormalTok{(x, y, }\DataTypeTok{by =} \StringTok{"ID"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

There are other functions(\texttt{intersect()}, \texttt{union()} and \texttt{setdiff()}). Also the data frame version of \texttt{rbind} and \texttt{cbind} which are \texttt{bind\_rows()} and \texttt{bind\_col()}. We are not going to go through them all. You can try them yourself. If you understand the functions we introduced so far. It should be easy for you to figure out the rest.

\hypertarget{tidy-and-reshape-data}{%
\section{Tidy and Reshape Data}\label{tidy-and-reshape-data}}

``Tidy data'' represent the information from a dataset as data frames where each row is an observation, and each column contains the values of a variable (i.e., an attribute of what we are observing). Depending on the situation, the requirements on what to present as rows and columns may change. To make data easy to work with for the problem at hand, in practice, we often need to convert data between the ``wide'' and the ``long'' format. The process feels like kneading the dough.

There are two commonly used packages for this kind of manipulations: \texttt{tidyr} and \texttt{reshape2}. We will show how to tidy and reshape data using the two packages. By comparing the functions to show how they overlap and where they differ.

\hypertarget{reshape2-package}{%
\subsection{\texorpdfstring{\texttt{reshape2} package}{reshape2 package}}\label{reshape2-package}}

It is a reboot of the previous package \texttt{reshape}. Take a baby subset of our exemplary clothes consumers data to illustrate:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sdat<-sim.dat[}\DecValTok{1}\OperatorTok{:}\DecValTok{5}\NormalTok{,}\DecValTok{1}\OperatorTok{:}\DecValTok{6}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

For the above data \texttt{sdat}, what if we want to have a variable indicating the purchasing channel (i.e.~online or in-store) and another column with the corresponding expense amount? Assume we want to keep the rest of the columns the same. It is a task to change data from ``wide'' to ``long''. There are two general ways to shape data:

\begin{itemize}
\tightlist
\item
  Use \texttt{melt()} to convert an object into a molten data frame, i.e., from wide to long
\item
  Use \texttt{dcast()} to cast a molten data frame into the shape you want, i.e., from long to wide
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(mdat <-}\StringTok{ }\KeywordTok{melt}\NormalTok{(sdat, }\DataTypeTok{measure.vars =} \KeywordTok{c}\NormalTok{(}\StringTok{"store_exp"}\NormalTok{, }\StringTok{"online_exp"}\NormalTok{),}
              \DataTypeTok{variable.name =} \StringTok{"Channel"}\NormalTok{, }
              \DataTypeTok{value.name =} \StringTok{"Expense"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    age gender income house    Channel Expense
## 1   57 Female 120963   Yes  store_exp   529.1
## 2   63 Female 122008   Yes  store_exp   478.0
## 3   59   Male 114202   Yes  store_exp   490.8
## 4   60   Male 113616   Yes  store_exp   347.8
## 5   51   Male 124253   Yes  store_exp   379.6
## 6   57 Female 120963   Yes online_exp   303.5
## 7   63 Female 122008   Yes online_exp   109.5
## 8   59   Male 114202   Yes online_exp   279.2
## 9   60   Male 113616   Yes online_exp   141.7
## 10  51   Male 124253   Yes online_exp   112.2
\end{verbatim}

You melted the data frame \texttt{sdat} by two variables: \texttt{store\_exp} and \texttt{online\_exp} (\texttt{measure.vars=c("store\_exp","online\_exp")}). The new variable name is \texttt{Channel} set by command \texttt{variable.name\ =\ "Channel"}. The value name is \texttt{Expense} set by command \texttt{value.name\ =\ "Expense"}.

You can run a regression to study the effect of purchasing channel as follows:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Here we use all observations from sim.dat}
\CommentTok{# Don't show result here}
\NormalTok{mdat <-}\StringTok{ }\KeywordTok{melt}\NormalTok{(sim.dat[, }\DecValTok{1}\OperatorTok{:}\DecValTok{6}\NormalTok{], }\DataTypeTok{measure.vars =} \KeywordTok{c}\NormalTok{(}\StringTok{"store_exp"}\NormalTok{, }\StringTok{"online_exp"}\NormalTok{), }
    \DataTypeTok{variable.name =} \StringTok{"Channel"}\NormalTok{, }\DataTypeTok{value.name =} \StringTok{"Expense"}\NormalTok{)}
\NormalTok{fit <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(Expense }\OperatorTok{~}\StringTok{ }\NormalTok{gender }\OperatorTok{+}\StringTok{ }\NormalTok{house }\OperatorTok{+}\StringTok{ }\NormalTok{income }\OperatorTok{+}\StringTok{ }\NormalTok{Channel }\OperatorTok{+}\StringTok{ }\NormalTok{age, }\DataTypeTok{data =}\NormalTok{ mdat)}
\KeywordTok{summary}\NormalTok{(fit)}
\end{Highlighting}
\end{Shaded}

You can \texttt{melt()} list, matrix, table too. The syntax is similar, and we won't go through every situation. Sometimes we want to convert the data from ``long'' to ``wide''. For example, \textbf{you want to compare the online and in-store expense between male and female based on the house ownership. }

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{dcast}\NormalTok{(mdat, house }\OperatorTok{+}\StringTok{ }\NormalTok{gender }\OperatorTok{~}\StringTok{ }\NormalTok{Channel, sum)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Using Expense as value column: use value.var to override.
\end{verbatim}

\begin{verbatim}
##   house gender store_exp online_exp
## 1   Yes Female      1007      413.0
## 2   Yes   Male      1218      533.2
\end{verbatim}

In the above code, what is the left side of \texttt{\textasciitilde{}} are variables that you want to group by. The right side is the variable you want to spread as columns. It will use the column indicating value from \texttt{melt()} before. Here is ``\texttt{Expense}'' .

\hypertarget{tidyr-package}{%
\subsection{\texorpdfstring{\texttt{tidyr} package}{tidyr package}}\label{tidyr-package}}

The other package that will do similar manipulations is \texttt{tidyr}. Let's get a subset to illustrate the usage.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# practice functions we learnt before}
\NormalTok{sdat <-}\StringTok{ }\NormalTok{sim.dat[}\DecValTok{1}\OperatorTok{:}\DecValTok{5}\NormalTok{, ] }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{select}\NormalTok{(age, gender, store_exp, store_trans)}
\NormalTok{sdat }\OperatorTok{%>%}\StringTok{ }\KeywordTok{tbl_df}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 5 x 4
##     age gender store_exp store_trans
##   <int> <fct>      <dbl>       <int>
## 1    57 Female      529.           2
## 2    63 Female      478.           4
## 3    59 Male        491.           7
## 4    60 Male        348.          10
## 5    51 Male        380.           4
\end{verbatim}

\texttt{gather()} function in \texttt{tidyr} is analogous to \texttt{melt()} in \texttt{reshape2}. The following code will do the same thing as we did before using \texttt{melt()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{msdat <-}\StringTok{ }\NormalTok{tidyr}\OperatorTok{::}\KeywordTok{gather}\NormalTok{(sdat, }\StringTok{"variable"}\NormalTok{,}\StringTok{"value"}\NormalTok{, store_exp, store_trans)}
\NormalTok{msdat }\OperatorTok{%>%}\StringTok{ }\KeywordTok{tbl_df}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 10 x 4
##      age gender variable    value
##    <int> <fct>  <chr>       <dbl>
##  1    57 Female store_exp    529.
##  2    63 Female store_exp    478.
##  3    59 Male   store_exp    491.
##  4    60 Male   store_exp    348.
##  5    51 Male   store_exp    380.
##  6    57 Female store_trans    2 
##  7    63 Female store_trans    4 
##  8    59 Male   store_trans    7 
##  9    60 Male   store_trans   10 
## 10    51 Male   store_trans    4
\end{verbatim}

Or if we use the pipe operation, we can write the above code as:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sdat }\OperatorTok{%>%}\StringTok{ }\KeywordTok{gather}\NormalTok{(}\StringTok{"variable"}\NormalTok{, }\StringTok{"value"}\NormalTok{, store_exp, store_trans)}
\end{Highlighting}
\end{Shaded}

It is identical with the following code using \texttt{melt()}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{melt}\NormalTok{(sdat, }\DataTypeTok{measure.vars =} \KeywordTok{c}\NormalTok{(}\StringTok{"store_exp"}\NormalTok{, }\StringTok{"store_trans"}\NormalTok{), }
     \DataTypeTok{variable.name =} \StringTok{"variable"}\NormalTok{, }
     \DataTypeTok{value.name =} \StringTok{"value"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The opposite operation to \texttt{gather()} is \texttt{spread()}. The previous one stacks columns and the latter one spread the columns.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{msdat }\OperatorTok{%>%}\StringTok{ }\KeywordTok{spread}\NormalTok{(variable, value)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   age gender store_exp store_trans
## 1  51   Male     379.6           4
## 2  57 Female     529.1           2
## 3  59   Male     490.8           7
## 4  60   Male     347.8          10
## 5  63 Female     478.0           4
\end{verbatim}

Another pair of functions that do opposite manipulations are \texttt{separate()} and \texttt{unite()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sepdat<-}\StringTok{ }\NormalTok{msdat }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{separate}\NormalTok{(variable,}\KeywordTok{c}\NormalTok{(}\StringTok{"Source"}\NormalTok{,}\StringTok{"Type"}\NormalTok{))}
\NormalTok{sepdat }\OperatorTok{%>%}\StringTok{ }\KeywordTok{tbl_df}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 10 x 5
##      age gender Source Type  value
##    <int> <fct>  <chr>  <chr> <dbl>
##  1    57 Female store  exp    529.
##  2    63 Female store  exp    478.
##  3    59 Male   store  exp    491.
##  4    60 Male   store  exp    348.
##  5    51 Male   store  exp    380.
##  6    57 Female store  trans    2 
##  7    63 Female store  trans    4 
##  8    59 Male   store  trans    7 
##  9    60 Male   store  trans   10 
## 10    51 Male   store  trans    4
\end{verbatim}

You can see that the function separates the original column ``\texttt{variable}'' to two new columns ``\texttt{Source}'' and ``\texttt{Type}''. You can use \texttt{sep=} to set the string or regular expression to separate the column. By default, it is ``\texttt{\_}''.

The \texttt{unite()} function will do the opposite: combining two columns. It is the generalization of \texttt{paste()} to a data frame.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sepdat }\OperatorTok{%>%}\StringTok{ }
\StringTok{  }\KeywordTok{unite}\NormalTok{(}\StringTok{"variable"}\NormalTok{, Source, Type, }\DataTypeTok{sep =} \StringTok{"_"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    age gender    variable value
## 1   57 Female   store_exp 529.1
## 2   63 Female   store_exp 478.0
## 3   59   Male   store_exp 490.8
## 4   60   Male   store_exp 347.8
## 5   51   Male   store_exp 379.6
## 6   57 Female store_trans   2.0
## 7   63 Female store_trans   4.0
## 8   59   Male store_trans   7.0
## 9   60   Male store_trans  10.0
## 10  51   Male store_trans   4.0
\end{verbatim}

The reshaping manipulations may be the trickiest part. You have to practice a lot to get familiar with those functions. Unfortunately, there is no shortcut.

\hypertarget{modeltuningstrategy}{%
\chapter{Model Tuning Strategy}\label{modeltuningstrategy}}

When training a machine learning model, there are many decisions to make. For example, when training a random forest, you need to decide the number of trees and the number of variables at each node. For lasso method, you need to determine the penalty parameter. There may be standard settings for some of the parameters, but it's unlikely to guess the right values for all of these correctly. Other than that, making good choices on how you split the data into training and testing sets can make a huge difference in helping you find a high-performance model efficiently.

This chapter will illustrate the practical aspects of model tuning. We will talk about different types of model error, sources of model error, hyperparameter tuning, how to set up your data and how to make sure your model implementation is correct. In practice applying machine learning is a highly iterative process.

\hypertarget{systematic-error-and-random-error}{%
\section{Systematic Error and Random Error}\label{systematic-error-and-random-error}}

Assume \(\mathbf{X}\) is \(n \times p\) observation matrix and \(\mathbf{y}\) is response variable, we have:

\begin{equation}
\mathbf{y}=f(\mathbf{X})+\mathbf{\epsilon}
\label{eq:generalmodeleq}
\end{equation}

where \(\mathbf{\epsilon}\) is the random error with a mean of zero. The function \(f(\cdot)\) is our modeling target, which represents the information in the response variable that predictors can explain. The main goal of estimating \(f(\cdot)\) is inference or prediction, or sometimes both. In general, there is a trade-off between flexibility and interpretability of the model. So data scientists need to comprehend the delicate balance between these two.

Depending on the modeling purposes, the requirement for interpretability varies. If the prediction is the only goal, then as long as the prediction is accurate enough, the interpretability is not under consideration. In this case, people can use ``black box'' model, such as random forest, boosting tree, neural network and so on. These models are very flexible but nearly impossible to explain. Their accuracy is usually higher on the training set, but not necessary when it predicts. It is not surprising since those models have a huge number of parameters and high flexibility that they can ``memorize'' the entire training data. A paper by Chiyuan Zhang et al.~in 2017 pointed out that ``Deep neural networks (even just two-layer net) easily fit random labels'' \citep{rethinkDL}. The traditional forms of regularization, such as weight decay, dropout, and data augmentation, fail to control generalization error. It poses a conceptual challenge to statistical theory and also calls our attention when we use such black-box models.

There are two kinds of application problems: complete information problem and incomplete information problem. The complete information problem has all the information you need to know the correct response. Take the famous cat recognition, for example, all the information you need to identify a cat is in the picture. In this situation, the algorithm that penetrates the data the most wins. There are some other similar problems such as the self-driving car, chess game, facial recognition and speech recognition. But in most of the data science applications, the information is incomplete. If you want to know whether a customer is going to purchase again or not, it is unlikely to have 360-degree of the customer's information. You may have their historical purchasing record, discounts and service received. But you don't know if the customer sees your advertisement, or has a friend recommends competitor's product, or encounters some unhappy purchasing experience somewhere. There could be a myriad of factors that will influence the customer's purchase decision while what you have as data is only a small part. To make things worse, in many cases, you don't even know what you don't know. Deep learning doesn't have any advantage in solving those problems. Instead, some parametric models often work better in this situation. You will comprehend this more after learning the different types of model error.
Assume we have \(\hat{f}\) which is an estimator of \(f\). Then we can further get \(\mathbf{\hat{y}}=\hat{f}(\mathbf{X})\). The predicted error is divided into two parts, systematic error, and random error:

\begin{equation}
E(\mathbf{y}-\hat{\mathbf{y}})^{2}=E[f(\mathbf{X})+\mathbf{\epsilon}-\hat{f}(\mathbf{X})]^{2}=\underset{\text{(1)}}{\underbrace{E[f(\mathbf{X})-\hat{f}(\mathbf{X})]^{2}}}+\underset{\text{(2)}}{\underbrace{Var(\mathbf{\epsilon})}}
\label{eq:error}
\end{equation}

It is also called Mean Square Error (MSE) where (1) is the systematic error. It exists because \(\hat{f}\) usually does not entirely describe the ``systematic relation'' between X and y which refers to the stable relationship that exists across different samples or time. Model improvement can help reduce this kind of error; (2) is the random error which represents the part of y that cannot be explained by X. A more complex model does not reduce the error. There are three reasons for random error:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  the current sample is not representative, so the pattern in one sample set does not generalize to a broader scale.
\item
  The information is incomplete. In other words, you don't have all variables needed to explain the response.
\item
  Measurement error in the variables.
\end{enumerate}

Deep learning has significant success solving problems with complete information and usually low measurement error. As mentioned before, in a task like image recognition, all you need are the pixels in the pictures. So in deep learning applications, increasing the sample size can improve the model performance significantly. But it may not perform well in problems with incomplete information. The biggest problem with the black-box model is that it fits random error, i.e., over-fitting. The notable feature of random error is that it varies over different samples. So one way to determine whether overfitting happens is to reserve a part of the data as the test set and then check the performance of the trained model on the test data. Note that overfitting is a general problem from which any model could suffer. However, since black-box models usually have a large number of parameters, it is much more suspectable to over-fitting.

\begin{figure}
\centering
\includegraphics{images/ModelError.png}
\caption{Types of Model Error}
\end{figure}

\hypertarget{vbtradeoff}{%
\subsection{Variance-Bias Trade-Off}\label{vbtradeoff}}

The systematic error can be further decomposed as:

\begin{equation}
\begin{array}{ccc}
E[f(\mathbf{X})-\hat{f}(\mathbf{X})]^{2} & = & E\left(f(\mathbf{X})-E[\hat{f}(\mathbf{X})]+E[\hat{f}(\mathbf{X})]-\hat{f}(\mathbf{X})\right)^{2}\\
 & = & E\left(E[\hat{f}(\mathbf{X})]-f(\mathbf{X})\right)^{2}+E\left(\hat{f}(\mathbf{X})-E[\hat{f}(\mathbf{X})]\right)^{2}\\
 & = & [Bias(\hat{f}(\mathbf{X}))]^{2}+Var(\hat{f}(\mathbf{X}))
\end{array}
\label{eq:biasvariance}
\end{equation}

The systematic error consists of two parts, \(Bias(\hat{f}(\mathbf{X}))\) and \(Var (\hat{f}(\mathbf{X}))\). To minimize the systematic error, we need to minimize both. The bias represents the error caused by the model's approximation of the reality, i.e., systematic relation, which may be very complex. For example, linear regression assumes a linear relationship between the predictors and the response, but rarely is there a perfect linear relationship in real life. So linear regression is more likely to have a high bias. In general, the more flexible the model is, the higher the variance. However, this does not guarantee complex models will outperform a much simpler one, such as linear regression. If the real relationship \(f\) is linear, then linear regression is unbiased. It is difficult for a more flexible model to compete. Good learning method requires low variance and bias. However, it is easy to find a model with extremely low bias but high variance (by fitting a tree) or a method with low variance but high variance (by fitting a straight line). That why we call it trade-off.

To explore bias and variance, let's begin with a simple simulation. We will simulate a data with a non-linear relationship and fit different models on it. An intuitive way to show these is to compare the plots of various models.

The code below simulates one predictor (\texttt{x}) and one response variable (\texttt{fx}). The relationship between \texttt{x} and \texttt{fx} is non-linear.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{source}\NormalTok{(}\KeywordTok{ids_url}\NormalTok{(}\StringTok{'R/multiplot.r'}\NormalTok{))}
\CommentTok{# randomly simulate some non-linear samples}
\NormalTok{x =}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{10}\NormalTok{, }\FloatTok{0.01}\NormalTok{) }\OperatorTok{*}\StringTok{ }\NormalTok{pi}
\NormalTok{e =}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(}\KeywordTok{length}\NormalTok{(x), }\DataTypeTok{mean =} \DecValTok{0}\NormalTok{, }\DataTypeTok{sd =} \FloatTok{0.2}\NormalTok{)}
\NormalTok{fx <-}\StringTok{ }\KeywordTok{sin}\NormalTok{(x) }\OperatorTok{+}\StringTok{ }\NormalTok{e }\OperatorTok{+}\StringTok{ }\KeywordTok{sqrt}\NormalTok{(x)}
\NormalTok{dat =}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(x, fx)}
\end{Highlighting}
\end{Shaded}

Then fit a simple linear regression on these data:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# plot fitting result}
\KeywordTok{library}\NormalTok{(ggplot2)}
\KeywordTok{ggplot}\NormalTok{(dat, }\KeywordTok{aes}\NormalTok{(x, fx)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_point}\NormalTok{() }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{method =} \StringTok{"lm"}\NormalTok{, }\DataTypeTok{se =} \OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{IDS_files/figure-latex/linearbias-1} 

}

\caption{High bias model}\label{fig:linearbias}
\end{figure}

Despite a large sample size, trained linear regression cannot describe the relationship very well. In other words, in this case, the model has a high bias (Fig. \ref{fig:linearbias}). People also call it underfitting.

Since the estimated parameters will be somewhat different for different samples, there is the variance of estimates. Intuitively, it gives you some sense that if we fit the same model with different samples (presumably, they are from the same population), how much will the estimates change. Ideally, the change is trivial. For high variance models, small changes in the training data result in very different estimates. In general, a model with high flexibility also has high variance., such as the CART tree, and the initial boosting method. To overcome that problem, the Random Forest and Gradient Boosting Model aim to reduce the variance by summarizing the results obtained from different samples.

Let's fit the above data using a smoothing method which is highly flexible and can fit the current data tightly:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(dat, }\KeywordTok{aes}\NormalTok{(x, fx)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{span =} \FloatTok{0.03}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{IDS_files/figure-latex/linearvar-1} 

}

\caption{High variance model}\label{fig:linearvar}
\end{figure}

The resulting plot (Fig. \ref{fig:linearvar}) indicates the smoothing method fit the data much better so it has a much smaller bias. However, this method has a high variance. If we simulate different subsets of the sample, the result curve will change significantly:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# set random seed}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{2016}\NormalTok{)}
\CommentTok{# sample part of the data to fit model sample 1}
\NormalTok{idx1 =}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\KeywordTok{length}\NormalTok{(x), }\DecValTok{100}\NormalTok{)}
\NormalTok{dat1 =}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{x1 =}\NormalTok{ x[idx1], }\DataTypeTok{fx1 =}\NormalTok{ fx[idx1])}
\NormalTok{p1 =}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(dat1, }\KeywordTok{aes}\NormalTok{(x1, fx1)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{span =} \FloatTok{0.03}\NormalTok{)}
\CommentTok{# sample 2}
\NormalTok{idx2 =}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\KeywordTok{length}\NormalTok{(x), }\DecValTok{100}\NormalTok{)}
\NormalTok{dat2 =}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{x2 =}\NormalTok{ x[idx2], }\DataTypeTok{fx2 =}\NormalTok{ fx[idx2])}
\NormalTok{p2 =}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(dat2, }\KeywordTok{aes}\NormalTok{(x2, fx2)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{span =} \FloatTok{0.03}\NormalTok{)}
\CommentTok{# sample 3}
\NormalTok{idx3 =}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\KeywordTok{length}\NormalTok{(x), }\DecValTok{100}\NormalTok{)}
\NormalTok{dat3 =}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{x3 =}\NormalTok{ x[idx3], }\DataTypeTok{fx3 =}\NormalTok{ fx[idx3])}
\NormalTok{p3 =}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(dat3, }\KeywordTok{aes}\NormalTok{(x3, fx3)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{span =} \FloatTok{0.03}\NormalTok{)}
\CommentTok{# sample 4}
\NormalTok{idx4 =}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\KeywordTok{length}\NormalTok{(x), }\DecValTok{100}\NormalTok{)}
\NormalTok{dat4 =}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{x4 =}\NormalTok{ x[idx4], }\DataTypeTok{fx4 =}\NormalTok{ fx[idx4])}
\NormalTok{p4 =}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(dat4, }\KeywordTok{aes}\NormalTok{(x4, fx4)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{span =} \FloatTok{0.03}\NormalTok{)}
\KeywordTok{multiplot}\NormalTok{(p1, p2, p3, p4, }\DataTypeTok{cols =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{IDS_files/figure-latex/unnamed-chunk-67-1.pdf}

The fitted lines (blue) change over different samples which means it has high variance. People also call it overfitting. Fitting the linear model using the same four subsets, the result barely changes:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p1 =}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(dat1, }\KeywordTok{aes}\NormalTok{(x1, fx1)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_point}\NormalTok{() }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{method =} \StringTok{"lm"}\NormalTok{, }
    \DataTypeTok{se =} \OtherTok{FALSE}\NormalTok{)}
\NormalTok{p2 =}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(dat2, }\KeywordTok{aes}\NormalTok{(x2, fx2)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_point}\NormalTok{() }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{method =} \StringTok{"lm"}\NormalTok{, }
    \DataTypeTok{se =} \OtherTok{FALSE}\NormalTok{)}
\NormalTok{p3 =}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(dat3, }\KeywordTok{aes}\NormalTok{(x3, fx3)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_point}\NormalTok{() }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{method =} \StringTok{"lm"}\NormalTok{, }
    \DataTypeTok{se =} \OtherTok{FALSE}\NormalTok{)}
\NormalTok{p4 =}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(dat4, }\KeywordTok{aes}\NormalTok{(x4, fx4)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_point}\NormalTok{() }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_smooth}\NormalTok{(}\DataTypeTok{method =} \StringTok{"lm"}\NormalTok{, }
    \DataTypeTok{se =} \OtherTok{FALSE}\NormalTok{)}
\KeywordTok{multiplot}\NormalTok{(p1, p2, p3, p4, }\DataTypeTok{cols =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{IDS_files/figure-latex/unnamed-chunk-68-1.pdf}

In general, the variance (\(Var(\hat{f}(\mathbf{X}))\)) \textbf{increases} and the bias (\(Bias(\hat{f}(\mathbf{X}))\)) \textbf{decreases} as the model flexibility increases. Variance and bias together determine the systematic error. As we increase the flexibility of the model, at first the rate at which \(Bias(\hat{f}(\mathbf{X}))\) decreases is faster than \(Var (\hat{f} (\mathbf{X}))\), so the MSE decreases. However, to some degree, higher flexibility has little effect on \(Bias(\hat{f}(\mathbf{X}))\) but \(Var(\hat{f} (\mathbf{X}))\) increases significantly, so the MSE increases.

\hypertarget{measurement-error-in-the-response}{%
\subsection{Measurement Error in the Response}\label{measurement-error-in-the-response}}

The measurement error in the response contributes to the random error (\(\mathbf{\epsilon}\)). This part of the error is irreducible if you change the data collection mechanism, and so it makes the root mean square error (RMSE) and \(R^2\) have the corresponding upper and lower limits. RMSE and \(R^2\) are commonly used performance measures for the regression model which we will talk in more detail later. Therefore, the random error term not only represents the part of fluctuations the model cannot explain but also contains measurement error in the response variables. Section 20.2 of Applied Predictive Modeling \citep{APM} has an example that shows the effect of the measurement error in the response variable on the model performance (RMSE and \(R^2\)).

The authors increased the error in the response proportional to a base level error which was gotten using the original data without introducing extra noise. Then fit a set of models repeatedly using the ``contaminated'' data sets to study the change of \(RMSE\) and \(R^2\) as the level of noise. Here we use clothing consumer data for a similar illustration. Suppose many people do not want to disclose their income and so we need to use other variables to establish a model to predict income. We set up the following model:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# load data}
\NormalTok{sim.dat <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"http://bit.ly/2P5gTw4"}\NormalTok{)}
\NormalTok{ymad <-}\StringTok{ }\KeywordTok{mad}\NormalTok{(}\KeywordTok{na.omit}\NormalTok{(sim.dat}\OperatorTok{$}\NormalTok{income))}
\CommentTok{# calculate z-score}
\NormalTok{zs <-}\StringTok{ }\NormalTok{(sim.dat}\OperatorTok{$}\NormalTok{income }\OperatorTok{-}\StringTok{ }\KeywordTok{mean}\NormalTok{(}\KeywordTok{na.omit}\NormalTok{(sim.dat}\OperatorTok{$}\NormalTok{income)))}\OperatorTok{/}\NormalTok{ymad}
\CommentTok{# which(na.omit(zs>3.5)): identify outliers which(is.na(zs)):}
\CommentTok{# identify missing values}
\NormalTok{idex <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\KeywordTok{which}\NormalTok{(}\KeywordTok{na.omit}\NormalTok{(zs }\OperatorTok{>}\StringTok{ }\FloatTok{3.5}\NormalTok{)), }\KeywordTok{which}\NormalTok{(}\KeywordTok{is.na}\NormalTok{(zs)))}
\CommentTok{# delete rows with outliers and missing values}
\NormalTok{sim.dat <-}\StringTok{ }\NormalTok{sim.dat[}\OperatorTok{-}\NormalTok{idex, ]}
\NormalTok{fit <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(income }\OperatorTok{~}\StringTok{ }\NormalTok{store_exp }\OperatorTok{+}\StringTok{ }\NormalTok{online_exp }\OperatorTok{+}\StringTok{ }\NormalTok{store_trans }\OperatorTok{+}\StringTok{ }\NormalTok{online_trans, }
    \DataTypeTok{data =}\NormalTok{ sim.dat)}
\end{Highlighting}
\end{Shaded}

The output shows that without additional noise, the root mean square error (RMSE) of the model is 29567, \(R^2\) is 0.6.

Let's add various degrees of noise (0 to 3 times the RMSE) to the variable \texttt{income}:

\[ RMSE \times (0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0) \]

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{noise <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\KeywordTok{rep}\NormalTok{(}\OtherTok{NA}\NormalTok{, }\DecValTok{7} \OperatorTok{*}\StringTok{ }\KeywordTok{nrow}\NormalTok{(sim.dat)), }\DataTypeTok{nrow =} \KeywordTok{nrow}\NormalTok{(sim.dat), }
    \DataTypeTok{ncol =} \DecValTok{7}\NormalTok{)}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\KeywordTok{nrow}\NormalTok{(sim.dat)) \{}
\NormalTok{    noise[i, ] <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(}\DecValTok{7}\NormalTok{, }\KeywordTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{7}\NormalTok{), }\KeywordTok{summary}\NormalTok{(fit)}\OperatorTok{$}\NormalTok{sigma }\OperatorTok{*}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }
        \DecValTok{3}\NormalTok{, }\DataTypeTok{by =} \FloatTok{0.5}\NormalTok{))}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

We then examine the effect of noise intensity on \(R^2\) for models with different complexity. The models with complexity from low to high are: ordinary linear regression, partial least square regression(PLS), multivariate adaptive regression spline (MARS), support vector machine (SVM, the kernel function is radial basis function), and random forest.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# fit ordinary linear regression}
\NormalTok{rsq_linear <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, }\KeywordTok{ncol}\NormalTok{(noise))}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\DecValTok{7}\NormalTok{) \{}
\NormalTok{    withnoise <-}\StringTok{ }\NormalTok{sim.dat}\OperatorTok{$}\NormalTok{income }\OperatorTok{+}\StringTok{ }\NormalTok{noise[, i]}
\NormalTok{    fit0 <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(withnoise }\OperatorTok{~}\StringTok{ }\NormalTok{store_exp }\OperatorTok{+}\StringTok{ }\NormalTok{online_exp }\OperatorTok{+}\StringTok{ }\NormalTok{store_trans }\OperatorTok{+}\StringTok{ }
\StringTok{        }\NormalTok{online_trans, }\DataTypeTok{data =}\NormalTok{ sim.dat)}
\NormalTok{    rsq_linear[i] <-}\StringTok{ }\KeywordTok{summary}\NormalTok{(fit0)}\OperatorTok{$}\NormalTok{adj.r.squared}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

PLS is a method of linearizing nonlinear relationships through hidden layers. It is similar to the principal component regression (PCR), except that PCR does not take into account the information of the dependent variable when selecting the components, and its purpose is to find the linear combinations (i.e., unsupervised) that capture the most variance of the independent variables. When the independent variables and response variables are related, PCR can well identify the systematic relationship between them. However, when there exist independent variables not associated with response variable, it will undermine PCR's performance. And PLS maximizes the linear combination of dependencies with the response variable. In the current case, the more complicated PLS does not perform better than simple linear regression.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# pls: conduct PLS and PCR}
\KeywordTok{library}\NormalTok{(pls)}
\NormalTok{rsq_pls <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, }\KeywordTok{ncol}\NormalTok{(noise))}
\CommentTok{# fit PLS}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\DecValTok{7}\NormalTok{) \{}
\NormalTok{    withnoise <-}\StringTok{ }\NormalTok{sim.dat}\OperatorTok{$}\NormalTok{income }\OperatorTok{+}\StringTok{ }\NormalTok{noise[, i]}
\NormalTok{    fit0 <-}\StringTok{ }\KeywordTok{plsr}\NormalTok{(withnoise }\OperatorTok{~}\StringTok{ }\NormalTok{store_exp }\OperatorTok{+}\StringTok{ }\NormalTok{online_exp }\OperatorTok{+}\StringTok{ }\NormalTok{store_trans }\OperatorTok{+}\StringTok{ }
\StringTok{        }\NormalTok{online_trans, }\DataTypeTok{data =}\NormalTok{ sim.dat)}
\NormalTok{    rsq_pls[i] <-}\StringTok{ }\KeywordTok{max}\NormalTok{(}\KeywordTok{drop}\NormalTok{(}\KeywordTok{R2}\NormalTok{(fit0, }\DataTypeTok{estimate =} \StringTok{"train"}\NormalTok{, }\DataTypeTok{intercept =} \OtherTok{FALSE}\NormalTok{)}\OperatorTok{$}\NormalTok{val))}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# earth: fit mars}
\KeywordTok{library}\NormalTok{(earth)}
\NormalTok{rsq_mars <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, }\KeywordTok{ncol}\NormalTok{(noise))}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\DecValTok{7}\NormalTok{) \{}
\NormalTok{    withnoise <-}\StringTok{ }\NormalTok{sim.dat}\OperatorTok{$}\NormalTok{income }\OperatorTok{+}\StringTok{ }\NormalTok{noise[, i]}
\NormalTok{    fit0 <-}\StringTok{ }\KeywordTok{earth}\NormalTok{(withnoise }\OperatorTok{~}\StringTok{ }\NormalTok{store_exp }\OperatorTok{+}\StringTok{ }\NormalTok{online_exp }\OperatorTok{+}\StringTok{ }\NormalTok{store_trans }\OperatorTok{+}\StringTok{ }
\StringTok{        }\NormalTok{online_trans, }\DataTypeTok{data =}\NormalTok{ sim.dat)}
\NormalTok{    rsq_mars[i] <-}\StringTok{ }\NormalTok{fit0}\OperatorTok{$}\NormalTok{rsq}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# caret: awesome package for tuning predictive model}
\KeywordTok{library}\NormalTok{(caret)}
\NormalTok{rsq_svm <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, }\KeywordTok{ncol}\NormalTok{(noise))}
\CommentTok{# Need some time to run}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\DecValTok{7}\NormalTok{) \{}
\NormalTok{    idex <-}\StringTok{ }\KeywordTok{which}\NormalTok{(}\KeywordTok{is.na}\NormalTok{(sim.dat}\OperatorTok{$}\NormalTok{income))}
\NormalTok{    withnoise <-}\StringTok{ }\NormalTok{sim.dat}\OperatorTok{$}\NormalTok{income }\OperatorTok{+}\StringTok{ }\NormalTok{noise[, i]}
\NormalTok{    trainX <-}\StringTok{ }\NormalTok{sim.dat[, }\KeywordTok{c}\NormalTok{(}\StringTok{"store_exp"}\NormalTok{, }\StringTok{"online_exp"}\NormalTok{, }\StringTok{"store_trans"}\NormalTok{, }
        \StringTok{"online_trans"}\NormalTok{)]}
\NormalTok{    trainY <-}\StringTok{ }\NormalTok{withnoise}
\NormalTok{    fit0 <-}\StringTok{ }\KeywordTok{train}\NormalTok{(trainX, trainY, }\DataTypeTok{method =} \StringTok{"svmRadial"}\NormalTok{, }\DataTypeTok{tuneLength =} \DecValTok{15}\NormalTok{, }
        \DataTypeTok{trControl =} \KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method =} \StringTok{"cv"}\NormalTok{))}
\NormalTok{    rsq_svm[i] <-}\StringTok{ }\KeywordTok{max}\NormalTok{(fit0}\OperatorTok{$}\NormalTok{results}\OperatorTok{$}\NormalTok{Rsquared)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# randomForest: random forest model}
\KeywordTok{library}\NormalTok{(randomForest)}
\NormalTok{rsq_rf <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, }\KeywordTok{ncol}\NormalTok{(noise))}
\CommentTok{# ntree=500 number of trees na.action = na.omit ignore}
\CommentTok{# missing value}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\DecValTok{7}\NormalTok{) \{}
\NormalTok{    withnoise <-}\StringTok{ }\NormalTok{sim.dat}\OperatorTok{$}\NormalTok{income }\OperatorTok{+}\StringTok{ }\NormalTok{noise[, i]}
\NormalTok{    fit0 <-}\StringTok{ }\KeywordTok{randomForest}\NormalTok{(withnoise }\OperatorTok{~}\StringTok{ }\NormalTok{store_exp }\OperatorTok{+}\StringTok{ }\NormalTok{online_exp }\OperatorTok{+}\StringTok{ }
\StringTok{        }\NormalTok{store_trans }\OperatorTok{+}\StringTok{ }\NormalTok{online_trans, }\DataTypeTok{data =}\NormalTok{ sim.dat, }\DataTypeTok{ntree =} \DecValTok{500}\NormalTok{, }
        \DataTypeTok{na.action =}\NormalTok{ na.omit)}
\NormalTok{    rsq_rf[i] <-}\StringTok{ }\KeywordTok{tail}\NormalTok{(fit0}\OperatorTok{$}\NormalTok{rsq, }\DecValTok{1}\NormalTok{)}
\NormalTok{\}}
\KeywordTok{library}\NormalTok{(reshape2)}
\NormalTok{rsq <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\KeywordTok{cbind}\NormalTok{(}\DataTypeTok{Noise =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\DecValTok{1}\NormalTok{, }\FloatTok{1.5}\NormalTok{, }\DecValTok{2}\NormalTok{, }\FloatTok{2.5}\NormalTok{, }\DecValTok{3}\NormalTok{), }
\NormalTok{    rsq_linear, rsq_pls, rsq_mars, rsq_svm, rsq_rf))}
\NormalTok{rsq <-}\StringTok{ }\KeywordTok{melt}\NormalTok{(rsq, }\DataTypeTok{id.vars =} \StringTok{"Noise"}\NormalTok{, }\DataTypeTok{measure.vars =} \KeywordTok{c}\NormalTok{(}\StringTok{"rsq_linear"}\NormalTok{, }
    \StringTok{"rsq_pls"}\NormalTok{, }\StringTok{"rsq_mars"}\NormalTok{, }\StringTok{"rsq_svm"}\NormalTok{, }\StringTok{"rsq_rf"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}



\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(ggplot2)}
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ rsq, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ Noise, }\DataTypeTok{y =}\NormalTok{ value, }\DataTypeTok{group =}\NormalTok{ variable, }
    \DataTypeTok{colour =}\NormalTok{ variable)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_line}\NormalTok{() }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_point}\NormalTok{() }\OperatorTok{+}\StringTok{ }\KeywordTok{ylab}\NormalTok{(}\StringTok{"R2"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{IDS_files/figure-latex/error-1} 

}

\caption{Test set \(R^2\) profiles for income models when measurement system noise increases. \texttt{rsq\_linear}: linear regression, \texttt{rsq\_pls}: Partial Least Square, \texttt{rsq\_mars}: Multiple Adaptive Regression Spline Regression, \texttt{rsq\_svm}: Support Vector Machine\texttt{rsq\_rf}: Random Forest}\label{fig:error}
\end{figure}

Fig. \ref{fig:error} shows that:

All model performance decreases sharply with increasing noise intensity. To better anticipate model performance, it helps to understand the way variable is measured. It is something need to make clear at the beginning of an analytical project. A data scientist should be aware of the quality of the data in the database. For data from the clients, it is an important to understand the quality of the data by communication.

More complex model is not necessarily better. The best model in this situation is MARS, not random forests or SVM. Simple linear regression and PLS perform the worst when noise is low. MARS is more complicated than the linear regression and PLS, but it is simpler and easier to explain than random forest and SVM.

When noise increases to a certain extent, the potential structure becomes vaguer, and complex random forest model starts to fail. When the systematic measurement error is significant, a more straightforward but not naive model may be a better choice. It is always a good practice to try different models, and select the simplest model in the case of similar performance. Model evaluation and selection represent the career ``maturity'' of a data scientist.

\hypertarget{measurement-error-in-the-independent-variables}{%
\subsection{Measurement Error in the Independent Variables}\label{measurement-error-in-the-independent-variables}}

The traditional statistical model usually assumes that the measurement of the independent variable has no error which is not possible in practice. Considering the error in the independent variables is necessary. The impact of the error depends on the following factors: (1) the magnitude of the randomness; (2) the importance of the corresponding variable in the model, and (3) the type of model used. Use variable \texttt{online\_exp} as an example. The approach is similar to the previous section. Add varying degrees of noise and see its impact on the model performance. We add the following different levels of noise (0 to 3 times the standard deviation) to\texttt{online\_exp}:

\[\sigma_{0} \times (0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0)\]

where \(\sigma_{0}\) is the standard error of \texttt{online\_exp}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{noise<-}\KeywordTok{matrix}\NormalTok{(}\KeywordTok{rep}\NormalTok{(}\OtherTok{NA}\NormalTok{,}\DecValTok{7}\OperatorTok{*}\KeywordTok{nrow}\NormalTok{(sim.dat)),}\DataTypeTok{nrow=}\KeywordTok{nrow}\NormalTok{(sim.dat),}\DataTypeTok{ncol=}\DecValTok{7}\NormalTok{)}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\KeywordTok{nrow}\NormalTok{(sim.dat))\{}
\NormalTok{noise[i,]<-}\KeywordTok{rnorm}\NormalTok{(}\DecValTok{7}\NormalTok{,}\KeywordTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{7}\NormalTok{),}\KeywordTok{sd}\NormalTok{(sim.dat}\OperatorTok{$}\NormalTok{online_exp)}\OperatorTok{*}\KeywordTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DataTypeTok{by=}\FloatTok{0.5}\NormalTok{))}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Likewise, we examine the effect of noise intensity on different models (\(R^2\)). The models with complexity from low to high are: ordinary linear regression, partial least square regression(PLS), multivariate adaptive regression spline (MARS), support vector machine (SVM, the Kernel function is radial basis function), and random forest. The code is similar as before so not shown here.



\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{IDS_files/figure-latex/errorvariable-1} 

}

\caption{Test set \(R^2\) profiles for income models when noise in \texttt{online\_exp} increases. \texttt{rsq\_linear} : linear regression, \texttt{rsq\_pls} : Partial Least Square, \texttt{rsq\_mars}: Multiple Adaptive Regression Spline Regression, \texttt{rsq\_svm}: Support Vector Machine\texttt{rsq\_rf}: Random Forest}\label{fig:errorvariable}
\end{figure}

Comparing Fig. \ref{fig:errorvariable} and Fig. \ref{fig:error}, the influence of the two types of error is very different. The error in response cannot be overcome for any model, but it is not the case for the independent variables. Imagine an extreme case, if \texttt{online\_exp} is completely random, that is, no information in it, the impact on the performance of random forest and support vector machine is marginal. Linear regression and PLS still perform similarly. With the increase of noise, the performance starts to decline faster. To a certain extent, it becomes steady. In general, if an independent variable contains error, other variables associated with it can compensate to some extent.

\hypertarget{data-splitting-and-resampling}{%
\section{Data Splitting and Resampling}\label{data-splitting-and-resampling}}

Those highly adaptable models can model complex relationships. However, they tend to overfit which leads to the poor prediction by learning too much from the data. It means that the model is susceptible to the specific sample used to fit it. When future data is not exactly like the past data, the model prediction may have big mistakes. A simple model like ordinary linear regression tends instead to underfit which leads to a bad prediction by learning too little from the data. It systematically over-predicts or under-predicts the data regardless of how well future data resemble past data. Without evaluating models, the modeler will not know about the problem before the future samples. Data splitting and resampling are fundamental techniques to build sound models for prediction.

\hypertarget{data-splitting}{%
\subsection{Data Splitting}\label{data-splitting}}

\emph{Data splitting} is to put part of the data aside as testing set (or Hold-outs, out of bag samples) and use the rest for model training. Training samples are also called in-sample. Model performance metrics evaluated using in-sample are retrodictive, not predictive.

The traditional business intelligence usually handles data description. Answer simple questions by querying and summarizing the data, such as:

\begin{itemize}
\tightlist
\item
  What is the monthly sales of a product in 2015?
\item
  What is the number of visits to our site in the past month?\\
\item
  What is the sales difference in 2015 for two different product designs?
\end{itemize}

There is no need to go through the tedious process of splitting the data, tuning and testing model to answer questions of this kind. Instead, people usually use as complete data as possible and then sum or average the parts of interest.

Many models have parameters which cannot be directly estimated from the data, such as \(\lambda\) in the lasso (penalty parameter), the number of trees in the random forest. This type of model parameter is called tuning parameter, and there is no analytical formula available to calculate the optimized value. Tuning parameters often control the complexity of the model. A poor choice can result in over-fitting or under-fitting. A standard approach to estimate tuning parameters is through cross-validation which is a data resampling approach.

To get a reasonable precision of the performance based on a single test set, the size of the test set may need to be large. So a conventional approach is to use a subset of samples to fit the model and use the rest to evaluate model performance. This process will repeat multiple times to get a performance profile. In that sense, resampling is based on splitting. The general steps are:

\begin{itemize}
\tightlist
\item
  Define a set of candidate values for tuning parameter(s)

  \begin{itemize}
  \tightlist
  \item
    For each candidate value in the set

    \begin{itemize}
    \tightlist
    \item
      Resample data
    \item
      Fit model
    \item
      Predict hold-out
    \item
      Calculate performance
    \end{itemize}
  \end{itemize}
\item
  Aggregate the results
\item
  Determine the final tuning parameter
\item
  Refit the model with the entire data set
\end{itemize}

\begin{figure}
\centering
\includegraphics[width=0.8\textwidth,height=\textheight]{images/ParameterTuningProcess.png}
\caption{Parameter Tuning Process}
\end{figure}

The above is an outline of the general procedure to tune parameters. Now let's focus on the critical part of the process: data splitting. Ideally, we should evaluate model using samples that were not used to build or fine-tune the model. So it provides an unbiased sense of model effectiveness. When the sample size is large, it is a good practice to set aside part of the samples to evaluate the final model. People use ``training'' data to indicate samples used to fit or fine-tune the model and ``test'' or ``validation'' data set is used to validate performance.

The first decision to make for data splitting is to decide the proportion of data in the test set. There are two factors to consider here: (1) sample size; (2) computation intensity. If the sample size is large enough which is the most common situation according to my experience, you can try to use 20\%, 30\% and 40\% of the data as the test set, and see which one works the best. If the model is computationally intense, then you may consider starting from a smaller sample of data to train the model hence will have a higher portion of data in the test set. Depending on how it performs, you may need to increase the training set. If the sample size is small, you can use cross-validation or bootstrap which is the topic in the next section.

The next decision is to decide which samples are in the test set. There is a desire to make the training and test sets as similar as possible. A simple way is to split data by random sampling which, however, does not control for any of the data attributes, such as the percentage of the retained customer in the data. So it is possible that the distribution of outcomes is substantially different between the training and test sets. There are three main ways to split the data that account for the similarity of resulted data sets. We will describe the three approaches using the clothing company customer data as examples.

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\tightlist
\item
  Split data according to the outcome variable
\end{enumerate}

Assume the outcome variable is customer segment (column \texttt{segment}) and we decide to use 80\% as training and 20\% test. The goal is to make the proportions of the categories in the two sets as similar as possible. The \texttt{createDataPartition()} function in \texttt{caret} will return a balanced splitting based on assigned variable.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# load data}
\NormalTok{sim.dat <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"https://raw.githubusercontent.com/happyrabbit/DataScientistR/master/Data/SegData.csv"}\NormalTok{)}
\KeywordTok{library}\NormalTok{(caret)}
\CommentTok{# set random seed to make sure reproducibility}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{3456}\NormalTok{)}
\NormalTok{trainIndex <-}\StringTok{ }\KeywordTok{createDataPartition}\NormalTok{(sim.dat}\OperatorTok{$}\NormalTok{segment, }\DataTypeTok{p =} \FloatTok{0.8}\NormalTok{, }\DataTypeTok{list =} \OtherTok{FALSE}\NormalTok{, }
    \DataTypeTok{times =} \DecValTok{1}\NormalTok{)}
\KeywordTok{head}\NormalTok{(trainIndex)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      Resample1
## [1,]         1
## [2,]         2
## [3,]         3
## [4,]         4
## [5,]         6
## [6,]         7
\end{verbatim}

The \texttt{list\ =\ FALSE} in the call to \texttt{createDataPartition} is to return a data frame. The \texttt{times\ =\ 1} tells R how many times you want to split the data. Here we only do it once, but you can repeat the splitting multiple times. In that case, the function will return multiple vectors indicating the rows to training/test. You can set \texttt{times2} and rerun the above code to see the result. Then we can use the returned indicator vector \texttt{trainIndex} to get training and test sets:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# get training set}
\NormalTok{datTrain <-}\StringTok{ }\NormalTok{sim.dat[trainIndex, ]}
\CommentTok{# get test set}
\NormalTok{datTest <-}\StringTok{ }\NormalTok{sim.dat[}\OperatorTok{-}\NormalTok{trainIndex, ]}
\end{Highlighting}
\end{Shaded}

According to the setting, there are 800 samples in the training set and 200 in test set. Let's check the distribution of the two sets:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(plyr)}
\KeywordTok{ddply}\NormalTok{(datTrain, }\StringTok{"segment"}\NormalTok{, summarise, }\DataTypeTok{count =} \KeywordTok{length}\NormalTok{(segment), }
    \DataTypeTok{percentage =} \KeywordTok{round}\NormalTok{(}\KeywordTok{length}\NormalTok{(segment)}\OperatorTok{/}\KeywordTok{nrow}\NormalTok{(datTrain), }\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       segment count percentage
## 1 Conspicuous   160       0.20
## 2       Price   200       0.25
## 3     Quality   160       0.20
## 4       Style   280       0.35
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ddply}\NormalTok{(datTest, }\StringTok{"segment"}\NormalTok{, summarise, }\DataTypeTok{count =} \KeywordTok{length}\NormalTok{(segment), }
    \DataTypeTok{percentage =} \KeywordTok{round}\NormalTok{(}\KeywordTok{length}\NormalTok{(segment)}\OperatorTok{/}\KeywordTok{nrow}\NormalTok{(datTest), }\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       segment count percentage
## 1 Conspicuous    40       0.20
## 2       Price    50       0.25
## 3     Quality    40       0.20
## 4       Style    70       0.35
\end{verbatim}

The percentages are the same for these two sets. In practice, it is possible that the distributions are not exactly identical but should be close.

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  Divide data according to predictors
\end{enumerate}

An alternative way is to split data based on the predictors. The goal is to get a diverse subset from a dataset so that the sample is representative. In other words, we need an algorithm to identify the \(n\) most diverse samples from a dataset with size \(N\). However, the task is generally infeasible for non-trivial values of \(n\) and \(N\) \citep{willett}. And hence practicable approaches to dissimilarity-based selection involve approximate methods that are sub-optimal. A major class of algorithms split the data on \emph{maximum dissimilarity sampling}. The process starts from:

\begin{itemize}
\tightlist
\item
  Initialize a single sample as starting test set
\item
  Calculate the dissimilarity between this initial sample and each remaining samples in the dataset
\item
  Add the most dissimilar unallocated sample to the test set
\end{itemize}

To move forward, we need to define the dissimilarity between groups. Each definition results in a different version of the algorithm and hence a different subset. It is the same problem as in hierarchical clustering where you need to define a way to measure the distance between clusters. The possible approaches are to use minimum, maximum, sum of all distances, the average of all distances, etc. Unfortunately, there is not a single best choice, and you may have to try multiple methods and check the resulted sample sets. R users can implement the algorithm using \texttt{maxDissim()} function from \texttt{caret} package. The \texttt{obj} argument is to set the definition of dissimilarity. Refer to the help documentation for more details (\texttt{?maxDissim}).

Let's use two variables (\texttt{age} and \texttt{income}) from the customer data as an example to illustrate how it works in R and compare maximum dissimilarity sampling with random sampling.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(lattice)}
\CommentTok{# select variables}
\NormalTok{testing <-}\StringTok{ }\KeywordTok{subset}\NormalTok{(sim.dat, }\DataTypeTok{select =} \KeywordTok{c}\NormalTok{(}\StringTok{"age"}\NormalTok{, }\StringTok{"income"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Random select 5 samples as initial subset (\texttt{start}) , the rest will be in \texttt{samplePool}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{5}\NormalTok{)}
\CommentTok{# select 5 random samples}
\NormalTok{startSet <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\KeywordTok{dim}\NormalTok{(testing)[}\DecValTok{1}\NormalTok{], }\DecValTok{5}\NormalTok{)}
\NormalTok{start <-}\StringTok{ }\NormalTok{testing[startSet, ]}
\CommentTok{# save the rest in data frame 'samplePool'}
\NormalTok{samplePool <-}\StringTok{ }\NormalTok{testing[}\OperatorTok{-}\NormalTok{startSet, ]}
\end{Highlighting}
\end{Shaded}

Use \texttt{maxDissim()} to select another 5 samples from \texttt{samplePool} that are as different as possible with the initical set \texttt{start}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{selectId <-}\StringTok{ }\KeywordTok{maxDissim}\NormalTok{(start, samplePool, }\DataTypeTok{obj =}\NormalTok{ minDiss, }\DataTypeTok{n =} \DecValTok{5}\NormalTok{)}
\NormalTok{minDissSet <-}\StringTok{ }\NormalTok{samplePool[selectId, ]}
\end{Highlighting}
\end{Shaded}

The \texttt{obj\ =\ minDiss} in the above code tells R to use minimum dissimilarity to define the distance between groups. Next, random select 5 samples from \texttt{samplePool} in data frame \texttt{RandomSet}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{selectId <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\KeywordTok{dim}\NormalTok{(samplePool)[}\DecValTok{1}\NormalTok{], }\DecValTok{5}\NormalTok{)}
\NormalTok{RandomSet <-}\StringTok{ }\NormalTok{samplePool[selectId, ]}
\end{Highlighting}
\end{Shaded}

Plot the resulted set to compare different sampling methods:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{start}\OperatorTok{$}\NormalTok{group <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\StringTok{"Initial Set"}\NormalTok{, }\KeywordTok{nrow}\NormalTok{(start))}
\NormalTok{minDissSet}\OperatorTok{$}\NormalTok{group <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\StringTok{"Maximum Dissimilarity Sampling"}\NormalTok{, }\KeywordTok{nrow}\NormalTok{(minDissSet))}
\NormalTok{RandomSet}\OperatorTok{$}\NormalTok{group <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\StringTok{"Random Sampling"}\NormalTok{, }\KeywordTok{nrow}\NormalTok{(RandomSet))}
\KeywordTok{xyplot}\NormalTok{(age }\OperatorTok{~}\StringTok{ }\NormalTok{income, }\DataTypeTok{data =} \KeywordTok{rbind}\NormalTok{(start, minDissSet, RandomSet), }\DataTypeTok{grid =} \OtherTok{TRUE}\NormalTok{, }
    \DataTypeTok{group =}\NormalTok{ group, }\DataTypeTok{auto.key =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{IDS_files/figure-latex/maxdis-1} 

}

\caption{Compare Maximum Dissimilarity Sampling with  Random Sampling}\label{fig:maxdis}
\end{figure}

The points from maximum dissimilarity sampling are far away from the initial samples ( Fig. \ref{fig:maxdis}, while the random samples are much closer to the initial ones. Why do we need a diverse subset? Because we hope the test set to be representative. If all test set samples are from respondents younger than 30, model performance on the test set has a high risk to fail to tell you how the model will perform on more general population.

\begin{itemize}
\tightlist
\item
  Divide data according to time
\end{itemize}

For time series data, random sampling is usually not the best way. There is an approach to divide data according to time-series. Since time series is beyond the scope of this book, there is not much discussion here. For more detail of this method, see \citep{Hyndman}. We will use a simulated first-order autoregressive model {[}AR (1){]} time-series data with 100 observations to show how to implement using the function \texttt{createTimeSlices\ ()} in the \texttt{caret} package.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# simulte AR(1) time series samples}
\NormalTok{timedata =}\StringTok{ }\KeywordTok{arima.sim}\NormalTok{(}\KeywordTok{list}\NormalTok{(}\DataTypeTok{order=}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{), }\DataTypeTok{ar=}\OperatorTok{-}\NormalTok{.}\DecValTok{9}\NormalTok{), }\DataTypeTok{n=}\DecValTok{100}\NormalTok{)}
\CommentTok{# plot time series}
\KeywordTok{plot}\NormalTok{(timedata, }\DataTypeTok{main=}\NormalTok{(}\KeywordTok{expression}\NormalTok{(}\KeywordTok{AR}\NormalTok{(}\DecValTok{1}\NormalTok{)}\OperatorTok{~}\ErrorTok{~~}\NormalTok{phi}\OperatorTok{==-}\NormalTok{.}\DecValTok{9}\NormalTok{)))     }
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{IDS_files/figure-latex/times-1} 

}

\caption{Divide data according to time}\label{fig:times}
\end{figure}

Fig. \ref{fig:times} shows 100 simulated time series observation. The goal is to make sure both training and test set to cover the whole period.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{timeSlices <-}\StringTok{ }\KeywordTok{createTimeSlices}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\KeywordTok{length}\NormalTok{(timedata), }
                   \DataTypeTok{initialWindow =} \DecValTok{36}\NormalTok{, }\DataTypeTok{horizon =} \DecValTok{12}\NormalTok{, }\DataTypeTok{fixedWindow =}\NormalTok{ T)}
\KeywordTok{str}\NormalTok{(timeSlices,}\DataTypeTok{max.level =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## List of 2
##  $ train:List of 53
##  $ test :List of 53
\end{verbatim}

There are three arguments in the above \texttt{createTimeSlices()}.

\begin{itemize}
\tightlist
\item
  \texttt{initialWindow}: The initial number of consecutive values in each training set sample
\item
  \texttt{horizon}: the number of consecutive values in test set sample
\item
  \texttt{fixedWindow}: if FALSE, all training samples start at 1
\end{itemize}

The function returns two lists, one for the training set, the other for the test set. Let's look at the first training sample:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# get result for the 1st training set}
\NormalTok{trainSlices <-}\StringTok{ }\NormalTok{timeSlices[[}\DecValTok{1}\NormalTok{]]}
\CommentTok{# get result for the 1st test set}
\NormalTok{testSlices <-}\StringTok{ }\NormalTok{timeSlices[[}\DecValTok{2}\NormalTok{]]}
\CommentTok{# check the index for the 1st training and test set}
\NormalTok{trainSlices[[}\DecValTok{1}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17
## [18] 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34
## [35] 35 36
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{testSlices[[}\DecValTok{1}\NormalTok{]]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] 37 38 39 40 41 42 43 44 45 46 47 48
\end{verbatim}

The first training set is consist of sample 1-36 in the dataset (\texttt{initialWindow\ =\ 36}). Then sample 37-48 are in the first test set ( \texttt{horizon\ =\ 12}). Type \texttt{head(trainSlices)} or \texttt{head(testSlices)} to check the later samples. If you are not clear about the argument \texttt{fixedWindow}, try to change the setting to be \texttt{F} and check the change in \texttt{trainSlices} and \texttt{testSlices}.

Understand and implement data splitting is not difficult. But there are two things to note:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The randomness in the splitting process will lead to uncertainty in performance measurement.
\item
  When the dataset is small, it can be too expensive to leave out test set. In this situation, if collecting more data is just not possible, the best shot is to use leave-one-out cross-validation which is in the next section.
\end{enumerate}

\hypertarget{resampling}{%
\subsection{Resampling}\label{resampling}}

You can consider resampling as repeated splitting. The basic idea is: use part of the data to fit model and then use the rest of data to calculate model performance. Repeat the process multiple times and aggregate the results. The differences in resampling techniques usually center around the ways to choose subsamples. There are two main reasons that we may need resampling:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Estimate tuning parameters through resampling. Some examples of models with such parameters are Support Vector Machine (SVM), models including the penalty (LASSO) and random forest.
\item
  For models without tuning parameter, such as ordinary linear regression and partial least square regression, the model fitting doesn't require resampling. But you can study the model stability through resampling.
\end{enumerate}

We will introduce three most common resampling techniques: k-fold cross-validation, repeated training/test splitting, and bootstrap.

\hypertarget{k-fold-cross-validation}{%
\subsubsection{k-fold cross-validation}\label{k-fold-cross-validation}}

k-fold cross-validation is to partition the original sample into \(k\) equal size subsamples (folds). Use one of the \(k\) folds to validate the model and the rest \(k-1\) to train model. Then repeat the process \(k\) times with each of the \(k\) folds as the test set. Aggregate the results into a performance profile.

Denote by \(\hat{f}^{-\kappa}(X)\) the fitted function, computed with the \(\kappa^{th}\) fold removed and \(x_i^\kappa\) the predictors for samples in left-out fold. The process of k-fold cross-validation is as follows:

\begin{quote}
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Partition the original sample into \(k\) equal size folds
\item
  for \(\kappa=1k\)
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Use data other than fold \(\kappa\) to train the model \(\hat{f}^{-\kappa}(X)\)
\item
  Apply \(\hat{f}^{-\kappa}(X)\) to predict fold \(\kappa\) to get \(\hat{f}^{-\kappa}(x_i^\kappa)\)
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Aggregate the results
  \[\hat{Error} = \frac{1}{N}\Sigma_{\kappa=1}^k\Sigma_{x_i^{\kappa}}L(y_i^{\kappa},\hat{f}^{-\kappa}(x_i^\kappa))\]
\end{enumerate}
\end{quote}

It is a standard way to find the value of tuning parameter that gives you the best performance. It is also a way to study the variability of model performance.

The following figure represents a 5-fold cross-validation example.

\begin{figure}
\centering
\includegraphics{images/cv5fold.png}
\caption{5-fold cross-validation}
\end{figure}

A special case of k-fold cross-validation is Leave One Out Cross Validation (LOOCV) where \(k=1\). When sample size is small, it is desired to use as many data to train the model. Most of the functions have default setting \(k=10\). The choice is usually 5-10 in practice, but there is no standard rule. The more folds to use, the more samples are used to fit model, and then the performance estimate is closer to the theoretical performance. Meanwhile, the variance of the performance is larger since the samples to fit model in different iterations are more similar. However, LOOCV has high computational cost since the number of interactions is the same as the sample size and each model fit uses a subset that is nearly the same size of the training set. On the other hand, when k is small (such as 2 or 3), the computation is more efficient, but the bias will increase. When the sample size is large, the impact of \(k\) becomes marginal.

Chapter 7 of \citep{Hastie2008} presents a more in-depth and more detailed discussion about the bias-variance trade-off in k-fold cross-validation.

You can implement k-fold cross-validation using \texttt{createFolds()} in \texttt{caret}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(caret)}
\NormalTok{class<-sim.dat}\OperatorTok{$}\NormalTok{segment}
\CommentTok{# creat k-folds}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{cv<-}\KeywordTok{createFolds}\NormalTok{(class,}\DataTypeTok{k=}\DecValTok{10}\NormalTok{,}\DataTypeTok{returnTrain=}\NormalTok{T)}
\KeywordTok{str}\NormalTok{(cv)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## List of 10
##  $ Fold01: int [1:900] 1 2 3 4 5 6 7 8 9 10 ...
##  $ Fold02: int [1:900] 1 2 3 4 5 6 7 9 10 11 ...
##  $ Fold03: int [1:900] 1 2 3 4 5 6 7 8 10 11 ...
##  $ Fold04: int [1:900] 1 2 3 4 5 6 7 8 9 11 ...
##  $ Fold05: int [1:900] 1 3 4 6 7 8 9 10 11 12 ...
##  $ Fold06: int [1:900] 1 2 3 4 5 6 7 8 9 10 ...
##  $ Fold07: int [1:900] 2 3 4 5 6 7 8 9 10 11 ...
##  $ Fold08: int [1:900] 1 2 3 4 5 8 9 10 11 12 ...
##  $ Fold09: int [1:900] 1 2 4 5 6 7 8 9 10 11 ...
##  $ Fold10: int [1:900] 1 2 3 5 6 7 8 9 10 11 ...
\end{verbatim}

The above code creates ten folds (\texttt{k=10}) according to the customer segments (we set \texttt{class} to be the categorical variable \texttt{segment}). The function returns a list of 10 with the index of rows in training set.

\hypertarget{repeated-trainingtest-splits}{%
\subsubsection{Repeated Training/Test Splits}\label{repeated-trainingtest-splits}}

In fact, this method is nothing but repeating the training/test set division on the original data. Fit the model with the training set, and evaluate the model with the test set. Unlike k-fold cross-validation, the test set generated by this procedure may have duplicate samples. A sample usually shows up in more than one test sets. There is no standard rule for split ratio and number of repetitions. The most common choice in practice is to use 75\% to 80\% of the total sample for training. The remaining samples are for validation. The more sample in the training set, the less biased the model performance estimate is. Increasing the repetitions can reduce the uncertainty in the performance estimates. Of course, it is at the cost of computational time when the model is complex. The number of repetitions is also related to the sample size of the test set. If the size is small, the performance estimate is more volatile. In this case, the number of repetitions needs to be higher to deal with the uncertainty of the evaluation results.

We can use the same function (\texttt{createDataPartition\ ()}) as before. If you look back, you will see \texttt{times\ =\ 1}. The only thing to change is to set it to the number of repetitions.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{trainIndex <-}\StringTok{ }\KeywordTok{createDataPartition}\NormalTok{(sim.dat}\OperatorTok{$}\NormalTok{segment, }\DataTypeTok{p =} \FloatTok{.8}\NormalTok{, }\DataTypeTok{list =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{times =} \DecValTok{5}\NormalTok{)}
\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{glimpse}\NormalTok{(trainIndex)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  int [1:800, 1:5] 1 3 4 5 6 7 8 9 10 11 ...
##  - attr(*, "dimnames")=List of 2
##   ..$ : NULL
##   ..$ : chr [1:5] "Resample1" "Resample2" "Resample3" "Resample4" ...
\end{verbatim}

Once know how to split the data, the repetition comes naturally.

\hypertarget{bootstrap-methods}{%
\subsubsection{Bootstrap Methods}\label{bootstrap-methods}}

Bootstrap is a powerful statistical tool (a little magic too). It can be used to analyze the uncertainty of parameter estimates \citep{bootstrap1986} quantitatively. For example, estimate the standard deviation of linear regression coefficients. The power of this method is that the concept is so simple that it can be easily applied to any model as long as the computation allows. However, you can hardly obtain the standard deviation for some models by using the traditional statistical inference.

Since it is with replacement, a sample can be selected multiple times, and the bootstrap sample size is the same as the original data. So for every bootstrap set, there are some left-out samples, which is also called ``out-of-bag samples.'' The out-of-bag sample is used to evaluate the model. Efron points out that under normal circumstances \citep{efron1983}, bootstrap estimates the error rate of the model with more certainty.The probability of an observation \(i\) in bootstrap sample B is:

\(\begin{array}{ccc} Pr{i\in B} & = & 1-\left(1-\frac{1}{N}\right)^{N}\\  & \approx & 1-e^{-1}\\  & = & 0.632 \end{array}\)

On average, 63.2\% of the observations appeared at least once in a bootstrap sample, so the estimation bias is similar to 2-fold cross-validation. As mentioned earlier, the smaller the number of folds, the larger the bias. Increasing the sample size will ease the problem. In general, bootstrap has larger bias and smaller uncertainty than cross-validation. Efron came up the following ``.632 estimator'' to alleviate this bias:

\[(0.632  original\ bootstrap\ estimate) + (0.368  apparent\ error\ rate)\]

The apparent error rate is the error rate when the data is used twice, both to fit the model and to check its accuracy and it is apparently over-optimistic. The modified bootstrap estimate reduces the bias but can be unstable with small samples size. This estimate can also be unduly optimistic when the model severely over-fits since the apparent error rate will be close to zero. Efron and Tibshirani \citep{b632plus} discuss another technique, called the ``632+ method,'' for adjusting the bootstrap estimates.

\hypertarget{measuring-performance}{%
\chapter{Measuring Performance}\label{measuring-performance}}

To compare different models, we need a way to measure model performance. There are various metrics to use. To better understand the strengths and weaknesses of a model, you need to look at it through multiple metrics. In this chapter, we will introduce some of the most common performance measurement metrics.

\hypertarget{regression-model-performance}{%
\section{Regression Model Performance}\label{regression-model-performance}}

\textbf{MSE and RMSE}

Mean Squared Error (MSE) measures the average of the squares of the errors---that is, the average squared difference between the estimated values and the actual value. The Root Mean Squared Error (RMSE) is the root square of the MSE.

\[MSE=\frac{1}{n}\sum_{i=1}^{n}(y_{i}-\hat{y}*{i})^{2}\]
\[RMSE=\sqrt{\frac{1}{n}\sum*{i=1}^{n}(y_{i}-\hat{y}_{i})^{2}}\]

Both are the common measurements for the regression model performance. Let's use the previous \texttt{income} prediction as an example. Fit a simple linear model:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sim.dat <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"http://bit.ly/2P5gTw4"}\NormalTok{)}
\NormalTok{fit<-}\StringTok{ }\KeywordTok{lm}\NormalTok{(}\DataTypeTok{formula =}\NormalTok{ income }\OperatorTok{~}\StringTok{ }\NormalTok{store_exp }\OperatorTok{+}\StringTok{ }\NormalTok{online_exp }\OperatorTok{+}\StringTok{ }\NormalTok{store_trans }\OperatorTok{+}\StringTok{ }
\StringTok{    }\NormalTok{online_trans, }\DataTypeTok{data =}\NormalTok{ sim.dat)}
\KeywordTok{summary}\NormalTok{(fit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = income ~ store_exp + online_exp + store_trans + 
##     online_trans, data = sim.dat)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -128768  -15804     441   13375  150945 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(>|t|)    
## (Intercept)  85711.680   3651.599   23.47  < 2e-16 ***
## store_exp        3.198      0.475    6.73  3.3e-11 ***
## online_exp       8.995      0.894   10.06  < 2e-16 ***
## store_trans   4631.751    436.478   10.61  < 2e-16 ***
## online_trans -1451.162    178.835   -8.11  1.8e-15 ***
## ---
## Signif. codes:  
## 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 31500 on 811 degrees of freedom
##   (184 observations deleted due to missingness)
## Multiple R-squared:  0.602,  Adjusted R-squared:   0.6 
## F-statistic:  306 on 4 and 811 DF,  p-value: <2e-16
\end{verbatim}

The fitted results \texttt{fit} shows the RMSE is 31530 (at the bottom of the output after \texttt{Residual\ standard\ error:}).

Another common performance measure for the regression model is R-Squared, often denoted as \(R^2\). It is the square of the correlation between the fitted value and the observed value. It is often explained as the percentage of the information in the data that can be explained by the model. The above model returns a R-squared0.6, which indicates the model can explain 60\% of the variance in variable \texttt{income}. While \(R^2\) is easy to explain, it is not a direct measure of model accuracy but correlation. Here the \(R^2\) value is not low but the RMSE is 0.6 which means the average difference between model fitting and the observation is \ensuremath{3.153\times 10^{4}}. It is a big discrepancy from an application point of view. When the response variable has a large scale and high variance, a high \(R^2\) doesn't mean the model has enough accuracy. It is also important to remember that \(R^2\) is dependent on the variation of the outcome variable. If the data has a response variable with a higher variance, the model based on it tends to have a higher \(R^2\).

We used \(R^2\) to show the impact of the error from independent and response variables in Chapter \ref{modeltuningstrategy} where we didn't consider the impact of the number of parameters (because the number of parameters is very small compared to the number of observations). However, \(R^2\) increases as the number of parameters increases. So people usually use Adjusted R-squared which is designed to mitigate the issue. The original \(R^2\) is defined as:

\[R^{2}=1-\frac{RSS}{TSS}\]

where \(RSS=\sum_{i=1}^{n}(y_{i}-\hat{y_{i}})^{2}\) and \(TSS=\sum_{i=1}^{n}(y_{i}-\bar{y})^{2}\).

Since RSS is always decreasing as the number of parameters increases, \(R^2\) increases as a result. For a model with \(p\) parameters, the adjusted \(R^2\) is defined as:

\[Adjusted\ R^{2}=1-\frac{RSS/(n-p-1)}{TSS/(n-1)}\]

To maximize the adjusted \(R^{2}\) is identical to minimize \(RSS/(n-p-1)\). Since the number of parameters \(p\) is reflected in the equation, \(RSS/(n-p-1)\) can increase or decrease as \(p\) increases. The idea behind this is that the adjusted R-squared increases if the new variable improves the model more than would be expected by chance. It decreases when a predictor improves the model by less than expected by chance. While~values are usually positive,~they can be~negative~as well.

Another measurement is \(C_{p}\). For a least squared model with \(p\) parameters:

\[C_{p}=\frac{1}{n}(RSS+2p\hat{\sigma}^{2})\]

where \(\hat{\sigma}^{2}\) is the estimator of the model random effect \(\epsilon\). \(C_{p}\) is to add penalty \(2p\hat{\sigma}^{2}\) to the training set \(RSS\). The goal is to adjust the over-optimistic measurement based on training data. As the number of parameters increases, the penalty increases. It counteracts the decrease of \(RSS\) due to increasing the number of parameters. We choose the model with a smaller \(C_{p}\).

Both AIC and BIC are based on the maximum likelihood. In linear regression, the maximum likelihood estimate is the least squared estimate. The definitions of the two are:

\[AIC=n+nlog(2\pi)+nlog(RSS/n)+2(p+1)\]

\[BIC=n+nlog(2\pi)+nlog(RSS/n)+log(n)(p+1)\]

R function AIC() and BIC() will calculate the AIC and BIC value according to the above equations. Many textbooks ignore content item \(n+nlog(2\pi)\), and use \(p\) instead of \(p+1\). Those slightly different versions give the same results since we are only interested in the relative value. Comparing to AIC, BIC puts a heavier penalty on the number of parameters.

\hypertarget{classification-model-performance}{%
\section{Classification Model Performance}\label{classification-model-performance}}

This section focuses on performance measurement for models with a categorical response. The metrics in the previous section are for models with a continuous response and they are not appropriate in the context of classification. Most of the classification problems are dichotomous, such as an outbreak of disease, spam email, etc. There are also cases with more than two categories as the segments in the clothing company data. We use swine disease data to illustrate different metrics. Let's train a random forest model as an example. We will discuss the model in Chapter \ref{treemodel}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(dplyr)}
\KeywordTok{library}\NormalTok{(randomForest)}
\KeywordTok{library}\NormalTok{(caret)}
\KeywordTok{library}\NormalTok{(readr)}
\NormalTok{disease_dat <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"http://bit.ly/2KXb1Qi"}\NormalTok{)}
\CommentTok{# you can check the data using glimpse()}
\CommentTok{# glimpse(disease_dat)}
\end{Highlighting}
\end{Shaded}

Separate the data to be training and testing sets. Fit model using training data (\texttt{xTrain} and \texttt{yTrain}) and applied the trained model on testing data (\texttt{xTest} and \texttt{yTest}) to evaluate model performance. We use 70\% of the sample as training and the rest 30\% as testing.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{100}\NormalTok{)}
\CommentTok{# separate the data to be training and testing}
\NormalTok{trainIndex <-}\StringTok{ }\KeywordTok{createDataPartition}\NormalTok{(disease_dat}\OperatorTok{$}\NormalTok{y, }\DataTypeTok{p =} \FloatTok{0.8}\NormalTok{, }
    \DataTypeTok{list =}\NormalTok{ F, }\DataTypeTok{times =} \DecValTok{1}\NormalTok{)}
\NormalTok{xTrain <-}\StringTok{ }\NormalTok{disease_dat[trainIndex, ] }\OperatorTok{%>%}\StringTok{ }\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\NormalTok{y)}
\NormalTok{xTest <-}\StringTok{ }\NormalTok{disease_dat[}\OperatorTok{-}\NormalTok{trainIndex, ] }\OperatorTok{%>%}\StringTok{ }\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\NormalTok{y)}
\CommentTok{# the response variable need to be factor}
\NormalTok{yTrain <-}\StringTok{ }\NormalTok{disease_dat}\OperatorTok{$}\NormalTok{y[trainIndex] }\OperatorTok{%>%}\StringTok{ }\KeywordTok{as.factor}\NormalTok{()}
\NormalTok{yTest <-}\StringTok{ }\NormalTok{disease_dat}\OperatorTok{$}\NormalTok{y[}\OperatorTok{-}\NormalTok{trainIndex] }\OperatorTok{%>%}\StringTok{ }\KeywordTok{as.factor}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

Train a random forest model:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{train_rf <-}\StringTok{ }\KeywordTok{randomForest}\NormalTok{(yTrain }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =}\NormalTok{ xTrain, }\DataTypeTok{mtry =} \KeywordTok{trunc}\NormalTok{(}\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{ncol}\NormalTok{(xTrain) }\OperatorTok{-}\StringTok{ }
\StringTok{    }\DecValTok{1}\NormalTok{)), }\DataTypeTok{ntree =} \DecValTok{1000}\NormalTok{, }\DataTypeTok{importance =}\NormalTok{ T)}
\end{Highlighting}
\end{Shaded}

Apply the trained random forest model to the testing data to get two types of predictions:

\begin{itemize}
\tightlist
\item
  probability (a value between 0 to 1)
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{yhatprob <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(train_rf, xTest, }\StringTok{"prob"}\NormalTok{)}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{100}\NormalTok{)}
\NormalTok{car}\OperatorTok{::}\KeywordTok{some}\NormalTok{(yhatprob)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##         0     1
## 47  0.831 0.169
## 101 0.177 0.823
## 196 0.543 0.457
## 258 0.858 0.142
## 274 0.534 0.466
## 369 0.827 0.173
## 389 0.852 0.148
## 416 0.183 0.817
## 440 0.523 0.477
## 642 0.836 0.164
\end{verbatim}

\begin{itemize}
\tightlist
\item
  category prediction (0 or 1)
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{yhat <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(train_rf, xTest)}
\NormalTok{car}\OperatorTok{::}\KeywordTok{some}\NormalTok{(yhat)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 146 232 269 302 500 520 521 575 738 781 
##   0   0   1   0   0   0   1   0   0   0 
## Levels: 0 1
\end{verbatim}

We will use the above two types of predictions to show different performance metrics.

\hypertarget{confusion-matrix}{%
\subsection{Confusion Matrix}\label{confusion-matrix}}

\textbf{Confusion Matrix} is a counting table to describe the performance of a classification model. For the true response \texttt{yTest} and prediction \texttt{yhat}, the confusion matrix is:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{yhat =}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(yhat) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{relevel}\NormalTok{(}\StringTok{"1"}\NormalTok{)}
\NormalTok{yTest =}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(yTest) }\OperatorTok{%>%}\StringTok{ }\KeywordTok{relevel}\NormalTok{(}\StringTok{"1"}\NormalTok{)}
\KeywordTok{table}\NormalTok{(yhat,yTest)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     yTest
## yhat  1  0
##    1 56  1
##    0 15 88
\end{verbatim}

The top-left and bottom-right are the numbers of correctly classified samples. The top-right and bottom-left are the numbers of wrongly classified samples. A general confusion matrix for a binary classifier is following:

\begin{longtable}[]{@{}ccc@{}}
\toprule
& Predicted Yes & Predicted No\tabularnewline
\midrule
\endhead
Actual Yes & TP & FN\tabularnewline
Actual No & FP & TN\tabularnewline
\bottomrule
\end{longtable}

where TP is true positive, FP is false positive, TN is true negative, FN is false negative. The cells along the diagonal line from top-left to bottom-right contain the counts of correctly classified samples. The cells along the other diagonal line contain the counts of wrongly classified samples. The most straightforward performance measure is the \textbf{total accuracy} which is the percentage of correctly classified samples:

\[Total\ accuracy = \frac{TP+TN}{TP+TN+FP+FN}\]

You can calculate the total accuracy when there are more than two categories. This statistic is straightforward but has some disadvantages. First, it doesn't differentiate different error types. In a real application, different types of error may have different impacts. For example, it is much worse to tag an important email as spam and miss it than failing to filter out a spam email. Provost et al. \citep{Provost1998} discussed in detail the problem of using total accuracy on different classifiers. There are some other metrics based on the confusion matrix that measure different types of error.

\textbf{Precision} is a metric to measure how accurate positive predictions are (i.e.~among those emails predicted as spam, how many percentages of them are spam emails?):

\[precision = \frac{TP}{TP+FP}\]

\textbf{Sensitivity} is to measure the coverage of actual positive samples (i.e.~among those spam emails, how many percentages of them are predicted as spam) :

\[Sensitivity = \frac{TP}{TP+FN}\]

\textbf{Specificity} is to measure the coverage of actual negative samples (i.e.~among those non-spam emails, how many percentages of them pass the filter):

\[Specificity = \frac{TN}{TN+FP}\]

Since wrongly tagging an important email as spam has a bigger impact, in the spam email case, we want to make sure the model specificity is high enough.

Second, total accuracy doesn't reflect the natural frequencies of each class. For example, the percentage of fraud cases for insurance may be very low, like 0.1\%. A model can achieve nearly perfect accuracy (99.9\%) by predicting all samples to be negative. The percentage of the largest class in the training set is also called the no-information rate. In this example, the no-information rate is 99.9\%. You need to get a model that at least beats this rate.

\hypertarget{kappa-statistic}{%
\subsection{Kappa Statistic}\label{kappa-statistic}}

Another metric is the Kappa statistic. It measures the agreement between the observed and predicted classes. It was originally come up by Cohen etc. \citep{Cohen1960}. Kappa takes into account the accuracy generated simply by chance. It is defined as:

\[Kappa=\frac{P_{0}-P_{e}}{1-P_{e}}\]

Let \(n=TP+TN+FP+FN\) be the total number of samples, where \(P_{0}=\frac{TP+TN}{n}\) is the observed accuracy, \(P_{e}=\frac{(TP+FP)(TP+FN)+(FN+TN)(FP+TN)}{n^{2}}\) is the expected accuracy based on the marginal totals of the confusion matrix. Kappa can take on a value from -1 to 1. The higher the value, the higher the agreement. A value of 0 means there is no agreement between the observed and predicted classes, while a value of 1 indicates perfect agreement. A negative value indicates that the prediction is in the opposite direction of the observed value. The following table may help you ``visualize'' the interpretation of kappa \citep{landis1977}:

\begin{longtable}[]{@{}cc@{}}
\toprule
Kappa & Agreement\tabularnewline
\midrule
\endhead
\textless{} 0 & Less than chance agreement\tabularnewline
0.01--0.20 & Slight agreement\tabularnewline
0.21-- 0.40 & Fair agreement\tabularnewline
0.41--0.60 & Moderate agreement\tabularnewline
0.61--0.80 & Substantial agreement\tabularnewline
0.81--0.99 & Almost perfect agreement\tabularnewline
\bottomrule
\end{longtable}

In general, a value between 0.3 to 0.5 indicates a reasonable agreement. If a model has a high accuracy of 90\%, while the expected accuracy is also high, say 85\%. The Kappa statistics is \(\frac{1}{3}\). It means the prediction and the observation have a fair agreement. You can calculate Kappa when the number of categories is larger than 2. The package \texttt{fmsb} has a function \texttt{Kappa.test()} to calculate Cohen's Kappa statistics. The function can also return the hypothesis test result and a confidence interval. Use the above observation vector \texttt{yTest} and prediction vector \texttt{yhat} as an example, you can calculate the statistics:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# install.packages("fmsb")}
\NormalTok{kt<-fmsb}\OperatorTok{::}\KeywordTok{Kappa.test}\NormalTok{(}\KeywordTok{table}\NormalTok{(yhat,yTest))}
\NormalTok{kt}\OperatorTok{$}\NormalTok{Result}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Estimate Cohen's kappa statistics and test the
##  null hypothesis that the extent of agreement is
##  same as random (kappa=0)
## 
## data:  table(yhat, yTest)
## Z = 9.7, p-value <2e-16
## 95 percent confidence interval:
##  0.6972 0.8894
## sample estimates:
## [1] 0.7933
\end{verbatim}

The output of the above function contains an object named \texttt{Judgement}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{kt}\OperatorTok{$}\NormalTok{Judgement}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Substantial agreement"
\end{verbatim}

\hypertarget{roc}{%
\subsection{ROC}\label{roc}}

Receiver Operating Characteristic (ROC) curve uses the predicted class probabilities and determines an effective threshold such that values above the threshold are indicative of a specific event. We have shown the definitions of sensitivity and specificity above. The sensitivity is the true positive rate and specificity is true negative rate. ``1 - specificity'' is the false negative rate. ROC is a graph of pairs of true positive rate (sensitivity) and false positive rate (1-specificity) values that result as the test's cutoff value is varied. The Area Under the Curve(AUC) is a common measure for two-class problem. There is usually a trade-off between sensitivity and specificity. If the threshold is set lower, then there are more samples predicted as positive and hence the sensitivity is higher. Let's look at the predicted probability \texttt{yhatprob} in the swine disease example. The predicted probability object \texttt{yhatprob} has two columns, one is the predicted probability that a farm will have an outbreak, the other is the probability that farm will NOT have an outbreak. So the two add up to have value 1. We use the probability of outbreak (the 2nd column) for further illustration. You can use \texttt{roc()} function to get an ROC object (\texttt{rocCurve}) and then apply different functions on that object to get needed plot or ROC statistics. For example, the following code produces the ROC curve:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(pROC)}
\NormalTok{rocCurve<-}\KeywordTok{roc}\NormalTok{(}\DataTypeTok{response=}\NormalTok{yTest,}
              \DataTypeTok{predictor=}\NormalTok{yhatprob[,}\DecValTok{2}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Setting levels: control = 1, case = 0
\end{verbatim}

\begin{verbatim}
## Setting direction: controls > cases
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(rocCurve)}
\end{Highlighting}
\end{Shaded}

\includegraphics{IDS_files/figure-latex/unnamed-chunk-92-1.pdf}

The first argument of the \texttt{roc()} is, \texttt{response}, the observation vector. The second argument is \texttt{predictor} is the continuous prediction (probability or link function value). The x-axis of ROC curve is ``1 - specificity'' and the y-axis is ``sensitivity''. ROC curve starts from (0, 0) and ends with (1, 1). A perfect model that correctly identifies all the samples will have 100\% sensitivity and specificity which corresponds to the curve that also goes through (0, 1). The area under the perfect curve is 1. A model that is totally useless corresponds to a curve that is close to the diagonal line and an area under the curve about 0.5.

You can visually compare different models by putting their ROC curves on one plot. Or use the AUC to compare them. DeLong et al.~came up a statistic test to compare AUC based on U-statistics \citep{delong1988} which can give a p-value and confidence interval. You can also use bootstrap to get a confidence interval for AUC \citep{hall2004}.

We can use the following code in R to get an estimate of AUC and its confidence interval:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# get the estimate of AUC}
\KeywordTok{auc}\NormalTok{(rocCurve)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Area under the curve: 0.989
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# get a confidence interval based on DeLong et al.}
\KeywordTok{ci.auc}\NormalTok{(rocCurve)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 95% CI: 0.979-0.999 (DeLong)
\end{verbatim}

AUC is robust to class imbalance\citep{Provost1998, Fawcett2006} hence a popular measurement. But it still boils a lot of information down to one number so there is inevitably a loss of information. It is better to double check by comparing the curve at the same time. If you care more about getting a model that will have high specificity, which is the lower part of the curve, as in the spam filtering case, you can use the area of the lower part of the curve as the performance measurement \citep{McClish1989}. ROC is only for two-class case. Some researchers generalized it to situations with more than two categories \citep{Hand2001, Lachiche2003, Li2008}.

\hypertarget{gain-and-lift-charts}{%
\subsection{Gain and Lift Charts}\label{gain-and-lift-charts}}

Gain and lift chart is a visual tool for evaluating the performance of a classification model. In the previous swine disease example, there are 160 samples in the testing data and 89 of them have a positive outcome.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{table}\NormalTok{(yTest)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## yTest
##  1  0 
## 71 89
\end{verbatim}

If we order the testing samples by the predicted probability, one would hope that the positive samples are ranked higher than the negative ones. That is what the lift charts do: rank the samples by their scores and calculate the cumulative positive rate as more samples are evaluated. In the perfect scenario, the highest-ranked 71 samples would contain all 71 positive samples. When the model is totally random, the highest-ranked x\% of the data would contain about x\% of the positive sample. The gain/lift charts compare the ratio between the results obtained with and without a model.

Let's plot the lift charts to compare the predicted outbreak probability (\texttt{modelscore\ \textless{}-\ yhatprob{[}\ ,2{]}}) from random forest model we fit before with some random scores generated from a uniform distribution (\texttt{randomscore\ \textless{}-\ runif(length(yTest))}).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# predicted outbreak probability}
\NormalTok{modelscore <-}\StringTok{ }\NormalTok{yhatprob[ ,}\DecValTok{2}\NormalTok{]}
\CommentTok{# randomly sample from a uniform distribution}
\NormalTok{randomscore <-}\StringTok{ }\KeywordTok{runif}\NormalTok{(}\KeywordTok{length}\NormalTok{(yTest))}
\NormalTok{labs<-}\KeywordTok{c}\NormalTok{(}\DataTypeTok{modelscore=}\StringTok{"Random Forest Prediction"}\NormalTok{,}
        \DataTypeTok{randomscore=}\StringTok{"Random Number"}\NormalTok{)}
\NormalTok{liftCurve <-}\StringTok{ }\NormalTok{caret}\OperatorTok{::}\KeywordTok{lift}\NormalTok{(yTest }\OperatorTok{~}\StringTok{ }\NormalTok{modelscore }\OperatorTok{+}\StringTok{ }\NormalTok{randomscore,}
                  \DataTypeTok{class =} \StringTok{"1"}\NormalTok{, }
                  \DataTypeTok{labels =}\NormalTok{ labs)}
\KeywordTok{xyplot}\NormalTok{(liftCurve, }\DataTypeTok{auto.key =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{columns =} \DecValTok{2}\NormalTok{, }\DataTypeTok{lines =}\NormalTok{ T, }\DataTypeTok{points =}\NormalTok{ F))}
\end{Highlighting}
\end{Shaded}

\includegraphics{IDS_files/figure-latex/unnamed-chunk-95-1.pdf}

The x-axis is the percentage of samples tested and the y-axis is the percentage of positive samples that are detected by the model. For example, the point on the curve of random forest prediction, 8.125,18.31, indicates that if you order the testing samples by the predicted probability from high to low, the top 8.125\% of the samples contain 0.1831\% of the total positive outcomes.

Similar to the ROC curve, we can choose the model by comparing their lift charts. Some parts of the lift curves may be more interesting than the rest. For example, if you only have a budget to clean 50\% of the farms, then you should pick the model that gives the highest point when the x-axis is 50\%.

\hypertarget{regression-models}{%
\chapter{Regression Models}\label{regression-models}}

\hypertarget{ordinary-least-squares}{%
\section{Ordinary Least Squares}\label{ordinary-least-squares}}

\hypertarget{multivariate-adaptive-regression-splines}{%
\section{Multivariate Adaptive Regression Splines}\label{multivariate-adaptive-regression-splines}}

\hypertarget{generalized-linear-model}{%
\section{Generalized Linear Model}\label{generalized-linear-model}}

\hypertarget{pcr-and-pls}{%
\section{PCR and PLS}\label{pcr-and-pls}}

\hypertarget{regularization-methods}{%
\chapter{Regularization Methods}\label{regularization-methods}}

The regularization method is also known as the shrinkage method. It is a technique that constrains or regularizes the coefficient estimates. By imposing a penalty on the size of coefficients, it shrinks the coefficient estimates towards zero. It also intrinsically conduct feature selection and is naturally resistant to non-informative predictors. It may not be obvious why this technique improves model performance, but it turns out to be a very effective modeling technique. In this chapter, we will introduce two best-known regularization methods: ridge regression and lasso. The elastic net is a combination of ridge and lasso, or it is a general representation of the two.

We talked about the variance bias trade-off in section \ref{vbtradeoff}. The variance of a learning model is the amount by which \(\hat{f}\) would change if we estimated it using a different training data set. In general, model variance increases as flexibility increases. The regularization technique decreases the model flexibility by shrinking the coefficient and hence significantly reduce the model variance.

\hypertarget{ridge-regression}{%
\section{Ridge Regression}\label{ridge-regression}}

Recall that the least square estimates minimize RSS:

\[RSS=\Sigma_{i=1}^{n}(y_{i}-\beta_{0}-\Sigma_{j=1}^{p}\beta_{j}x_{ij})^{2}\]

Ridge regression \citep{Hoerl1970} is similar but it finds \(\hat{\beta}^{R}\) that optimizes a slightly different function:

\begin{equation}
\Sigma_{i=1}^{n}(y_{i}-\beta_{0}-\Sigma_{j=1}^{p}\beta_{j}x_{ij})^{2}+\lambda\Sigma_{j=1}^{p}\beta_{j}^{2}=RSS+\lambda\Sigma_{j=1}^{p}\beta_{j}^{2}
\label{eq:ridge}
\end{equation}

where \(\lambda >0\) is a tuning parameter. As with the least squares, ridge regression considers minimizing RSS. However, it adds a shrinkage penalty \(\lambda\Sigma_{j=1}^{p}\beta_{j}^{2}\) that takes account of the number of parameters in the model. When \(\lambda = 0\), it is identical to least squares. As \(\lambda\) gets larger, the coefficients start shrinking towards 0. When \(\lambda\rightarrow\infty\), the rest of the coefficients \(\beta_{1},...,\beta_{p}\) are close to 0. Here, the penalty is not applied to \(\beta_{0}\). The tuning parameter \(\lambda\) is used to adjust the impact of the two parts in equation \eqref{eq:ridge}. Every value of \(\lambda\) corresponds to a set of parameter estimates.

There are many R packages for ridge regression, such as lm.ridge() function from MASS, function enet() from, elasticnet. If you know the value of \(\lambda\), you can use either of the function to fit ridge regression. A more convenient way is to use train() function from caret. Let's use the 10 survey questions to predict the total purchase amount (sum of online and store purchase).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# install packages from CRAN}
\NormalTok{p_needed <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{'caret'}\NormalTok{, }\StringTok{'elasticnet'}\NormalTok{, }\StringTok{'glmnet'}\NormalTok{, }\StringTok{'devtools'}\NormalTok{)}
\NormalTok{packages <-}\StringTok{ }\KeywordTok{rownames}\NormalTok{(}\KeywordTok{installed.packages}\NormalTok{())}
\NormalTok{p_to_install <-}\StringTok{ }\NormalTok{p_needed[}\OperatorTok{!}\NormalTok{(p_needed }\OperatorTok{%in%}\StringTok{ }\NormalTok{packages)]}
\ControlFlowTok{if}\NormalTok{ (}\KeywordTok{length}\NormalTok{(p_to_install) }\OperatorTok{>}\StringTok{ }\DecValTok{0}\NormalTok{) \{}
    \KeywordTok{install.packages}\NormalTok{(p_to_install)}
\NormalTok{\}}

\KeywordTok{lapply}\NormalTok{(p_needed, require, }\DataTypeTok{character.only =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dat <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"http://bit.ly/2P5gTw4"}\NormalTok{)}
\CommentTok{# data cleaning: delete wrong observations since expense can't be negative}
\NormalTok{dat <-}\StringTok{ }\KeywordTok{subset}\NormalTok{(dat, store_exp }\OperatorTok{>}\StringTok{ }\DecValTok{0} \OperatorTok{&}\StringTok{ }\NormalTok{online_exp }\OperatorTok{>}\StringTok{ }\DecValTok{0}\NormalTok{)}
\CommentTok{# get predictors}
\NormalTok{trainx <-}\StringTok{ }\NormalTok{dat[ , }\KeywordTok{grep}\NormalTok{(}\StringTok{"Q"}\NormalTok{, }\KeywordTok{names}\NormalTok{(dat))]}
\CommentTok{# get response}
\NormalTok{trainy <-}\StringTok{ }\NormalTok{dat}\OperatorTok{$}\NormalTok{store_exp }\OperatorTok{+}\StringTok{ }\NormalTok{dat}\OperatorTok{$}\NormalTok{online_exp}
\end{Highlighting}
\end{Shaded}

Use \texttt{train()} function to tune parameter. Since ridge regression adds the penalty parameter \(\lambda\) in front of the sum of squares of the parameters, the scale of the parameters matters. So here it is better to center and scale the predictors. This preprocessing is recommended for all techniques that put penalty to parameter estimates. In this example, the 10 survey questions are already with the same scale so data preprocessing doesn't make too much different. It is a good idea to set the preprocessing as a standard.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# set cross validation}
\NormalTok{ctrl <-}\StringTok{ }\KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method =} \StringTok{"cv"}\NormalTok{, }\DataTypeTok{number =} \DecValTok{10}\NormalTok{)}
\CommentTok{# set the parameter range }
\NormalTok{ridgeGrid <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{.lambda =} \KeywordTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{.1}\NormalTok{, }\DataTypeTok{length =} \DecValTok{20}\NormalTok{))}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{100}\NormalTok{)}
\NormalTok{ridgeRegTune <-}\StringTok{ }\KeywordTok{train}\NormalTok{(trainx, trainy,}
                      \DataTypeTok{method =} \StringTok{"ridge"}\NormalTok{,}
                      \DataTypeTok{tuneGrid =}\NormalTok{ ridgeGrid,}
                      \DataTypeTok{trControl =}\NormalTok{ ctrl,}
                      \CommentTok{## center and scale predictors}
                      \DataTypeTok{preProc =} \KeywordTok{c}\NormalTok{(}\StringTok{"center"}\NormalTok{, }\StringTok{"scale"}\NormalTok{))}
\NormalTok{ridgeRegTune}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Ridge Regression 
## 
## 999 samples
##  10 predictor
## 
## Pre-processing: centered (10), scaled (10) 
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 899, 899, 899, 899, 899, 900, ... 
## Resampling results across tuning parameters:
## 
##   lambda    RMSE  Rsquared  MAE  
##   0.000000  1744  0.7952    754.0
##   0.005263  1744  0.7954    754.9
##   0.010526  1744  0.7955    755.9
##   0.015789  1744  0.7955    757.3
##   0.021053  1745  0.7956    758.8
##   0.026316  1746  0.7956    760.6
##   0.031579  1747  0.7956    762.4
##   0.036842  1748  0.7956    764.3
##   0.042105  1750  0.7956    766.4
##   0.047368  1751  0.7956    768.5
##   0.052632  1753  0.7956    770.6
##   0.057895  1755  0.7956    772.7
##   0.063158  1757  0.7956    774.9
##   0.068421  1759  0.7956    777.2
##   0.073684  1762  0.7956    779.6
##   0.078947  1764  0.7955    782.1
##   0.084211  1767  0.7955    784.8
##   0.089474  1769  0.7955    787.6
##   0.094737  1772  0.7955    790.4
##   0.100000  1775  0.7954    793.3
## 
## RMSE was used to select the optimal model using
##  the smallest value.
## The final value used for the model was lambda
##  = 0.005263.
\end{verbatim}

The results show that the best value of \(\lambda\) is 0.005 and the RMSE and \(R^{2}\) are 1744 and 0.7954 correspondingly. You can see from the figure \ref{fig:ridgeregtune}, as the \(\lambda\) increase, the RMSE first slightly decreases and then increases.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(ridgeRegTune)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{IDS_files/figure-latex/ridgeregtune-1} 

}

\caption{Test mean squared error for the ridge regression}\label{fig:ridgeregtune}
\end{figure}

Once you have the tuning parameter value, there are different functions to fit a ridge regression. Let's look at how to use \texttt{enet()} in \texttt{elasticnet} package.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ridgefit =}\StringTok{ }\KeywordTok{enet}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{as.matrix}\NormalTok{(trainx), }\DataTypeTok{y =}\NormalTok{ trainy, }\DataTypeTok{lambda =} \FloatTok{0.01}\NormalTok{,}
                \CommentTok{# center and scale predictors}
                \DataTypeTok{normalize =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Note here \texttt{ridgefit} only assigns the value of the tuning parameter for ridge regression. Since the elastic net model include both ridge and lasso penalty, we need to use \texttt{predict()} function to get the model fit. You can get the fitted results by setting \texttt{s\ =\ 1} and \texttt{mode\ =\ "fraction"}. Here \texttt{s\ =\ 1} means we only use the ridge parameter. We will come back to this when we get to lasso regression.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ridgePred <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(ridgefit, }\DataTypeTok{newx =} \KeywordTok{as.matrix}\NormalTok{(trainx), }
                     \DataTypeTok{s =} \DecValTok{1}\NormalTok{, }\DataTypeTok{mode =} \StringTok{"fraction"}\NormalTok{, }\DataTypeTok{type =} \StringTok{"fit"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

By setting \texttt{type\ =\ "fit"}, the above returns a list object. The \texttt{fit} item has the predictions:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{names}\NormalTok{(ridgePred)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "s"        "fraction" "mode"     "fit"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{head}\NormalTok{(ridgePred}\OperatorTok{$}\NormalTok{fit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      1      2      3      4      5      6 
## 1290.5  224.2  591.4 1220.6  853.4  908.2
\end{verbatim}

If you want to check the estimated coefficients, you can set \texttt{type="coefficients"}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ridgeCoef<-}\KeywordTok{predict}\NormalTok{(ridgefit,}\DataTypeTok{newx =} \KeywordTok{as.matrix}\NormalTok{(trainx), }
                   \DataTypeTok{s=}\DecValTok{1}\NormalTok{, }\DataTypeTok{mode=}\StringTok{"fraction"}\NormalTok{, }\DataTypeTok{type=}\StringTok{"coefficients"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

It also returns a list and the estimates are in the \texttt{coefficients} item:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# didn't show the results}
\NormalTok{RidgeCoef =}\StringTok{ }\NormalTok{ridgeCoef}\OperatorTok{$}\NormalTok{coefficients}
\end{Highlighting}
\end{Shaded}

Comparing to the least square regression, ridge regression performs better because of the bias-variance-trade-off we mentioned in section \ref{vbtradeoff}. As the penalty parameter \(\lambda\) increases, the flexibility of the ridge regression decreases. It decreases the variance of the model but increases the bias at the same time.

\hypertarget{lasso}{%
\section{LASSO}\label{lasso}}

Even though the ridge regression shrinks the parameter estimates towards 0, it won't shink any estimates to be exactly 0 which means it includes all predictors in the final model. So it can't select variables. It may not be a problem for prediction but it is a huge disadvantage if you want to interpret the model especially when the number of variables is large. A popular alternative to the ridge penalty is the \textbf{Least Absolute Shrinkage and Selection Operator} (LASSO) \citep{Tibshirani1996}.

Similar to ridge regression, lasso adds a penalty. The lasso coefficients \(\hat{\beta}_{\lambda}^{L}\) minimize the following:

\begin{equation}
\Sigma_{i=1}^{n}(y_{i}-\beta_{0}-\Sigma_{j=1}^{p}\beta_{j}x_{ij})^{2}+\lambda\Sigma_{j=1}^{p}|\beta_{j}|=RSS+\lambda\Sigma_{j=1}^{p}|\beta_{j}|
\label{eq:lasso}
\end{equation}

The only difference between lasso and ridge is the penalty. In statistical parlance, ridge uses \(L_2\) penalty (\(\beta_{j}^{2}\)) and lasso uses \(L_1\) penalty (\(|\beta_{j}|\)). \(L_1\) penalty can shrink the estimates to 0 when \(\lambda\) is big enough. So lasso can be used as a feature selection tool. It is a huge advantage because it leads to a more explainable model.

Similar to other models with tuning parameters, lasso regression requires cross-validation to tune the parameter. You can use \texttt{train()} in a similar way as we showed in the ridge regression section. To tune parameter, we need to set cross-validation and parameter range. Also, it is advised to standardize the predictors:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ctrl <-}\StringTok{ }\KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method =} \StringTok{"cv"}\NormalTok{, }\DataTypeTok{number =} \DecValTok{10}\NormalTok{)}
\NormalTok{lassoGrid <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{fraction =} \KeywordTok{seq}\NormalTok{(.}\DecValTok{8}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DataTypeTok{length =} \DecValTok{20}\NormalTok{))}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{100}\NormalTok{)}
\NormalTok{lassoTune <-}\StringTok{ }\KeywordTok{train}\NormalTok{(trainx, trainy,}
                      \CommentTok{## set the method to be lasso}
                      \DataTypeTok{method =} \StringTok{"lars"}\NormalTok{,}
                      \DataTypeTok{tuneGrid =}\NormalTok{ lassoGrid,}
                      \DataTypeTok{trControl =}\NormalTok{ ctrl,}
                      \CommentTok{## standardize the predictors}
                      \DataTypeTok{preProc =} \KeywordTok{c}\NormalTok{(}\StringTok{"center"}\NormalTok{, }\StringTok{"scale"}\NormalTok{))}
\NormalTok{lassoTune}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Least Angle Regression 
## 
## 999 samples
##  10 predictor
## 
## Pre-processing: centered (10), scaled (10) 
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 899, 899, 899, 899, 899, 900, ... 
## Resampling results across tuning parameters:
## 
##   fraction  RMSE  Rsquared  MAE  
##   0.8000    1763  0.7921    787.5
##   0.8105    1760  0.7924    784.1
##   0.8211    1758  0.7927    780.8
##   0.8316    1756  0.7930    777.7
##   0.8421    1754  0.7933    774.6
##   0.8526    1753  0.7936    771.8
##   0.8632    1751  0.7939    769.1
##   0.8737    1749  0.7942    766.6
##   0.8842    1748  0.7944    764.3
##   0.8947    1746  0.7947    762.2
##   0.9053    1745  0.7949    760.1
##   0.9158    1744  0.7951    758.3
##   0.9263    1743  0.7952    756.7
##   0.9368    1743  0.7953    755.5
##   0.9474    1742  0.7954    754.5
##   0.9579    1742  0.7954    754.0
##   0.9684    1742  0.7954    753.6
##   0.9789    1743  0.7953    753.4
##   0.9895    1743  0.7953    753.5
##   1.0000    1744  0.7952    754.0
## 
## RMSE was used to select the optimal model using
##  the smallest value.
## The final value used for the model was fraction
##  = 0.9579.
\end{verbatim}

The results show that the best value of the tuning parameter (\texttt{fraction} from the output) is 0.957 and the RMSE and \(R^{2}\) are 1742 and 0.7954 correspondingly. The performance is nearly the same with ridge regression. You can see from the figure \ref{fig:lassoregtune}, as the \(\lambda\) increase, the RMSE first decreases and then increases.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(lassoTune)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{IDS_files/figure-latex/lassoregtune-1} 

}

\caption{Test mean squared error for the lasso regression}\label{fig:lassoregtune}
\end{figure}

Once you select a value for tuning parameter, there are different functions to fit lasso regression, such as \texttt{lars()} in \texttt{lars}, \texttt{enet()} in \texttt{elasticnet}, \texttt{glmnet()} in \texttt{glmnet}. They all have very similar syntax.

Here we continue using \texttt{enet()}. The syntax is similar to ridge regression. The only difference is that you need to set \texttt{lambda\ =\ 0} because the argument \texttt{lambda} here is to control the ridge penalty. When it is 0, the function will return the lasso model object.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lassoModel<-}\StringTok{ }\KeywordTok{enet}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{as.matrix}\NormalTok{(trainx), }\DataTypeTok{y =}\NormalTok{ trainy, }\DataTypeTok{lambda =} \DecValTok{0}\NormalTok{, }\DataTypeTok{normalize =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Set the fraction value to be 0.957 (the value we got above):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lassoFit <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(lassoModel, }\DataTypeTok{newx =} \KeywordTok{as.matrix}\NormalTok{(trainx), }\DataTypeTok{s =} \FloatTok{0.957}\NormalTok{, }\DataTypeTok{mode =} \StringTok{"fraction"}\NormalTok{, }\DataTypeTok{type =} \StringTok{"fit"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Again by setting \texttt{type\ =\ "fit"}, the above returns a list object. The \texttt{fit} item has the predictions:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{head}\NormalTok{(lassoFit}\OperatorTok{$}\NormalTok{fit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      1      2      3      4      5      6 
## 1357.3  300.5  690.2 1228.2  838.4 1010.1
\end{verbatim}

You need to set \texttt{type\ =\ "coefficients"} to get parameter estimates:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lassoCoef <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(lassoModel, }\DataTypeTok{newx =} \KeywordTok{as.matrix}\NormalTok{(trainx), }\DataTypeTok{s =} \FloatTok{0.95}\NormalTok{, }\DataTypeTok{mode =} \StringTok{"fraction"}\NormalTok{, }\DataTypeTok{type =} \StringTok{"coefficients"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

It also returns a list and the estimates are in the \texttt{coefficients} item:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# didn't show the results}
\NormalTok{LassoCoef =}\StringTok{ }\NormalTok{lassoCoef}\OperatorTok{$}\NormalTok{coefficients}
\end{Highlighting}
\end{Shaded}

Many researchers applied lasso to other learning methods, such as linear discriminant analysis \citep{Clem2011}, partial least squares regression\citep{chun2010}. However, since the \(L_1\) norm is not differentiable, optimization for lasso regression is more complicated. People come up with different algorithms to solve the computation problem. The biggest breakthrough is Least Angle Regression {[}LARS{]} from Bradley Efron etc. This algorithm works well for lasso regression especially when the dimension is high.

\hypertarget{variable-selection-property-of-the-lasso}{%
\section{Variable selection property of the lasso}\label{variable-selection-property-of-the-lasso}}

You may ask why lasso (\(L_1\) penalty) has the feature selection property but not the ridge (\(L_2\) penalty). To answer that question, let's look at the alternative representations of the optimization problem for lasso and ridge. For lasso regression, it is identical to optimize the following two functions:

\begin{equation}
\Sigma_{i=1}^{n}(y_{i}-\beta_{0}-\Sigma_{j=1}^{p}\beta_{j}x_{ij})^{2}+\lambda\Sigma_{j=1}^{p}|\beta_{j}|=RSS+\lambda\Sigma_{j=1}^{p}|\beta_{j}|
\label{eq:lasso1}
\end{equation}

\begin{equation}
\underset{\beta}{min}\left\{ \Sigma_{i=1}^{n}\left(y_{i}-\beta_{0}-\Sigma_{j=1}^{p}\beta_{j}x_{ij}\right)^{2}\right\} ,\ \Sigma_{j=1}^{p}|\beta_{j}|\leq s
\label{eq:lasso2}
\end{equation}

For any value of tuning parameter \(\lambda\), there exists a \(s\) such that the coefficient estimates optimize equation \eqref{eq:lasso1} also optimize equation \eqref{eq:lasso2}. Similarly, for ridge regression, the two representations are identical:

\begin{equation}
\Sigma_{i=1}^{n}(y_{i}-\beta_{0}-\Sigma_{j=1}^{p}\beta_{j}x_{ij})^{2}+\lambda\Sigma_{j=1}^{p}\beta_{j}^{2}=RSS+\lambda\Sigma_{j=1}^{p}\beta_{j}^{2}
\label{eq:ridge1}
\end{equation}

\begin{equation}
\underset{\beta}{min}\left\{ \Sigma_{i=1}^{n}\left(y_{i}-\beta_{0}-\Sigma_{j=1}^{p}\beta_{j}x_{ij}\right)^{2}\right\} ,\ \Sigma_{j=1}^{p}\beta_{j}^{2}\leq s
\label{eq:ridge2}
\end{equation}

When \(p2\), lasso estimates (\(\hat{\beta}_{lasso}\) in figure \ref{fig:lassoridge}) have the smallest RSS among all points that satisfy \(|\beta*{1}|+|\beta_{2}|\leq s\) (i.e.~within the diamond in figure \ref{fig:lassoridge}). Ridge estimates have the smallest RSS among all points that satisfy \(\beta_{1}^{2}+\beta_{2}^{2}\leq s\) (i.e.~within the circle in figure \ref{fig:lassoridge}). As s increases, the diamond and circle regions expand and get less restrictive. If s is large enough, the restrictive region will cover the least squares estimate (\(\hat{\beta}_{LSE}\)). Then, equations \eqref{eq:lasso2} and \eqref{eq:ridge2} will simply yield the least squares estimate. In contrast, if s is small, the grey region in figure \ref{fig:lassoridge} will be small and hence restrict the magnitude of \(\beta\). With the alternative formulations, we can answer the question of feature selection property.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{images/LassoRidge} 

}

\caption{Contours of the RSS and constrain functions for the lasso (left) and ridge regression (right)}\label{fig:lassoridge}
\end{figure}

Figure \ref{fig:lassoridge} illustrates the situations for lasso (left) and ridge (right). The least square estimate is marked as \(\hat{\beta}\). The restrictive regions are in grey, the diamond region is for the lasso, the circle region is for the ridge. The least square estimates lie outside the grey region, so they are different from the lasso and ridge estimates. The ellipses centered around \(\hat{\beta}\) represent contours of RSS. All the points on a given ellipse share an RSS. As the ellipses expand, the RSS increases. The lasso and ridge estimates are the first points at which an ellipse contracts the grey region. Since ridge regression has a circular restrictive region that doesn't have a sharp point, the intersecting point can't drop on the axis. But it is possible for lasso since it has corners at each of the axes. When the intersecting point is on an axis, one of the parameter estimates is 0. If p \textgreater{} 2, the restrictive regions become sphere or hypersphere. In that case, when the intersecting point drops on an axis, multiple coefficient estimates can equal 0 simultaneously.

\hypertarget{elastic-net}{%
\section{Elastic Net}\label{elastic-net}}

Elastic Net is a generalization of lasso and ridge regression\citep{zou2005}. It combines the two penalties. The estimates of coefficients optimize the following function:

\begin{equation}
\Sigma_{i=1}^{n}(y_{i}-\hat{y}_{i})^{2}+\lambda_{1}\Sigma_{j=1}^{p}\beta_{j}^{2}+\lambda_{2}\Sigma_{j=1}^{p}|\beta_{j}|
\label{eq:elasticnet}
\end{equation}

Ridge penalty shrinks the coefficients of correlated predictors towards each other while the lasso tends to pick one and discard the others. So lasso estimates have a higher variance. However, ridge regression doesn't have a variable selection property. The advantage of the elastic net is that it keeps the feature selection quality from the lasso penalty as well as the effectiveness of the ridge penalty. \citep{zou2005} suggest that it deals with highly correlated variables more effectively.

We can still use train() function to tune the parameters in the elastic net. As before, set the cross-validation and parameter range. Standardize the predictors:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{enetGrid <-}\StringTok{ }\KeywordTok{expand.grid}\NormalTok{(}\DataTypeTok{.lambda =} \KeywordTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{,}\FloatTok{0.2}\NormalTok{,}\DataTypeTok{length=}\DecValTok{20}\NormalTok{), }
                        \DataTypeTok{.fraction =} \KeywordTok{seq}\NormalTok{(.}\DecValTok{8}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DataTypeTok{length =} \DecValTok{20}\NormalTok{))}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{100}\NormalTok{)}
\NormalTok{enetTune <-}\StringTok{ }\KeywordTok{train}\NormalTok{(trainx, trainy,}
                  \DataTypeTok{method =} \StringTok{"enet"}\NormalTok{,}
                  \DataTypeTok{tuneGrid =}\NormalTok{ enetGrid,}
                  \DataTypeTok{trControl =}\NormalTok{ ctrl,}
                  \DataTypeTok{preProc =} \KeywordTok{c}\NormalTok{(}\StringTok{"center"}\NormalTok{, }\StringTok{"scale"}\NormalTok{))}
\NormalTok{enetTune}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Elasticnet 

999 samples
 10 predictor

Pre-processing: centered (10), scaled (10) 
Resampling: Cross-Validated (10 fold) 
Summary of sample sizes: 899, 899, 899, 899, 899, 900, ... 
Resampling results across tuning parameters:

  lambda   fraction  RMSE  Rsquared  MAE  
  0.00000  0.8000    1763  0.7921    787.5
  0.00000  0.8105    1760  0.7924    784.1
  .
  .
  .
  0.09474  0.9158    1760  0.7945    782.5
  0.09474  0.9263    1761  0.7947    782.5
  0.09474  0.9368    1761  0.7949    782.7
  0.09474  0.9474    1763  0.7950    783.3
  0.09474  0.9579    1764  0.7951    784.3
  0.09474  0.9684    1766  0.7953    785.7
  0.09474  0.9789    1768  0.7954    787.1
  0.09474  0.9895    1770  0.7954    788.8
  0.09474  1.0000    1772  0.7955    790.4
 [ reached getOption("max.print") -- omitted 200 rows ]

RMSE was used to select the optimal model using the smallest value.
The final values used for the model were fraction = 0.9579 and lambda = 0.
\end{verbatim}

The results show that the best values of the tuning parameters are fraction is 0.9579 and lambda is 0 It also indicates that the final model is lasso only (the ridge penalty parameter lambda is 0). The RMSE and \(R^{2}\) are 1742.2843 and 0.7954 correspondingly.

\hypertarget{penalized-generalized-linear-model}{%
\section{Penalized Generalized Linear Model}\label{penalized-generalized-linear-model}}

Adding penalties is a general technique that can be applied to many methods other than linear regression. In this section, we will introduce the penalized generalized linear model. It is to fit the generalized linear model by minimizing a penalized maximum likelihood. The penalty can be \(L_1\), \(L_2\) or a combination of the two. The estimates of coefficients minimize the following:

\[\underset{\beta_{0},\mathbf{\beta}}{min}\frac{1}{N}\Sigma_{i=1}^{N}w_{i}l(y_{i},\beta_{0}+\mathbf{\beta^{T}x_{i}})+\lambda[(1-\alpha)\parallel\mathbf{\beta}\parallel_{2}^{2}/2+\alpha\parallel\mathbf{\beta}\parallel_{1}]\]

where

\[l(y_{i},\beta_{0}+\mathbf{\beta^{T}x_{i}})=-log[\mathcal{L}(y_{i},\beta_{0}+\mathbf{\beta^{T}x_{i}})]\]

It is the negative logarithm of the likelihood, \(\mathcal{L}(y_{i},\beta_{0}+\mathbf{\beta^{T}x_{i}})\). Maximize likelihood is to minimize \(l(y_{i},\beta_{0}+\mathbf{\beta^{T}x_{i}})\).

Parameter \(\alpha\) decides the penalty, i.e, between \(L_2\) (\(\alpha=0\)) and \(L_1\) (\(\alpha=1\)). \(\lambda\) controls the weight of the whole penalty item. The higher \(\lambda\) is, the more weight the penalty carries comparing to likelihood. As discussed above, the ridge penalty shrinks the coefficients towards 0 but can't be exactly 0. The lasso penalty can set 0 estimates so it has the property of feature selection. The elastic net combines both. Here we have two tuning parameters, \(\alpha\) and \(\lambda\).

\hypertarget{introduction-to-glmnet-package}{%
\subsection{\texorpdfstring{Introduction to \texttt{glmnet} package}{Introduction to glmnet package}}\label{introduction-to-glmnet-package}}

\texttt{glmnet} is a package that fits a penalized generalized linear model using \emph{cyclical coordinate descent}. It successively optimizes the objective function over each parameter with others fixed, and cycles repeatedly until convergence. Since the linear model is a special case of the generalized linear model, \texttt{glmnet} can also fit a penalized linear model. Other than that, it can also fit penalized logistic regression, multinomial, Poisson, and Cox regression models.

The default family option in the function \texttt{glmnet()} is \texttt{gaussian}. It is the linear regression we discussed so far in this chapter. But the parameterization is a little different in the generalized linear model framework (we have \(\alpha\) and \(\lambda\)). Let's start from our previous example, using the same training data but \texttt{glmnet()} to fit model:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(glmnet)}
\NormalTok{dat <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"http://bit.ly/2P5gTw4"}\NormalTok{)}
\CommentTok{# data cleaning: delete wrong observations since expense can't be negative}
\NormalTok{dat <-}\StringTok{ }\KeywordTok{subset}\NormalTok{(dat, store_exp }\OperatorTok{>}\StringTok{ }\DecValTok{0} \OperatorTok{&}\StringTok{ }\NormalTok{online_exp }\OperatorTok{>}\StringTok{ }\DecValTok{0}\NormalTok{)}
\CommentTok{# get predictors}
\NormalTok{trainx <-}\StringTok{ }\NormalTok{dat[, }\KeywordTok{grep}\NormalTok{(}\StringTok{"Q"}\NormalTok{, }\KeywordTok{names}\NormalTok{(dat))]}
\CommentTok{# get response}
\NormalTok{trainy <-}\StringTok{ }\NormalTok{dat}\OperatorTok{$}\NormalTok{store_exp }\OperatorTok{+}\StringTok{ }\NormalTok{dat}\OperatorTok{$}\NormalTok{online_exp}
\NormalTok{glmfit =}\StringTok{ }\KeywordTok{glmnet}\NormalTok{(}\KeywordTok{as.matrix}\NormalTok{(trainx), trainy)}
\end{Highlighting}
\end{Shaded}

The object \texttt{glmfit} returned by \texttt{glmnet()} has the information of the fitted model for the later operations. An easy way to extract the components is through various functions on \texttt{glmfit}, such as \texttt{plot()}, \texttt{print()}, \texttt{coef()} and \texttt{predict()} . For example, the following code visualizes the path of coefficients as penalty increases:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(glmfit, }\DataTypeTok{label =}\NormalTok{ T)}
\end{Highlighting}
\end{Shaded}

\includegraphics{IDS_files/figure-latex/unnamed-chunk-113-1.pdf}

Each curve in the plot represents one predictor. The default setting is \(\alpha=1\) which means there is only lasso penalty. From left to right, \(L_I\) norm is increasing which means \(\lambda\) is decreasing. The bottom x-axis is \(L_1\) norm (i.e. \(\parallel\mathbf{\beta}\parallel_{1}\)). The upper x-axis is the effective degrees of freedom (df) for the lasso. You can check the detail for every step by:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{print}\NormalTok{(glmfit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Call:  glmnet(x = as.matrix(trainx), y = trainy) 

   Df  %Dev Lambda
1   0 0.000   3040
2   2 0.104   2770
3   2 0.192   2530
4   2 0.265   2300
5   3 0.326   2100
6   3 0.389   1910
7   3 0.442   1740
8   3 0.485   1590
9   3 0.521   1450
...
\end{verbatim}

The first column \texttt{Df} is the degree of freedom (i.e.~the number of non-zero coefficients), \texttt{\%Dev} is the percentage of deviance explained and \texttt{Lambda} is the value of tuning parameter \(\lambda\). By default, the function will try 100 different values of \(\lambda\). However, if as \(\lambda\) changes, the \texttt{\%Dev} doesn't change sufficiently, the algorithm will stop before it goes through all the values of \(\lambda\). We didn't show the full output above. But it only uses 68 different values of \(\lambda\). You can also set the value of \(\lambda\) using \texttt{s=} :

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{coef}\NormalTok{(glmfit, }\DataTypeTok{s =} \DecValTok{1200}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 11 x 1 sparse Matrix of class "dgCMatrix"
##                  1
## (Intercept) 2255.2
## Q1          -390.9
## Q2           653.6
## Q3           624.4
## Q4             .  
## Q5             .  
## Q6             .  
## Q7             .  
## Q8             .  
## Q9             .  
## Q10            .
\end{verbatim}

When \(\lambda=1200\), there are three coefficients with non-zero estimates(\texttt{Q1}, \texttt{Q2} and \texttt{Q3}). You can apply models with different values of tuning parameter to new data using \texttt{predict()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{newdat =}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\KeywordTok{sample}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{9}\NormalTok{, }\DecValTok{30}\NormalTok{, }\DataTypeTok{replace =}\NormalTok{ T), }\DataTypeTok{nrow =} \DecValTok{3}\NormalTok{)}
\KeywordTok{predict}\NormalTok{(glmfit, newdat, }\DataTypeTok{s =} \KeywordTok{c}\NormalTok{(}\DecValTok{1741}\NormalTok{, }\DecValTok{2000}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##         1    2
## [1,] 6004 5968
## [2,] 7101 6674
## [3,] 9158 8411
\end{verbatim}

Each column corresponds to a value of \(\lambda\). To tune the value of \(\lambda\), we can easily use \texttt{cv.glmnet()} function to do cross-validation.\texttt{cv.glmnet()} returns the cross-validation results as a list object. We store the object in \texttt{cvfit} and use it for further operations.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cvfit =}\StringTok{ }\KeywordTok{cv.glmnet}\NormalTok{(}\KeywordTok{as.matrix}\NormalTok{(trainx), trainy)}
\end{Highlighting}
\end{Shaded}

We can plot the object using \texttt{plot()}. The red dotted line is the cross-validation curve. Each red point is the cross-validation mean squared error for a value of \(\lambda\). The grey bars around the red points indicate the upper and lower standard deviation. The two gray dotted vertical lines represent the two selected values of \(\lambda\), one gives the minimum mean cross-validated error (\texttt{lambda.min}), the other gives the error that is within one standard error of the minimum (\texttt{lambda.1se}).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(cvfit)}
\end{Highlighting}
\end{Shaded}

\includegraphics{IDS_files/figure-latex/unnamed-chunk-117-1.pdf}

You can check the two selected \(\lambda\) values by:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# lambda with minimum mean cross-validated error}
\NormalTok{cvfit}\OperatorTok{$}\NormalTok{lambda.min}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 12.57
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# lambda with one standard error of the minimum}
\NormalTok{cvfit}\OperatorTok{$}\NormalTok{lambda}\FloatTok{.1}\NormalTok{se}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1200
\end{verbatim}

You can look at the coefficient estimates for different \(\lambda\) by:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# coefficient estimates for model with the error }
\CommentTok{# that is within one standard error of the minimum}
\KeywordTok{coef}\NormalTok{(cvfit, }\DataTypeTok{s =} \StringTok{"lambda.1se"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 11 x 1 sparse Matrix of class "dgCMatrix"
##                  1
## (Intercept) 2255.3
## Q1          -391.1
## Q2           653.7
## Q3           624.5
## Q4             .  
## Q5             .  
## Q6             .  
## Q7             .  
## Q8             .  
## Q9             .  
## Q10            .
\end{verbatim}

\hypertarget{penalized-logistic-regression}{%
\subsection{Penalized logistic regression}\label{penalized-logistic-regression}}

\hypertarget{multivariate-logistic-regression-model}{%
\subsubsection{Multivariate logistic regression model}\label{multivariate-logistic-regression-model}}

Logistic regression is a traditional statistical method for a two-category classification problem. It is simple yet useful. Here we use the swine disease breakout data as an example to illustrate the learning method and code implementation. Refer to section \ref{swinediseasedata} for more details about the dataset. The goal is to predict if a farm will have a swine disease outbreak (i.e build a risk scoring system).

Consider risk scoring system construction using a sample of \(n\) observations, with information collected for \(G\) categorical predictors and one binary response variable for each observation. The predictors are 120 survey questions (i.e.~G=120). There were three possible answers for each question (A, B and C). So each predictor is encoded to two dummy variables (we consider C as the baseline.). Let \(\mathbf{x_{i,g}}\) be the vector of dummy variables associated with the \(g^{th}\) categorical predictor for the \(i^{th}\) observation, where \(i=1,\cdots,n\), \(g=1,\cdots,G\). For example, if the first farm chooses B for question 2, then the corresponding observation is \(\mathbf{x_{12}}=(0,1)^{T}\). Each question has a degree of freedom of 2.

We denote the degrees of freedom of the \(g^{th}\) predictor by \(df_g\), which is also the length of vector \(\mathbf{x_{i,g}}\). Let \(y_i\) (= 1, diseased; or 0, not diseased) be the binary response for the \(i\)th observation. Denote the probability
of disease for \(i\)th subject by \(\theta_i\), the model can be formulated as:

\[y_{i}\sim Bounoulli(\theta_{i})\]

\[log\left(\frac{\theta_{i}}{1-\theta_{i}}\right)=\eta_{\mathbf{\beta}}(x_{i})=\beta_{0}+\sum_{g=1}^{G}\mathbf{x_{i,g}}^{T}\mathbf{\mathbf{\beta_{g}}}\]

where \(\beta_{0}\) is the intercept and \(\mathbf{\beta_{g}}\) is the parameter vector corresponding to the \(g^{th}\) predictor. As we mentioned, here \(\mathbf{\beta_{g}}\) has length 2.

Traditional estimation of logistic
parameters \(\mathbf{\beta}=(\beta_{0}^{T},\mathbf{\beta_{1}}^{T},\mathbf{\beta_{2}}^{T},...,\mathbf{\beta_{G}}^{T})^{T}\) is done through maximizing the log-likelihood

\begin{eqnarray*}
l(\mathbf{\beta})&=&log[\prod_{i=1}^{n}\theta_{i}^{y_{i}}(1-\theta_{i})^{1-y_{i}}]\\
&=&\sum_{i=1}^{n}\{y_{i}log(\theta_{i})+(1-y_{i})log(1-\theta_{i})\}\\
&=&\sum_{i=1}^{n}\{\ y_{i}\eta_{\mathbf{\beta}}(\mathbf{x_{i}})-log[1+exp(\eta_{\mathbf{\beta}}(\mathbf{x_{i}}))]\ \}
\label{eq:logisticlikelihood}
\end{eqnarray*}

For logistic regression analysis with a large number of explanatory variables, complete- or quasi-complete-separation may result which makes
the maximum likelihood estimation unstable \citep{Wed1976}, \citep{albert1984}. For example:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(MASS)}
\NormalTok{dat <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"http://bit.ly/2KXb1Qi"}\NormalTok{)}
\NormalTok{fit <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(y}\OperatorTok{~}\NormalTok{., dat, }\DataTypeTok{family =} \StringTok{"binomial"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: glm.fit: algorithm did not converge
\end{verbatim}

\begin{verbatim}
## Warning: glm.fit: fitted probabilities numerically 0 or
## 1 occurred
\end{verbatim}

There is an error saying ``\texttt{algorithm\ did\ not\ converge}''. It is because there is complete separation. It happens when there are a large number of explanatory variables which makes the estimation of the coefficients unstable. To stabilize the estimation of parameter coefficients, one popular approach is the lasso algorithm with \(L_1\) norm penalty proposed by Tibshirani\citep{Tibshirani1996}. Because the lasso algorithm can estimate some variable coefficients to be 0, it can also be used as a variable selection tool.

\hypertarget{penalized-logistic-regression-1}{%
\subsubsection{Penalized logistic regression}\label{penalized-logistic-regression-1}}

Penalized logistic regression adds penalty to the likelihood function:

\[
\underset{\mathbf{\beta}\in \mathbb{R}^{p+1}}{min} -\sum_{i=1}^{n}\{\ y_{i}\eta_{\mathbf{\beta}}(\mathbf{x_{i}})-log[1+exp(\eta_{\mathbf{\beta}}(\mathbf{x_{i}}))]\ \}+\lambda [(1-\alpha) \parallel \mathbf{\beta}\parallel _{2}^{2}/2] + \alpha \parallel \mathbf{\beta}\parallel _{1} ]
\]

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dat <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"http://bit.ly/2KXb1Qi"}\NormalTok{)}
\NormalTok{trainx =}\StringTok{ }\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{select}\NormalTok{(dat, }\OperatorTok{-}\NormalTok{y)}
\NormalTok{trainy =}\StringTok{ }\NormalTok{dat}\OperatorTok{$}\NormalTok{y}
\NormalTok{fit <-}\StringTok{ }\KeywordTok{glmnet}\NormalTok{(}\KeywordTok{as.matrix}\NormalTok{(trainx), trainy, }\DataTypeTok{family =} \StringTok{"binomial"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The error message is gone when we use penalized regression. We can visualize the shrinking path of coefficients as penalty increases. The use of \texttt{predict()} function is a little different. For the generalized linear model, you can return different results by setting the \texttt{type} argument. The choices are:

\begin{itemize}
\tightlist
\item
  \texttt{link}: return the link function value
\item
  \texttt{response}: return the probability
\item
  \texttt{class}: return the category (0/1)
\item
  \texttt{coefficients}: return the coefficient estimates
\item
  \texttt{nonzero}: return an indicator for non-zero estimates (i.e.~which variables are selected)
\end{itemize}

The default setting is to predict the probability of the second level of the response variable. For example, the second level of the response variable for \texttt{trainy} here is:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{levels}\NormalTok{(}\KeywordTok{as.factor}\NormalTok{(trainy))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "0" "1"
\end{verbatim}

The first column of the above output is the predicted link function value when \(\lambda=0.02833\). The second column of the output is the predicted link function when \(\lambda=0.0311\). Similarly, you can change the setting for \texttt{type} to produce different outputs. You can use the \texttt{cv.glmnet()} function to tune parameters. The parameter setting is nearly the same as before, the only difference is the setting of \texttt{type.measure}. Since the response is categorical, not continuous, we have different performance measurements. The most common settings of \texttt{type.measure} for classification are:

\begin{itemize}
\tightlist
\item
  \texttt{class}: error rate
\item
  \texttt{auc}: it is the area under the ROC for the dichotomous problem
  So the model is to predict the probability of outcome ``1''. Take a baby example of 3 observations and 2 values of \(\lambda\) to show the usage of \texttt{predict()} function:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{newdat =}\StringTok{ }\KeywordTok{as.matrix}\NormalTok{(trainx[}\DecValTok{1}\OperatorTok{:}\DecValTok{3}\NormalTok{, ])}
\KeywordTok{predict}\NormalTok{(fit, newdat, }\DataTypeTok{type =} \StringTok{"link"}\NormalTok{, }\DataTypeTok{s =} \KeywordTok{c}\NormalTok{(}\FloatTok{2.833e-02}\NormalTok{, }\FloatTok{3.110e-02}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##         1       2
## 1  0.1943  0.1443
## 2 -0.9913 -1.0077
## 3 -0.5841 -0.5496
\end{verbatim}

For example:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cvfit =}\StringTok{ }\KeywordTok{cv.glmnet}\NormalTok{(}\KeywordTok{as.matrix}\NormalTok{(trainx), trainy, }\DataTypeTok{family =} \StringTok{"binomial"}\NormalTok{, }\DataTypeTok{type.measure =} \StringTok{"class"}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(cvfit)}
\end{Highlighting}
\end{Shaded}

\includegraphics{IDS_files/figure-latex/unnamed-chunk-124-1.pdf}

The above uses error rate as performance criteria and use 10-fold cross-validation. Similarly, you can get the \(\lambda\) value for the minimum error rate and the error rate that is 1 standard error from the minimum:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cvfit}\OperatorTok{$}\NormalTok{lambda.min}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 2.643e-05
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cvfit}\OperatorTok{$}\NormalTok{lambda}\FloatTok{.1}\NormalTok{se}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.003334
\end{verbatim}

You can use the same way to get the parameter estimates and make prediction.

\hypertarget{group-lasso-logistic-regression}{%
\subsubsection{Group lasso logistic regression}\label{group-lasso-logistic-regression}}

For models with categorical survey questions (explanatory variables), however, the original lasso algorithm only selects individual dummy variables instead of sets of the dummy variables grouped by the question in the survey. Another disadvantage of applying lasso to grouped variables is that the estimates are affected by the way dummy variables are encoded. Thus the group lasso \citep{Yuan2007} method has been proposed to enable variable selection in linear regression models on groups of variables, instead of on single variables. For logistic regression models, the group lasso algorithm was first studied by Kim et al. \citep{Kim2006}. They proposed a gradient descent algorithm to solve the corresponding constrained problem, which does, however, depend on unknown constants. Meier et al. \citep{Meier2008} proposed a new algorithm that could work directly on the penalized problem and its convergence property does not depend on unknown constants. The algorithm is especially suitable for high-dimensional problems. It can also be applied to solve the corresponding convex optimization problem in generalized linear models. The group lasso estimator proposed by Meier et al. \citep{Meier2008} for logistic regression has been shown to be statistically consistent, even with a large number of categorical predictors.

In this section, we illustrate how to use the logistic group lasso algorithm to construct risk scoring systems for predicting disease. Instead of maximizing the log-likelihood in the maximum likelihood method, the logistic group lasso estimates are calculated by minimizing the convex function:

\[
S_{\lambda}(\mathbf{\beta})=-l(\mathbf{\beta})+\lambda\sum_{g=1}^{G}s(df_{g})\parallel\mathbf{\beta_{g}}\parallel_{2}
\]

where \(\lambda\) is a tuning parameter for the penalty and \(s(\cdot)\) is a function to rescale the penalty. In lasso algorithms, the selection of \(\lambda\) is usually determined by cross-validation using data. For \(s(\cdot)\), we use the square root function \(s(df_g)=df_g^{0.5}\) as suggested in Meier et al.\citep{Meier2008}. It ensures the penalty is of the order of the number of parameters \(df_g\) as used in \citep{Yuan2007}.

Here we consider selection of the tuning parameter \(\lambda\) from a multiplicative grid of 100 values \(\{0.96\lambda_{max},0.96^{2}\lambda_{max},0.96^{3}\lambda_{max},...,0.96^{100}\lambda_{max}\}\). Here \(\lambda_{max}\) is defined as

\begin{equation}
\lambda_{max}=\underset{g\in\{1,...,G\}}{max}\left\{\frac{1}{s(df_{g})}\parallel \mathbf{x_{g}}^{T}(\mathbf{y}-\bar{\mathbf{y}} )\parallel_{2}\right\},
\end{equation}

such that when \(\lambda=\lambda_{max}\), only the intercept is
in the model. When \(\lambda\) goes to \(0\), the model is equivalent to ordinary
logistic regression.

Three criteria may be used to select the optimal value of \(\lambda\). One is AUC which you should have seem many times in this book by now. The log-likelihood score used in Meier et al. \citep{Meier2008} is taken as the average of log-likelihood of the validation data overall cross-validation sets. Another one is the maximum correlation coefficient in Yeo and Burge \citep{Yeo2004} that is defined as:

\[
\rho_{max}=max\{\rho_{\tau}|\tau\in(0,1)\},
\]

where \(\tau\in(0,1)\) is a threshold to classify the predicted probability into binary disease status and \(\rho_\tau\) is the Pearson correlation coefficient between the true binary disease status and the predictive disease status with threshold \(\tau\).

You can use the following package to implement the model. Install the package using:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{devtools}\OperatorTok{::}\KeywordTok{install_github}\NormalTok{(}\StringTok{"netlify/NetlifyDS"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Load the package:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(}\StringTok{"NetlifyDS"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The package includes the swine disease breakout data and you can load the data by:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{data}\NormalTok{(}\StringTok{"sim1_da1"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

You can use \texttt{cv\_glasso()} function to tune the parameters:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# the last column of sim1_da1 response variable y}
\CommentTok{# trainx is the explanatory variable matrix}
\NormalTok{trainx =}\StringTok{ }\NormalTok{dplyr}\OperatorTok{::}\KeywordTok{select}\NormalTok{(sim1_da1, }\OperatorTok{-}\NormalTok{y)}
\CommentTok{# save response variable as as trainy}
\NormalTok{trainy =}\StringTok{ }\NormalTok{sim1_da1}\OperatorTok{$}\NormalTok{y}
\CommentTok{# get the group indicator}
\NormalTok{index <-}\StringTok{ }\KeywordTok{gsub}\NormalTok{(}\StringTok{"}\CharTok{\textbackslash{}\textbackslash{}}\StringTok{..*"}\NormalTok{, }\StringTok{""}\NormalTok{, }\KeywordTok{names}\NormalTok{(trainx))}
\end{Highlighting}
\end{Shaded}

Dummy variables from the same question are in the same group:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{index[}\DecValTok{1}\OperatorTok{:}\DecValTok{50}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] "Q1"  "Q1"  "Q2"  "Q2"  "Q3"  "Q3"  "Q4"  "Q4" 
##  [9] "Q5"  "Q5"  "Q6"  "Q6"  "Q7"  "Q7"  "Q8"  "Q8" 
## [17] "Q9"  "Q9"  "Q10" "Q10" "Q11" "Q11" "Q12" "Q12"
## [25] "Q13" "Q13" "Q14" "Q14" "Q15" "Q15" "Q16" "Q16"
## [33] "Q17" "Q17" "Q18" "Q18" "Q19" "Q19" "Q20" "Q20"
## [41] "Q21" "Q21" "Q22" "Q22" "Q23" "Q23" "Q24" "Q24"
## [49] "Q25" "Q25"
\end{verbatim}

Set a series of tuning parameter values. nlam is the number of values we want to tune. It is the parameter \(m\) in \{0.96\lambda\emph{\{max\},0.96\textsuperscript{\{2\}\lambda\emph{\{max\},0.96\^{}\{3\}\lambda}\{max\},\ldots{},0.96}\{m\}\lambda}\{max\}\}\$. The tuning process returns a long output and we will not report all:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Tune over 100 values}
\NormalTok{nlam <-}\StringTok{ }\DecValTok{100}
\CommentTok{# set the type of prediction}
\CommentTok{# - `link`: return the predicted link function}
\CommentTok{# - `response`: return the predicted probability}
\CommentTok{# number of cross-validation folds}
\NormalTok{kfold <-}\StringTok{ }\DecValTok{10}
\NormalTok{cv_fit <-}\StringTok{ }\KeywordTok{cv_glasso}\NormalTok{(trainx, trainy, }\DataTypeTok{nlam =}\NormalTok{ nlam, }\DataTypeTok{kfold =}\NormalTok{ kfold, }\DataTypeTok{type =} \StringTok{"link"}\NormalTok{)}
\CommentTok{# }
\KeywordTok{str}\NormalTok{(cv_fit)}
\end{Highlighting}
\end{Shaded}

Here we only show part of the output:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{...}
\NormalTok{ $ auc               : num [1:100] 0.573 0.567 0.535 0.484 0.514 ...}
\NormalTok{ $ log_likelihood    : num [1:100] -554 -554 -553 -553 -552 ...}
\NormalTok{ $ maxrho            : num [1:100] -0.0519 0.00666 0.04631 0.0486 0.06269 ...}
\NormalTok{ $ lambda.max.auc    : Named num [1:2] 0.922 0.94}
\NormalTok{  ..- attr(*, "names")= chr [1:2] "lambda" "auc"}
\NormalTok{ $ lambda.1se.auc    : Named num [1:2] 16.74 0.81}
\NormalTok{  ..- attr(*, "names")= chr [1:2] "" "se.auc"}
\NormalTok{ $ lambda.max.loglike: Named num [1:2] 1.77 -248.86}
\NormalTok{  ..- attr(*, "names")= chr [1:2] "lambda" "loglike"}
\NormalTok{ $ lambda.1se.loglike: Named num [1:2] 9.45 -360.13}
\NormalTok{  ..- attr(*, "names")= chr [1:2] "lambda" "se.loglike"}
\NormalTok{ $ lambda.max.maxco  : Named num [1:2] 0.922 0.708}
\NormalTok{  ..- attr(*, "names")= chr [1:2] "lambda" "maxco"}
\NormalTok{ $ lambda.1se.maxco  : Named num [1:2] 14.216 0.504}
\NormalTok{  ..- attr(*, "names")= chr [1:2] "lambda" "se.maxco"}
\end{Highlighting}
\end{Shaded}

In the returned results:

\begin{itemize}
\tightlist
\item
  \texttt{\$\ auc}: the AUC values
\item
  \texttt{\$\ log\_likelihood}: log-likelihood
\item
  \texttt{\$\ maxrho}: maximum correlation coefficient
\item
  \texttt{\$\ lambda.max.auc}: the max AUC and the corresponding value of \(\lambda\)
\item
  \texttt{\$\ lambda.1se.auc}: one standard error to the max AUC and the corresponding \(\lambda\)
\item
  \texttt{\$\ lambda.max.loglike}: max log-likelihood and the corresponding \(\lambda\)
\item
  \texttt{\$\ lambda.1se.loglike}: one standard error to the max log-likelihood and the corresponding \(\lambda\)
\item
  \texttt{\$\ lambda.max.maxco}: maximum correlation coefficient and the corresponding \(\lambda\)
\item
  \texttt{\$\ lambda.1se.maxco}: one standard error to the maximum correlation coefficient and the corresponding \(\lambda\)
\end{itemize}

The most common criterion is AUC. You can compare the selections from different criteria. If they all point to the same value of the tuning parameter, you can have more confidence about the choice. If they suggest very different values, then you need to concern if the tuning process is stable. You can visualize the cross validation result:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(cv_fit)}
\end{Highlighting}
\end{Shaded}

\includegraphics{IDS_files/figure-latex/unnamed-chunk-131-1.pdf}

The x-axis is the value of the tuning parameter, the y-axis is AUC. The two dash lines are the value of \(\lambda\) for max AUC and the value for the one standard deviation to the max AUC. Once you choose the value of the tuning parameter, you can use \texttt{fitglasso()} to fit the model. For example, we can fit the model using the parameter value that gives the max AUC, which is \(\lambda=0.922\):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitgl <-}\StringTok{ }\KeywordTok{fitglasso}\NormalTok{(trainx, trainy, }\DataTypeTok{lambda =} \FloatTok{0.922}\NormalTok{, }\DataTypeTok{na_action =}\NormalTok{ na.pass)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Lambda: 0.922  nr.var: 229}
\end{Highlighting}
\end{Shaded}

You can use \texttt{coef()} to get the estimates of coefficients:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{coef}\NormalTok{(fitgl)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
               0.922
Intercept -5.318e+01
Q1.A       1.757e+00
Q1.B       1.719e+00
Q2.A       2.170e+00
Q2.B       6.939e-01
Q3.A       2.102e+00
Q3.B       1.359e+00
...
\end{verbatim}

Use \texttt{predict\_glasso()} to predict new samples:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{prey <-}\StringTok{ }\KeywordTok{predict_glasso}\NormalTok{(fitgl, trainx)}
\end{Highlighting}
\end{Shaded}

\hypertarget{treemodel}{%
\chapter{Tree-Based Methods}\label{treemodel}}

The tree-based models can be used for regression and classification. The goal is to stratify or segment the predictor space into a number of sub-regions. For a given observation, use the mean or the mode of the training observations in the sub-region as the prediction. Tree-based methods are conceptually simple yet powerful. This type of model is often referred to as Classification And Regression Trees (CART). They are popular tools for many reasons:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Do not require user to specify the form of the relationship between predictors and response
\item
  Do not require (or if they do, very limited) data preprocessing and can handle different types of predictors (sparse, skewed, continuous, categorical, etc.)
\item
  Robust to co-linearity
\item
  Can handle missing data
\item
  Many pre-built packages make implementation as easy as a button push
\end{enumerate}

CART can refer to the tree model in general, but most of the time, it represents the algorithm initially proposed by Breiman \citep{Breiman1984}. After Breiman, there are many new algorithms, such as ID3, C4.5, and C5.0. C5.0 is an improved version of C4.5, but since C5.0 is not open source, the C4.5 algorithm is more popular. C4.5 was a major competitor of CART. But now, all those seem outdated. The most popular tree models are Random Forest (RF) and Gradient Boosting Machine (GBM). Despite being out of favor in application, it is important to understand the mechanism of the basic tree algorithm. Because the later models are based on the same foundation.

The original CART algorithm targets binary classification, and the later algorithms can handle multi-category classification. A single tree is easy to explain but has poor accuracy. More complicated tree models, such as RF and GBM, can provide much better prediction at the cost of explainability. As the model becoming more complicated, it is more like a black-box which makes it very difficult to explain the relationship among predictors. There is always a trade-off between explainability and predictability.

\includegraphics{../linhui.org/book/Figure/treeEN.png}

The reason why it is called ``tree'' is of course because the structure has similarities. But the direction of the decision tree is opposite to the real tree, the root is on the top, and the leaf is on the bottom. From the root node, a decision tree divides to different branches and generates more nodes. The new nodes are child nodes, and the previous node is the parent node. At each child node, the algorithm will decide whether to continue dividing. If it stops, the node is called a leaf node. If it continues, then the node becomes the new parent node and splits to produce the next layer of child nodes. At each non-leaf node, the algorithm needs to decide split into branches. The leaf node contains the final ``decision,'' final class the sample belongs to or the sample's value has. Here are the important definitions in the tree model:

\begin{itemize}
\tightlist
\item
  \textbf{Classification tree}: the outcome is discrete
\item
  \textbf{Regression tree}: the outcome is continuous (e.g.~the price of a house, or a patient's length of stay in a hospital)
\item
  \textbf{Non-leaf node (or split node)}: the algorithm needs to decide a split at each non-leaf node (eg: \(age \leq 25\), \(25 < age \leq 35\), \(age > 35\))
\item
  \textbf{Root node}the beginning node where the tree starts
\item
  \textbf{Leaf node (or Terminal node)}: the node stops splitting. It has the final decision of the model
\item
  \textbf{Degree of the node}: the number of subtrees of a node
\item
  \textbf{Degree of the tree}: the maximum degree of a node in the tree
\item
  \textbf{Pruning}: remove parts of the tree that do not provide power to classify instances
\item
  \textbf{Branch (or Subtree)}: the whole part under a non-leaf node
\item
  \textbf{Child}: the node directly after and connected to another node
\item
  \textbf{Parent}: the converse notion of a child
\end{itemize}

Single tree is easy to explain but has high variance and low accuracy, and hence is very limited. Minor changes in the training data can lead to large changes in the fitted tree. A series of rectangular decision regions defined by a single tree is often too naive to represent the relationship between the dependent variable and the predictors. To overcome these shortcomings, researchers have proposed ensemble methods which combine many trees. Ensemble tree models typically have much better predictive performance than a single tree. We will introduce those models in later sections.

\hypertarget{splitting-criteria}{%
\section{Splitting Criteria}\label{splitting-criteria}}

The splitting criteria used by the regression tree and the classification tree are different. Like the regression tree, the goal of the classification tree is to divide the data into smaller, more homogeneous groups. Homogeneity means that most of the samples at each node are from one class. The original CART algorithm uses Gini impurity as the splitting criterion; The later ID3, C4.5, and C5.0 use entropy. We will look at three most common splitting criteria.

\textbf{Gini impurity}

Gini impurity\citep{Breiman1984} is a measure of non-homogeneity. It is widely used in classification tree. For a two-class problem, the Gini impurity for a given node is defined as:

\[p_{1}(1-p_{1})+p_{2}(1-p_{2})\]

where \(p_{1}\) and \(p_{2}\) are probabilities for the two classes respectively. It is easy to see that when the sample set is pure, one of the probability is 0 and the Gini score is the smallest. Conversely, when \(p_{1}=p_{2}=0.5\), the Gini score is the largest, in which case the purity of the node is the smallest. Let's look at an example. Suppose we want to determine which students are computer science (CS) majors. Here is the simple hypothetical classification tree result obtained with the gender variable.

\includegraphics{images/giniEN.PNG}

Let's calculate the Gini impurity for splitting node ``Gender'':

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Gini impurity for ``Female'' = \(\frac{1}{6}\times\frac{5}{6}+\frac{5}{6}\times\frac{1}{6}=\frac{5}{18}\)
\item
  Gini impurity for ``Male'' = \(0\times1+1\times 0=0\)
\end{enumerate}

The Gini impurity for the node ``Gender'' is the following weighted average of the above two scores:

\[\frac{3}{5}\times\frac{5}{18}+\frac{2}{5}\times 0=\frac{1}{6}\]

The Gini impurity for the 50 samples in the parent node is \(\frac{1}{2}\). It is easy to calculate the Gini impurity drop from \(\frac{1}{2}\) to \(\frac{1}{6}\) after splitting. The split using ``gender'' causes a Gini impurity decrease of \(\frac{1}{3}\). The algorithm will use different variables to split the data and choose the one that causes the most substantial Gini impurity decrease.

\textbf{Information gain}

Looking at the samples in the following three nodes, which one is the easiest to describe? It is obviously C. Because all the samples in C are of the same type, so the description requires the least amount of information. On the contrary, B needs more information, and A needs the most information. In other words, C has the highest purity, B is the second, and A has the lowest purity. We need less information to describe nodes with higher purity.

\includegraphics{images/InfoGainEN.PNG}

A measure of the degree of disorder is entropy which is defined as:

\[Entropy=-plog_{2}p-(1-p)log_{2}(1-p)\]

where p is the percentage of one type of samples. If all the samples in one node are of one type (such as C), the entropy is 0. If the proportion of each type in a node is 50\%50\%, the entropy is 1. We can use entropy as splitting criteria. The goal is to decrease entropy as the tree grows.

Similarly, the entropy of a splitting node is the weighted average of the entropy of each child. In the above tree for the students, the entropy of the root node with all 50 students is \(-\frac{25}{50}log_{2}\frac{25}{50}-\frac{25}{50}log_{2}\frac{25}{50}=1\). Here an entropy of 1 indicates that the purity of the node is the lowest, that is, each type takes up half of the samples.

The entropy of the split using variable ``gender'' can be calculated in three steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Entropy for ``Female'' = \(-\frac{5}{30}log_{2}\frac{5}{30}-\frac{25}{30}log_{2}\frac{25}{30}=0.65\)
\item
  Entropy for ``Male'' = \(0\times1+1\times 0=0\)
\item
  Entropy for the node ``Gender'' is the weighted average of the above two entropy numbers: \(\frac{3}{5}\times 0.65+\frac{2}{5}\times 0=0.39\)
\end{enumerate}

So entropy decreases from 1 to 0.39 after the split.

\textbf{Sum of Square Error (SSE)}

The previous two metrics are for classification tree. The SSE is the most widely used splitting metric for regression. Suppose you want to divide the data set \(S\) into two groups of \(S_{1}\) and \(S_{2}\), where the selection of \(S_{1}\) and \(S_{2}\) needs to minimize the sum of squared errors:

\begin{equation}
SSE=\Sigma_{i\in S_{1}}(y_{i}-\bar{y}_{1})^{2}+\Sigma_{i\in S_{2}}(y_{i}-\bar{y}_{2})^{2}
\label{eq:treesse}
\end{equation}

In equation \eqref{eq:treesse}, \(\bar{y}_{1}\) and \(\bar{y}_{1}\) are the average of the sample in \(S_{1}\) and \(S_{2}\). The way regression tree grows is to automatically decide on the splitting variables and split points that can maximize \textbf{SSE reduction}. Since this process is essentially a recursive segmentation, this approach is also called recursive partitioning.

Take a look at this simple regression tree for the height of 10 students:

\includegraphics{images/varEN.png}

You can calculate the SSE using the following code:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  SSE for ``Female'' is 136
\item
  SSE for ``Male'' is 32
\item
  SSE for splitting node ``Gender'' is the sum of the above two numbers which is 168
\end{enumerate}

SSE for the 10 students in root node is 522.9. After the split, SSE decreases from 522.9 to 168.

If there is another possible way of splitting, divide it by major, as follows:

\includegraphics{images/varEN2.png}

In this situation:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  SSE for ``Math'' is 184
\item
  SSE for ``English'' is 302.8
\item
  SSE for splitting node ``Major'' is the sum of the above two numbers which is 486.8
\end{enumerate}

Splitting data using variable ``gender'' reduced SSE from 522.9 to 168; using variable ``major'' reduced SSE from 522.9 to 486.8. Based on SSE reduction, you should use gender to split the data.

The three splitting criteria mentioned above are the basis for building a tree model.

\hypertarget{tree-pruning}{%
\section{Tree Pruning}\label{tree-pruning}}

Pruning is the process that reduces the size of decision trees. It reduces the risk of overfitting by limiting the size of the tree or removing sections of the tree that provide little power.

\textbf{Limit the size}

You can limit the tree size by setting some parameters.

\begin{itemize}
\item
  Minimum sample size at each node: Defining the minimum sample size at the node helps to prevent the leaf nodes having only one sample. The sample size can be a tuning parameter. If it is too large, the model tends to under-fit. If it is too small, the model tends to over-fit. In the case of severe class imbalance, the minimum sample size may need to be smaller because the number of samples in a particular class is small.
\item
  Maximum depth of the tree: If the tree grows too deep, the model tends to over-fit. It can be a tuning parameter.
\item
  Maximum number of terminal nodes: Limit on the terminal nodes works the same as the limit on the depth of the tree. They are proportional.
\item
  The number of variables considered for each split: the algorithm randomly selects variables used in finding the optimal split point at each level. In general, the square root of the number of all variables works best, which is also the default setting for many functions. However, people often treat it as a tuning parameter.
\end{itemize}

\textbf{Remove branches}

Another way is to first let the tree grow as much as possible and then go back to remove insignificant branches. The process reduces the depth of the tree. The idea is to overfit the training set and then correct using the testing set. There are different implementations.

\begin{itemize}
\tightlist
\item
  cost/complexity penalty
\end{itemize}

The idea is that the pruning minimizes the penalized error \(SSE_{\lambda}\) with a certain value of tuning parameter \(\lambda\).

\[SSE_{\lambda} = SSE+\lambda \times (complexity)\]

Here complexity is a function of the number of leaves. For every given \(\lambda\), we want to find the smallest tree that minimizes this penalized error. Breiman presents the algorithm to solve the optimization\citep{Breiman1984}.

To find the optimal pruning tree, you need to iterate through a series of values of \(\lambda\) and calculate the corresponding SSE. For the same \(\lambda\), SSE changes over different samples. Breiman et al.~suggested using cross-validation \citep{Breiman1984} to study the variation of SSE under each \(\lambda\) value. They also proposed a standard deviation criterion to give the simplest tree: within one standard deviation, find the simplest tree that minimizes the absolute error. Another method is to choose the tree size that minimizes the numerical error \citep{Hastie2008}.

\begin{itemize}
\tightlist
\item
  Error-based pruning
\end{itemize}

This method was first proposed by Quinlan \citep{Quinlan1999}. The idea behind is intuitive. All split nodes of the tree are included in the initial candidate pool. Pruning a split node means removing the entire subtree under the node and setting the node as a terminal node. The data is divided into 3 subsets for:

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\item
  training a complete tree;
\item
  pruning;
\item
  testing the final model.
\end{enumerate}

You train a complete tree using the subset (1) and apply the tree on the subset (2) to calculate the accuracy. Then prune the tree based on a node and apply that on the subset (2) to calculate another accuracy. If the accuracy after pruning is higher or equal to that from the complete tree, then we set the node as a terminal node. Otherwise, keep the subtree under the node. The advantage of this method is that it is easy to compute. However, when the size of the subset (2) is much smaller than that of the subset (1), there is a risk of over-pruning. Some researchers found that this method results in more accurate trees than pruning process based on tree size \citep{Espoito1997}.

\begin{itemize}
\tightlist
\item
  Error-complexity pruning
\end{itemize}

This method is to search for a trade-off between error and complexity. Assume we have a splitting node \(t\), and the corresponding subtree \(T\). The error cost of the node is defined as:

\[R(t)=r(t)\times p(t)\]

where \(r(t)\) is the error rate associate with the node:

\[r(t)=\frac{misclassified\ sample\ size\ of\ the\ node}{sample\ size\ of\ the\ node}\]

\(p(t)\) is the ratio of the sample of the node to the total sample

\[p(t)=\frac{ sample\ size\ of\ the\ node}{total\ sample\ size}\]

If we keep note \(t\), the error cost of the subtree \(T\) is:

\[R(T)=\Sigma_{i = no.\ of\ leaves} R(i)\]

The error-complexity measure of the node is:

\[a(t)=\frac{R(t)-R(T)_{t}}{no.\ of\ leaves - 1}\]

Based on the metrics above, the pruning process is \citep{Nikita2012}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Calculate \(a\) for every node \(t\)
\item
  Prune the node with the lowest value
\item
  Repeat 1 and 2. It produces a pruned tree each time and they form a forest.
\item
  Select the tree with the best overall accuracy
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Minium error pruning
\end{itemize}

Niblett and Brotko came it up in 1991\citep{Cestnik1991}. The process is bottom-up which seeks a single tree that minimizes the expected error rate on new samples. If we prune a splitting point \(t\), all the samples under \(t\) will be classified as from one category, say category \(c\). If we prune the subtree, the expected error rate is:

\[E(t)=\frac{n_{t}-n_{t,c}+k-1}{n_{t}+k}\]

where:

\[k=number\ of\ categories\]
\[n_{t}=sample\ size\ under\ node\ t\]
\[n_{t,c}= number\ of\ sample\ under\ t\ that\ belong\ to\ category\ c\]

Based on the above definition, the pruning process is\citep{Espoito1997}:

\begin{itemize}
\tightlist
\item
  Calculate the expected error rate for each non-leave node if that subtree is pruned
\item
  Calculate the expected error rate for each non-leave node if that subtree is not pruned
\item
  If pruning the node leads to higher expected rate, then keep the subtree; otherwise, prune it.
\end{itemize}

\hypertarget{regression-and-decision-tree-basic}{%
\section{Regression and Decision Tree Basic}\label{regression-and-decision-tree-basic}}

\hypertarget{regression-tree}{%
\subsection{Regression Tree}\label{regression-tree}}

Let's look at the process of building a regression tree\citep{ISLR15}. There are two steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Divide predictors space --- that is a set of possible values of \(X_1,X_2,\dots,X_p\)--- into \(J\) distinct and non-overlapping regions: \(R_1,R_2,\dots,R_J\)
\item
  For every observation that falls into the region \(R_j\), the prediction in the mean of the response values for the training observations in \(R_j\)
\end{enumerate}

Let's go back to the previous baby example. If we use the variable ``Gender'' to divide the observations, we obtain two regions \(R_1\) (female) and \(R_2\) (male).

\includegraphics{images/varEN.png}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y1=}\KeywordTok{c}\NormalTok{(}\DecValTok{156}\NormalTok{,}\DecValTok{167}\NormalTok{,}\DecValTok{165}\NormalTok{,}\DecValTok{163}\NormalTok{,}\DecValTok{160}\NormalTok{,}\DecValTok{170}\NormalTok{,}\DecValTok{160}\NormalTok{)}
\NormalTok{y2=}\KeywordTok{c}\NormalTok{(}\DecValTok{172}\NormalTok{,}\DecValTok{180}\NormalTok{,}\DecValTok{176}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The sample average for region \(R_1\) is 163, for region \(R_2\) is 176. For a new observation, if it is female, the model predicts the height to be 163, if it is male, the predicted height is 176. Calculating the mean is easy. Let's look at the first step in more detail which is to divide the space into \(R_1, R_2, \dots, R_J\).

In theory, the region can be any shape. However, to simplify the problem, we divide the predictor space into high-dimensional rectangles. The goal is to divide the space in a way that minimize RSS. Practically, it is nearly impossible to consider all possible partitions of the feature space. So we use an approach named recursive binary splitting, a top-down , greedy algorithm. The process starts from the top of the tree (root node) and then successively splits the predictor space. Each split produces two branches (hence binary). At each step of the process, it chooses the best split at that particular step, rather than looking ahead and picking a split that leads to a better tree in general (hence greedy).

\[R_{1}(j, s)=\{X|X_j<s\}\ and\ R_{2}(j, s)=\{X|X_j\geq s\}\]

Calculate the RSS decrease after the split. For different \((j,s)\), search for the combination that minimizes the RSS, that is to minimize the following:

\[\Sigma_{i:x_i\in R_1(j,s)}(y_i-\hat{y}_{R_{1}})^2+\Sigma_{i:x_i\in R_2(j,s)}(y_i-\hat{y}_{R_{2}})^2\]

where \(\hat{y}_{R_1}\) is the mean of all samples in \(R_1\), \(\hat{y}_{R_2}\) is the mean of samples in \(R_2\). It can be quick to optimize the equation above. Especially when \(p\) is not too large.

Next, we continue to search for the split that optimize the RSS. Note that the optimization is limited in the sub-region. The process keeps going until a stopping criterion is reaches. For example, continue until no region contains more than 5 samples or the RSS decreases less than 1\%. The process is like a tree growing.

\includegraphics{images/BinaryTree.png}

There are multiple R packages for building regression tree, such as \texttt{ctree}, \texttt{rpart} and \texttt{tree}. \texttt{rpart} is widely used for building single tree. The split is based on CART algorithm, using \texttt{rpart()} function from the package. There are some parameters that controls the model fitting, such as the minimum number of observations that must exist in a node in order for a split to be attempted, the minimum number of observations in any leaf node etc. You can can set those parameter using \texttt{rpart.control}.

A more convenient way is to use \texttt{train()} function in \texttt{caret} package. The package can call \texttt{rpart()} function and train the model through cross-validation. In this case, the most common parameters are \texttt{cp} (complexity parameter) and \texttt{maxdepth} (the maximum depth of any node of the final tree). To tune the complexity parameter, set \texttt{method\ =\ "rpart"}. To tune the maximum tree depth, set \texttt{method\ =\ "rpart2"} :

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(rpart)}
\NormalTok{dat <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"http://bit.ly/2P5gTw4"}\NormalTok{)}
\CommentTok{# data cleaning: delete wrong observations}
\NormalTok{dat <-}\StringTok{ }\KeywordTok{subset}\NormalTok{(dat, store_exp }\OperatorTok{>}\StringTok{ }\DecValTok{0} \OperatorTok{&}\StringTok{ }\NormalTok{online_exp }\OperatorTok{>}\StringTok{ }\DecValTok{0}\NormalTok{)}
\CommentTok{# use the 10 survey questions as predictors}
\NormalTok{trainx <-}\StringTok{ }\NormalTok{dat[, }\KeywordTok{grep}\NormalTok{(}\StringTok{"Q"}\NormalTok{, }\KeywordTok{names}\NormalTok{(dat))]}
\CommentTok{# use the sum of store and online expenditure as response variable}
\CommentTok{# total expenditure = store expenditure + online expenditure}
\NormalTok{trainy <-}\StringTok{ }\NormalTok{dat}\OperatorTok{$}\NormalTok{store_exp }\OperatorTok{+}\StringTok{ }\NormalTok{dat}\OperatorTok{$}\NormalTok{online_exp}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{100}\NormalTok{)}
\NormalTok{rpartTune <-}\StringTok{ }\KeywordTok{train}\NormalTok{(trainx, trainy, }\DataTypeTok{method =} \StringTok{"rpart2"}\NormalTok{, }\DataTypeTok{tuneLength =} \DecValTok{10}\NormalTok{, }
    \DataTypeTok{trControl =} \KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method =} \StringTok{"cv"}\NormalTok{))}
\KeywordTok{plot}\NormalTok{(rpartTune)}
\end{Highlighting}
\end{Shaded}

\includegraphics{IDS_files/figure-latex/unnamed-chunk-135-1.pdf}

RMSE doesn't change much when the maximum is larger than 2. So we set the maximum depth to be 2 and refit the model:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rpartTree <-}\StringTok{ }\KeywordTok{rpart}\NormalTok{(trainy }\OperatorTok{~}\StringTok{ }\NormalTok{., }\DataTypeTok{data =}\NormalTok{ trainx, }\DataTypeTok{maxdepth =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

You can check the result using \texttt{print()}:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{print}\NormalTok{(rpartTree)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## n= 999 
## 
## node), split, n, deviance, yval
##       * denotes terminal node
## 
## 1) root 999 1.581e+10  3479.0  
##   2) Q3< 3.5 799 2.374e+09  1819.0  
##     4) Q5< 1.5 250 3.534e+06   705.2 *
##     5) Q5>=1.5 549 1.919e+09  2326.0 *
##   3) Q3>=3.5 200 2.436e+09 10110.0 *
\end{verbatim}

You can see that the final model picks \texttt{Q3} and \texttt{Q5} to predict total expenditure. To visualize the tree, you can convert \texttt{rpart} object to \texttt{party} object using \texttt{partykit} then use \texttt{plot()} function:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(partykit)}
\NormalTok{rpartTree2 <-}\StringTok{ }\KeywordTok{as.party}\NormalTok{(rpartTree)}
\KeywordTok{plot}\NormalTok{(rpartTree2)}
\end{Highlighting}
\end{Shaded}

\includegraphics{IDS_files/figure-latex/unnamed-chunk-138-1.pdf}

\hypertarget{decision-tree}{%
\subsection{Decision Tree}\label{decision-tree}}

Similar to a regression tree, the goal of a classification tree is to stratifying the predictor space into a number of sub-regions that are more homogeneous. The difference is that a classification tree is used to predict a categorical response rather than a continuous one. For a classification tree, the prediction is the most commonly occurring class of training observations in the region to which an observation belongs. The splitting criteria for a classification tree are different. The most common criteria are entropy and Gini impurity. CART uses Gini impurity and C4.5 uses entropy.

When the predictor is continuous, the splitting process is straightforward. When the predictor is categorical, the process can take different approaches:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Keep the variable as categorical and group some categories on either side of the split. In this way, the model can make more dynamic splits but must treat the categorical predictor as an ordered set of bits.
\item
  Encode the categorical variable to be a set of binary dummy variables. In this way, the information in the categorical variable is decomposed to independent bits of information. The model considers these dummy variables separately and evaluates for each of these on one split point (because there are only two possible values: 0/1).
\end{enumerate}

When fitting tree models, people need to choose the way to treat categorical predictors. If you know some of the categories have higher predictability, then the first approach may be better. In the rest of this section, we will build tree models using the above two approaches and compare them.

Let's build a classification model to identify the gender of the customer:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(caret)}
\KeywordTok{library}\NormalTok{(pROC)}
\NormalTok{dat <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"http://bit.ly/2P5gTw4"}\NormalTok{)}
\CommentTok{# use the 10 survey questions as predictors}
\NormalTok{trainx1 <-}\StringTok{ }\NormalTok{dat[, }\KeywordTok{grep}\NormalTok{(}\StringTok{"Q"}\NormalTok{, }\KeywordTok{names}\NormalTok{(dat))]}
\CommentTok{# add a categorical predictor}
\CommentTok{# use two ways to treat categorical predictor}
\CommentTok{# trainx1: use approach 1, without encoding}
\NormalTok{trainx1}\OperatorTok{$}\NormalTok{segment <-}\StringTok{ }\NormalTok{dat}\OperatorTok{$}\NormalTok{segment}

\CommentTok{# trainx2: use approach 2, encode it to a set of dummy variables}
\NormalTok{dumMod<-}\KeywordTok{dummyVars}\NormalTok{(}\OperatorTok{~}\NormalTok{.,}
                  \DataTypeTok{data=}\NormalTok{trainx1,}
                  \CommentTok{# Combine the previous variable name and the level name}
                  \CommentTok{# as the new dummy variable name}
                  \DataTypeTok{levelsOnly=}\NormalTok{F)}
\NormalTok{trainx2 <-}\StringTok{ }\KeywordTok{predict}\NormalTok{(dumMod,trainx1)}
\CommentTok{# the response variable is gender}
\NormalTok{trainy <-}\StringTok{ }\NormalTok{dat}\OperatorTok{$}\NormalTok{gender}
\end{Highlighting}
\end{Shaded}

Here we use \texttt{train()} function in \texttt{caret} package to call \texttt{rpart} to build the model. We can compare the model results from the two approaches:

\begin{verbatim}
CART 

1000 samples
  11 predictor
   2 classes: 'Female', 'Male' 

No pre-processing
Resampling: Cross-Validated (10 fold) 
Summary of sample sizes: 901, 899, 900, 900, 901, 900, ... 
Resampling results across tuning parameters:

  cp       ROC     Sens    Spec  
  0.00000  0.6937  0.6517  0.6884
  0.00835  0.7026  0.6119  0.7355
  0.01670  0.6852  0.5324  0.8205
  0.02505  0.6803  0.5107  0.8498
  0.03340  0.6803  0.5107  0.8498
  
  ......

  0.23380  0.6341  0.5936  0.6745
  0.24215  0.5556  0.7873  0.3240

ROC was used to select the optimal model using the largest value.
The final value used for the model was cp = 0.00835.
\end{verbatim}

The above keeps the variable as categorical without encoding. Here \texttt{cp} is the complexity parameter. It is used to decide when to stop growing the tree. \texttt{cp\ =\ 0.01} means the algorithm only keeps the split that improves the corresponding metric by more than 0.01. Next, let's encode the categorical variable to be a set of dummy variables and fit the model again:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rpartTune2 <-}\StringTok{ }\NormalTok{caret}\OperatorTok{::}\KeywordTok{train}\NormalTok{(trainx2, trainy, }\DataTypeTok{method =} \StringTok{"rpart"}\NormalTok{,}
                       \DataTypeTok{tuneLength =} \DecValTok{30}\NormalTok{,}
                       \DataTypeTok{metric =} \StringTok{"ROC"}\NormalTok{, }
                       \DataTypeTok{trControl =} \KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method =} \StringTok{"cv"}\NormalTok{,}
                                                \DataTypeTok{summaryFunction =}\NormalTok{ twoClassSummary,}
                                                \DataTypeTok{classProbs =} \OtherTok{TRUE}\NormalTok{,}
                                                \DataTypeTok{savePredictions =} \OtherTok{TRUE}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Compare the results of the two approaches.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rpartRoc <-}\StringTok{ }\KeywordTok{roc}\NormalTok{(}\DataTypeTok{response =}\NormalTok{ rpartTune1}\OperatorTok{$}\NormalTok{pred}\OperatorTok{$}\NormalTok{obs,}
                \DataTypeTok{predictor =}\NormalTok{ rpartTune1}\OperatorTok{$}\NormalTok{pred}\OperatorTok{$}\NormalTok{Female,}
                \DataTypeTok{levels =} \KeywordTok{rev}\NormalTok{(}\KeywordTok{levels}\NormalTok{(rpartTune1}\OperatorTok{$}\NormalTok{pred}\OperatorTok{$}\NormalTok{obs)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Setting direction: controls < cases
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rpartFactorRoc <-}\StringTok{ }\KeywordTok{roc}\NormalTok{(}\DataTypeTok{response =}\NormalTok{ rpartTune2}\OperatorTok{$}\NormalTok{pred}\OperatorTok{$}\NormalTok{obs,}
                      \DataTypeTok{predictor =}\NormalTok{ rpartTune2}\OperatorTok{$}\NormalTok{pred}\OperatorTok{$}\NormalTok{Female,}
                      \DataTypeTok{levels =} \KeywordTok{rev}\NormalTok{(}\KeywordTok{levels}\NormalTok{(rpartTune1}\OperatorTok{$}\NormalTok{pred}\OperatorTok{$}\NormalTok{obs)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Setting direction: controls < cases
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(rpartRoc, }\DataTypeTok{type =} \StringTok{"s"}\NormalTok{, }\DataTypeTok{print.thres =} \KeywordTok{c}\NormalTok{(.}\DecValTok{5}\NormalTok{),}
     \DataTypeTok{print.thres.pch =} \DecValTok{3}\NormalTok{,}
     \DataTypeTok{print.thres.pattern =} \StringTok{""}\NormalTok{,}
     \DataTypeTok{print.thres.cex =} \FloatTok{1.2}\NormalTok{,}
     \DataTypeTok{col =} \StringTok{"red"}\NormalTok{, }\DataTypeTok{legacy.axes =} \OtherTok{TRUE}\NormalTok{,}
     \DataTypeTok{print.thres.col =} \StringTok{"red"}\NormalTok{)}

\KeywordTok{plot}\NormalTok{(rpartFactorRoc,}
     \DataTypeTok{type =} \StringTok{"s"}\NormalTok{,}
     \DataTypeTok{add =} \OtherTok{TRUE}\NormalTok{,}
     \DataTypeTok{print.thres =} \KeywordTok{c}\NormalTok{(.}\DecValTok{5}\NormalTok{),}
     \DataTypeTok{print.thres.pch =} \DecValTok{16}\NormalTok{, }\DataTypeTok{legacy.axes =} \OtherTok{TRUE}\NormalTok{,}
     \DataTypeTok{print.thres.pattern =} \StringTok{""}\NormalTok{,}
     \DataTypeTok{print.thres.cex =} \FloatTok{1.2}\NormalTok{)}
\KeywordTok{legend}\NormalTok{(.}\DecValTok{75}\NormalTok{, }\FloatTok{.2}\NormalTok{,}
       \KeywordTok{c}\NormalTok{(}\StringTok{"Grouped Categories"}\NormalTok{, }\StringTok{"Independent Categories"}\NormalTok{),}
       \DataTypeTok{lwd =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{),}
       \DataTypeTok{col =} \KeywordTok{c}\NormalTok{(}\StringTok{"black"}\NormalTok{, }\StringTok{"red"}\NormalTok{),}
       \DataTypeTok{pch =} \KeywordTok{c}\NormalTok{(}\DecValTok{16}\NormalTok{, }\DecValTok{3}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\includegraphics{IDS_files/figure-latex/unnamed-chunk-142-1.pdf}

You can see that in this case, the two approaches lead to similar model performance.

Single tree is straightforward and easy to interpret but it has problems:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Low accuracy
\item
  Unstable: little change in the training data leads to very different trees.
\end{enumerate}

One way to overcome those is to use an ensemble of trees. In the rest of this chapter, we will introduce three ensemble methods (combine many models' predictions): bagging tree, random forest, and gradient boosted machine. Those ensemble approaches have significant higher accuracy and stability. However, it comes with the cost of interpretability.

\hypertarget{bagging-tree-1}{%
\section{Bagging Tree}\label{bagging-tree-1}}

As mentioned before, a single tree is unstable. If you randomly separate the sample to be two parts and fit tree model on each, you can get two very different trees. A stable model should give a similar result on different random samples. Some traditional statistical models have high stability, such as linear regression. \emph{Ensemble methods} appeared in the 1990s which can effectively stabilize the model. \emph{Bootstrapping} is a type of process where you repeated draw samples of the same size from a single original sample with replacement \citep{Efron1986}. \emph{Bootstrap aggregation} (Bagged) is an ensemble technique proposed by Leo Breiman (Breiman 1996a). It uses bootstrapping in conjunction with any model to construct an ensemble. The process is very straightforward:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Build a model on different bootstrap samples to form an ensemble, say \(B\) samples
\item
  For a new sample, each model will give a prediction: \(\hat{f}^1(x),\hat{f}^2(x)\dots,\hat{f}^B(x)\)
\item
  The bagged model's prediction is the average of all the predictions:
\end{enumerate}

\[\hat{f}_{avg}(x)=\frac{1}{B}\Sigma^B_{b=1}\hat{f}^b(x)\]

Assume that there are \(n\) independent random variables \(Z_1,\dots,Z_n\) with variance \(\sigma^2\). Then the variance of the mean \(\bar{Z}\) is \(\frac{\sigma^2}{n}\). It is easy to see why bagged models have less variance. Since bootstrapping is to sample with replacement, it means some samples are selected multiple times and some not at all. Those left out samples are called out-of-bag. You can use the out-of-bag sample to access the model performance. For regression, the prediction is a simple average. For classification, the prediction is the category with the most ``votes''. Here, the number of trees, \(B\) is a parameter you need to decide, i.e.~tuning parameter. \textbf{Bagging is a general approach that can be applied to different learners. Here we only discuss in the context of decision trees.}

The advantages of bagging tree are:

\begin{itemize}
\item
  Bagging stabilizes the model predictions by averaging the results. If we have 10 bootstrap samples and fit a single tree on each of those, we may get 10 trees with very different structures and leading to different predictions for a new sample. But if we use the average of the 10 predictions as the final prediction, then the result should be much more stable. It means if we have another 10 samples and do it all-over again, we will get very similar averaged prediction.
\item
  Bagging provides more accurate predictions. If the goal is to predict rather than interpret, then the ensemble approach definitely has an advantage, especially for unstable models. However, for stable models (such as regression, MARS), bagging may bring marginal improvement for the model performance.
\item
  Bagging can use out-of-bag samples to evaluate model performance. For each model in the ensemble, we can calculate the value of the model performance metric (you can decide what metric to use). You can use the average of all the out-of-bag performance values to gauge the predictive performance of the entire ensemble. This correlates well with either cross-validation estimates or test set estimates. On average, each tree uses about 2/3 of the samples, and the rest 1/3 is used as out-of-bag. When the number of bootstrap samples is large enough, the out-of-bag performance estimate approximates that from leave one out cross-validation.
\end{itemize}

You need to choose the number of bootstrap samples. The author of ``Applied Predictive Modeling'' \citep{APM} points out that often people see an exponential decrease in predictive improvement as the number of iterations increases. Most of the predictive power is from a small portion of the trees. Based on their experience, model performance can have small improvements up to 50 bagging iterations. If it is still not satisfying, they suggest trying other more powerfully predictive ensemble methods such as random forests and boosting which will be described in the following sections.

The disadvantages of bagging tree are:

\begin{itemize}
\item
  As the number of bootstrap samples increases, the computation and memory requirements increase as well. You can mitigate this disadvantage by parallel computing. Since each bootstrap sample and modeling is independent of any other sample and model, you can easily parallelize the bagging process by building those models separately and bring back the results in the ed to generate the prediction.
\item
  The bagged model is difficult to explain which is common for all ensemble approaches. However, you can still get variable importance by combining measures of importance across the ensemble. For example, we can calculate the RSS decrease for each variable across all trees and use the average as the measurement of the importance.
\item
  Since the bagging tree uses all of the original predictors as everey split of every tree, those trees are related with each other. The tree correlation prevents bagging from optimally reducing the variance of the predicted values. See \citep{Hastie2008} for a mathematical illustration of the tree correlation phenomenon.
\end{itemize}

Let's look at how to use R to build bagging tree. Get the predictors and response variable first:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dat <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"http://bit.ly/2P5gTw4"}\NormalTok{)}
\CommentTok{# use the 10 survey questions as predictors}
\NormalTok{trainx <-}\StringTok{ }\NormalTok{dat[, }\KeywordTok{grep}\NormalTok{(}\StringTok{"Q"}\NormalTok{, }\KeywordTok{names}\NormalTok{(dat))]}
\CommentTok{# add segment as a predictor }
\CommentTok{# don't need to encode it to dummy variables}
\NormalTok{trainx}\OperatorTok{$}\NormalTok{segment <-}\StringTok{ }\NormalTok{dat}\OperatorTok{$}\NormalTok{segment}
\CommentTok{# use gender as the response variable}
\NormalTok{trainy <-}\StringTok{ }\KeywordTok{as.factor}\NormalTok{(dat}\OperatorTok{$}\NormalTok{gender)}
\end{Highlighting}
\end{Shaded}

Then fit the model using train function in caret package. Here we just set the number of trees to be 1000. You can tune that parameter.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{100}\NormalTok{)}
\NormalTok{bagTune <-}\StringTok{ }\NormalTok{caret}\OperatorTok{::}\KeywordTok{train}\NormalTok{(trainx, trainy, }
                           \DataTypeTok{method =} \StringTok{"treebag"}\NormalTok{,}
                           \DataTypeTok{nbagg =} \DecValTok{2000}\NormalTok{,}
                           \DataTypeTok{metric =} \StringTok{"ROC"}\NormalTok{,}
                           \DataTypeTok{trControl =} \KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method =} \StringTok{"cv"}\NormalTok{,}
                           \DataTypeTok{summaryFunction =}\NormalTok{ twoClassSummary,}
                           \DataTypeTok{classProbs =} \OtherTok{TRUE}\NormalTok{,}
                           \DataTypeTok{savePredictions =} \OtherTok{TRUE}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

The model results are:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bagTune}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Bagged CART 
## 
## 1000 samples
##   11 predictor
##    2 classes: 'Female', 'Male' 
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 901, 899, 900, 900, 901, 900, ... 
## Resampling results:
## 
##   ROC     Sens    Spec  
##   0.7093  0.6533  0.6774
\end{verbatim}

Since we only have a handful of variables in this example, the maximum AUC doesn't improve by using bagging tree. But it makes a difference when we have more predictors.

\hypertarget{random-forest}{%
\section{Random Forest}\label{random-forest}}

Since the tree correlation prevents bagging from optimally reducing the variance of the predicted values, a natural way to improve the model performance is to reduce the correlation among trees. That is what random forest aims to do: improve the performance of bagging by de-correlating trees.

From a statistical perspective, you can de-correlate trees by introducing randomness when you build each tree. One approach \citep{Ho1998, amit1997} is to randomly choose \(m\) variables to use each time you build a tree. Dietterich\citep{Dietterich2000} came up with the idea of random split selection which is to randomly choose \(m\) variables to use at each splitting node. Based on the different generalizations to the original bagging algorithm, Breiman \citep{Breiman2001} came up with a unified algorithm called \emph{random forest.}

When building a tree, the algorithm randomly chooses \(m\) variables to use at each splitting node. Then choose the best one out of the \(m\) to use at that node. In general, people use \(m=\sqrt{p}\). For example, if we use 10 questions from the questionnaire as predictors, then at each node, the algorithm will randomly choose 4 candidate variables. Since those trees in the forest don't always use the same variables, tree correlation is less than that in bagging. It tends to work better when there are more predictors. Since we only have 10 predictors here, the improvement from the random forest is marginal. The number of randomly selected predictors is a tuning parameter in the random forest. Since random forest is computationally intensive, we suggest starting with 5 value around \(m=\sqrt{p}\). Another tuning parameter is the number of trees in the forest. You can start with 1000 trees and then increase the number until performance levels off.

The basic random forest is as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Select the number of trees, B
\item
  \textbf{for} \emph{i=1 to B} \textbf{do}

  \begin{itemize}
  \tightlist
  \item
    generate a bootstrap sample of the original data
  \item
    train a tree on this sample

    \begin{itemize}
    \tightlist
    \item
      \textbf{for} \emph{each split} \textbf{do}

      \begin{itemize}
      \tightlist
      \item
        randomly select m\textless{}ppredictors
      \item
        choose the best one out of the \(m\) and partition the data
      \end{itemize}
    \item
      \textbf{end}
    \end{itemize}
  \item
    use typical tree model stopping criteria to determine when a tree is complete without pruning
  \end{itemize}
\item
  \textbf{end}
\end{enumerate}

When \(m=p\), random forest is equal to the bagging tree. When the predictors are highly correlated, then smaller \(m\) tends to work better. Let's use the \texttt{caret} package to train a random forest:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# tune across a list of numbers of predictors}
\NormalTok{mtryValues <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{5}\NormalTok{)}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{100}\NormalTok{)}
\NormalTok{rfTune <-}\StringTok{ }\KeywordTok{train}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ trainx, }
               \DataTypeTok{y =}\NormalTok{ trainy,}
               \CommentTok{# set the model to be random forest}
               \DataTypeTok{method =} \StringTok{"rf"}\NormalTok{,}
               \DataTypeTok{ntree =} \DecValTok{1000}\NormalTok{,}
               \DataTypeTok{tuneGrid =} \KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{.mtry =}\NormalTok{ mtryValues),}
               \DataTypeTok{importance =} \OtherTok{TRUE}\NormalTok{,}
               \DataTypeTok{metric =} \StringTok{"ROC"}\NormalTok{,}
               \DataTypeTok{trControl =} \KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method =} \StringTok{"cv"}\NormalTok{,}
                           \DataTypeTok{summaryFunction =}\NormalTok{ twoClassSummary,}
                           \DataTypeTok{classProbs =} \OtherTok{TRUE}\NormalTok{,}
                           \DataTypeTok{savePredictions =} \OtherTok{TRUE}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rfTune}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Random Forest 
## 
## 1000 samples
##   11 predictor
##    2 classes: 'Female', 'Male' 
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 899, 900, 900, 899, 899, 901, ... 
## Resampling results across tuning parameters:
## 
##   mtry  ROC     Sens    Spec  
##   1     0.7169  0.5341  0.8205
##   2     0.7137  0.6334  0.7175
##   3     0.7150  0.6478  0.6995
##   4     0.7114  0.6550  0.6950
##   5     0.7092  0.6514  0.6882
## 
## ROC was used to select the optimal model using
##  the largest value.
## The final value used for the model was mtry = 1.
\end{verbatim}

In this example, since the number of predictors is small, the result of the model indicates that the optimal number of candidate variables at each node is 1. The optimal AUC is not too much higher than that from bagging tree:

If you have selected the values of tuning parameters, you can also use the randomForest package to fit a random forest.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(randomForest)}
\NormalTok{rfit =}\StringTok{ }\KeywordTok{randomForest}\NormalTok{(trainy }\OperatorTok{~}\StringTok{ }\NormalTok{., trainx, }\DataTypeTok{mtry =} \DecValTok{1}\NormalTok{, }\DataTypeTok{ntree =} \DecValTok{1000}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Since bagging tree is a special case of random forest, you can fit the bagging tree by setting \(mtry=p\). Function \texttt{importance()} can return the importance of each predictor:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{importance}\NormalTok{(rfit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##         MeanDecreaseGini
## Q1                 9.056
## Q2                 7.582
## Q3                 7.611
## Q4                12.308
## Q5                 5.628
## Q6                 9.740
## Q7                 6.638
## Q8                 7.829
## Q9                 5.955
## Q10                4.781
## segment           11.185
\end{verbatim}

You can use varImpPlot() function to visualize the predictor importance:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{varImpPlot}\NormalTok{(rfit)}
\end{Highlighting}
\end{Shaded}

\includegraphics{IDS_files/figure-latex/unnamed-chunk-150-1.pdf}
It is easy to see from the plot that \texttt{segment} and \texttt{Q4} are the top two variables to classify gender.

\hypertarget{gradient-boosted-machine}{%
\section{Gradient Boosted Machine}\label{gradient-boosted-machine}}

Boosting models were developed in the 1980s \citep{Valiant1984, KV1989} and were originally for classification problems. Due to the excellent model performance, they were widely used for a variety of applications, such as gene expression \citep{dudoit2002, bendor2000}, chemical substructure classification \citep{Varmuza2003}, music classification \citep{Bergstra2006}, etc. The first effective implementation of boosting is Adaptive Boosting (AdaBoost) algorithm came up by Yoav Freund and Robert Schapire in 1996 \citep{Schapire1999}. After that, some researchers \citep{Friedman2000} started to connect the boosting algorithm with some statistical concepts, such as loss function, additive model, logistic regression. Friedman pointed out that boosting can be considered as a forward stagewise additive model that minimizes exponential loss. The new view of boosting in a statistical framework enabled the method to be extended to regression problems.

The idea is to combine a group of weak learners (a classifier that is marginally better than random guess) to produce a strong learner. Like bagging, boosting is a general approach that can be applied to different learners. Here we focus on the decision tree. Recall that both bagging and random forest create multiple copies of the original training data using the bootstrap, fitting a separate decision tree to each copy and combining all the results to create a single prediction. Boosting also creates different trees but the trees are grown sequentially and each tree is a weak learner. Any modeling technique with tuning parameters can produce a range of learners, from weak to strong. You can easily make a weak learner by restricting the depth of the tree. There are different types of boosting. Here we introduce two main types: adaptive boosting and stochastic gradient boosting.

\hypertarget{adaptive-boosting}{%
\subsection{Adaptive Boosting}\label{adaptive-boosting}}

Yoav Freund and Robert Schapire \citep{Freund1997} came up the AdaBoost.M1 algorithm. Consider a binary classification problem where the response variable has two categories \(Y \in \{-1, 1\}\). Given predictor matrix, \(X\), construct a classifier \(G(X)\) that predicts \(1\) or \(-1\). The corresponding error rate in the training set is:

\[\bar{err}=\frac{1}{N}\Sigma_{i=1}^NI(y_i\neq G(x_i))\]

The algorithm produces a series of classifiers \(G_m(x),\ m=1,2,...,M\) from different iterations. In each iteration, it finds the best classifier based on the current weights. The misclassified samples in the \(m^{th}\) iteration will have higher weights in the \(m+1^{th}\) iteration and the correctly classified samples will have lower weights. As it moves on, the algorithm will put more effort into the ``difficult'' samples until it can correctly classify them. So it requires the algorithm to change focus at each iteration. At each iteration, the algorithm will calculate a stage weight based on the error rate. The final prediction is a weighted average of all those weak classifiers using stage weights from all the iterations:

\[G(x)=sign ( \Sigma_{m=1}^M \alpha_{m}G_m(x))\]
where \(\alpha_1,\alpha_2,...,\alpha_M\) are the weights from different iterations.

\textbf{AdaBoost.M1}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Response variables have two values: +1 and -1
\item
  Initialize the observation to have the same weights: \(w_i=\frac{1}{N},i=1,...,N\)
\item
  for m = 1 to M:

  \begin{enumerate}
  \def\labelenumii{(\alph{enumii})}
  \item
    Fit a classifier \(G_m(x)\) to the training data using weights \(w_i\)
  \item
    Compute the error rate for the \(m^th\) classifier: \(err_m=\frac{\Sigma_{i=1}^Nw_i I(y_i\neq G_m(x_i))}{\Sigma_{i=1}^Nw_i}\)
  \item
    Compute the stage weight for \(m^{th}\) iteration: \(\alpha_m=log\frac{1-err_m}{err_m}\)
  \item
    Update \(w_i = w_i\cdot exp[\alpha_m\cdot I(y_i \neq G_m(x_i))],\ i=1,2,\dots,N\)
  \end{enumerate}
\item
  Calculate the prediction\(G(x)=sign[\Sigma_{m=1}^M\alpha_mG_m(x)]\)where \(sign(\cdot)\) means if \(\cdot\) is positive, then the sample is classified as +1, -1 otherwise.
\end{enumerate}

Since the classifier \(G_m(x)\) returns discrete value, the AdaBoost.M1 algorithm is known as ``Discrete AdaBoost'' \citep{Friedman2000}. You can revise the above algorithm if it returns continuous value, for example, a probability\citep{Friedman2000}. As mentioned before, boosting is a general approach that can be applied to different learners. Since you can easily create weak learners by limiting the depth of the tree, the boosting tree is a common method. Since the classification tree is a low bias/high variance technique, ensemble decreases model variance and lead to low bias/low variance model. See Breinman\citep{Breiman1998} for more explanation about why the boosting tree performs well in general. However, boosting can not significantly improve the low variance model. So applying boosting to Latent Dirichlet Allocation (LDA) or K-Nearest Neighbor (KNN) doesn't lead to as good improvement as applying boosting to statistical learning methods like naive Bayes \citep{Bauer1999}.

\hypertarget{stochastic-gradient-boosting}{%
\subsection{Stochastic Gradient Boosting}\label{stochastic-gradient-boosting}}

As mentioned before, Friedman \citep{Friedman2000} provided a statistical framework for the AdaBoost algorithm and pointed out that boosting can be considered as a forward stagewise additive model that minimizes exponential loss. The framework led to some generalized algorithms such as Real AdaBoost, Gentle AdaBoost, and LogitBoost. Those algorithms later were unified under a framework called gradient boosting machine. The last section of the chapter illustrates how boosting can be considered as an additive model.

Consider a 2-class classification problem. You have the response \(y \in \{0, 1\}\) and the sample proportion of class 1 from the training set is \(p\). \(f(x)\) is the model prediction in the range of \([-\infty, +\infty]\) and the predicted event probability is \(\hat{p}=\frac{1}{1+exp[-f(x)]}\). The gradient boosting for this problem is as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Initialize all predictions to the sample log-odds: \(f_{i} = log \frac{\hat{p}}{1- \hat{p}}\)
\item
  \textbf{for} j=1 \ldots{} M \textbf{do}

  \begin{itemize}
  \tightlist
  \item
    Compute predicted event probability: \(\hat{p}_i=\frac{1}{1+exp[-f_{i}(x)]}\).
  \item
    Compute the residual (i.e.~gradient): \(z_i=y_i-\hat{p}_i\)
  \item
    Randomly sample the training data
  \item
    Train a tree model on the random subset using the residuals as the outcome
  \item
    Compute the terminal node estimates of the Pearson residuals: \(r_i=\frac{1/n\Sigma_i^n(y_i-\hat{p}_i)}{1/n\Sigma_i^n\hat{p}_i(1-\hat{p}_i)}\)
  \item
    Update f\(f_i=f_i+\lambda f_i^{(j)}\)
  \end{itemize}
\item
  end
\end{enumerate}

When using the tree as the base learner, basic gradient boosting has two tuning parameters: tree depth and the number of iterations. You can further customize the algorithm by selecting a different loss function and gradient\citep{Hastie2008}. The final line of the loop includes a regularization strategy. Instead of adding \(f_i^{(j)}\) to the previous iteration's \(f_i\), only a fraction of the value is added. This fraction is called learning rate which is \(\lambda\) in the algorithm. It can take values between 0 and 1 which is another tuning parameter of the model.

The way to calculate variable importance in boosting is similar to a bagging model. You get variable importance by combining measures of importance across the ensemble. For example, we can calculate the Gini index improvement for each variable across all trees and use the average as the measurement of the importance.

Boosting is a very popular method for classification. It is one of the methods that can be directly applied to the data without requiring a great deal of time-consuming data preprocessing. Applying boosting on tree models significantly improves predictive accuracy. Some advantages of trees that are sacrificed by boosting are speed and interpretability.

Let's look at the R implementation.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gbmGrid <-}\StringTok{ }\KeywordTok{expand.grid}\NormalTok{(}\DataTypeTok{interaction.depth =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{9}\NormalTok{),}
                       \DataTypeTok{n.trees =} \DecValTok{1}\OperatorTok{:}\DecValTok{5}\NormalTok{,}
                       \DataTypeTok{shrinkage =} \KeywordTok{c}\NormalTok{(.}\DecValTok{01}\NormalTok{, }\FloatTok{.1}\NormalTok{),}
                       \DataTypeTok{n.minobsinnode =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{10}\NormalTok{))}

\KeywordTok{set.seed}\NormalTok{(}\DecValTok{100}\NormalTok{)}
\NormalTok{gbmTune <-}\StringTok{ }\KeywordTok{train}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ trainx, }
                \DataTypeTok{y =}\NormalTok{ trainy,}
                \DataTypeTok{method =} \StringTok{"gbm"}\NormalTok{,}
                \DataTypeTok{tuneGrid =}\NormalTok{ gbmGrid,}
                \DataTypeTok{metric =} \StringTok{"ROC"}\NormalTok{,}
                \DataTypeTok{verbose =} \OtherTok{FALSE}\NormalTok{,}
                \DataTypeTok{trControl =} \KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method =} \StringTok{"cv"}\NormalTok{,}
                           \DataTypeTok{summaryFunction =}\NormalTok{ twoClassSummary,}
                           \DataTypeTok{classProbs =} \OtherTok{TRUE}\NormalTok{,}
                           \DataTypeTok{savePredictions =} \OtherTok{TRUE}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# only show part of the output}
\NormalTok{gbmTune}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Stochastic Gradient Boosting }

\NormalTok{1000 samples}
\NormalTok{  11 predictor}
\NormalTok{   2 classes: 'Female', 'Male' }

\NormalTok{No pre-processing}
\NormalTok{Resampling: Cross-Validated (10 fold) }
\NormalTok{Summary of sample sizes: 899, 900, 900, 899, 899, 901, ... }
\NormalTok{Resampling results across tuning parameters:}

\NormalTok{  shrinkage  interaction.depth  n.minobsinnode  n.trees  ROC     Sens    Spec    }
\NormalTok{  0.01       1                   1              1        0.6821  1.0000  0.000000}
\NormalTok{  0.01       1                   1              2        0.6882  1.0000  0.000000}
\NormalTok{  0.01       1                   1              3        0.6936  1.0000  0.000000}
\NormalTok{  .}
\NormalTok{  .}
\NormalTok{  .}
\NormalTok{  0.01       5                   8              2        0.7127  1.0000  0.000000}
\NormalTok{  0.01       5                   8              3        0.7148  1.0000  0.000000}
\NormalTok{  0.01       5                   8              4        0.7096  1.0000  0.000000}
\NormalTok{  0.01       5                   8              5        0.7100  1.0000  0.000000}
\NormalTok{  0.01       5                   9              1        0.7006  1.0000  0.000000}
\NormalTok{  0.01       5                   9              2        0.7055  1.0000  0.000000}
\NormalTok{ [ reached getOption("max.print") -- omitted 358 rows ]}

\NormalTok{ROC was used to select the optimal model using the largest value.}
\NormalTok{The final values used for the model were n.trees = 4, interaction.depth = 3, shrinkage}
\NormalTok{ = 0.01 and n.minobsinnode = 6.}
\end{Highlighting}
\end{Shaded}

The results show that the tuning parameter settings that lead to the best ROC are \texttt{n.trees\ =\ 4} (number of trees), \texttt{interaction.depth\ =\ 3} (depth of tree), \texttt{shrinkage\ =\ 0.01} (learning rate) and \texttt{n.minobsinnode\ =\ 6} (minimum number of observations in each node).

Now, let's compare the results from the three tree models.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{treebagRoc <-}\StringTok{ }\KeywordTok{roc}\NormalTok{(}\DataTypeTok{response =}\NormalTok{ bagTune}\OperatorTok{$}\NormalTok{pred}\OperatorTok{$}\NormalTok{obs,}
                        \DataTypeTok{predictor =}\NormalTok{ bagTune}\OperatorTok{$}\NormalTok{pred}\OperatorTok{$}\NormalTok{Female,}
                        \DataTypeTok{levels =} \KeywordTok{rev}\NormalTok{(}\KeywordTok{levels}\NormalTok{(bagTune}\OperatorTok{$}\NormalTok{pred}\OperatorTok{$}\NormalTok{obs)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Setting direction: controls < cases
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rfRoc <-}\StringTok{ }\KeywordTok{roc}\NormalTok{(}\DataTypeTok{response =}\NormalTok{ rfTune}\OperatorTok{$}\NormalTok{pred}\OperatorTok{$}\NormalTok{obs,}
             \DataTypeTok{predictor =}\NormalTok{ rfTune}\OperatorTok{$}\NormalTok{pred}\OperatorTok{$}\NormalTok{Female,}
             \DataTypeTok{levels =} \KeywordTok{rev}\NormalTok{(}\KeywordTok{levels}\NormalTok{(rfTune}\OperatorTok{$}\NormalTok{pred}\OperatorTok{$}\NormalTok{obs)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Setting direction: controls < cases
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gbmRoc <-}\StringTok{ }\KeywordTok{roc}\NormalTok{(}\DataTypeTok{response =}\NormalTok{ gbmTune}\OperatorTok{$}\NormalTok{pred}\OperatorTok{$}\NormalTok{obs,}
              \DataTypeTok{predictor =}\NormalTok{ gbmTune}\OperatorTok{$}\NormalTok{pred}\OperatorTok{$}\NormalTok{Female,}
              \DataTypeTok{levels =} \KeywordTok{rev}\NormalTok{(}\KeywordTok{levels}\NormalTok{(gbmTune}\OperatorTok{$}\NormalTok{pred}\OperatorTok{$}\NormalTok{obs)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Setting direction: controls < cases
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(rpartRoc, }\DataTypeTok{type =} \StringTok{"s"}\NormalTok{, }\DataTypeTok{print.thres =} \KeywordTok{c}\NormalTok{(.}\DecValTok{5}\NormalTok{),}
     \DataTypeTok{print.thres.pch =} \DecValTok{16}\NormalTok{,}
     \DataTypeTok{print.thres.pattern =} \StringTok{""}\NormalTok{,}
     \DataTypeTok{print.thres.cex =} \FloatTok{1.2}\NormalTok{,}
     \DataTypeTok{col =} \StringTok{"black"}\NormalTok{, }\DataTypeTok{legacy.axes =} \OtherTok{TRUE}\NormalTok{,}
     \DataTypeTok{print.thres.col =} \StringTok{"black"}\NormalTok{)}

\KeywordTok{plot}\NormalTok{(treebagRoc, }\DataTypeTok{type =} \StringTok{"s"}\NormalTok{, }\DataTypeTok{add =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{print.thres =} \KeywordTok{c}\NormalTok{(.}\DecValTok{5}\NormalTok{), }
     \DataTypeTok{print.thres.pch =} \DecValTok{3}\NormalTok{, }\DataTypeTok{legacy.axes =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{print.thres.pattern =} \StringTok{""}\NormalTok{, }
     \DataTypeTok{print.thres.cex =} \FloatTok{1.2}\NormalTok{,}
     \DataTypeTok{col =} \StringTok{"red"}\NormalTok{, }\DataTypeTok{print.thres.col =} \StringTok{"red"}\NormalTok{)}

\KeywordTok{plot}\NormalTok{(rfRoc, }\DataTypeTok{type =} \StringTok{"s"}\NormalTok{, }\DataTypeTok{add =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{print.thres =} \KeywordTok{c}\NormalTok{(.}\DecValTok{5}\NormalTok{), }
     \DataTypeTok{print.thres.pch =} \DecValTok{1}\NormalTok{, }\DataTypeTok{legacy.axes =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{print.thres.pattern =} \StringTok{""}\NormalTok{, }
     \DataTypeTok{print.thres.cex =} \FloatTok{1.2}\NormalTok{,}
     \DataTypeTok{col =} \StringTok{"green"}\NormalTok{, }\DataTypeTok{print.thres.col =} \StringTok{"green"}\NormalTok{)}

\KeywordTok{plot}\NormalTok{(gbmRoc, }\DataTypeTok{type =} \StringTok{"s"}\NormalTok{, }\DataTypeTok{add =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{print.thres =} \KeywordTok{c}\NormalTok{(.}\DecValTok{5}\NormalTok{), }
     \DataTypeTok{print.thres.pch =} \DecValTok{10}\NormalTok{, }\DataTypeTok{legacy.axes =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{print.thres.pattern =} \StringTok{""}\NormalTok{, }
     \DataTypeTok{print.thres.cex =} \FloatTok{1.2}\NormalTok{,}
     \DataTypeTok{col =} \StringTok{"blue"}\NormalTok{, }\DataTypeTok{print.thres.col =} \StringTok{"blue"}\NormalTok{)}

\KeywordTok{legend}\NormalTok{(}\FloatTok{0.2}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\DataTypeTok{cex =} \FloatTok{0.8}\NormalTok{,}
       \KeywordTok{c}\NormalTok{(}\StringTok{"Single Tree"}\NormalTok{, }\StringTok{"Bagged Tree"}\NormalTok{, }\StringTok{"Random Forest"}\NormalTok{, }\StringTok{"Boosted Tree"}\NormalTok{),}
       \DataTypeTok{lwd =} \KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{),}
       \DataTypeTok{col =} \KeywordTok{c}\NormalTok{(}\StringTok{"black"}\NormalTok{, }\StringTok{"red"}\NormalTok{, }\StringTok{"green"}\NormalTok{, }\StringTok{"blue"}\NormalTok{),}
       \DataTypeTok{pch =} \KeywordTok{c}\NormalTok{(}\DecValTok{16}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{10}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\includegraphics{IDS_files/figure-latex/unnamed-chunk-153-1.pdf}

Since the data here doesn't have many variables, we don't see a significant difference among the models. But you can still see those ensemble methods are better than a single tree. In most of the real applications, ensemble methods perform much better. Random forest and boosting trees can be a baseline model. Before exploring different models, you can quickly run a random forest to see the performance and then try to improve that performance. If the performance you got from the random forest is not too much better than guessing, you should consider collecting more data or reviewing the problem to frame it a different way instead of trying other models. Because it usually means the current data is not enough to solve the problem.

\hypertarget{boosting-as-additive-model}{%
\subsection{Boosting as Additive Model}\label{boosting-as-additive-model}}

This section illustrates how boosting is a forward stagewise additive model that minimizes exponential loss \citep{Friedman2000}. Many seemingly different models can be represented as a \textbf{basis expansion model}. Recall the classifier obtained by the AdaBoost.M1 algorithm:

\[G(x)=sign ( \Sigma_{m=1}^M \alpha_{m}G_m(x))\]

The above expression fits in the framework of basis expansion which is as following:

\begin{equation}
f(x)=\Sigma_{m=1}^M \beta_m b(x,\gamma_m)
\label{eq:basisexp}
\end{equation}

where \(\beta_m,\ m=1,\dots,M\) is expansion coefficient and \(b(x,\gamma)\in \mathbb{R}\) is basis function. Many of the learning methods fit into this additive framework. Hastie et al discuss basis expansion in detail in Chapter 5 of the book \citep{Hastie2008} and cover the additive expansion for different learning techniques \citep{Hastie2008}, such as single-hidden-layer neural networks (Chapter 11) and MARS (Section 9.4). These models are fit by minimizing a designated loss function averaged over the training data:

\begin{equation}
\underset{\{\beta_m,\gamma_m\}_i^M}{min}\Sigma_{i=1}^N L\left(y_i,\Sigma_{m=1}^M\beta_{m}b(x_i;\gamma_m)\right)
\label{eq:additiveloss}
\end{equation}

Different models have different basis function \(b(x_i;\gamma_m)\). You can also customize loss function \(L(\cdot)\), such as squared-error loss, or likelihood-based loss. Optimizing the loss function across the whole training set is usually computation costly no matter the choice of basis and loss. The good news is that the problem can be simplified as fitting a single basis function.

\[\underset{\beta,\gamma}{min}=\Sigma_{i=1}^N L(y_i,\beta b(x_i;\gamma))\]

The forward stagewise algorithm approximates the optimal solution of equation \eqref{eq:additiveloss}. It adds new basis functions to the expansion without adjusting the previous ones. The forward stagewise additive algorithm is as following (section 10.2, \citep{Hastie2008}):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Initialize \(f_0(x)=0\)
\item
  \textbf{for} \(m=1,\dots,M\) \textbf{do}

  \begin{itemize}
  \tightlist
  \item
    compute
    \[(\beta_m,\gamma_m)=\underset{\beta,\gamma}{argmin}\Sigma_{i=1}^NL(y_i,f_{m-1}(x_i)+\beta b(x_i;\gamma))\]
  \item
    set \(f_m(x)=f_{m-1}(x)+\beta_m b(x;\gamma_m)\)
  \end{itemize}
\end{enumerate}

At iteration m, it will search for the optimal \(b(x;\gamma_m)\) and \(\beta_m\) based on the previous basis function \(f_{m-1}(x)\). And then add the new basis \(b(x;\gamma_m)\beta_m\) to the previous basis function to get a new basis function \(f_m(x)\) without changing any parameters from previous steps. Assume we use squared-error loss:

\[L(y,f(x))=(y-f(x))^2\]

Then we have:

\[L(y_i,f_{m-1}(x_i)+\beta b(x_i;\gamma))=(y_i-f_{m-1}(x_i)-\beta b(x_i;\gamma))^2\]

where \(y_i-f_{m-1}(x_i)\) is the residual for sample i based on the previous model. That is to say, it is fitting the new basis using the residual of the previous step.

However, the squared-error loss is generally not a good choice. For a regression problem, it is susceptible to outliers. It doesn't fit the classification problem since the response is categorical. Hence we often consider other loss functions.

Now let is go back to the AdaBoost.M1 algorithm. It is actually a special case of the above forward stagewise model when the loss function is:

\[L(y,f(x))=exp(-yf(x))\]

In AdaBoost.M1, the basis function is the classifier from each iteration \(G_m(x)\in \{-1,1\}\). If we use the exponential loss, the optimization problem is

\begin{equation}
\begin{array}{ccc}
(\beta_m,G_m) & = & \underset{\beta,G}{argmin}\Sigma_{i=1}^N exp[-y_i(f_{m-1}(x_i)+\beta G(x_i))]\\
& = & \underset{\beta, G}{argmin}\Sigma_{i=1}^N exp[-y_i \beta G(x_i)]\cdot exp[-y_if_{m-1}(x_i)]\\
& = & \underset{\beta, G}{argmin}\Sigma_{i=1}^N w_i^m exp[-y_i\beta G(x_i)]
\end{array}
\label{eq:explossadaboost1}
\end{equation}

where \(w_i^m= exp[-y_if_{m-1}(x_i)]\). It does not depend on \(\beta\) and \(G(x)\). So we can consider it as the weight for each sample. Since the weight is related to \(f_{m-1}(x_i)\), it changes each iteration. We can further decompose equation \eqref{eq:explossadaboost1}:

\begin{equation}
\begin{array}{ccc}
(\beta_{m},G_{m}) & = & \underset{\beta,G}{argmin}\Sigma_{i=1}^{N}w_{i}^{m}exp[-y_{i}\beta G(x_{i})]\\
& = & \underset{\beta,G}{argmin}\Sigma_{i=1}^{N}\left\{ w_{i}^{m}e^{-\beta}I(y_{i}=G(x))+w_{i}^{m}e^{\beta}I(y_{i}\neq G(x))\right\} \\
& = & \underset{\beta,G}{argmin}\Sigma_{i=1}^{N}\left\{ w_{i}^{m}e^{-\beta}[1-I(y_{i}\neq G(x))]+w_{i}^{m}e^{\beta}I(y_{i}\neq G(x))\right\} \\
& = & \underset{\beta,G}{argmin}\left\{ (e^{\beta}-e^{-\beta})\cdot\Sigma_{i=1}^{N}w_{i}^{m}I(y_{i}\neq G(x_{i}))+e^{-\beta}\cdot\Sigma_{i=1}^{N}w_{i}^{m}\right\}
\end{array}
\label{eq:explossadaboost2}
\end{equation}

when \(\beta >0\), the solution for equation \eqref{eq:explossadaboost2} is:

\[G_{m} = \underset{G}{argmin}\Sigma_{i=1}^{N}w_{i}^{m}I(y_{i}\neq G(x))\]

It is the classifier that minimizes the weighted error. Plug the above \(G_m\) into equation \eqref{eq:explossadaboost2}. Take the derivative with respect to \(\beta\) and set it to be 0 to solve the optimal \(\beta_m\):

\[\beta_m =\frac{1}{2}ln\frac{1-err_m}{err_m}\]

where

\[err_m = \frac{\Sigma_{i=1}^N w_i^{m}I(y_i \neq G_m(x_i))}{\Sigma_{i=1}^N w_i^{m}}\]

According to the forward stagewise algorithm, the result is updated as:

\[f_m(x)=f_{m-1}(x)+\beta_m G_m(x)\]

We can go ahead and get the weight for the next iteration:

\begin{equation}
\begin{array}{ccc}
w_i^{m+1} & = & exp[-y_if_m (x_i)]\\
& = & exp[-y_if_{m-1}(x)-y_i \beta_m G_m(x)]\\
& = & w_{i}^{m}\cdot exp[-\beta_m y_i G_m(x_i)]
\end{array}
\label{eq:explossadaboost3}
\end{equation}

Since \(-y_i G_m(x_i)=2\cdot I(y_i \neq G_m(x_i))-1\), equation \eqref{eq:explossadaboost2} can be written as:

\[w_i^{m+1}=w_i^m \cdot exp[\alpha_mI(y_i\neq G_m(x_i))] \cdot exp[-\beta_m]\]

where \(\alpha_m=2\beta_m=log\frac{1-err_m}{err_m}\) is the same with the \(\alpha_m\) in AdaBoost.M1 algorithm we showed before. So AdaBoost.M1 is a special case of a forward stagewise additive model using exponential loss. For comparing and selecting different loss functions, you can refer to section 10.5 and 10.6 in \citep{Hastie2008}.

\hypertarget{deep-learning}{%
\chapter{Deep Learning}\label{deep-learning}}

\hypertarget{deep-learning-introduction-and-history}{%
\section{Deep Learning Introduction and History}\label{deep-learning-introduction-and-history}}

With everyday applications in language, voice, image and automatic driving cars, deep learning becomes a popular concept to the general public in the past few years. However, many of the concepts of deep learning started as early as the 1940s. For example, the concept of binary classifier of perceptron used a linear combination of input signals and a step activation function. It is basically the same as the single neuron in a modern deep learning framework that uses the exact same linear combination of input signals from neurons at previously layer and a more efficient non-linear activation function. The perceptron model was further defined by minimizing the classification error and it was trained by using one data point at a time to update the model parameters during the optimization process. In the modern neural network framework, there is a loss function to be minimized based on the problem to solve and the stochastics gradience descent and its variations are the major optimization algorithms.

Even though the theoretical foundation of deep learning has gradually developed in the past few decades, real-world applications of deep learning started in the past few years primarily due to the following constraints: data, network structure, algorithm, and computation power.

\textbf{Data}

We are all familiar with all sorts of data today: structured tabulated data in database tables or CSV files, free form text, images, and other unstructured datasets. However, historical data are relatively small in size, especially for data with accurately labeled ground truth. Statisticians have been working on datasets that only have a few thousand rows and a few dozen columns for decades to solve business problems. Even with modern computers, the size of the data is usually limited to the memory of a computer. Now we know, to enable deep learning applications, the algorithm needs a much larger dataset than traditional machine learning methods. It is usually at the order of millions of samples with high-quality ground truth labels for supervised deep learning models.

The first widely used large dataset with accurate labels is the ImageNet dataset which was first created in 2010. It now contains more than 14 million images with more than 20k synsets (i.e.~meaningful categories). Every image in the dataset was human-annotated with quality control to ensure the ground truth labels are accurate. One of the direct results of ImageNet was the Large Scale Visual Recognition Challenge (ILSVRC) which evaluated different algorithms in image-related tasks. The ILSVRC competition provided a perfect stage for deep learning applications to debut to the general public. For 2010 and 2011, the best record of error from traditional image classifications methods was around 26\%. In 2012, a method based on the convolutional neural network became the state of the art with an error rate of around 16\%, a dramatic improvement from the traditional methods.

With the prevalence of consumer internet, users created much high-quality content and historical text, voice, image, and video were digitized. The quality and quantity of data for deep learning applications became available and it exceeded the threshold of the requirement for many deep learning applications such as image classification, voice recognition, and natural language understanding. Data is the fuel for deep learning engines. With more and more varieties of data created, captured, and saved, there will be more deep learning applications to create values across many business sectors.

\textbf{Network Structure}

Lacking high-quality high-volume data was not the only constraint for early deep learning years. For perceptron with just one single neural, it is just a linear classifier. Real applications are nearly always non-linear. To solve that problem, we have to grow one neural to multiple layers of neural network with multiple neurons per layer - the multiple layer perceptron (MLP) and it is called feedforward neural network. In the 1990s, the universal approximation theorem was proved and it assured us that a feedforward network with a single hidden layer containing a finite number of neurons can approximate continuous functions. Even though the one-layer neural network theoretically can solve a general non-linear problem, the reality is that we have grown the neural network to many layers of neural network. The number of layers in the network is the ``depth'' of a network. Loosely speaking, deep learning is a neural network with the many layers (i.e.~the depth is deep).

The MLP is the basic structure for the modern deep learning applications. MLP can be used for classification problems or regression problems with response variables as the output and a collection of explanatory variables as the input (i.e.~the traditionally structured datasets). Many of the problems that can be solved using classical classification methods such as random forest can be solved by MLP. However, MLP is not the best option for image and language-related tasks. For image-related tasks, pixels for a local neighbor region collectively provide useful information to solve a task. To take advantage of the 2D spatial relationship among pixels, the convolutional neural network (CNN) structure is a better choice. For language-related tasks, the sequence of the text provides additional information than just a collection of single words. The recurrent neural network (RNN) is a better structure for such sequence-related data. There are other more complicated neural network structures and it is still a fast-developing area. MLP, CNN, and RNN are just the starting point of deep learning methods.

\textbf{Algorithm}

In addition to data and neural network structure, there were a few key algorithm breakthroughs before deep learning became a household terminology. For an entry-level neural network structure, there are hundreds of thousands of parameters to be estimated from the data. With a large amount of training data, stochastic gradience decent and mini-batch gradience decent are efficient ways to utilize a subset of training data to update the model parameters. Within the process, one of the key steps is back-propagation which was introduced in the 1980s for efficient weight update. There is a non-linear activation for each neuron in deep learning models, and \textbf{sigmoid} or \textbf{hyperbolic tangent} functions were often used. However, it has the problem of gradient vanishing when the number of layers of the network grows large (i.e.~deeper network). To solve this problem, the \textbf{rectified linear unit (ReLu)} was introduced to deep learning in the 2000s and it increases the convergence speed dramatically. ReLu is so simple (i.e.~y = x when x \textgreater{}= 0 and y = 0 otherwise), but it indeed cleverly solved one of the big headaches in deep learning. With hundreds of thousands of parameters in the model, deep learning is easy to get overfitted. The dropout concept was introduced in 2012 to mitigate overfitting. It randomly drops out a certain percentage of neurons in the network during the optimization process to achieve more robust model performance. It is similar to the concept of random forest where features and training data are randomly chosen. There are many other algorithm improvements to get better models such as batch normalization and using residuals from previous layers. With backpropagation in stochastic gradience decent, ReLu activation function, dropout, and other technics, modern deep learning methods begin to outperform traditional machine learning methods.

\textbf{Computation Power}

With data, network structure and algorithms ready, it still requires certain computation power to train a deep learning model. The entire framework involves heavy linear algebra operations with large matrixes and tensors. The general CPU architecture is not the ideal platform and GPU is a much faster choice. With the vast potential application of deep learning, major tech companies contribute heavily to open-source deep learning frameworks. For example, Google has open-sourced its TensorFlow framework; Facebook has open-sourced its PyTorch framework, and Amazon has significantly contributed to the MXNet open-source framework. With thousands of well-trained software developers and scientists behind these deep learning frameworks, users can confidently pick one framework and start training their deep learning models right away in popular cloud environments. Much of the heavy lifting to train a deep learning model has been embedded in these open-source frameworks and there are also many pre-trained models available for users to adopt. Users can now enjoy the relatively easy access to software and hardware to develop their own deep learning applications. In this book, we will demo deep learning examples using Keras, a high-level abstraction of TensorFlow, using the Databricks Community Edition platform.

In summary, deep learning applications are not developed just in the past few years and it has been ongoing research in the past few decades. The accumulation of data, the advance of algorithm and the improvement of computation power finally enable every day deep learning applications. In the foreseeable future, deep learning will continue to revolutionize machine learning methods across more areas to provide significant improvement.

\hypertarget{projection-pursuit-regression}{%
\section{Projection Pursuit Regression}\label{projection-pursuit-regression}}

Before moving onto neural networks, let us start with a broader framework, Projection Pursuit Regression (PPR). It has a form of \textbf{additive model} of the derived features rather than the inputs themselves. Another widely used algorithm, AdaBoost, also fits an additive model in a base learner.

Assume \(\mathbf{X^{T}}=(X_1,X_2,\dots,X_p)\) is a vector with \(p\) variables. \(Y\) is the corresponding response variable. \(\mathbf{\omega_{m}},m=1,2,\dots,M\) is parameter vector with \(p\) elements.

\[f(\mathbf{X})=\sum_{m=1}^{M}g_{m}(\mathbf{\omega_{m}^{T}X})\]

The new feature \(\mathbf{V_{m}}=\mathbf{\omega_{m}^{T}X}\) is a linear combination of input variables \(\mathbf{X}\). The additive model is based on the new features. Here \(\mathbf{\omega_{m}}\) is a unit vector, and the new feature \(\mathbf{v_m}\) is actually the projection of \(\mathbf{X}\) on \(\mathbf{\omega_{m}}\). It projects the p-dimensional independent variable space onto the new M-dimensional feature space. This is similar to the principal component analysis except that the principal component is orthogonal projection but it is not necessarily orthogonal here.

I know it is very abstract. Let's look at some examples. Assume \(p=2\), i.e.~there are two variables \(x_1\) and \(x_2\). If \(M=1\), \(\mathbf{\omega^{T}}=(\frac{1}{2},\frac{\sqrt{3}}{2})\), then the corresponding \(v=\frac{1}{2}x_{1}+\frac{\sqrt{3}}{2}x_{2}\). Let's try different setings and compare the results:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \(\mathbf{\omega^{T}}=(\frac{1}{2},\frac{\sqrt{3}}{2})\), \(v=\frac{1}{2}x_{1}+\frac{\sqrt{3}}{2}x_{2}\) , \(g(v)=\frac{1}{1+e^{-v}}\)
\item
  \(\mathbf{\omega^{T}}=(1,0)\), \(v = x_1\), \(g(v)=(v+5)sin(\frac{1}{\frac{v}{3}+0.1})\)
\item
  \(\mathbf{\omega^{T}}=(0,1)\), \(v = x_2\), \(g(v)=e^{\frac{v^2}{5}}\)
\item
  \(\mathbf{\omega^{T}}=(1,0)\), \(v = x_1\), \(g(v)=(v+0.1)sin(\frac{1}{\frac{v}{3}+0.1})\)
\end{enumerate}

Here is how you can simulate the data and plot it using R:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# use plot3D package to generate 3D plot}
\KeywordTok{library}\NormalTok{(plot3D)}
\CommentTok{# get x1 and x2 note here x1 and x2 need to be matrix if}
\CommentTok{# you check the two objects, you will find: columns in}
\CommentTok{# x1 are identical rows in x2 are identical mesh() is}
\CommentTok{# funciton from plot3D package you may need to think a}
\CommentTok{# little here}
\NormalTok{M <-}\StringTok{ }\KeywordTok{mesh}\NormalTok{(}\KeywordTok{seq}\NormalTok{(}\OperatorTok{-}\FloatTok{13.2}\NormalTok{, }\FloatTok{13.2}\NormalTok{, }\DataTypeTok{length.out =} \DecValTok{50}\NormalTok{), }\KeywordTok{seq}\NormalTok{(}\OperatorTok{-}\FloatTok{37.4}\NormalTok{, }
    \FloatTok{37.4}\NormalTok{, }\DataTypeTok{length.out =} \DecValTok{50}\NormalTok{))}
\NormalTok{x1 <-}\StringTok{ }\NormalTok{M}\OperatorTok{$}\NormalTok{x}
\NormalTok{x2 <-}\StringTok{ }\NormalTok{M}\OperatorTok{$}\NormalTok{y}
\CommentTok{## setting 1 map X using w to get v}
\NormalTok{v <-}\StringTok{ }\NormalTok{(}\DecValTok{1}\OperatorTok{/}\DecValTok{2}\NormalTok{) }\OperatorTok{*}\StringTok{ }\NormalTok{x1 }\OperatorTok{+}\StringTok{ }\NormalTok{(}\KeywordTok{sqrt}\NormalTok{(}\DecValTok{3}\NormalTok{)}\OperatorTok{/}\DecValTok{2}\NormalTok{) }\OperatorTok{*}\StringTok{ }\NormalTok{x2}
\CommentTok{# apply g() on v}
\NormalTok{g1 <-}\StringTok{ }\DecValTok{1}\OperatorTok{/}\NormalTok{(}\DecValTok{1} \OperatorTok{+}\StringTok{ }\KeywordTok{exp}\NormalTok{(}\OperatorTok{-}\NormalTok{v))}
\KeywordTok{par}\NormalTok{(}\DataTypeTok{mfrow =} \KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{), }\DataTypeTok{mar =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{))}
\KeywordTok{surf3D}\NormalTok{(x1, x2, g1, }\DataTypeTok{colvar =}\NormalTok{ g1, }\DataTypeTok{border =} \StringTok{"black"}\NormalTok{, }\DataTypeTok{colkey =} \OtherTok{FALSE}\NormalTok{, }
    \DataTypeTok{box =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{main =} \StringTok{"Setting 1"}\NormalTok{)}
\CommentTok{## setting 2}
\NormalTok{v <-}\StringTok{ }\NormalTok{x1}
\NormalTok{g2 <-}\StringTok{ }\NormalTok{(v }\OperatorTok{+}\StringTok{ }\DecValTok{5}\NormalTok{) }\OperatorTok{*}\StringTok{ }\KeywordTok{sin}\NormalTok{(}\DecValTok{1}\OperatorTok{/}\NormalTok{(v}\OperatorTok{/}\DecValTok{3} \OperatorTok{+}\StringTok{ }\FloatTok{0.1}\NormalTok{))}
\KeywordTok{surf3D}\NormalTok{(x1, x2, g2, }\DataTypeTok{colvar =}\NormalTok{ g2, }\DataTypeTok{border =} \StringTok{"black"}\NormalTok{, }\DataTypeTok{colkey =} \OtherTok{FALSE}\NormalTok{, }
    \DataTypeTok{box =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{main =} \StringTok{"Setting 2"}\NormalTok{)}
\CommentTok{## setting 3}
\NormalTok{v <-}\StringTok{ }\NormalTok{x2}
\NormalTok{g3 <-}\StringTok{ }\KeywordTok{exp}\NormalTok{(v}\OperatorTok{^}\DecValTok{2}\OperatorTok{/}\DecValTok{5}\NormalTok{)}
\KeywordTok{surf3D}\NormalTok{(x1, x2, g3, }\DataTypeTok{colvar =}\NormalTok{ g3, }\DataTypeTok{border =} \StringTok{"black"}\NormalTok{, }\DataTypeTok{colkey =} \OtherTok{FALSE}\NormalTok{, }
    \DataTypeTok{box =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{main =} \StringTok{"Setting 3"}\NormalTok{)}
\CommentTok{## setting 4}
\NormalTok{v <-}\StringTok{ }\NormalTok{x1}
\NormalTok{g4 <-}\StringTok{ }\NormalTok{(v }\OperatorTok{+}\StringTok{ }\FloatTok{0.1}\NormalTok{) }\OperatorTok{*}\StringTok{ }\KeywordTok{sin}\NormalTok{(}\DecValTok{1}\OperatorTok{/}\NormalTok{(v}\OperatorTok{/}\DecValTok{3} \OperatorTok{+}\StringTok{ }\FloatTok{0.1}\NormalTok{))}
\KeywordTok{surf3D}\NormalTok{(x1, x2, g4, }\DataTypeTok{colvar =}\NormalTok{ g4, }\DataTypeTok{border =} \StringTok{"black"}\NormalTok{, }\DataTypeTok{colkey =} \OtherTok{FALSE}\NormalTok{, }
    \DataTypeTok{box =} \OtherTok{FALSE}\NormalTok{, }\DataTypeTok{main =} \StringTok{"Setting 4"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{IDS_files/figure-latex/nnet_simulate_data-1.pdf}

You can see that this framework is very flexible. In essence, it is to do a non-linear transformation of the linear combination. You can use this way to capture varies of relationships. For example,\(x_{1}x_{2}\) can be written as \(\frac{(x_{1}+x_{2})^{2}-(x_{1}-x_{2})^{2}}{4}\), where \(M=2\). All the higher order factors of \(x_1\) and \(x_2\) can be represented similarly. If \(M\) is large enough, this framework can approximate any continuous function on \(\mathbb{R}^{p}\). So the model family covers a board area, but with a price. That is the interpretability. Because the number of parameters increases with M and the mode is nested.

PPR in 1981 was a new idea then which lead to the debut of the neural network model. The basic technical idea behind deep learning has been around for decades. However, why did they take off in recent years? Here are some main drivers behind the rise.

First, thanks to the digitalization where lots of human activities are now in the digital realm and captured as data. So in the last 10 year, for many problems, we went from having a relatively small amount of data to accumulating a large amount of data. The traditional learning algorithms, like Logistic Regression, Support Vector Machine, Random Forest cannot effectively take advantage of such big data. Second, the increasing computation power enables us to train a large neural network either on a CPU or GPU using big data. The scale of data and computation ability lead to much progress, but tremendous algorithmic innovation is also an important driver. Many of the innovations are about speeding up the optimization of neural network. One of the examples is to use ReLU as intermediate layer activation function instead of the previous sigmoid function. The change has made the optimization process much faster because the previous sigmoid function suffers from vanishing gradient. We will talk more about that in the following sections. Here we just want to show an example of the impact of algorithmic innovation.

\hypertarget{feedforward-neural-network}{%
\section{Feedforward Neural Network}\label{feedforward-neural-network}}

\hypertarget{logistic_reg_as_neural_network}{%
\subsection{Logistic Regression as Neural Network}\label{logistic_reg_as_neural_network}}

Let's look at logistic regression from the lens of neural network. For a binary classification problem, for example spam classifier, given \(m\) samples \(\{(x^{(1)}, y^{(1)}),(x^{(2)}, y^{(2)}),...,(x^{(m)}, y^{(m)})\}\), we need to use the input feature \(x^{(i)}\) (they may be the frequency of various words such as ``money'', special characters like dollar signs, and the use of capital letters in the message etc.) to predict the output \(y^{(i)}\) (if it is a spam email). Assume that for each sample \(i\), there are \(n_{x}\) input features. Then we have:

\begin{equation}
X=\left[\begin{array}{cccc}
x_{1}^{(1)} & x_{1}^{(2)} & \dotsb & x_{1}^{(m)}\\
x_{2}^{(1)} & x_{2}^{(2)} & \dotsb & x_{2}^{(m)}\\
\vdots & \vdots & \vdots & \vdots\\
x_{n_{x}}^{(1)} & x_{n_{x}}^{(2)} & \dots & x_{n_{x}}^{(m)}
\end{array}\right]\in\mathbb{R}^{n_{x}\times m}
\label{eq:input}
\end{equation}

\[y=[y^{(1)},y^{(2)},\dots,y^{(m)}] \in \mathbb{R}^{1 \times m}\]

To predict if sample \(i\) is a spam email, we first get the inactivated \textbf{neuro} \(z^{(i)}\) by a linear transformation of the input \(x^{(i)}\), which is \(z^{(i)}=w^Tx^{(i)} + b\). Then we apply a function to ``activate'' the neuro \(z^{(i)}\) and we call it ``activation function''. In logistic regression, the activation function is sigmoid function and the ``activated'' \(z^{(i)}\) is the prediction:

\[\hat{y}^{(i)} = \sigma(w^Tx^{(i)} + b)\]

where \(\sigma(z) = \frac{1}{1+e^{-z}}\). The following figure summarizes the process:

\includegraphics[width=0.3\textwidth,height=\textheight]{images/dnn0.png}

There are two types of layers. The last layer connects directly to the output. All the rest are \emph{intermediate layers}. Depending on your definition, we call it ``0-layer neural network'' where the layer count only considers \emph{intermediate layers}. To train the model, you need a cost function which is defined as equation \eqref{eq:costlogistic}.

\begin{equation}
J(w,b)=\frac{1}{m} \Sigma_{i=1}^m L(\hat{y}^{(i)}, y^{(i)})
\label{eq:costlogistic}
\end{equation}

where

\[L(\hat{y}^{(i)}, y^{(i)}) =  -y^{(i)}log(\hat{y}^{(i)})-(1-y^{(i)})log(1-\hat{y}^{(i)})\]

To fit the model is to minimize the cost function.

\hypertarget{gradient-descent}{%
\subsection{Gradient Descent}\label{gradient-descent}}

The general approach to minimize \(J(w,b)\) is by gradient descent, also known as \emph{back-propagation}. In logistic regression, it is easy to calculate the gradient w.r.t the parameters \((w, b)\) using the chain rule for differentiation. The optimization process is a forward and backward sweep over the network. Let's look at the gradient descent for logistic regression across m sample. The non-vectorized process is as follows.

\includegraphics[width=1\textwidth,height=\textheight]{images/GradientDescent.png}

First initialize \(w_1\), \(w_2\), \ldots{} , \(w_{n_x}\), and \(b\). Then plug in the initialized value to the forward and backward propagation. The forward propagation takes the current weights and calculates the prediction \(\hat{h}^{(i)}\) and cost \(J^{(i)}\). The backward propagation calculates the gradient descent for the parameters. After iterating through all \(m\) samples, you can calculate gradient descent for the parameters. Then update the parameter by:
\[w := w - \gamma \frac{\partial J}{\partial w}\]
\[b := b - \gamma \frac{\partial J}{\partial b}\]

Repeat the progapation process using the updated parameter until the cost \(J\) stabilizes.

\hypertarget{deep-neural-network}{%
\subsection{Deep Neural Network}\label{deep-neural-network}}

Before people coined the term \emph{deep learning}, a neural network refers to \emph{single hidden layer network}. Neural networks with more than one layers are called \emph{deep learning}. Network with the structure in figure \ref{fig:ffnn} is the \textbf{multiple layer perceptron (MLP)} or \textbf{feedforward neural network (FFNN)}.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{images/dnn_str} 

}

\caption{Feedforward Neural Network}\label{fig:ffnn}
\end{figure}

Let's look at a simple one-hidden-layer neural network (figure \ref{fig:onelayernn}). First only consider one sample. From left to right, there is an input layer with 3 features (\(x_1, x_2, x_3\)), a hidden layer with four neurons and an output later to produce a prediction \(\hat{y}\).

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{images/onelayerNN} 

}

\caption{1-layer Neural Network}\label{fig:onelayernn}
\end{figure}

\textbf{From input to the first hidden layer}

Each inactivated neuron on the first layer is a linear transformation of the input vector \(x\). For example, \(z^{[1]}_1 = w^{[1]T}_1x^{(i)} + b_1^{[1]}\) is the first inactivated neuron for hidden layer one. \textbf{We use superscript \texttt{{[}l{]}} to denote a quantity associated with the \(l^{th}\) layer and the subscript \texttt{i} to denote the \(i^{th}\) entry of a vector (a neuron or feature).} Here \(w^{[1]}\) and \(b_1^{[1]}\) are the weight and bias parameters for layer 1. \(w^{[1]}\) is a \(4 \times 1\) vector and hence \(w^{[1]T}_1x^{(i)}\) us a linear combination of the four input features. Then use a sigmoid function \(\sigma(\cdot)\) to activate the neuron \(z^{[1]}_1\) to get \(a^{[1]}_1\).

\textbf{From the first hidden layer to the output}

Next, do a linear combination of the activated neurons from the first layer to get inactivated output, \(z^{[2]}_1\). And then activate the neuron to get the predicted output \(\hat{y}\). The parameters to estimate in this step are \(w^{[2]}\) and \(b_1^{[2]}\).

If you fully write out the process, it is the bottom right of figure \ref{fig:onelayernn}. When you implement a neural network, you need to do similar calculation four times to get the activated neurons in the first hidden layer. Doing this with a \texttt{for} loop is inefficient. So people vectorize the four equations. Take an input and compute the corresponding \(z\) and \(a\) as a vector. You can vectorize each step and get the following representation:

\[\begin{array}{cc}
z^{[1]}=W^{[1]}x+b^{[1]} & \ \ \sigma^{[1]}(z^{[1]})=a^{[1]}\\
z^{[2]}=W^{[2]}a^{[1]}+b^{[2]} & \ \ \ \ \ \sigma^{[2]}(z^{[2]})=a^{[2]}=\hat{y}
\end{array}\]

\(b^{[1]}\) is the column vector of the four bias parameters shown above. \(z^{[1]}\) is a column vector of the four non-active neurons. When you apply an activation function to a matrix or vector, you apply it element-wise. \(W^{[1]}\) is the matrix by stacking the four row-vectors:

\[W^{[1]}=\left[\begin{array}{c}
w_{1}^{[1]T}\\
w_{2}^{[1]T}\\
w_{3}^{[1]T}\\
w_{4}^{[1]T}
\end{array}\right]\]

So if you have one sample, you can go through the above forward propagation process to calculate the output \(\hat{y}\) for that sample. If you have \(m\) training samples, you need to repeat this process each of the \(m\) samples. \textbf{We use superscript \texttt{(i)} to denote a quantity associated with \(i^{th}\) sample.} You need to do the same calculation for all \(m\) samples.

For i = 1 to m, do:

\[\begin{array}{cc}
z^{[1](i)}=W^{[1]}x^{(i)}+b^{[1]} & \ \ \sigma^{[1]}(z^{[1](i)})=a^{[1](i)}\\
z^{[2](i)}=W^{[2]}a^{[1](i)}+b^{[2]} & \ \ \ \ \ \sigma^{[2]}(z^{[2](i)})=a^{[2](i)}=\hat{y}^{(i)}
\end{array}\]

Recall that we defined the matrix X to be equal to our training samples stacked up as column vectors in equation \eqref{eq:input}. We do a similar thing here to stack vectors with the superscript (i) together across \(m\) samples. This way, the neural network computes the outputs on all the samples on at the same time:

\[\begin{array}{cc}
Z^{[1]}=W^{[1]}X+b^{[1]} & \ \ \sigma^{[1]}(Z^{[1]})=A^{[1]}\\
Z^{[2]}=W^{[2]}A^{[1]}+b^{[2]} & \ \ \ \ \ \sigma^{[2]}(Z^{[2]})=A^{[2]}=\hat{Y}
\end{array}\]

where
\[X=\left[\begin{array}{cccc}
| & | &  & |\\
x^{(1)} & x^{(1)} & \cdots & x^{(m)}\\
| & | &  & |
\end{array}\right],\]

\[A^{[l]}=\left[\begin{array}{cccc}
| & | &  & |\\
a^{[l](1)} & a^{[l](1)} & \cdots & a^{[l](m)}\\
| & | &  & |
\end{array}\right]_{l=1\ or\ 2},\]

\[Z^{[l]}=\left[\begin{array}{cccc}
| & | &  & |\\
z^{[l](1)} & z^{[l](1)} & \cdots & z^{[l](m)}\\
| & | &  & |
\end{array}\right]_{l=1\ or\ 2}\]

You can add layers like this to get a deeper neural network as shown in the bottom right of figure \ref{fig:ffnn}.

When build a neural network with many layers, one of the choices you get to make is the activation function to use in the hidden layers and the output layer. So far, we only see sigmoid activation function. But there are other choices. Intermediate layers usually use different activation function than the output layer. Let's look at some of the common options in the next section.

\hypertarget{activation-function}{%
\subsection{Activation Function}\label{activation-function}}

\begin{itemize}
\tightlist
\item
  Sigmoid and Softmax Function
\end{itemize}

We have used the sigmoid (or logistic) activation function. The function is S-shape with an output value between 0 to 1. Therefore it is used as the output layer activation function to predict the probability \textbf{when the response \(y\) is binary}. However, it is rarely used as an intermediate layer activation function. One of the main reasons is that when \(z\) is away from 0, then the derivative of the function drops fast which slows down the optimization process through gradient descent. Even the fact that it is differentiable provides some convenience, the decreasing slope can cause a neural network to get stuck at the training time.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{IDS_files/figure-latex/activationsigmoid-1} 

}

\caption{Sigmoid Function}\label{fig:activationsigmoid}
\end{figure}

When the output has more than 2 categories, people use softmax function as the output layer activation function.

\[f_i(\mathbf{z}) = \frac{e^{z_i}}{\Sigma_{j=1}^{J} e^{z_j} } \]
where \(\mathbf{z}\) is a vector.

\begin{itemize}
\tightlist
\item
  Hyperbolic Tangent Function (tanh)
\end{itemize}

Another activation function with a similar S-shape is the hyperbolic tangent function. It works better than the sigmoid function as the intermediate layer \footnote{``The tanh function is almost always strictly superior.'' ---- by Andrew Ng from his coursera course ``Neural Networks and Deep Learning''}.

\begin{equation}
tanh(z) = \frac{e^{z} - e^{-z}}{e^{z} + e^{-z}}
\label{eq:tanh}
\end{equation}

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{IDS_files/figure-latex/activationtanh-1} 

}

\caption{Hyperbolic Tangent Function}\label{fig:activationtanh}
\end{figure}

The tanh function crosses point (0, 0) and the value of the function is between 1 and -1 which makes the mean of the activated neurons closer to 0. The sigmoid function doesn't have that property. When you preprocess the training input data, you sometimes center the data so that the mean is 0. The tanh function is kind of doing that data processing for you which makes learning for the next layer a little easier. This activation function is used a lot in the recurrent neural networks where you want to polarize the results.

\begin{itemize}
\tightlist
\item
  Rectified Linear Unit (ReLU) Function
\end{itemize}

The most popular activation function is the Rectified Linear Unit (ReLU) function. It is a piecewise function, or a half rectified function:

\[R(z) = max(0, z)\]

The derivative is 1 when z is positive and 0 when z is negative. You can define the derivative as either 0 or 1 when z is 0. When you implement this, it is unlikely that z equals to exactly 0 even it can be very close to 0.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{IDS_files/figure-latex/activationrelu-1} 

}

\caption{Rectified Linear Unit Function}\label{fig:activationrelu}
\end{figure}

The advantage of the ReLU is that when z is positive, the derivative doesn't vanish as z getting larger. So it leads to faster computation than sigmoid or tanh. It is non-linear with an unconstrained response. However, the disadvantage is that when z is negative, the derivative is 0. It may not map the negative values appropriately. In practice, this doesn't cause too much trouble but there is another version of ReLu called leaky ReLu that attempts to solve the dying ReLU problem. The leaky ReLu is

\[R(z)_{Leaky}=\begin{cases}
\begin{array}{c}
z\\
az
\end{array} & \begin{array}{c}
z\geq0\\
z<0
\end{array}\end{cases}\]

Instead of being 0 when z is negative, it adds a slight slope such as \(a=0.01\) as shown in figure \ref{fig:activationleakyrelu} (can you see the leaky part there? : ).

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{IDS_files/figure-latex/activationleakyrelu-1} 

}

\caption{Rectified Linear Unit Function}\label{fig:activationleakyrelu}
\end{figure}

You may notice that all activation functions are non-linear. Since the composition of two linear functions is still linear, using a linear activation function doesn't help to capture more information. That is why you don't see people use a linear activation function in the intermediate layer. One exception is when the output \(y\) is continuous, you may use linear activation function at the output layer. To sum up, for intermediate layers:

\begin{itemize}
\tightlist
\item
  ReLU is usually a good choice. If you don't know what to choose, then start with ReLU. Leaky ReLu usually works better than the ReLU but it is not used as much in practice. Either one works fine. Also, people usually use a=0.01 as the slope for leaky ReLU. You can try different parameters but most of the people a = 0.01.
\item
  tanh is used sometimes especially in recurrent neural network. But you nearly never see people use sigmoid function as intermediate layer activation function.
\end{itemize}

For the output layer:

\begin{itemize}
\tightlist
\item
  When it is binary classification, use sigmoid with binary cross-entropy as loss function
\item
  When there are multiple classes, use softmax function with categorical cross-entropy as loss function
\item
  When the response is continuous, use identity function (i.e.~y = x)
\end{itemize}

\hypertarget{deal-with-overfitting}{%
\subsection{Deal with Overfitting}\label{deal-with-overfitting}}

The biggest problem for deep learning is overfitting.

\hypertarget{regularization}{%
\subsubsection{Regularization}\label{regularization}}

For logistic regression,

\[\underset{w,b}{min}J(w,b)= \frac{1}{m} \Sigma_{i=1}^{m}L(\hat{y}^{(i)}, y^{(i)}) + penalty\]

Common penalties are L1 or L2 as follows:

\[L_2\ penalty=\frac{\lambda}{2m}\parallel w \parallel_2^2 = \frac{\lambda}{2m}\Sigma_{i=1}^{n_x}w_i^2\]

\[L_1\ penalty = \frac{\lambda}{m}\Sigma_{i=1}^{n_x}|w|\]

For neural network,

\[J(w^{[1]},b^{[1]},\dots,w^{[L]},b^{[L]})=\frac{1}{m}\Sigma_{i=1}^{m}L(\hat{y}^{(i)},y^{(i)}) + \frac{\lambda}{2m}\Sigma_{l=1}^{L} \parallel w^{[l]} \parallel^2_F\]

where

\[\parallel w^{[l]} \parallel^2_F = \Sigma_{i=1}^{l}\Sigma_{j=1}^{l-1} (w^{[l]}_{ij})^2\]

Many people call it ``Frobenius Norm'' instead of L2-norm.

\hypertarget{dropout}{%
\subsubsection{Dropout}\label{dropout}}

\hypertarget{optimization}{%
\subsection{Optimization}\label{optimization}}

\hypertarget{batch-mini-batch-stochastic-gradient-descent}{%
\subsubsection{Batch, Mini-batch, Stochastic Gradient Descent}\label{batch-mini-batch-stochastic-gradient-descent}}

\[\begin{array}{ccc} x= & [\underbrace{x^{(1)},x^{(2)},\cdots,x^{(1000)}}/ & \cdots/\cdots x^{(m)}]\\ (n_{x},m) & mini-batch\ 1 \end{array}\]

\[\begin{array}{ccc} y= & [\underbrace{y^{(1)},y^{(2)},\cdots,y^{(1000)}}/ & \cdots/\cdots y^{(m)}]\\ (1,m) & mini-batch\ 1 \end{array}\]

\begin{itemize}
\tightlist
\item
  Mini-batch size = m: batch gradient descent, too long per iteration
\item
  Mini-batch size = 1: stochastic gradient descent, lose speed from vectorization
\item
  Mini-batch size in between: mini-batch gradient descent, make progress without processing all training set, typical batch sizes are \(2^6=64\), \(2^7=128\), \(2^7=256\), \(2^8=512\)
\end{itemize}

\hypertarget{optimization-algorithms}{%
\subsubsection{Optimization Algorithms}\label{optimization-algorithms}}

In the history of deep learning, researchers proposed different optimization algorithms and showed that they worked well in a specific scenario. But the optimization algorithms didn't generalize well to a wide range of neural networks. So you will need to try different optimizers in your application. We will introduce three commonly used optimizers here.

\textbf{Exponentially Weighted Averages}

\hypertarget{image-recognition-using-ffnn}{%
\subsection{Image Recognition Using FFNN}\label{image-recognition-using-ffnn}}

In this section, we will walk through a toy example of image classification problem using \textbf{\texttt{keras}} package. We use R in the section to illustrate the process and also provide the python notebook on the book website. Please check the \href{https://keras.rstudio.com/}{\texttt{keras} R package website} for the most recent development. We are using the Databrick community edition with the following consideration:

\begin{itemize}
\tightlist
\item
  Minimum language barrier in coding for most users
\item
  Zero setup to save time using cloud environment
\item
  Help you get familiar with current trend of cloud computing in corporate setup
\end{itemize}

Refer to section \ref{CloudEnvironment} for how to set up an account, create a notebook (R or Python) and start a cluster.

\hypertarget{convolutional-neural-network}{%
\section{Convolutional Neural Network}\label{convolutional-neural-network}}

This section introduces the Convolutional Neural Network (CNN), the deep learning model that is almost universally used in computer vision applications. Computer vision has been advancing rapidly which enables many new applications such as self-driving cars and unlocking a phone using face. The application is not limited to the tech industry but some traditional industries such as agriculture and healthcare. Precision agriculture utilizes advanced hardware and computer vision algorithms to increase efficiency and reduce costs for farmers. For example, analyze images from cameras in the greenhouse to track plant growth state. Use sensory data from drones, satellites, and tractors to track soil conditions, detect herbs and pests, automate irrigation, etc. In health care, computer vision helps clinicians to diagnose disease, identify cancer sites with high accuracy \citep{gloria2019}. Even if you don't work on computer vision, you may find some of the ideas inspiring and borrow them into your area.

Some popular computer vision problems are:

\begin{itemize}
\tightlist
\item
  Image classification (or image recognition): Recognize the object in the image. Is there a cat in the image? Who is this person?
\item
  Object detection: Detect the position and boarder of a specific object. For example, if you are building a self-driving car, you need to know the positions of other cars around you.
\item
  Neural Style Transfer (NST): Given a ``content'' image and a ``style'' image, generate an image that merges the two.
\end{itemize}

In this chapter, we illustrate the fundamentals of CNN using the example of image classification.

\hypertarget{recurrent-neural-network}{%
\section{Recurrent Neural Network}\label{recurrent-neural-network}}

\hypertarget{appendix-appendix}{%
\appendix \addcontentsline{toc}{chapter}{\appendixname}}


\hypertarget{r-code-for-data-simulation}{%
\chapter{R code for data simulation}\label{r-code-for-data-simulation}}

\hypertarget{customer-data-for-clothing-company}{%
\section{Customer Data for Clothing Company}\label{customer-data-for-clothing-company}}

The simulation is not very straightforward and we will break it into three parts:
1. Define data structure: variable names, variable distribution, customer segment names, segment size
1. Variable distribution parameters: mean and variance
1. Iterate across segments and variables. Simulate data according to specific parameters assigned

By organizing code this way, it makes easy for us to change specific parts of the simulation. For example, if we want to change the distribution of one variable, we can just change the corresponding part of the code.

Here is code to define data structure:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# set a random number seed to }
\CommentTok{# make the process repeatable}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{12345}\NormalTok{)}
\CommentTok{# define the number of observations}
\NormalTok{ncust <-}\StringTok{ }\DecValTok{1000}
\CommentTok{# create a data frmae for simulated data}
\NormalTok{seg_dat <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{id =} \KeywordTok{as.factor}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\NormalTok{ncust)))}
\CommentTok{# assign the variable names}
\NormalTok{vars <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"age"}\NormalTok{, }\StringTok{"gender"}\NormalTok{, }\StringTok{"income"}\NormalTok{, }\StringTok{"house"}\NormalTok{, }\StringTok{"store_exp"}\NormalTok{, }
    \StringTok{"online_exp"}\NormalTok{, }\StringTok{"store_trans"}\NormalTok{, }\StringTok{"online_trans"}\NormalTok{)}
\CommentTok{# assign distribution for each variable}
\NormalTok{vartype <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"norm"}\NormalTok{, }\StringTok{"binom"}\NormalTok{, }\StringTok{"norm"}\NormalTok{, }\StringTok{"binom"}\NormalTok{, }\StringTok{"norm"}\NormalTok{, }\StringTok{"norm"}\NormalTok{, }
    \StringTok{"pois"}\NormalTok{, }\StringTok{"pois"}\NormalTok{)}
\CommentTok{# names of 4 segments}
\NormalTok{group_name <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Price"}\NormalTok{, }\StringTok{"Conspicuous"}\NormalTok{, }\StringTok{"Quality"}\NormalTok{, }\StringTok{"Style"}\NormalTok{)}
\CommentTok{# size of each segments}
\NormalTok{group_size <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{250}\NormalTok{, }\DecValTok{200}\NormalTok{, }\DecValTok{200}\NormalTok{, }\DecValTok{350}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The next step is to define variable distribution parameters. There are 4 segments of customers and 8 parameters. Different segments correspond to different parameters. Let's store the parameters in a 48 matrix:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# matrix for mean}
\NormalTok{mus <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{( }\KeywordTok{c}\NormalTok{(}
  \CommentTok{# Price}
  \DecValTok{60}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\DecValTok{120000}\NormalTok{,}\FloatTok{0.9}\NormalTok{, }\DecValTok{500}\NormalTok{,}\DecValTok{200}\NormalTok{,}\DecValTok{5}\NormalTok{,}\DecValTok{2}\NormalTok{,}
  \CommentTok{# Conspicuous}
  \DecValTok{40}\NormalTok{, }\FloatTok{0.7}\NormalTok{, }\DecValTok{200000}\NormalTok{,}\FloatTok{0.9}\NormalTok{, }\DecValTok{5000}\NormalTok{,}\DecValTok{5000}\NormalTok{,}\DecValTok{10}\NormalTok{,}\DecValTok{10}\NormalTok{,}
  \CommentTok{# Quality}
  \DecValTok{36}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\DecValTok{70000}\NormalTok{, }\FloatTok{0.4}\NormalTok{, }\DecValTok{300}\NormalTok{, }\DecValTok{2000}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{15}\NormalTok{,}
  \CommentTok{# Style}
  \DecValTok{25}\NormalTok{, }\FloatTok{0.2}\NormalTok{, }\DecValTok{90000}\NormalTok{, }\FloatTok{0.2}\NormalTok{, }\DecValTok{200}\NormalTok{, }\DecValTok{2000}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{20}\NormalTok{), }
  \DataTypeTok{ncol=}\KeywordTok{length}\NormalTok{(vars), }\DataTypeTok{byrow=}\OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# matrix for variance}
\NormalTok{sds<-}\StringTok{ }\KeywordTok{matrix}\NormalTok{( }\KeywordTok{c}\NormalTok{(}
  \CommentTok{# Price}
  \DecValTok{3}\NormalTok{,}\OtherTok{NA}\NormalTok{,}\DecValTok{8000}\NormalTok{,}\OtherTok{NA}\NormalTok{,}\DecValTok{100}\NormalTok{,}\DecValTok{50}\NormalTok{,}\OtherTok{NA}\NormalTok{,}\OtherTok{NA}\NormalTok{,}
  \CommentTok{# Conspicuous}
  \DecValTok{5}\NormalTok{,}\OtherTok{NA}\NormalTok{,}\DecValTok{50000}\NormalTok{,}\OtherTok{NA}\NormalTok{,}\DecValTok{1000}\NormalTok{,}\DecValTok{1500}\NormalTok{,}\OtherTok{NA}\NormalTok{,}\OtherTok{NA}\NormalTok{,}
  \CommentTok{# Quality}
  \DecValTok{7}\NormalTok{,}\OtherTok{NA}\NormalTok{,}\DecValTok{10000}\NormalTok{,}\OtherTok{NA}\NormalTok{,}\DecValTok{50}\NormalTok{,}\DecValTok{200}\NormalTok{,}\OtherTok{NA}\NormalTok{,}\OtherTok{NA}\NormalTok{,}
  \CommentTok{# Style}
  \DecValTok{2}\NormalTok{,}\OtherTok{NA}\NormalTok{,}\DecValTok{5000}\NormalTok{,}\OtherTok{NA}\NormalTok{,}\DecValTok{10}\NormalTok{,}\DecValTok{500}\NormalTok{,}\OtherTok{NA}\NormalTok{,}\OtherTok{NA}\NormalTok{), }
  \DataTypeTok{ncol=}\KeywordTok{length}\NormalTok{(vars), }\DataTypeTok{byrow=}\OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Now we are ready to simulate data using the parameters defined above:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# simulate non-survey data}
\NormalTok{sim.dat <-}\StringTok{ }\OtherTok{NULL}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{2016}\NormalTok{)}
\CommentTok{# loop on customer segment (i)}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \KeywordTok{seq_along}\NormalTok{(group_name)) \{}
    
    \CommentTok{# add this line in order to moniter the process}
    \KeywordTok{cat}\NormalTok{(i, group_name[i], }\StringTok{"}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
    
    \CommentTok{# create an empty matrix to store relevent data}
\NormalTok{    seg <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\KeywordTok{matrix}\NormalTok{(}\OtherTok{NA}\NormalTok{, }\DataTypeTok{nrow =}\NormalTok{ group_size[i], }
    \DataTypeTok{ncol =} \KeywordTok{length}\NormalTok{(vars)))}
    
    \CommentTok{# Simulate data within segment i}
    \ControlFlowTok{for}\NormalTok{ (j }\ControlFlowTok{in} \KeywordTok{seq_along}\NormalTok{(vars)) \{}
        
        \CommentTok{# loop on every variable (j)}
        \ControlFlowTok{if}\NormalTok{ (vartype[j] }\OperatorTok{==}\StringTok{ "norm"}\NormalTok{) \{}
            \CommentTok{# simulate normal distribution}
\NormalTok{            seg[, j] <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(group_size[i], }\DataTypeTok{mean =}\NormalTok{ mus[i, }
\NormalTok{                j], }\DataTypeTok{sd =}\NormalTok{ sds[i, j])}
\NormalTok{        \} }\ControlFlowTok{else} \ControlFlowTok{if}\NormalTok{ (vartype[j] }\OperatorTok{==}\StringTok{ "pois"}\NormalTok{) \{}
            \CommentTok{# simulate poisson distribution}
\NormalTok{            seg[, j] <-}\StringTok{ }\KeywordTok{rpois}\NormalTok{(group_size[i], }\DataTypeTok{lambda =}\NormalTok{ mus[i, }
\NormalTok{                j])}
\NormalTok{        \} }\ControlFlowTok{else} \ControlFlowTok{if}\NormalTok{ (vartype[j] }\OperatorTok{==}\StringTok{ "binom"}\NormalTok{) \{}
            \CommentTok{# simulate binomial distribution}
\NormalTok{            seg[, j] <-}\StringTok{ }\KeywordTok{rbinom}\NormalTok{(group_size[i], }\DataTypeTok{size =} \DecValTok{1}\NormalTok{, }
                \DataTypeTok{prob =}\NormalTok{ mus[i, j])}
\NormalTok{        \} }\ControlFlowTok{else}\NormalTok{ \{}
            \CommentTok{# if the distribution name is not one of the above, stop}
            \CommentTok{# and return a message}
            \KeywordTok{stop}\NormalTok{(}\StringTok{"Don't have type:"}\NormalTok{, vartype[j])}
\NormalTok{        \}}
\NormalTok{    \}}
\NormalTok{    sim.dat <-}\StringTok{ }\KeywordTok{rbind}\NormalTok{(sim.dat, seg)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Now let's edit the data we just simulated a little by adding tags to 0/1 binomial variables:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# assign variable names}
\KeywordTok{names}\NormalTok{(sim.dat) <-}\StringTok{ }\NormalTok{vars}
\CommentTok{# assign factor levels to segment variable}
\NormalTok{sim.dat}\OperatorTok{$}\NormalTok{segment <-}\StringTok{ }\KeywordTok{factor}\NormalTok{(}\KeywordTok{rep}\NormalTok{(group_name, }\DataTypeTok{times =}\NormalTok{ group_size))}
\CommentTok{# recode gender and house variable}
\NormalTok{sim.dat}\OperatorTok{$}\NormalTok{gender <-}\StringTok{ }\KeywordTok{factor}\NormalTok{(sim.dat}\OperatorTok{$}\NormalTok{gender, }\DataTypeTok{labels =} \KeywordTok{c}\NormalTok{(}\StringTok{"Female"}\NormalTok{, }
    \StringTok{"Male"}\NormalTok{))}
\NormalTok{sim.dat}\OperatorTok{$}\NormalTok{house <-}\StringTok{ }\KeywordTok{factor}\NormalTok{(sim.dat}\OperatorTok{$}\NormalTok{house, }\DataTypeTok{labels =} \KeywordTok{c}\NormalTok{(}\StringTok{"No"}\NormalTok{, }
    \StringTok{"Yes"}\NormalTok{))}
\CommentTok{# store_trans and online_trans are at least 1}
\NormalTok{sim.dat}\OperatorTok{$}\NormalTok{store_trans <-}\StringTok{ }\NormalTok{sim.dat}\OperatorTok{$}\NormalTok{store_trans }\OperatorTok{+}\StringTok{ }\DecValTok{1}
\NormalTok{sim.dat}\OperatorTok{$}\NormalTok{online_trans <-}\StringTok{ }\NormalTok{sim.dat}\OperatorTok{$}\NormalTok{online_trans }\OperatorTok{+}\StringTok{ }\DecValTok{1}
\CommentTok{# age is integer}
\NormalTok{sim.dat}\OperatorTok{$}\NormalTok{age <-}\StringTok{ }\KeywordTok{floor}\NormalTok{(sim.dat}\OperatorTok{$}\NormalTok{age)}
\end{Highlighting}
\end{Shaded}

In the real world, the data always includes some noise such as missing, wrong imputation. So we will add some noise to the data:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# add missing values}
\NormalTok{idxm <-}\StringTok{ }\KeywordTok{as.logical}\NormalTok{(}\KeywordTok{rbinom}\NormalTok{(ncust, }\DataTypeTok{size =} \DecValTok{1}\NormalTok{, }\DataTypeTok{prob =}\NormalTok{ sim.dat}\OperatorTok{$}\NormalTok{age}\OperatorTok{/}\DecValTok{200}\NormalTok{))}
\NormalTok{sim.dat}\OperatorTok{$}\NormalTok{income[idxm] <-}\StringTok{ }\OtherTok{NA}
\CommentTok{# add wrong imputations and outliers}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{idx <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\NormalTok{ncust, }\DecValTok{5}\NormalTok{)}
\NormalTok{sim.dat}\OperatorTok{$}\NormalTok{age[idx[}\DecValTok{1}\NormalTok{]] <-}\StringTok{ }\DecValTok{300}
\NormalTok{sim.dat}\OperatorTok{$}\NormalTok{store_exp[idx[}\DecValTok{2}\NormalTok{]] <-}\StringTok{ }\DecValTok{-500}
\NormalTok{sim.dat}\OperatorTok{$}\NormalTok{store_exp[idx[}\DecValTok{3}\OperatorTok{:}\DecValTok{5}\NormalTok{]] <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{50000}\NormalTok{, }\DecValTok{30000}\NormalTok{, }\DecValTok{30000}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

So far we have created part of the data. You can check it using \texttt{summary(sim.dat)}. Next, we will move on to simulate survey data.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# number of survey questions}
\NormalTok{nq <-}\StringTok{ }\DecValTok{10}

\CommentTok{# mean matrix for different segments }
\NormalTok{mus2 <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{( }\KeywordTok{c}\NormalTok{( }\DecValTok{5}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{4}\NormalTok{, }\CommentTok{# Price}
  \DecValTok{1}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{5}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{2}\NormalTok{, }\CommentTok{# Conspicuous}
  \DecValTok{5}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{3}\NormalTok{, }\CommentTok{# Quality}
  \DecValTok{3}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{5}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{4}\NormalTok{,}\DecValTok{2}\NormalTok{), }\CommentTok{# Style}
\DataTypeTok{ncol=}\NormalTok{nq, }\DataTypeTok{byrow=}\OtherTok{TRUE}\NormalTok{) }

\CommentTok{# assume the variance is 0.2 for all}
\NormalTok{sd2 <-}\StringTok{ }\FloatTok{0.2}
\NormalTok{sim.dat2 <-}\StringTok{ }\OtherTok{NULL}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1000}\NormalTok{)}
\CommentTok{# loop for customer segment (i)}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \KeywordTok{seq_along}\NormalTok{(group_name)) \{}
    \CommentTok{# the following line is used for checking the}
    \CommentTok{# progress cat (i, group_name[i],'\textbackslash{}n') create an}
    \CommentTok{# empty data frame to store data}
\NormalTok{    seg <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\KeywordTok{matrix}\NormalTok{(}\OtherTok{NA}\NormalTok{, }\DataTypeTok{nrow =}\NormalTok{ group_size[i], }
        \DataTypeTok{ncol =}\NormalTok{ nq))}
    \CommentTok{# simulate data within segment}
    \ControlFlowTok{for}\NormalTok{ (j }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{nq) \{}
        \CommentTok{# simulate normal distribution}
\NormalTok{        res <-}\StringTok{ }\KeywordTok{rnorm}\NormalTok{(group_size[i], }\DataTypeTok{mean =}\NormalTok{ mus2[i, }
\NormalTok{            j], }\DataTypeTok{sd =}\NormalTok{ sd2)}
        \CommentTok{# set upper and lower limit}
\NormalTok{        res[res }\OperatorTok{>}\StringTok{ }\DecValTok{5}\NormalTok{] <-}\StringTok{ }\DecValTok{5}
\NormalTok{        res[res }\OperatorTok{<}\StringTok{ }\DecValTok{1}\NormalTok{] <-}\StringTok{ }\DecValTok{1}
        \CommentTok{# convert continuous values to discrete integers}
\NormalTok{        seg[, j] <-}\StringTok{ }\KeywordTok{floor}\NormalTok{(res)}
\NormalTok{    \}}
\NormalTok{    sim.dat2 <-}\StringTok{ }\KeywordTok{rbind}\NormalTok{(sim.dat2, seg)}
\NormalTok{\}}

\KeywordTok{names}\NormalTok{(sim.dat2) <-}\StringTok{ }\KeywordTok{paste}\NormalTok{(}\StringTok{"Q"}\NormalTok{, }\DecValTok{1}\OperatorTok{:}\DecValTok{10}\NormalTok{, }\DataTypeTok{sep =} \StringTok{""}\NormalTok{)}
\NormalTok{sim.dat <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(sim.dat, sim.dat2)}
\NormalTok{sim.dat}\OperatorTok{$}\NormalTok{segment <-}\StringTok{ }\KeywordTok{factor}\NormalTok{(}\KeywordTok{rep}\NormalTok{(group_name, }\DataTypeTok{times =}\NormalTok{ group_size))}
\end{Highlighting}
\end{Shaded}

\hypertarget{customer-satisfaction-survey-data-from-airline-company-1}{%
\section{Customer Satisfaction Survey Data from Airline Company}\label{customer-satisfaction-survey-data-from-airline-company-1}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Create a matrix of factor loadings This pattern}
\CommentTok{# is called bifactor because it has a general}
\CommentTok{# factor for separate components.  For example,}
\CommentTok{# 'Ease of making reservation' has general factor}
\CommentTok{# loading 0.33, specific factor loading 0.58 The}
\CommentTok{# outcome variables are formed as combinations of}
\CommentTok{# these general and specific factors}

\NormalTok{loadings <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\KeywordTok{c}\NormalTok{ ( }
  \CommentTok{# Ticketing}
  \FloatTok{.33}\NormalTok{, }\FloatTok{.58}\NormalTok{, }\FloatTok{.00}\NormalTok{, }\FloatTok{.00}\NormalTok{,  }\CommentTok{# Ease of making reservation }
  \FloatTok{.35}\NormalTok{, }\FloatTok{.55}\NormalTok{, }\FloatTok{.00}\NormalTok{, }\FloatTok{.00}\NormalTok{,  }\CommentTok{# Availability of preferred seats}
  \FloatTok{.30}\NormalTok{, }\FloatTok{.52}\NormalTok{, }\FloatTok{.00}\NormalTok{, }\FloatTok{.00}\NormalTok{,  }\CommentTok{# Variety of flight options}
  \FloatTok{.40}\NormalTok{, }\FloatTok{.50}\NormalTok{, }\FloatTok{.00}\NormalTok{, }\FloatTok{.00}\NormalTok{,  }\CommentTok{# Ticket prices}
  \CommentTok{# Aircraft}
  \FloatTok{.50}\NormalTok{, }\FloatTok{.00}\NormalTok{, }\FloatTok{.55}\NormalTok{, }\FloatTok{.00}\NormalTok{,  }\CommentTok{# Seat comfort}
  \FloatTok{.41}\NormalTok{, }\FloatTok{.00}\NormalTok{, }\FloatTok{.51}\NormalTok{, }\FloatTok{.00}\NormalTok{,  }\CommentTok{# Roominess of seat area}
  \FloatTok{.45}\NormalTok{, }\FloatTok{.00}\NormalTok{, }\FloatTok{.57}\NormalTok{, }\FloatTok{.00}\NormalTok{,  }\CommentTok{# Availability of Overhead}
  \FloatTok{.32}\NormalTok{, }\FloatTok{.00}\NormalTok{, }\FloatTok{.54}\NormalTok{, }\FloatTok{.00}\NormalTok{,  }\CommentTok{# Cleanliness of aircraft}
  \CommentTok{# Service}
  \FloatTok{.35}\NormalTok{, }\FloatTok{.00}\NormalTok{, }\FloatTok{.00}\NormalTok{, }\FloatTok{.50}\NormalTok{,  }\CommentTok{# Courtesy of flight attendant}
  \FloatTok{.38}\NormalTok{, }\FloatTok{.00}\NormalTok{, }\FloatTok{.00}\NormalTok{, }\FloatTok{.57}\NormalTok{,  }\CommentTok{# Friendliness}
  \FloatTok{.60}\NormalTok{, }\FloatTok{.00}\NormalTok{, }\FloatTok{.00}\NormalTok{, }\FloatTok{.50}\NormalTok{,  }\CommentTok{# Helpfulness}
  \FloatTok{.52}\NormalTok{, }\FloatTok{.00}\NormalTok{, }\FloatTok{.00}\NormalTok{, }\FloatTok{.58}\NormalTok{,  }\CommentTok{# Food and drinks}
  \CommentTok{# General   }
  \FloatTok{.43}\NormalTok{, }\FloatTok{.10}\NormalTok{, }\FloatTok{.30}\NormalTok{, }\FloatTok{.30}\NormalTok{,  }\CommentTok{# Overall satisfaction}
  \FloatTok{.35}\NormalTok{, }\FloatTok{.50}\NormalTok{, }\FloatTok{.40}\NormalTok{, }\FloatTok{.20}\NormalTok{,  }\CommentTok{# Purchase again}
  \FloatTok{.25}\NormalTok{, }\FloatTok{.50}\NormalTok{, }\FloatTok{.50}\NormalTok{, }\FloatTok{.20}\NormalTok{), }\CommentTok{# Willingness to recommend}
  \DataTypeTok{nrow=}\DecValTok{15}\NormalTok{,}\DataTypeTok{ncol=}\DecValTok{4}\NormalTok{, }\DataTypeTok{byrow=}\OtherTok{TRUE}\NormalTok{)}

\CommentTok{# Matrix multiplication produces the correlation}
\CommentTok{# matrix except for the diagonal}
\NormalTok{cor_matrix <-}\StringTok{ }\NormalTok{loadings }\OperatorTok{%*%}\StringTok{ }\KeywordTok{t}\NormalTok{(loadings)}
\CommentTok{# Diagonal set to ones}
\KeywordTok{diag}\NormalTok{(cor_matrix) <-}\StringTok{ }\DecValTok{1}

\CommentTok{# use the mvtnorm package to randomly generate a}
\CommentTok{# data set with a given correlation pattern}

\KeywordTok{library}\NormalTok{(mvtnorm)}
\CommentTok{# mean vectors of the 3 airline companies}
\NormalTok{mu1 =}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{6}\NormalTok{)}
\NormalTok{mu2 =}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{)}
\NormalTok{mu3 =}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{8}\NormalTok{)}

\CommentTok{# set random seed}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123456}\NormalTok{)}
\CommentTok{# respondent ID}
\NormalTok{resp.id <-}\StringTok{ }\DecValTok{1}\OperatorTok{:}\DecValTok{1000}

\KeywordTok{library}\NormalTok{(MASS)}
\NormalTok{rating1 <-}\StringTok{ }\KeywordTok{mvrnorm}\NormalTok{(}\KeywordTok{length}\NormalTok{(resp.id), }\DataTypeTok{mu =}\NormalTok{ mu1, }\DataTypeTok{Sigma =}\NormalTok{ cor_matrix)}
\NormalTok{rating2 <-}\StringTok{ }\KeywordTok{mvrnorm}\NormalTok{(}\KeywordTok{length}\NormalTok{(resp.id), }\DataTypeTok{mu =}\NormalTok{ mu2, }\DataTypeTok{Sigma =}\NormalTok{ cor_matrix)}
\NormalTok{rating3 <-}\StringTok{ }\KeywordTok{mvrnorm}\NormalTok{(}\KeywordTok{length}\NormalTok{(resp.id), }\DataTypeTok{mu =}\NormalTok{ mu3, }\DataTypeTok{Sigma =}\NormalTok{ cor_matrix)}


\CommentTok{# truncates scale to be between 1 and 9}
\NormalTok{rating1[rating1 }\OperatorTok{>}\StringTok{ }\DecValTok{9}\NormalTok{] <-}\StringTok{ }\DecValTok{9}
\NormalTok{rating1[rating1 }\OperatorTok{<}\StringTok{ }\DecValTok{1}\NormalTok{] <-}\StringTok{ }\DecValTok{1}
\NormalTok{rating2[rating2 }\OperatorTok{>}\StringTok{ }\DecValTok{9}\NormalTok{] <-}\StringTok{ }\DecValTok{9}
\NormalTok{rating2[rating2 }\OperatorTok{<}\StringTok{ }\DecValTok{1}\NormalTok{] <-}\StringTok{ }\DecValTok{1}
\NormalTok{rating3[rating3 }\OperatorTok{>}\StringTok{ }\DecValTok{9}\NormalTok{] <-}\StringTok{ }\DecValTok{9}
\NormalTok{rating3[rating3 }\OperatorTok{<}\StringTok{ }\DecValTok{1}\NormalTok{] <-}\StringTok{ }\DecValTok{1}

\CommentTok{# Round to single digit}
\NormalTok{rating1 <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\KeywordTok{round}\NormalTok{(rating1, }\DecValTok{0}\NormalTok{))}
\NormalTok{rating2 <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\KeywordTok{round}\NormalTok{(rating2, }\DecValTok{0}\NormalTok{))}
\NormalTok{rating3 <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\KeywordTok{round}\NormalTok{(rating3, }\DecValTok{0}\NormalTok{))}
\NormalTok{rating1}\OperatorTok{$}\NormalTok{ID <-}\StringTok{ }\NormalTok{resp.id}
\NormalTok{rating2}\OperatorTok{$}\NormalTok{ID <-}\StringTok{ }\NormalTok{resp.id}
\NormalTok{rating3}\OperatorTok{$}\NormalTok{ID <-}\StringTok{ }\NormalTok{resp.id}
\NormalTok{rating1}\OperatorTok{$}\NormalTok{Airline <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\StringTok{"AirlineCo.1"}\NormalTok{, }\KeywordTok{length}\NormalTok{(resp.id))}
\NormalTok{rating2}\OperatorTok{$}\NormalTok{Airline <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\StringTok{"AirlineCo.2"}\NormalTok{, }\KeywordTok{length}\NormalTok{(resp.id))}
\NormalTok{rating3}\OperatorTok{$}\NormalTok{Airline <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\StringTok{"AirlineCo.3"}\NormalTok{, }\KeywordTok{length}\NormalTok{(resp.id))}
\NormalTok{rating <-}\StringTok{ }\KeywordTok{rbind}\NormalTok{(rating1, rating2, rating3)}

\CommentTok{# assign names to the variables in the data frame}
\KeywordTok{names}\NormalTok{(rating) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Easy_Reservation"}\NormalTok{, }\StringTok{"Preferred_Seats"}\NormalTok{, }
    \StringTok{"Flight_Options"}\NormalTok{, }\StringTok{"Ticket_Prices"}\NormalTok{, }\StringTok{"Seat_Comfort"}\NormalTok{, }
    \StringTok{"Seat_Roominess"}\NormalTok{, }\StringTok{"Overhead_Storage"}\NormalTok{, }\StringTok{"Clean_Aircraft"}\NormalTok{, }
    \StringTok{"Courtesy"}\NormalTok{, }\StringTok{"Friendliness"}\NormalTok{, }\StringTok{"Helpfulness"}\NormalTok{, }\StringTok{"Service"}\NormalTok{, }
    \StringTok{"Satisfaction"}\NormalTok{, }\StringTok{"Fly_Again"}\NormalTok{, }\StringTok{"Recommend"}\NormalTok{, }\StringTok{"ID"}\NormalTok{, }
    \StringTok{"Airline"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{swine-disease-breakout-data}{%
\section{Swine Disease Breakout Data}\label{swine-disease-breakout-data}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# sim1_da1.csv the 1st simulated data similar}
\CommentTok{# sim1_da2 and sim1_da3 sim1.csv simulated data,}
\CommentTok{# the first simulation dummy.sim1.csv dummy}
\CommentTok{# variables for the first simulated data with all}
\CommentTok{# the baseline code for simulation}

\NormalTok{nf <-}\StringTok{ }\DecValTok{800}
\ControlFlowTok{for}\NormalTok{ (j }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\DecValTok{20}\NormalTok{) \{}
    \KeywordTok{set.seed}\NormalTok{(}\DecValTok{19870} \OperatorTok{+}\StringTok{ }\NormalTok{j)}
\NormalTok{    x <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"A"}\NormalTok{, }\StringTok{"B"}\NormalTok{, }\StringTok{"C"}\NormalTok{)}
\NormalTok{    sim.da1 <-}\StringTok{ }\OtherTok{NULL}
    \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{nf) \{}
        \CommentTok{# sample(x, 120, replace=TRUE)->sam}
\NormalTok{        sim.da1 <-}\StringTok{ }\KeywordTok{rbind}\NormalTok{(sim.da1, }\KeywordTok{sample}\NormalTok{(x, }\DecValTok{120}\NormalTok{, }\DataTypeTok{replace =} \OtherTok{TRUE}\NormalTok{))}
\NormalTok{    \}}
    
\NormalTok{    sim.da1 <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(sim.da1)}
\NormalTok{    col <-}\StringTok{ }\KeywordTok{paste}\NormalTok{(}\StringTok{"Q"}\NormalTok{, }\DecValTok{1}\OperatorTok{:}\DecValTok{120}\NormalTok{, }\DataTypeTok{sep =} \StringTok{""}\NormalTok{)}
\NormalTok{    row <-}\StringTok{ }\KeywordTok{paste}\NormalTok{(}\StringTok{"Farm"}\NormalTok{, }\DecValTok{1}\OperatorTok{:}\NormalTok{nf, }\DataTypeTok{sep =} \StringTok{""}\NormalTok{)}
    \KeywordTok{colnames}\NormalTok{(sim.da1) <-}\StringTok{ }\NormalTok{col}
    \KeywordTok{rownames}\NormalTok{(sim.da1) <-}\StringTok{ }\NormalTok{row}
    
    \CommentTok{# use class.ind() function in nnet package to}
    \CommentTok{# encode dummy variables}
    \KeywordTok{library}\NormalTok{(nnet)}
\NormalTok{    dummy.sim1 <-}\StringTok{ }\OtherTok{NULL}
    \ControlFlowTok{for}\NormalTok{ (k }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\KeywordTok{ncol}\NormalTok{(sim.da1)) \{}
\NormalTok{        tmp =}\StringTok{ }\KeywordTok{class.ind}\NormalTok{(sim.da1[, k])}
        \KeywordTok{colnames}\NormalTok{(tmp) =}\StringTok{ }\KeywordTok{paste}\NormalTok{(col[k], }\KeywordTok{colnames}\NormalTok{(tmp))}
\NormalTok{        dummy.sim1 =}\StringTok{ }\KeywordTok{cbind}\NormalTok{(dummy.sim1, tmp)}
\NormalTok{    \}}
\NormalTok{    dummy.sim1 <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(dummy.sim1)}
    
    \CommentTok{# set 'C' as the baseline delete baseline dummy}
    \CommentTok{# variable}
    
\NormalTok{    base.idx <-}\StringTok{ }\DecValTok{3} \OperatorTok{*}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{120}\NormalTok{)}
\NormalTok{    dummy1 <-}\StringTok{ }\NormalTok{dummy.sim1[, }\OperatorTok{-}\NormalTok{base.idx]}
    
    \CommentTok{# simulate independent variable for different}
    \CommentTok{# values of r simulate based on one value of r each}
    \CommentTok{# time r=0.1, get the link function}
\NormalTok{    s1 <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\KeywordTok{rep}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\OperatorTok{/}\DecValTok{10}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{-1}\OperatorTok{/}\DecValTok{10}\NormalTok{), }\DecValTok{40}\NormalTok{), }\KeywordTok{rep}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\OperatorTok{/}\DecValTok{10}\NormalTok{, }
        \DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{), }\DecValTok{40}\NormalTok{), }\KeywordTok{rep}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{), }\DecValTok{40}\NormalTok{))}
\NormalTok{    link1 <-}\StringTok{ }\KeywordTok{as.matrix}\NormalTok{(dummy.sim1) }\OperatorTok{%*%}\StringTok{ }\NormalTok{s1 }\OperatorTok{-}\StringTok{ }\DecValTok{40}\OperatorTok{/}\DecValTok{3}\OperatorTok{/}\DecValTok{10}
    
    \CommentTok{# r=0.25}
    \CommentTok{# c(rep(c(1/4,0,-1/4),40),rep(c(1/4,0,0),40),rep(c(0,0,0),40))->s1}
    \CommentTok{# as.matrix(dummy.sim1)%*%s1-40/3/4->link1}
    
    \CommentTok{# r=0.5}
    \CommentTok{# c(rep(c(1/2,0,-1/2),40),rep(c(1/2,0,0),40),rep(c(0,0,0),40))->s1}
    \CommentTok{# as.matrix(dummy.sim1)%*%s1-40/3/2->link1}
    
    \CommentTok{# r=1}
    \CommentTok{# c(rep(c(1,0,-1),40),rep(c(1,0,0),40),rep(c(0,0,0),40))->s1}
    \CommentTok{# as.matrix(dummy.sim1)%*%s1-40/3->link1}
    
    \CommentTok{# r=2}
    \CommentTok{# c(rep(c(2,0,-2),40),rep(c(2,0,0),40),rep(c(0,0,0),40))->s1}
    \CommentTok{# as.matrix(dummy.sim1)%*%s1-40/3/0.5->link1}
    
    
    \CommentTok{# calculate the outbreak probability}
\NormalTok{    hp1 <-}\StringTok{ }\KeywordTok{exp}\NormalTok{(link1)}\OperatorTok{/}\NormalTok{(}\KeywordTok{exp}\NormalTok{(link1) }\OperatorTok{+}\StringTok{ }\DecValTok{1}\NormalTok{)}
    
    \CommentTok{# based on the probability hp1, simulate response}
    \CommentTok{# variable: res}
\NormalTok{    res <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\DecValTok{9}\NormalTok{, nf)}
    \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\NormalTok{nf) \{}
\NormalTok{        res[i] <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{), }\DecValTok{1}\NormalTok{, }\DataTypeTok{prob =} \KeywordTok{c}\NormalTok{(hp1[i], }
            \DecValTok{1} \OperatorTok{-}\StringTok{ }\NormalTok{hp1[i]))}
\NormalTok{    \}}
    
    \CommentTok{# da1 with response variable, without group}
    \CommentTok{# indicator da2 without response variable, with}
    \CommentTok{# group indicator da3 without response variable,}
    \CommentTok{# without group indicator}
    
\NormalTok{    dummy1}\OperatorTok{$}\NormalTok{y <-}\StringTok{ }\NormalTok{res}
\NormalTok{    da1 <-}\StringTok{ }\NormalTok{dummy1}
\NormalTok{    y <-}\StringTok{ }\NormalTok{da1}\OperatorTok{$}\NormalTok{y}
\NormalTok{    ind <-}\StringTok{ }\OtherTok{NULL}
    \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\DecValTok{120}\NormalTok{) \{}
\NormalTok{        ind <-}\StringTok{ }\KeywordTok{c}\NormalTok{(ind, }\KeywordTok{rep}\NormalTok{(i, }\DecValTok{2}\NormalTok{))}
\NormalTok{    \}}
    
\NormalTok{    da2 <-}\StringTok{ }\KeywordTok{rbind}\NormalTok{(da1[, }\DecValTok{1}\OperatorTok{:}\DecValTok{240}\NormalTok{], ind)}
\NormalTok{    da3 <-}\StringTok{ }\NormalTok{da1[, }\DecValTok{1}\OperatorTok{:}\DecValTok{240}\NormalTok{]}
    
    \CommentTok{# save simulated data}
    \KeywordTok{write.csv}\NormalTok{(da1, }\KeywordTok{paste}\NormalTok{(}\StringTok{"sim"}\NormalTok{, j, }\StringTok{"_da"}\NormalTok{, }\DecValTok{1}\NormalTok{, }\StringTok{".csv"}\NormalTok{, }
        \DataTypeTok{sep =} \StringTok{""}\NormalTok{), }\DataTypeTok{row.names =}\NormalTok{ F)}
    \KeywordTok{write.csv}\NormalTok{(da2, }\KeywordTok{paste}\NormalTok{(}\StringTok{"sim"}\NormalTok{, j, }\StringTok{"_da"}\NormalTok{, }\DecValTok{2}\NormalTok{, }\StringTok{".csv"}\NormalTok{, }
        \DataTypeTok{sep =} \StringTok{""}\NormalTok{), }\DataTypeTok{row.names =}\NormalTok{ F)}
    \KeywordTok{write.csv}\NormalTok{(da3, }\KeywordTok{paste}\NormalTok{(}\StringTok{"sim"}\NormalTok{, j, }\StringTok{"_da"}\NormalTok{, }\DecValTok{3}\NormalTok{, }\StringTok{".csv"}\NormalTok{, }
        \DataTypeTok{sep =} \StringTok{""}\NormalTok{), }\DataTypeTok{row.names =}\NormalTok{ F)}
    \KeywordTok{write.csv}\NormalTok{(sim.da1, }\KeywordTok{paste}\NormalTok{(}\StringTok{"sim"}\NormalTok{, j, }\StringTok{".csv"}\NormalTok{, }\DataTypeTok{sep =} \StringTok{""}\NormalTok{), }
        \DataTypeTok{row.names =}\NormalTok{ F)}
    \KeywordTok{write.csv}\NormalTok{(dummy.sim1, }\KeywordTok{paste}\NormalTok{(}\StringTok{"dummy.sim"}\NormalTok{, j, }\StringTok{".csv"}\NormalTok{, }
        \DataTypeTok{sep =} \StringTok{""}\NormalTok{), }\DataTypeTok{row.names =}\NormalTok{ F)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\bibliography{bibliography.bib}

\backmatter
\printindex

\end{document}
