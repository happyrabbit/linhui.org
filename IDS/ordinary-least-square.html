<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>9.1 Ordinary Least Square | Introduction to Data Science</title>
  <meta name="description" content="9.1 Ordinary Least Square | Introduction to Data Science" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="9.1 Ordinary Least Square | Introduction to Data Science" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="http://scientistcafe.com/IDS/" />
  
  <meta property="og:description" content="9.1 Ordinary Least Square | Introduction to Data Science" />
  <meta name="github-repo" content="happyrabbit/IntroDataScience" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="9.1 Ordinary Least Square | Introduction to Data Science" />
  
  <meta name="twitter:description" content="9.1 Ordinary Least Square | Introduction to Data Science" />
  

<meta name="author" content="Hui Lin and Ming Li" />


<meta name="date" content="2023-02-15" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="regression-models.html"/>
<link rel="next" href="principal-component-regression-and-partial-least-square.html"/>
<script src="libs/header-attrs/header-attrs.js"></script>
<script src="libs/jquery/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="http://scientistcafe.com">Homepage</a></li>

<li class="divider"></li>
<li><a href="index.html#preface">Preface<span></span></a>
<ul>
<li><a href="goal-of-the-book.html#goal-of-the-book">Goal of the Book<span></span></a></li>
<li><a href="what-this-book-covers.html#what-this-book-covers">What This Book Covers<span></span></a></li>
<li><a href="who-this-book-is-for.html#who-this-book-is-for">Who This Book Is For<span></span></a></li>
<li><a href="how-to-use-this-book.html#how-to-use-this-book">How to Use This Book<span></span></a>
<ul>
<li><a href="how-to-use-this-book.html#what-the-book-assumes">What the Book Assumes<span></span></a></li>
<li><a href="how-to-use-this-book.html#how-to-run-r-and-python-code">How to Run R and Python Code<span></span></a></li>
</ul></li>
<li><a href="complementary-reading.html#complementary-reading">Complementary Reading<span></span></a></li>
</ul></li>
<li><a href="about-the-authors.html#about-the-authors">About the Authors<span></span></a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction<span></span></a>
<ul>
<li class="chapter" data-level="1.1" data-path="a-brief-history-of-data-science.html"><a href="a-brief-history-of-data-science.html"><i class="fa fa-check"></i><b>1.1</b> A brief history of data science<span></span></a></li>
<li class="chapter" data-level="1.2" data-path="data-science-role-and-skill-tracks.html"><a href="data-science-role-and-skill-tracks.html"><i class="fa fa-check"></i><b>1.2</b> Data science role and skill tracks<span></span></a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="data-science-role-and-skill-tracks.html"><a href="data-science-role-and-skill-tracks.html#engineering"><i class="fa fa-check"></i><b>1.2.1</b> Engineering<span></span></a></li>
<li class="chapter" data-level="1.2.2" data-path="data-science-role-and-skill-tracks.html"><a href="data-science-role-and-skill-tracks.html#analysis"><i class="fa fa-check"></i><b>1.2.2</b> Analysis<span></span></a></li>
<li class="chapter" data-level="1.2.3" data-path="data-science-role-and-skill-tracks.html"><a href="data-science-role-and-skill-tracks.html#modelinginference"><i class="fa fa-check"></i><b>1.2.3</b> Modeling/inference<span></span></a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="what-kind-of-questions-can-data-science-solve.html"><a href="what-kind-of-questions-can-data-science-solve.html"><i class="fa fa-check"></i><b>1.3</b> What kind of questions can data science solve?<span></span></a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="what-kind-of-questions-can-data-science-solve.html"><a href="what-kind-of-questions-can-data-science-solve.html#prerequisites"><i class="fa fa-check"></i><b>1.3.1</b> Prerequisites<span></span></a></li>
<li class="chapter" data-level="1.3.2" data-path="what-kind-of-questions-can-data-science-solve.html"><a href="what-kind-of-questions-can-data-science-solve.html#problem-type"><i class="fa fa-check"></i><b>1.3.2</b> Problem type<span></span></a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="structure-of-data-science-team.html"><a href="structure-of-data-science-team.html"><i class="fa fa-check"></i><b>1.4</b> Structure of data science team<span></span></a></li>
<li class="chapter" data-level="1.5" data-path="data-science-roles.html"><a href="data-science-roles.html"><i class="fa fa-check"></i><b>1.5</b> Data science roles<span></span></a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="SoftSkillsforDataScientists.html"><a href="SoftSkillsforDataScientists.html"><i class="fa fa-check"></i><b>2</b> Soft Skills for Data Scientists<span></span></a>
<ul>
<li class="chapter" data-level="2.1" data-path="comparison-between-statistician-and-data-scientist.html"><a href="comparison-between-statistician-and-data-scientist.html"><i class="fa fa-check"></i><b>2.1</b> Comparison between Statistician and Data Scientist<span></span></a></li>
<li class="chapter" data-level="2.2" data-path="beyond-data-and-analytics.html"><a href="beyond-data-and-analytics.html"><i class="fa fa-check"></i><b>2.2</b> Beyond Data and Analytics<span></span></a></li>
<li class="chapter" data-level="2.3" data-path="three-pillars-of-knowledge.html"><a href="three-pillars-of-knowledge.html"><i class="fa fa-check"></i><b>2.3</b> Three Pillars of Knowledge<span></span></a></li>
<li class="chapter" data-level="2.4" data-path="data-science-project-cycle.html"><a href="data-science-project-cycle.html"><i class="fa fa-check"></i><b>2.4</b> Data Science Project Cycle<span></span></a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="data-science-project-cycle.html"><a href="data-science-project-cycle.html#types-of-data-science-projects"><i class="fa fa-check"></i><b>2.4.1</b> Types of Data Science Projects<span></span></a></li>
<li class="chapter" data-level="2.4.2" data-path="data-science-project-cycle.html"><a href="data-science-project-cycle.html#ProblemFormulationandProjectPlanningStage"><i class="fa fa-check"></i><b>2.4.2</b> Problem Formulation and Project Planning Stage<span></span></a></li>
<li class="chapter" data-level="2.4.3" data-path="data-science-project-cycle.html"><a href="data-science-project-cycle.html#project-modeling-stage"><i class="fa fa-check"></i><b>2.4.3</b> Project Modeling Stage<span></span></a></li>
<li class="chapter" data-level="2.4.4" data-path="data-science-project-cycle.html"><a href="data-science-project-cycle.html#ModelImplementationandPostProductionStage"><i class="fa fa-check"></i><b>2.4.4</b> Model Implementation and Post Production Stage<span></span></a></li>
<li class="chapter" data-level="2.4.5" data-path="data-science-project-cycle.html"><a href="data-science-project-cycle.html#ProjectCycleSummary"><i class="fa fa-check"></i><b>2.4.5</b> Project Cycle Summary<span></span></a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="common-mistakes-in-data-science.html"><a href="common-mistakes-in-data-science.html"><i class="fa fa-check"></i><b>2.5</b> Common Mistakes in Data Science<span></span></a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="common-mistakes-in-data-science.html"><a href="common-mistakes-in-data-science.html#problem-formulation-stage"><i class="fa fa-check"></i><b>2.5.1</b> Problem Formulation Stage<span></span></a></li>
<li class="chapter" data-level="2.5.2" data-path="common-mistakes-in-data-science.html"><a href="common-mistakes-in-data-science.html#project-planning-stage"><i class="fa fa-check"></i><b>2.5.2</b> Project Planning Stage<span></span></a></li>
<li class="chapter" data-level="2.5.3" data-path="common-mistakes-in-data-science.html"><a href="common-mistakes-in-data-science.html#project-modeling-stage-1"><i class="fa fa-check"></i><b>2.5.3</b> Project Modeling Stage<span></span></a></li>
<li class="chapter" data-level="2.5.4" data-path="common-mistakes-in-data-science.html"><a href="common-mistakes-in-data-science.html#model-implementation-and-post-production-stage"><i class="fa fa-check"></i><b>2.5.4</b> Model Implementation and Post Production Stage<span></span></a></li>
<li class="chapter" data-level="2.5.5" data-path="common-mistakes-in-data-science.html"><a href="common-mistakes-in-data-science.html#summary-of-common-mistakes"><i class="fa fa-check"></i><b>2.5.5</b> Summary of Common Mistakes<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="introduction-to-the-data.html"><a href="introduction-to-the-data.html"><i class="fa fa-check"></i><b>3</b> Introduction to The Data<span></span></a>
<ul>
<li class="chapter" data-level="3.1" data-path="customer-data-for-a-clothing-company.html"><a href="customer-data-for-a-clothing-company.html"><i class="fa fa-check"></i><b>3.1</b> Customer Data for A Clothing Company<span></span></a></li>
<li class="chapter" data-level="3.2" data-path="swinediseasedata.html"><a href="swinediseasedata.html"><i class="fa fa-check"></i><b>3.2</b> Swine Disease Breakout Data<span></span></a></li>
<li class="chapter" data-level="3.3" data-path="mnist-dataset.html"><a href="mnist-dataset.html"><i class="fa fa-check"></i><b>3.3</b> MNIST Dataset<span></span></a></li>
<li class="chapter" data-level="3.4" data-path="imdb-dataset.html"><a href="imdb-dataset.html"><i class="fa fa-check"></i><b>3.4</b> IMDB Dataset<span></span></a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="bigdatacloudplatform.html"><a href="bigdatacloudplatform.html"><i class="fa fa-check"></i><b>4</b> Big Data Cloud Platform<span></span></a>
<ul>
<li class="chapter" data-level="4.1" data-path="power-of-cluster-of-computers.html"><a href="power-of-cluster-of-computers.html"><i class="fa fa-check"></i><b>4.1</b> Power of Cluster of Computers<span></span></a></li>
<li class="chapter" data-level="4.2" data-path="evolution-of-cluster-computing.html"><a href="evolution-of-cluster-computing.html"><i class="fa fa-check"></i><b>4.2</b> Evolution of Cluster Computing<span></span></a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="evolution-of-cluster-computing.html"><a href="evolution-of-cluster-computing.html#hadoop"><i class="fa fa-check"></i><b>4.2.1</b> Hadoop<span></span></a></li>
<li class="chapter" data-level="4.2.2" data-path="evolution-of-cluster-computing.html"><a href="evolution-of-cluster-computing.html#spark"><i class="fa fa-check"></i><b>4.2.2</b> Spark<span></span></a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="CloudEnvironment.html"><a href="CloudEnvironment.html"><i class="fa fa-check"></i><b>4.3</b> Introduction of Cloud Environment<span></span></a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="CloudEnvironment.html"><a href="CloudEnvironment.html#open-account-and-create-a-cluster"><i class="fa fa-check"></i><b>4.3.1</b> Open Account and Create a Cluster<span></span></a></li>
<li class="chapter" data-level="4.3.2" data-path="CloudEnvironment.html"><a href="CloudEnvironment.html#r-notebook"><i class="fa fa-check"></i><b>4.3.2</b> R Notebook<span></span></a></li>
<li class="chapter" data-level="4.3.3" data-path="CloudEnvironment.html"><a href="CloudEnvironment.html#markdown-cells"><i class="fa fa-check"></i><b>4.3.3</b> Markdown Cells<span></span></a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="leveragesparkr.html"><a href="leveragesparkr.html"><i class="fa fa-check"></i><b>4.4</b> Leverage Spark Using R Notebook<span></span></a></li>
<li class="chapter" data-level="4.5" data-path="databases-and-sql.html"><a href="databases-and-sql.html"><i class="fa fa-check"></i><b>4.5</b> Databases and SQL<span></span></a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="databases-and-sql.html"><a href="databases-and-sql.html#history"><i class="fa fa-check"></i><b>4.5.1</b> History<span></span></a></li>
<li class="chapter" data-level="4.5.2" data-path="databases-and-sql.html"><a href="databases-and-sql.html#database-table-and-view"><i class="fa fa-check"></i><b>4.5.2</b> Database, Table and View<span></span></a></li>
<li class="chapter" data-level="4.5.3" data-path="databases-and-sql.html"><a href="databases-and-sql.html#basic-sql-statement"><i class="fa fa-check"></i><b>4.5.3</b> Basic SQL Statement<span></span></a></li>
<li class="chapter" data-level="4.5.4" data-path="databases-and-sql.html"><a href="databases-and-sql.html#advanced-topics-in-database"><i class="fa fa-check"></i><b>4.5.4</b> Advanced Topics in Database<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="datapreprocessing.html"><a href="datapreprocessing.html"><i class="fa fa-check"></i><b>5</b> Data Pre-processing<span></span></a>
<ul>
<li class="chapter" data-level="5.1" data-path="data-cleaning.html"><a href="data-cleaning.html"><i class="fa fa-check"></i><b>5.1</b> Data Cleaning<span></span></a></li>
<li class="chapter" data-level="5.2" data-path="missing-values.html"><a href="missing-values.html"><i class="fa fa-check"></i><b>5.2</b> Missing Values<span></span></a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="missing-values.html"><a href="missing-values.html#impute-missing-values-with-medianmode"><i class="fa fa-check"></i><b>5.2.1</b> Impute missing values with median/mode<span></span></a></li>
<li class="chapter" data-level="5.2.2" data-path="missing-values.html"><a href="missing-values.html#k-nearest-neighbors"><i class="fa fa-check"></i><b>5.2.2</b> K-nearest neighbors<span></span></a></li>
<li class="chapter" data-level="5.2.3" data-path="missing-values.html"><a href="missing-values.html#bagging-tree"><i class="fa fa-check"></i><b>5.2.3</b> Bagging Tree<span></span></a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="centering-and-scaling.html"><a href="centering-and-scaling.html"><i class="fa fa-check"></i><b>5.3</b> Centering and Scaling<span></span></a></li>
<li class="chapter" data-level="5.4" data-path="resolve-skewness.html"><a href="resolve-skewness.html"><i class="fa fa-check"></i><b>5.4</b> Resolve Skewness<span></span></a></li>
<li class="chapter" data-level="5.5" data-path="outliers.html"><a href="outliers.html"><i class="fa fa-check"></i><b>5.5</b> Resolve Outliers<span></span></a></li>
<li class="chapter" data-level="5.6" data-path="collinearity.html"><a href="collinearity.html"><i class="fa fa-check"></i><b>5.6</b> Collinearity<span></span></a></li>
<li class="chapter" data-level="5.7" data-path="sparse-variables.html"><a href="sparse-variables.html"><i class="fa fa-check"></i><b>5.7</b> Sparse Variables<span></span></a></li>
<li class="chapter" data-level="5.8" data-path="re-encode-dummy-variables.html"><a href="re-encode-dummy-variables.html"><i class="fa fa-check"></i><b>5.8</b> Re-encode Dummy Variables<span></span></a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="datawrangline.html"><a href="datawrangline.html"><i class="fa fa-check"></i><b>6</b> Data Wrangling<span></span></a>
<ul>
<li class="chapter" data-level="6.1" data-path="summarize-data.html"><a href="summarize-data.html"><i class="fa fa-check"></i><b>6.1</b> Summarize Data<span></span></a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="summarize-data.html"><a href="summarize-data.html#dplyr-package"><i class="fa fa-check"></i><b>6.1.1</b> <code>dplyr</code> package<span></span></a></li>
<li class="chapter" data-level="6.1.2" data-path="summarize-data.html"><a href="summarize-data.html#applyfamilyinbaser"><i class="fa fa-check"></i><b>6.1.2</b> <code>apply()</code>, <code>lapply()</code> and <code>sapply()</code> in base R<span></span></a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="tidy-and-reshape-data.html"><a href="tidy-and-reshape-data.html"><i class="fa fa-check"></i><b>6.2</b> Tidy and Reshape Data<span></span></a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="modeltuningstrategy.html"><a href="modeltuningstrategy.html"><i class="fa fa-check"></i><b>7</b> Model Tuning Strategy<span></span></a>
<ul>
<li class="chapter" data-level="7.1" data-path="vbtradeoff.html"><a href="vbtradeoff.html"><i class="fa fa-check"></i><b>7.1</b> Variance-Bias Trade-Off<span></span></a></li>
<li class="chapter" data-level="7.2" data-path="datasplittingresampling.html"><a href="datasplittingresampling.html"><i class="fa fa-check"></i><b>7.2</b> Data Splitting and Resampling<span></span></a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="datasplittingresampling.html"><a href="datasplittingresampling.html#datasplitting"><i class="fa fa-check"></i><b>7.2.1</b> Data Splitting<span></span></a></li>
<li class="chapter" data-level="7.2.2" data-path="datasplittingresampling.html"><a href="datasplittingresampling.html#resampling"><i class="fa fa-check"></i><b>7.2.2</b> Resampling<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="measuring-performance.html"><a href="measuring-performance.html"><i class="fa fa-check"></i><b>8</b> Measuring Performance<span></span></a>
<ul>
<li class="chapter" data-level="8.1" data-path="regression-model-performance.html"><a href="regression-model-performance.html"><i class="fa fa-check"></i><b>8.1</b> Regression Model Performance<span></span></a></li>
<li class="chapter" data-level="8.2" data-path="classification-model-performance.html"><a href="classification-model-performance.html"><i class="fa fa-check"></i><b>8.2</b> Classification Model Performance<span></span></a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="classification-model-performance.html"><a href="classification-model-performance.html#confusion-matrix"><i class="fa fa-check"></i><b>8.2.1</b> Confusion Matrix<span></span></a></li>
<li class="chapter" data-level="8.2.2" data-path="classification-model-performance.html"><a href="classification-model-performance.html#kappa-statistic"><i class="fa fa-check"></i><b>8.2.2</b> Kappa Statistic<span></span></a></li>
<li class="chapter" data-level="8.2.3" data-path="classification-model-performance.html"><a href="classification-model-performance.html#roc"><i class="fa fa-check"></i><b>8.2.3</b> ROC<span></span></a></li>
<li class="chapter" data-level="8.2.4" data-path="classification-model-performance.html"><a href="classification-model-performance.html#gain-and-lift-charts"><i class="fa fa-check"></i><b>8.2.4</b> Gain and Lift Charts<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="regression-models.html"><a href="regression-models.html"><i class="fa fa-check"></i><b>9</b> Regression Models<span></span></a>
<ul>
<li class="chapter" data-level="9.1" data-path="ordinary-least-square.html"><a href="ordinary-least-square.html"><i class="fa fa-check"></i><b>9.1</b> Ordinary Least Square<span></span></a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="ordinary-least-square.html"><a href="ordinary-least-square.html#the-magic-p-value"><i class="fa fa-check"></i><b>9.1.1</b> The Magic P-value<span></span></a></li>
<li class="chapter" data-level="9.1.2" data-path="ordinary-least-square.html"><a href="ordinary-least-square.html#diagnostics-for-linear-regression"><i class="fa fa-check"></i><b>9.1.2</b> Diagnostics for Linear Regression<span></span></a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="principal-component-regression-and-partial-least-square.html"><a href="principal-component-regression-and-partial-least-square.html"><i class="fa fa-check"></i><b>9.2</b> Principal Component Regression and Partial Least Square<span></span></a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="regularization-methods.html"><a href="regularization-methods.html"><i class="fa fa-check"></i><b>10</b> Regularization Methods<span></span></a>
<ul>
<li class="chapter" data-level="10.1" data-path="ridge-regression.html"><a href="ridge-regression.html"><i class="fa fa-check"></i><b>10.1</b> Ridge Regression<span></span></a></li>
<li class="chapter" data-level="10.2" data-path="lasso.html"><a href="lasso.html"><i class="fa fa-check"></i><b>10.2</b> LASSO<span></span></a></li>
<li class="chapter" data-level="10.3" data-path="elastic-net.html"><a href="elastic-net.html"><i class="fa fa-check"></i><b>10.3</b> Elastic Net<span></span></a></li>
<li class="chapter" data-level="10.4" data-path="penalized-generalized-linear-model.html"><a href="penalized-generalized-linear-model.html"><i class="fa fa-check"></i><b>10.4</b> Penalized Generalized Linear Model<span></span></a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="penalized-generalized-linear-model.html"><a href="penalized-generalized-linear-model.html#introduction-to-glmnet-package"><i class="fa fa-check"></i><b>10.4.1</b> Introduction to <code>glmnet</code> package<span></span></a></li>
<li class="chapter" data-level="10.4.2" data-path="penalized-generalized-linear-model.html"><a href="penalized-generalized-linear-model.html#penalized-logistic-regression"><i class="fa fa-check"></i><b>10.4.2</b> Penalized logistic regression<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="treemodel.html"><a href="treemodel.html"><i class="fa fa-check"></i><b>11</b> Tree-Based Methods<span></span></a>
<ul>
<li class="chapter" data-level="11.1" data-path="tree-basics.html"><a href="tree-basics.html"><i class="fa fa-check"></i><b>11.1</b> Tree Basics<span></span></a></li>
<li class="chapter" data-level="11.2" data-path="splitting-criteria.html"><a href="splitting-criteria.html"><i class="fa fa-check"></i><b>11.2</b> Splitting Criteria<span></span></a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="splitting-criteria.html"><a href="splitting-criteria.html#gini-impurity"><i class="fa fa-check"></i><b>11.2.1</b> Gini impurity<span></span></a></li>
<li class="chapter" data-level="11.2.2" data-path="splitting-criteria.html"><a href="splitting-criteria.html#information-gain-ig"><i class="fa fa-check"></i><b>11.2.2</b> Information Gain (IG)<span></span></a></li>
<li class="chapter" data-level="11.2.3" data-path="splitting-criteria.html"><a href="splitting-criteria.html#information-gain-ratio-igr"><i class="fa fa-check"></i><b>11.2.3</b> Information Gain Ratio (IGR)<span></span></a></li>
<li class="chapter" data-level="11.2.4" data-path="splitting-criteria.html"><a href="splitting-criteria.html#sum-of-squared-error-sse"><i class="fa fa-check"></i><b>11.2.4</b> Sum of Squared Error (SSE)<span></span></a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="tree-pruning.html"><a href="tree-pruning.html"><i class="fa fa-check"></i><b>11.3</b> Tree Pruning<span></span></a></li>
<li class="chapter" data-level="11.4" data-path="regression-and-decision-tree-basic.html"><a href="regression-and-decision-tree-basic.html"><i class="fa fa-check"></i><b>11.4</b> Regression and Decision Tree Basic<span></span></a>
<ul>
<li class="chapter" data-level="11.4.1" data-path="regression-and-decision-tree-basic.html"><a href="regression-and-decision-tree-basic.html#regression-tree"><i class="fa fa-check"></i><b>11.4.1</b> Regression Tree<span></span></a></li>
<li class="chapter" data-level="11.4.2" data-path="regression-and-decision-tree-basic.html"><a href="regression-and-decision-tree-basic.html#decision-tree"><i class="fa fa-check"></i><b>11.4.2</b> Decision Tree<span></span></a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="bagging-tree-1.html"><a href="bagging-tree-1.html"><i class="fa fa-check"></i><b>11.5</b> Bagging Tree<span></span></a></li>
<li class="chapter" data-level="11.6" data-path="random-forest.html"><a href="random-forest.html"><i class="fa fa-check"></i><b>11.6</b> Random Forest<span></span></a></li>
<li class="chapter" data-level="11.7" data-path="gradient-boosted-machine.html"><a href="gradient-boosted-machine.html"><i class="fa fa-check"></i><b>11.7</b> Gradient Boosted Machine<span></span></a>
<ul>
<li class="chapter" data-level="11.7.1" data-path="gradient-boosted-machine.html"><a href="gradient-boosted-machine.html#adaptive-boosting"><i class="fa fa-check"></i><b>11.7.1</b> Adaptive Boosting<span></span></a></li>
<li class="chapter" data-level="11.7.2" data-path="gradient-boosted-machine.html"><a href="gradient-boosted-machine.html#stochastic-gradient-boosting"><i class="fa fa-check"></i><b>11.7.2</b> Stochastic Gradient Boosting<span></span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="deeplearning.html"><a href="deeplearning.html"><i class="fa fa-check"></i><b>12</b> Deep Learning<span></span></a>
<ul>
<li class="chapter" data-level="12.1" data-path="feedforward-neural-network.html"><a href="feedforward-neural-network.html"><i class="fa fa-check"></i><b>12.1</b> Feedforward Neural Network<span></span></a>
<ul>
<li class="chapter" data-level="12.1.1" data-path="feedforward-neural-network.html"><a href="feedforward-neural-network.html#logisticregasneuralnetwork"><i class="fa fa-check"></i><b>12.1.1</b> Logistic Regression as Neural Network<span></span></a></li>
<li class="chapter" data-level="12.1.2" data-path="feedforward-neural-network.html"><a href="feedforward-neural-network.html#stochastic-gradient-descent"><i class="fa fa-check"></i><b>12.1.2</b> Stochastic Gradient Descent<span></span></a></li>
<li class="chapter" data-level="12.1.3" data-path="feedforward-neural-network.html"><a href="feedforward-neural-network.html#deepneuralnetwork"><i class="fa fa-check"></i><b>12.1.3</b> Deep Neural Network<span></span></a></li>
<li class="chapter" data-level="12.1.4" data-path="feedforward-neural-network.html"><a href="feedforward-neural-network.html#activationfunction"><i class="fa fa-check"></i><b>12.1.4</b> Activation Function<span></span></a></li>
<li class="chapter" data-level="12.1.5" data-path="feedforward-neural-network.html"><a href="feedforward-neural-network.html#optimization"><i class="fa fa-check"></i><b>12.1.5</b> Optimization<span></span></a></li>
<li class="chapter" data-level="12.1.6" data-path="feedforward-neural-network.html"><a href="feedforward-neural-network.html#deal-with-overfitting"><i class="fa fa-check"></i><b>12.1.6</b> Deal with Overfitting<span></span></a></li>
<li class="chapter" data-level="12.1.7" data-path="feedforward-neural-network.html"><a href="feedforward-neural-network.html#ffnnexample"><i class="fa fa-check"></i><b>12.1.7</b> Image Recognition Using FFNN<span></span></a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="convolutional-neural-network.html"><a href="convolutional-neural-network.html"><i class="fa fa-check"></i><b>12.2</b> Convolutional Neural Network<span></span></a>
<ul>
<li class="chapter" data-level="12.2.1" data-path="convolutional-neural-network.html"><a href="convolutional-neural-network.html#convolution-layer"><i class="fa fa-check"></i><b>12.2.1</b> Convolution Layer<span></span></a></li>
<li class="chapter" data-level="12.2.2" data-path="convolutional-neural-network.html"><a href="convolutional-neural-network.html#padding-layer"><i class="fa fa-check"></i><b>12.2.2</b> Padding Layer<span></span></a></li>
<li class="chapter" data-level="12.2.3" data-path="convolutional-neural-network.html"><a href="convolutional-neural-network.html#pooling-layer"><i class="fa fa-check"></i><b>12.2.3</b> Pooling Layer<span></span></a></li>
<li class="chapter" data-level="12.2.4" data-path="convolutional-neural-network.html"><a href="convolutional-neural-network.html#convolution-over-volume"><i class="fa fa-check"></i><b>12.2.4</b> Convolution Over Volume<span></span></a></li>
<li class="chapter" data-level="12.2.5" data-path="convolutional-neural-network.html"><a href="convolutional-neural-network.html#cnnexample"><i class="fa fa-check"></i><b>12.2.5</b> Image Recognition Using CNN<span></span></a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="recurrent-neural-network.html"><a href="recurrent-neural-network.html"><i class="fa fa-check"></i><b>12.3</b> Recurrent Neural Network<span></span></a>
<ul>
<li class="chapter" data-level="12.3.1" data-path="recurrent-neural-network.html"><a href="recurrent-neural-network.html#rnn-model"><i class="fa fa-check"></i><b>12.3.1</b> RNN Model<span></span></a></li>
<li class="chapter" data-level="12.3.2" data-path="recurrent-neural-network.html"><a href="recurrent-neural-network.html#lstm"><i class="fa fa-check"></i><b>12.3.2</b> Long Short Term Memory<span></span></a></li>
<li class="chapter" data-level="12.3.3" data-path="recurrent-neural-network.html"><a href="recurrent-neural-network.html#embedding"><i class="fa fa-check"></i><b>12.3.3</b> Word Embedding<span></span></a></li>
<li class="chapter" data-level="12.3.4" data-path="recurrent-neural-network.html"><a href="recurrent-neural-network.html#rnnexample"><i class="fa fa-check"></i><b>12.3.4</b> Sentiment Analysis Using RNN<span></span></a></li>
</ul></li>
</ul></li>
<li class="appendix"><span><b>Appendix<span></span></b></span></li>
<li class="chapter" data-level="A" data-path="largelocaldata.html"><a href="largelocaldata.html"><i class="fa fa-check"></i><b>A</b> Handling Large Local Data<span></span></a>
<ul>
<li class="chapter" data-level="A.1" data-path="readr.html"><a href="readr.html"><i class="fa fa-check"></i><b>A.1</b> <code>readr</code><span></span></a></li>
<li class="chapter" data-level="A.2" data-path="data.table-enhanced-data.html"><a href="data.table-enhanced-data.html"><i class="fa fa-check"></i><b>A.2</b> <code>data.table</code>— enhanced <code>data.frame</code><span></span></a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="r-code-for-data-simulation.html"><a href="r-code-for-data-simulation.html"><i class="fa fa-check"></i><b>B</b> R code for data simulation<span></span></a>
<ul>
<li class="chapter" data-level="B.1" data-path="appendixdata1.html"><a href="appendixdata1.html"><i class="fa fa-check"></i><b>B.1</b> Customer Data for Clothing Company<span></span></a></li>
<li class="chapter" data-level="B.2" data-path="appendixdata3.html"><a href="appendixdata3.html"><i class="fa fa-check"></i><b>B.2</b> Swine Disease Breakout Data<span></span></a></li>
</ul></li>
<li><a href="references.html#references">References<span></span></a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Data Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ordinary-least-square" class="section level2 hasAnchor" number="9.1">
<h2><span class="header-section-number">9.1</span> Ordinary Least Square<a href="ordinary-least-square.html#ordinary-least-square" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>For a typical linear regression with <span class="math inline">\(p\)</span> explanatory variables, we have a linear combinations of these variables:</p>
<p><span class="math display">\[f(\mathbf{X})=\mathbf{X}\symbf{\beta}=\beta_{0}+\sum_{j=1}^{p}\mathbf{x_{.j}}\beta_{j}\]</span></p>
<p>where <span class="math inline">\(\symbf{\beta}\)</span> is the parameter vector with length <span class="math inline">\(p+1\)</span>. Here we use <span class="math inline">\(\mathbf{x_{.j}}\)</span> for column vector and <span class="math inline">\(\mathbf{x_{i.}}\)</span> for row vector. Least square is the method to find a set of value for <span class="math inline">\(\symbf{\beta^{T}}=(\beta_{0},\beta_{1},...,\beta_{p})\)</span> such that it minimizes the residual sum of square (RSS):</p>
<p><span class="math display">\[RSS(\beta)=\sum_{i=1}^{N}(y_{i}-f(\mathbf{x_{i.}}))^{2}=\sum_{i=1}^{N}(y_{i}-\beta_{0}-\sum_{j=1}^{p}x_{ij}\beta_{j})^{2}\]</span></p>
<p>The process of finding a set of values has been implemented in R. Now let’s load the data:</p>
<div class="sourceCode" id="cb202"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb202-1"><a href="ordinary-least-square.html#cb202-1" aria-hidden="true" tabindex="-1"></a>dat <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;http://bit.ly/2P5gTw4&quot;</span>)</span></code></pre></div>
<p>Before fitting the model, we need to clean the data, such as removing bad data points that are not logical (negative expense).</p>
<div class="sourceCode" id="cb203"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb203-1"><a href="ordinary-least-square.html#cb203-1" aria-hidden="true" tabindex="-1"></a>dat <span class="ot">&lt;-</span> <span class="fu">subset</span>(dat, store_exp <span class="sc">&gt;</span> <span class="dv">0</span> <span class="sc">&amp;</span> online_exp <span class="sc">&gt;</span> <span class="dv">0</span>)</span></code></pre></div>
<p>Use 10 survey question variables as our explanatory variables.</p>
<div class="sourceCode" id="cb204"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb204-1"><a href="ordinary-least-square.html#cb204-1" aria-hidden="true" tabindex="-1"></a>modeldat <span class="ot">&lt;-</span> dat[, <span class="fu">grep</span>(<span class="st">&quot;Q&quot;</span>, <span class="fu">names</span>(dat))]</span></code></pre></div>
<p>The response variable is the sum of in-store spending and online spending.</p>
<div class="sourceCode" id="cb205"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb205-1"><a href="ordinary-least-square.html#cb205-1" aria-hidden="true" tabindex="-1"></a><span class="co"># total expense = in store expense + online expense</span></span>
<span id="cb205-2"><a href="ordinary-least-square.html#cb205-2" aria-hidden="true" tabindex="-1"></a>modeldat<span class="sc">$</span>total_exp <span class="ot">&lt;-</span> dat<span class="sc">$</span>store_exp <span class="sc">+</span> dat<span class="sc">$</span>online_exp</span></code></pre></div>
<p>To fit a linear regression model, let us first check if there are any missing values or outliers:</p>
<div class="sourceCode" id="cb206"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb206-1"><a href="ordinary-least-square.html#cb206-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb206-2"><a href="ordinary-least-square.html#cb206-2" aria-hidden="true" tabindex="-1"></a><span class="fu">hist</span>(modeldat<span class="sc">$</span>total_exp, <span class="at">main =</span> <span class="st">&quot;&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;total_exp&quot;</span>)</span>
<span id="cb206-3"><a href="ordinary-least-square.html#cb206-3" aria-hidden="true" tabindex="-1"></a><span class="fu">boxplot</span>(modeldat<span class="sc">$</span>total_exp)</span></code></pre></div>
<p><img src="IDS_files/figure-html/unnamed-chunk-82-1.svg" width="672" /></p>
<p>There is no missing value in the response variable, but there are outliers. Outliers are usually best described by the problem to solve itself such that we know from domain knowledge that it is not possible to have such values. We can also use a statistical threshold to remove extremely large or small outlier values from the data. We use the Z-score to find and remove outliers described in section <a href="outliers.html#outliers">5.5</a>. Readers can refer to the section for more detail.</p>
<div class="sourceCode" id="cb207"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb207-1"><a href="ordinary-least-square.html#cb207-1" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> modeldat<span class="sc">$</span>total_exp</span>
<span id="cb207-2"><a href="ordinary-least-square.html#cb207-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Find data points with Z-score larger than 3.5</span></span>
<span id="cb207-3"><a href="ordinary-least-square.html#cb207-3" aria-hidden="true" tabindex="-1"></a>zs <span class="ot">&lt;-</span> (y <span class="sc">-</span> <span class="fu">mean</span>(y))<span class="sc">/</span><span class="fu">mad</span>(y)</span>
<span id="cb207-4"><a href="ordinary-least-square.html#cb207-4" aria-hidden="true" tabindex="-1"></a>modeldat <span class="ot">&lt;-</span> modeldat[<span class="sc">-</span><span class="fu">which</span>(zs <span class="sc">&gt;</span> <span class="fl">3.5</span>), ]</span></code></pre></div>
<p>We will not perform log-transformation for the response variable at this stage. Let us first check the correlation among explanatory variables:</p>
<div class="sourceCode" id="cb208"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb208-1"><a href="ordinary-least-square.html#cb208-1" aria-hidden="true" tabindex="-1"></a>correlation <span class="ot">&lt;-</span> <span class="fu">cor</span>(modeldat[, <span class="fu">grep</span>(<span class="st">&quot;Q&quot;</span>, <span class="fu">names</span>(modeldat))])</span>
<span id="cb208-2"><a href="ordinary-least-square.html#cb208-2" aria-hidden="true" tabindex="-1"></a>corrplot<span class="sc">::</span><span class="fu">corrplot.mixed</span>(correlation, <span class="at">order =</span> <span class="st">&quot;hclust&quot;</span>, <span class="at">tl.pos =</span> <span class="st">&quot;lt&quot;</span>, </span>
<span id="cb208-3"><a href="ordinary-least-square.html#cb208-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">upper =</span> <span class="st">&quot;ellipse&quot;</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:corplotlm"></span>
<img src="IDS_files/figure-html/corplotlm-1.svg" alt="Correlation Matrix Plot for Explanatory Variables" width="80%" />
<p class="caption">
FIGURE 9.1: Correlation Matrix Plot for Explanatory Variables
</p>
</div>
<p>As shown in figure <a href="ordinary-least-square.html#fig:corplotlm">9.1</a>, there are some highly correlated variables. Let us use the method described in section <a href="collinearity.html#collinearity">5.6</a> to find potential highly correlated explanatory variables to remove with a threshold of 0.75:</p>
<div class="sourceCode" id="cb209"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb209-1"><a href="ordinary-least-square.html#cb209-1" aria-hidden="true" tabindex="-1"></a>highcor <span class="ot">&lt;-</span> <span class="fu">findCorrelation</span>(correlation, <span class="at">cutoff =</span> <span class="fl">0.75</span>)</span></code></pre></div>
<div class="sourceCode" id="cb210"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb210-1"><a href="ordinary-least-square.html#cb210-1" aria-hidden="true" tabindex="-1"></a>modeldat <span class="ot">&lt;-</span> modeldat[, <span class="sc">-</span>highcor]</span></code></pre></div>
<p>The dataset is now ready to fit a linear regression model. The standard format to define a regression in R is:</p>
<ol style="list-style-type: decimal">
<li><p>response variable is at the left side of <code>~</code></p></li>
<li><p>the explanatory variables are at the right side of <code>~</code></p></li>
<li><p>if all the variables in the dataset except the response variable are included in the model, we can use <code>.</code> at the right side of <code>~</code></p></li>
<li><p>if we want to consider the interaction between two variables such as Q1 and Q2, we can add an interaction term <code>Q1*Q2</code></p></li>
<li><p>transformation of variables can be added directly to variable names such as <code>log(total_exp)</code>.</p></li>
</ol>
<div class="sourceCode" id="cb211"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb211-1"><a href="ordinary-least-square.html#cb211-1" aria-hidden="true" tabindex="-1"></a>lmfit <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="fu">log</span>(total_exp) <span class="sc">~</span> ., <span class="at">data =</span> modeldat)</span>
<span id="cb211-2"><a href="ordinary-least-square.html#cb211-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(lmfit)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = log(total_exp) ~ ., data = modeldat)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -1.1749 -0.1372  0.0128  0.1416  0.5623 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  8.09831    0.05429  149.18  &lt; 2e-16 ***
## Q1          -0.14534    0.00882  -16.47  &lt; 2e-16 ***
## Q2           0.10228    0.01949    5.25  2.0e-07 ***
## Q3           0.25445    0.01835   13.87  &lt; 2e-16 ***
## Q6          -0.22768    0.01152  -19.76  &lt; 2e-16 ***
## Q8          -0.09071    0.01650   -5.50  5.2e-08 ***
## ---
## Signif. codes:  
## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.226 on 805 degrees of freedom
## Multiple R-squared:  0.854,  Adjusted R-squared:  0.853 
## F-statistic:  943 on 5 and 805 DF,  p-value: &lt;2e-16</code></pre>
<p>The <code>summary(lmfit)</code> presents a summary of the model fit. It shows the point estimate of each explanatory variable (the <code>Estimate</code> column), their corresponding standard error (the <code>Std. Error</code> column), t values (<code>t value</code>), and p values (<code>Pr(&gt;|t|)</code>).</p>
<div id="the-magic-p-value" class="section level3 hasAnchor" number="9.1.1">
<h3><span class="header-section-number">9.1.1</span> The Magic P-value<a href="ordinary-least-square.html#the-magic-p-value" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let us pause a little to have a short discussion about p-value. Misuse of p-value is common in many research fields. There were heated discussions about P-value in the past. Siegfried commented in his 2010 Science News article: </p>
<blockquote>
<p>“It’s science’s dirtiest secret: The scientific method of testing hypotheses by statistical analysis stands on a flimsy foundation.”</p>
</blockquote>
<p>American Statistical Association (i.e., ASA) released an official statement on p-value in 2016 <span class="citation">(<a href="#ref-ASA_P" role="doc-biblioref">Ronald L. Wassersteina 2016</a>)</span>. It was the first time to have an organization level announcement about p-value. ASA stated that the goal to release this guidance was to</p>
<blockquote>
<p>“improve the conduct and interpretation of quantitative science and inform the growing emphasis on reproducibility of science research.”</p>
</blockquote>
<p>The statement also noted that</p>
<blockquote>
<p>“the increased quantification of scientific research and a proliferation of large, complex data sets has expanded the scope for statistics and the importance of appropriately chosen techniques, properly conducted analyses, and correct interpretation.”</p>
</blockquote>
<p>The statement’s six principles, many of which address misconceptions and misuse of the P-value, are the following:</p>
<ol style="list-style-type: decimal">
<li>P-values can indicate how incompatible the data are with a specified statistical model.</li>
<li>P-values do not measure the probability that the studied hypothesis is true or the probability that the data were produced by random chance alone.</li>
<li>Scientific conclusions and business or policy decisions should not be based only on whether a p-value passes a specific threshold.</li>
<li>Proper inference requires full reporting and transparency.</li>
<li>A p-value, or statistical significance, does not measure the size of an effect or the importance of a result.</li>
<li>By itself, a p-value does not provide a good measure of evidence regarding a model or hypothesis.</li>
</ol>
<p>The <span class="math inline">\(p = 0.05\)</span> threshold is not based on any scientific calculation but is an arbitrary number. It means that practitioners can use a different threshold if they think it better fits the problem to solve. We do not promote p-value in this book. However, the p-value is hard to avoid in classical statistical inference. In practice, when making classic statistical inferences, we recommend reporting confidence interval whenever possible instead of P-value.</p>
<p>The Bayesian paradigm is an alternative to the classical paradigm. A Bayesian can state probabilities about the parameters, which are considered random variables. However, it is not possible in the classical paradigm. In our work, we use hierarchical (generalize) linear models in practice instead of classical linear regression. Hierarchical models pool information across clusters (for example, you can treat each customer segment as a cluster). This pooling tends to improve estimates of each cluster, especially when sampling is imbalanced. Because the models automatically cope with differing uncertainty introduced by sampling imbalance (bigger cluster has smaller variance), it prevents over-sampled clusters from unfairly dominating inference.</p>
<p>This book does not cover the Bayesian framework. The best applied Bayesian book is <a href="https://xcelab.net/rm/statistical-rethinking/">Statistical Rethinking</a> by Richard McElreath <span class="citation">(<a href="#ref-rethinking2020" role="doc-biblioref">McElreath 2020</a>)</span>. The book provides outstanding conceptual explanations and a wide range of models from simple to advanced with detailed, repeatable code. The text uses R, but there are code examples for Python and Julia on the book website.</p>
<p>Now let us come back to our example. We will not spend too much time on p-values, while we will focus on the confidence interval for the parameter estimate for each explanatory variable. In R, the function <code>confint()</code> can produce the confidence interval for each parameter:</p>
<div class="sourceCode" id="cb213"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb213-1"><a href="ordinary-least-square.html#cb213-1" aria-hidden="true" tabindex="-1"></a><span class="fu">confint</span>(lmfit,<span class="at">level=</span><span class="fl">0.9</span>)</span></code></pre></div>
<pre><code>##                  5 %     95 %
## (Intercept)  8.00892  8.18771
## Q1          -0.15987 -0.13081
## Q2           0.07018  0.13437
## Q3           0.22424  0.28466
## Q6          -0.24665 -0.20871
## Q8          -0.11787 -0.06354</code></pre>
<p>The above output is for a 90% confidence level as <code>level=0.9</code> indicated in the function call. We can change the confidence level by adjusting the level setting.</p>
<p>Fitting a linear regression is so easy using R that many analysts directly write reports without thinking about whether the model is meaningful. On the other hand, we can easily use R to check model assumptions. In the following sections, we will introduce a few commonly used diagnostic methods for linear regression to check whether the model assumptions are reasonable.</p>
</div>
<div id="diagnostics-for-linear-regression" class="section level3 hasAnchor" number="9.1.2">
<h3><span class="header-section-number">9.1.2</span> Diagnostics for Linear Regression<a href="ordinary-least-square.html#diagnostics-for-linear-regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In linear regression , we would like the Ordinary Least Square (OLS) estimate to be the Best Linear Unbiased Estimate (BLUE). In other words, we hope the expected value of the estimate is the actual parameter value (i.e., unbiased) and achieving minimized residual (i.e., best). Based on the Gauss-Markov theorem, the OLS estimate is BLUE under the following conditions:</p>
<ol style="list-style-type: decimal">
<li><p>Explanatory variables (<span class="math inline">\(\mathbf{x_{.j}}\)</span>) and random error (<span class="math inline">\(\symbf{\epsilon}\)</span>) are independent: <span class="math inline">\(cov(\symbf{x_{.j},\epsilon})=0\)</span> for <span class="math inline">\(\forall j=j\in1...p\)</span>.</p></li>
<li><p>The expected value of random error is zero: <span class="math inline">\(E(\symbf{\epsilon|X})=0\)</span></p></li>
<li><p>Random errors are uncorrelated with each other, and the variance of random error is consistent: <span class="math inline">\(Var(\symbf{\epsilon})=\sigma^{2}I\)</span>, where <span class="math inline">\(\sigma\)</span> is positive and <span class="math inline">\(I\)</span> is a <span class="math inline">\(n \times n\)</span> identical matrix.</p></li>
</ol>
<p>We will introduce four graphic diagnostics for the above assumptions.</p>
<ol style="list-style-type: decimal">
<li>Residual plot </li>
</ol>
<p>It is a scatter plot with residual on the Y-axis and fitted value on the X-axis. We can also put any of the explanatory variables on the X-axis. Under the assumption, residuals are randomly distributed, and we need to check the following:</p>
<ul>
<li>Are residuals centered around zero?</li>
<li>Are there any patterns in the residual plots (such as residuals with x-values farther from <span class="math inline">\(\bar{x}\)</span> have greater variance than residuals with x-values closer to <span class="math inline">\(\bar{x}\)</span>)?</li>
<li>Are the variances of the residual consistent across a range of fitted values?</li>
</ul>
<p>Please note that even if the variance is not consistent, the regression parameter’s point estimate is still unbiased. However, the variance estimate is not unbiased. Because the significant test for regression parameters is based on the random error distribution, these tests are no longer valid if the variance is not constant.</p>
<ol start="2" style="list-style-type: decimal">
<li>Normal quantile-quantile Plot (Q-Q Plot) </li>
</ol>
<p>Q-Q Plot is used to check the normality assumption for the residual. For normally distributed residuals, the data points should follow a straight line along the Q-Q plot. The more departure from a straight line, the more departure from a normal distribution for the residual.</p>
<ol start="3" style="list-style-type: decimal">
<li>Standardized residuals plot </li>
</ol>
<p>Standardized residual is the residual normalized by an estimate of its standard deviation. Like the residual plot, the X-axis is still the fitted value, but the y-axis is now standardized residuals. Because of the normalization, the y-axis shows the number of standard deviations from zero. A value greater than 2 or less than -2 indicates observations with large standardized residuals. The plot is useful because when the variance is not consistent, it can be difficult to detect the outliers using the raw residuals plot.</p>
<ol start="4" style="list-style-type: decimal">
<li>Cook’s distance </li>
</ol>
<p>Cook’s distance can check influential points in OLS based linear regression models. In general, we need to pay attention to data points with Cook’s distance &gt; 0.5.</p>
<p>In R, these diagnostic graphs are built in the <code>plot()</code> function.</p>
<div class="sourceCode" id="cb215"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb215-1"><a href="ordinary-least-square.html#cb215-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb215-2"><a href="ordinary-least-square.html#cb215-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(lmfit, <span class="at">which =</span> <span class="dv">1</span>)</span>
<span id="cb215-3"><a href="ordinary-least-square.html#cb215-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(lmfit, <span class="at">which =</span> <span class="dv">2</span>)</span>
<span id="cb215-4"><a href="ordinary-least-square.html#cb215-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(lmfit, <span class="at">which =</span> <span class="dv">3</span>)</span>
<span id="cb215-5"><a href="ordinary-least-square.html#cb215-5" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(lmfit, <span class="at">which =</span> <span class="dv">4</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:lmdiagnostic"></span>
<img src="IDS_files/figure-html/lmdiagnostic-1.svg" alt="Linear Regression Diagnostic Plots: residual plot (top left), Q-Q plot (top right), standardized residuals plot (lower left), Cook's distance (lower right)" width="80%" />
<p class="caption">
FIGURE 9.2: Linear Regression Diagnostic Plots: residual plot (top left), Q-Q plot (top right), standardized residuals plot (lower left), Cook’s distance (lower right)
</p>
</div>
<p>The above diagnostic plot examples show:</p>
<ul>
<li><p>Residual plot: residuals are generally distributed around <span class="math inline">\(y=0\)</span> horizontal line. There are no significant trends or patterns in this residual plot (there are two bumps but does not seem too severe). So the linear relationship assumption between the response variable and explanatory variables is reasonable.</p></li>
<li><p>Q-Q plot: data points are pretty much along the diagonal line of Y=X, indicating no significant normality assumption departure for the residuals. Because we simulate the data, we know the response variable before log transformation follows a normal distribution. The shape of the distribution does not deviate from a normal distribution too much after log transformation.</p></li>
</ul>
<p>Note that the Gauss-Markov theorem does not require normality. We need the normal assumption to look for significant factors or define a confidence interval. However, as Andrew Gelman pointed out in section 3.6 of his book <span class="citation">(<a href="#ref-linear2006" role="doc-biblioref">Gelman and Hill 2006</a>)</span>, normality and equal variance are typically minor concerns.</p>
<ul>
<li>Standardized residual plot: if the constant variance assumption is valid, then the plot’s data points should be randomly distributed around the horizontal line. We can see there are three outliers on the plot. Let us check those points:</li>
</ul>
<div class="sourceCode" id="cb216"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb216-1"><a href="ordinary-least-square.html#cb216-1" aria-hidden="true" tabindex="-1"></a>modeldat[<span class="fu">which</span>(<span class="fu">row.names</span>(modeldat) <span class="sc">%in%</span> <span class="fu">c</span>(<span class="dv">960</span>, <span class="dv">678</span>, <span class="dv">155</span>)), ]</span></code></pre></div>
<pre><code>##     Q1 Q2 Q3 Q6 Q8 total_exp
## 155  4  2  1  4  4     351.9
## 678  2  1  1  1  2    1087.3
## 960  2  1  1  1  3     658.3</code></pre>
<p>It is not easy to see why those records are outliers from the above output. It will be clear conditional on the independent variables (<code>Q1</code>, <code>Q2</code>, <code>Q3</code>, <code>Q6</code>, and <code>Q8</code>). Let us examine the value of <code>total_exp</code> for samples with the same Q1, Q2, Q3, Q6, and Q8 answers as the 3rd row above.</p>
<div class="sourceCode" id="cb218"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb218-1"><a href="ordinary-least-square.html#cb218-1" aria-hidden="true" tabindex="-1"></a>datcheck <span class="ot">=</span> modeldat <span class="sc">%&gt;%</span> </span>
<span id="cb218-2"><a href="ordinary-least-square.html#cb218-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(Q1 <span class="sc">==</span><span class="dv">2</span> <span class="sc">&amp;</span> Q2 <span class="sc">==</span> <span class="dv">1</span> <span class="sc">&amp;</span> Q3 <span class="sc">==</span> <span class="dv">1</span> <span class="sc">&amp;</span> Q6 <span class="sc">==</span> <span class="dv">1</span> <span class="sc">&amp;</span> Q8 <span class="sc">==</span> <span class="dv">3</span>) </span>
<span id="cb218-3"><a href="ordinary-least-square.html#cb218-3" aria-hidden="true" tabindex="-1"></a><span class="fu">nrow</span>(datcheck)</span></code></pre></div>
<pre><code>## [1] 87</code></pre>
<p>There are 87 samples with the same values of independent variables. The response variable’s (<code>total_exp</code>) distribution is:</p>
<div class="sourceCode" id="cb220"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb220-1"><a href="ordinary-least-square.html#cb220-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(datcheck<span class="sc">$</span>total_exp)</span></code></pre></div>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##     658    1884    2215    2204    2554    3197</code></pre>
<p>Now it is easy to see why row 960 with <code>total_exp = 658.3</code> is an outlier. All the other 86 records with the same survey responses have a much higher total expense!</p>
<ul>
<li>Cook’s distance: the maximum of Cook’s distance is around 0.05. Even though the graph does not have any point with Cook’s distance of more than 0.5, we could spot some outliers.</li>
</ul>
<p>The graphs suggest some outliers, but it is our decision what to do. We can either remove it or investigate it further. If the values are not due to any data error, we should consider them in our analysis.</p>
</div>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-linear2006" class="csl-entry">
Gelman, Andrew, and Jennifer Hill. 2006. <em>Data Analysis Using Regression and Multilevel/Hierarchical Models</em>. Cambridge University Press.
</div>
<div id="ref-rethinking2020" class="csl-entry">
McElreath, Richard. 2020. <em>Statistical Rethinking: A Bayesian Course with Examples in r and STAN</em>. Edited by 2nd. Chapman; Hall/CRC.
</div>
<div id="ref-ASA_P" class="csl-entry">
Ronald L. Wassersteina, Nicole A. Lazara. 2016. <span>“Position on p-Values: Context, Process, and Purpose.”</span>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="regression-models.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="principal-component-regression-and-partial-least-square.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": true,
"twitter": true,
"linkedin": true,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/happyrabbit/IntroDataScience/09-Regression.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["IDS.pdf", "IDS.epub", "IDS.mobi"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section",
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
