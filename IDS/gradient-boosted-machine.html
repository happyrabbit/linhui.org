<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Introduction to Data Science</title>
  <meta name="description" content="Introduction to Data Science">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Introduction to Data Science" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="http://scientistcafe.com/IDS/" />
  
  <meta property="og:description" content="Introduction to Data Science" />
  <meta name="github-repo" content="happyrabbit/IntroDataScience" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Introduction to Data Science" />
  
  <meta name="twitter:description" content="Introduction to Data Science" />
  

<meta name="author" content="Hui Lin and Ming Li">


<meta name="date" content="2019-11-06">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="random-forest.html">
<link rel="next" href="neural-network.html">
<script src="libs/jquery/jquery.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="http://scientistcafe.com">Homepage</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="goal-of-the-book.html"><a href="goal-of-the-book.html"><i class="fa fa-check"></i>Goal of the Book</a></li>
<li class="chapter" data-level="" data-path="who-this-book-is-for.html"><a href="who-this-book-is-for.html"><i class="fa fa-check"></i>Who This Book Is For</a></li>
<li class="chapter" data-level="" data-path="what-this-book-covers.html"><a href="what-this-book-covers.html"><i class="fa fa-check"></i>What This Book Covers</a></li>
<li class="chapter" data-level="" data-path="conventions.html"><a href="conventions.html"><i class="fa fa-check"></i>Conventions</a></li>
<li class="chapter" data-level="" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="about-the-authors.html"><a href="about-the-authors.html"><i class="fa fa-check"></i>About the Authors</a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="blind-men-and-an-elephant.html"><a href="blind-men-and-an-elephant.html"><i class="fa fa-check"></i><b>1.1</b> Blind men and an elephant</a><ul>
<li class="chapter" data-level="1.1.1" data-path="blind-men-and-an-elephant.html"><a href="blind-men-and-an-elephant.html#data-science-roleskill-tracks"><i class="fa fa-check"></i><b>1.1.1</b> Data science role/skill tracks</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="what-should-data-science-do.html"><a href="what-should-data-science-do.html"><i class="fa fa-check"></i><b>1.2</b> What should data science do?</a><ul>
<li class="chapter" data-level="1.2.1" data-path="what-should-data-science-do.html"><a href="what-should-data-science-do.html#lets-dream-big"><i class="fa fa-check"></i><b>1.2.1</b> Let’s dream big</a></li>
<li class="chapter" data-level="1.2.2" data-path="what-should-data-science-do.html"><a href="what-should-data-science-do.html#what-kind-of-questions-can-data-science-solve"><i class="fa fa-check"></i><b>1.2.2</b> What kind of questions can data science solve?</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="structure-data-science-team.html"><a href="structure-data-science-team.html"><i class="fa fa-check"></i><b>1.3</b> Structure data science team</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="soft-skills-for-data-scientists.html"><a href="soft-skills-for-data-scientists.html"><i class="fa fa-check"></i><b>2</b> Soft Skills for Data Scientists</a><ul>
<li class="chapter" data-level="2.1" data-path="comparison-between-statistician-and-data-scientist.html"><a href="comparison-between-statistician-and-data-scientist.html"><i class="fa fa-check"></i><b>2.1</b> Comparison between Statistician and Data Scientist</a></li>
<li class="chapter" data-level="2.2" data-path="beyond-data-and-analytics.html"><a href="beyond-data-and-analytics.html"><i class="fa fa-check"></i><b>2.2</b> Beyond Data and Analytics</a></li>
<li class="chapter" data-level="2.3" data-path="three-pillars-of-knowledge.html"><a href="three-pillars-of-knowledge.html"><i class="fa fa-check"></i><b>2.3</b> Three Pillars of Knowledge</a></li>
<li class="chapter" data-level="2.4" data-path="data-science-project-cycle.html"><a href="data-science-project-cycle.html"><i class="fa fa-check"></i><b>2.4</b> Data Science Project Cycle</a><ul>
<li class="chapter" data-level="2.4.1" data-path="data-science-project-cycle.html"><a href="data-science-project-cycle.html#types-of-data-science-projects"><i class="fa fa-check"></i><b>2.4.1</b> Types of Data Science Projects</a></li>
<li class="chapter" data-level="2.4.2" data-path="data-science-project-cycle.html"><a href="data-science-project-cycle.html#at-the-planning-stage"><i class="fa fa-check"></i><b>2.4.2</b> At the Planning Stage</a></li>
<li class="chapter" data-level="2.4.3" data-path="data-science-project-cycle.html"><a href="data-science-project-cycle.html#at-the-modeling-stage"><i class="fa fa-check"></i><b>2.4.3</b> At the Modeling Stage</a></li>
<li class="chapter" data-level="2.4.4" data-path="data-science-project-cycle.html"><a href="data-science-project-cycle.html#at-the-production-stage"><i class="fa fa-check"></i><b>2.4.4</b> At the Production Stage</a></li>
<li class="chapter" data-level="2.4.5" data-path="data-science-project-cycle.html"><a href="data-science-project-cycle.html#summary"><i class="fa fa-check"></i><b>2.4.5</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="common-mistakes-in-data-science.html"><a href="common-mistakes-in-data-science.html"><i class="fa fa-check"></i><b>2.5</b> Common Mistakes in Data Science</a><ul>
<li class="chapter" data-level="2.5.1" data-path="common-mistakes-in-data-science.html"><a href="common-mistakes-in-data-science.html#problem-formulation-stage"><i class="fa fa-check"></i><b>2.5.1</b> Problem Formulation Stage</a></li>
<li class="chapter" data-level="2.5.2" data-path="common-mistakes-in-data-science.html"><a href="common-mistakes-in-data-science.html#problem-planning-stage"><i class="fa fa-check"></i><b>2.5.2</b> Problem Planning Stage</a></li>
<li class="chapter" data-level="2.5.3" data-path="common-mistakes-in-data-science.html"><a href="common-mistakes-in-data-science.html#modeling-stage"><i class="fa fa-check"></i><b>2.5.3</b> Modeling Stage</a></li>
<li class="chapter" data-level="2.5.4" data-path="common-mistakes-in-data-science.html"><a href="common-mistakes-in-data-science.html#production-stage"><i class="fa fa-check"></i><b>2.5.4</b> Production Stage</a></li>
<li class="chapter" data-level="2.5.5" data-path="common-mistakes-in-data-science.html"><a href="common-mistakes-in-data-science.html#summary-1"><i class="fa fa-check"></i><b>2.5.5</b> Summary</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="introduction-to-the-data.html"><a href="introduction-to-the-data.html"><i class="fa fa-check"></i><b>3</b> Introduction to The Data</a><ul>
<li class="chapter" data-level="3.1" data-path="customer-data-for-clothing-company.html"><a href="customer-data-for-clothing-company.html"><i class="fa fa-check"></i><b>3.1</b> Customer Data for Clothing Company</a></li>
<li class="chapter" data-level="3.2" data-path="customer-satisfaction-survey-data-from-airline-company.html"><a href="customer-satisfaction-survey-data-from-airline-company.html"><i class="fa fa-check"></i><b>3.2</b> Customer Satisfaction Survey Data from Airline Company</a></li>
<li class="chapter" data-level="3.3" data-path="swine-disease-breakout-data.html"><a href="swine-disease-breakout-data.html"><i class="fa fa-check"></i><b>3.3</b> Swine Disease Breakout Data</a></li>
<li class="chapter" data-level="3.4" data-path="mnist-dataset.html"><a href="mnist-dataset.html"><i class="fa fa-check"></i><b>3.4</b> MNIST Dataset</a></li>
<li class="chapter" data-level="3.5" data-path="imdb-dataset.html"><a href="imdb-dataset.html"><i class="fa fa-check"></i><b>3.5</b> IMDB Dataset</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="big-data-cloud-platform.html"><a href="big-data-cloud-platform.html"><i class="fa fa-check"></i><b>4</b> Big Data Cloud Platform</a><ul>
<li class="chapter" data-level="4.1" data-path="how-data-becomes-science.html"><a href="how-data-becomes-science.html"><i class="fa fa-check"></i><b>4.1</b> How Data Becomes Science?</a></li>
<li class="chapter" data-level="4.2" data-path="power-of-cluster-of-computers.html"><a href="power-of-cluster-of-computers.html"><i class="fa fa-check"></i><b>4.2</b> Power of Cluster of Computers</a></li>
<li class="chapter" data-level="4.3" data-path="evolution-of-cluster-computing.html"><a href="evolution-of-cluster-computing.html"><i class="fa fa-check"></i><b>4.3</b> Evolution of Cluster Computing</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="data-pre-processing.html"><a href="data-pre-processing.html"><i class="fa fa-check"></i><b>5</b> Data Pre-processing</a><ul>
<li class="chapter" data-level="5.1" data-path="data-cleaning.html"><a href="data-cleaning.html"><i class="fa fa-check"></i><b>5.1</b> Data Cleaning</a></li>
<li class="chapter" data-level="5.2" data-path="missing-values.html"><a href="missing-values.html"><i class="fa fa-check"></i><b>5.2</b> Missing Values</a><ul>
<li class="chapter" data-level="5.2.1" data-path="missing-values.html"><a href="missing-values.html#impute-missing-values-with-medianmode"><i class="fa fa-check"></i><b>5.2.1</b> Impute missing values with median/mode</a></li>
<li class="chapter" data-level="5.2.2" data-path="missing-values.html"><a href="missing-values.html#k-nearest-neighbors"><i class="fa fa-check"></i><b>5.2.2</b> K-nearest neighbors</a></li>
<li class="chapter" data-level="5.2.3" data-path="missing-values.html"><a href="missing-values.html#bagging-tree"><i class="fa fa-check"></i><b>5.2.3</b> Bagging Tree</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="centering-and-scaling.html"><a href="centering-and-scaling.html"><i class="fa fa-check"></i><b>5.3</b> Centering and Scaling</a></li>
<li class="chapter" data-level="5.4" data-path="resolve-skewness.html"><a href="resolve-skewness.html"><i class="fa fa-check"></i><b>5.4</b> Resolve Skewness</a></li>
<li class="chapter" data-level="5.5" data-path="resolve-outliers.html"><a href="resolve-outliers.html"><i class="fa fa-check"></i><b>5.5</b> Resolve Outliers</a></li>
<li class="chapter" data-level="5.6" data-path="collinearity.html"><a href="collinearity.html"><i class="fa fa-check"></i><b>5.6</b> Collinearity</a></li>
<li class="chapter" data-level="5.7" data-path="sparse-variables.html"><a href="sparse-variables.html"><i class="fa fa-check"></i><b>5.7</b> Sparse Variables</a></li>
<li class="chapter" data-level="5.8" data-path="re-encode-dummy-variables.html"><a href="re-encode-dummy-variables.html"><i class="fa fa-check"></i><b>5.8</b> Re-encode Dummy Variables</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="data-wrangling.html"><a href="data-wrangling.html"><i class="fa fa-check"></i><b>6</b> Data Wrangling</a><ul>
<li class="chapter" data-level="6.1" data-path="read-and-write-data.html"><a href="read-and-write-data.html"><i class="fa fa-check"></i><b>6.1</b> Read and write data</a><ul>
<li class="chapter" data-level="6.1.1" data-path="read-and-write-data.html"><a href="read-and-write-data.html#readr"><i class="fa fa-check"></i><b>6.1.1</b> <code>readr</code></a></li>
<li class="chapter" data-level="6.1.2" data-path="read-and-write-data.html"><a href="read-and-write-data.html#data.table-enhanced-data.frame"><i class="fa fa-check"></i><b>6.1.2</b> <code>data.table</code>— enhanced <code>data.frame</code></a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="summarize-data.html"><a href="summarize-data.html"><i class="fa fa-check"></i><b>6.2</b> Summarize data</a><ul>
<li class="chapter" data-level="6.2.1" data-path="summarize-data.html"><a href="summarize-data.html#apply-lapply-and-sapply-in-base-r"><i class="fa fa-check"></i><b>6.2.1</b> <code>apply()</code>, <code>lapply()</code> and <code>sapply()</code> in base R</a></li>
<li class="chapter" data-level="6.2.2" data-path="summarize-data.html"><a href="summarize-data.html#dplyr-package"><i class="fa fa-check"></i><b>6.2.2</b> <code>dplyr</code> package</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="tidy-and-reshape-data.html"><a href="tidy-and-reshape-data.html"><i class="fa fa-check"></i><b>6.3</b> Tidy and Reshape Data</a><ul>
<li class="chapter" data-level="6.3.1" data-path="tidy-and-reshape-data.html"><a href="tidy-and-reshape-data.html#reshape2-package"><i class="fa fa-check"></i><b>6.3.1</b> <code>reshape2</code> package</a></li>
<li class="chapter" data-level="6.3.2" data-path="tidy-and-reshape-data.html"><a href="tidy-and-reshape-data.html#tidyr-package"><i class="fa fa-check"></i><b>6.3.2</b> <code>tidyr</code> package</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="model-tuning-strategy.html"><a href="model-tuning-strategy.html"><i class="fa fa-check"></i><b>7</b> Model Tuning Strategy</a><ul>
<li class="chapter" data-level="7.1" data-path="systematic-error-and-random-error.html"><a href="systematic-error-and-random-error.html"><i class="fa fa-check"></i><b>7.1</b> Systematic Error and Random Error</a><ul>
<li class="chapter" data-level="7.1.1" data-path="systematic-error-and-random-error.html"><a href="systematic-error-and-random-error.html#measurement-error-in-the-response"><i class="fa fa-check"></i><b>7.1.1</b> Measurement Error in the Response</a></li>
<li class="chapter" data-level="7.1.2" data-path="systematic-error-and-random-error.html"><a href="systematic-error-and-random-error.html#measurement-error-in-the-independent-variables"><i class="fa fa-check"></i><b>7.1.2</b> Measurement Error in the Independent Variables</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="data-splitting-and-resampling.html"><a href="data-splitting-and-resampling.html"><i class="fa fa-check"></i><b>7.2</b> Data Splitting and Resampling</a><ul>
<li class="chapter" data-level="7.2.1" data-path="data-splitting-and-resampling.html"><a href="data-splitting-and-resampling.html#data-splitting"><i class="fa fa-check"></i><b>7.2.1</b> Data Splitting</a></li>
<li class="chapter" data-level="7.2.2" data-path="data-splitting-and-resampling.html"><a href="data-splitting-and-resampling.html#resampling"><i class="fa fa-check"></i><b>7.2.2</b> Resampling</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="measuring-performance.html"><a href="measuring-performance.html"><i class="fa fa-check"></i><b>8</b> Measuring Performance</a><ul>
<li class="chapter" data-level="8.1" data-path="regression-model-performance.html"><a href="regression-model-performance.html"><i class="fa fa-check"></i><b>8.1</b> Regression Model Performance</a></li>
<li class="chapter" data-level="8.2" data-path="classification-model-performance.html"><a href="classification-model-performance.html"><i class="fa fa-check"></i><b>8.2</b> Classification Model Performance</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="regression-models.html"><a href="regression-models.html"><i class="fa fa-check"></i><b>9</b> Regression Models</a><ul>
<li class="chapter" data-level="9.1" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html"><i class="fa fa-check"></i><b>9.1</b> Ordinary Least Squares</a></li>
<li class="chapter" data-level="9.2" data-path="multivariate-adaptive-regression-splines.html"><a href="multivariate-adaptive-regression-splines.html"><i class="fa fa-check"></i><b>9.2</b> Multivariate Adaptive Regression Splines</a></li>
<li class="chapter" data-level="9.3" data-path="generalized-linear-model.html"><a href="generalized-linear-model.html"><i class="fa fa-check"></i><b>9.3</b> Generalized Linear Model</a></li>
<li class="chapter" data-level="9.4" data-path="pcr-and-pls.html"><a href="pcr-and-pls.html"><i class="fa fa-check"></i><b>9.4</b> PCR and PLS</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="regularization-methods.html"><a href="regularization-methods.html"><i class="fa fa-check"></i><b>10</b> Regularization Methods</a><ul>
<li class="chapter" data-level="10.1" data-path="ridge-regression.html"><a href="ridge-regression.html"><i class="fa fa-check"></i><b>10.1</b> Ridge Regression</a></li>
<li class="chapter" data-level="10.2" data-path="lasso.html"><a href="lasso.html"><i class="fa fa-check"></i><b>10.2</b> LASSO</a></li>
<li class="chapter" data-level="10.3" data-path="elastic-net.html"><a href="elastic-net.html"><i class="fa fa-check"></i><b>10.3</b> Elastic Net</a></li>
<li class="chapter" data-level="10.4" data-path="lasso-generalized-linear-model.html"><a href="lasso-generalized-linear-model.html"><i class="fa fa-check"></i><b>10.4</b> LASSO Generalized Linear Model</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="tree-based-methods.html"><a href="tree-based-methods.html"><i class="fa fa-check"></i><b>11</b> Tree-Based Methods</a><ul>
<li class="chapter" data-level="11.1" data-path="splitting-criteria.html"><a href="splitting-criteria.html"><i class="fa fa-check"></i><b>11.1</b> Splitting Criteria</a></li>
<li class="chapter" data-level="11.2" data-path="tree-pruning.html"><a href="tree-pruning.html"><i class="fa fa-check"></i><b>11.2</b> Tree Pruning</a></li>
<li class="chapter" data-level="11.3" data-path="regression-and-decision-tree-basic.html"><a href="regression-and-decision-tree-basic.html"><i class="fa fa-check"></i><b>11.3</b> Regression and Decision Tree Basic</a><ul>
<li class="chapter" data-level="11.3.1" data-path="regression-and-decision-tree-basic.html"><a href="regression-and-decision-tree-basic.html#regression-tree"><i class="fa fa-check"></i><b>11.3.1</b> Regression Tree</a></li>
<li class="chapter" data-level="11.3.2" data-path="regression-and-decision-tree-basic.html"><a href="regression-and-decision-tree-basic.html#decision-tree"><i class="fa fa-check"></i><b>11.3.2</b> Decision Tree</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="bagging-tree-1.html"><a href="bagging-tree-1.html"><i class="fa fa-check"></i><b>11.4</b> Bagging Tree</a></li>
<li class="chapter" data-level="11.5" data-path="random-forest.html"><a href="random-forest.html"><i class="fa fa-check"></i><b>11.5</b> Random Forest</a></li>
<li class="chapter" data-level="11.6" data-path="gradient-boosted-machine.html"><a href="gradient-boosted-machine.html"><i class="fa fa-check"></i><b>11.6</b> Gradient Boosted Machine</a><ul>
<li class="chapter" data-level="11.6.1" data-path="gradient-boosted-machine.html"><a href="gradient-boosted-machine.html#adaptive-boosting"><i class="fa fa-check"></i><b>11.6.1</b> Adaptive Boosting</a></li>
<li class="chapter" data-level="11.6.2" data-path="gradient-boosted-machine.html"><a href="gradient-boosted-machine.html#stochastic-gradient-boosting"><i class="fa fa-check"></i><b>11.6.2</b> Stochastic Gradient Boosting</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="neural-network.html"><a href="neural-network.html"><i class="fa fa-check"></i><b>12</b> Neural Network</a><ul>
<li class="chapter" data-level="12.1" data-path="projection-pursuit-regression.html"><a href="projection-pursuit-regression.html"><i class="fa fa-check"></i><b>12.1</b> Projection Pursuit Regression</a></li>
<li class="chapter" data-level="12.2" data-path="standard-neural-network.html"><a href="standard-neural-network.html"><i class="fa fa-check"></i><b>12.2</b> Standard Neural Network</a><ul>
<li class="chapter" data-level="12.2.1" data-path="standard-neural-network.html"><a href="standard-neural-network.html#logistic_reg_as_neural_network"><i class="fa fa-check"></i><b>12.2.1</b> Logistic Regression as Neural Network</a></li>
<li class="chapter" data-level="12.2.2" data-path="standard-neural-network.html"><a href="standard-neural-network.html#deep-neural-network"><i class="fa fa-check"></i><b>12.2.2</b> Deep Neural Network</a></li>
<li class="chapter" data-level="12.2.3" data-path="standard-neural-network.html"><a href="standard-neural-network.html#activation-function"><i class="fa fa-check"></i><b>12.2.3</b> Activation Function</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="convolutional-neural-network.html"><a href="convolutional-neural-network.html"><i class="fa fa-check"></i><b>12.3</b> Convolutional Neural Network</a></li>
<li class="chapter" data-level="12.4" data-path="recurrent-neural-network.html"><a href="recurrent-neural-network.html"><i class="fa fa-check"></i><b>12.4</b> Recurrent Neural Network</a><ul>
<li class="chapter" data-level="12.4.1" data-path="recurrent-neural-network.html"><a href="recurrent-neural-network.html#section"><i class="fa fa-check"></i><b>12.4.1</b> </a></li>
</ul></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="r-code-for-data-simulation.html"><a href="r-code-for-data-simulation.html"><i class="fa fa-check"></i><b>A</b> R code for data simulation</a><ul>
<li class="chapter" data-level="A.1" data-path="customer-data-for-clothing-company-1.html"><a href="customer-data-for-clothing-company-1.html"><i class="fa fa-check"></i><b>A.1</b> Customer Data for Clothing Company</a></li>
<li class="chapter" data-level="A.2" data-path="customer-satisfaction-survey-data-from-airline-company-1.html"><a href="customer-satisfaction-survey-data-from-airline-company-1.html"><i class="fa fa-check"></i><b>A.2</b> Customer Satisfaction Survey Data from Airline Company</a></li>
<li class="chapter" data-level="A.3" data-path="swine-disease-breakout-data-1.html"><a href="swine-disease-breakout-data-1.html"><i class="fa fa-check"></i><b>A.3</b> Swine Disease Breakout Data</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Data Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="gradient-boosted-machine" class="section level2">
<h2><span class="header-section-number">11.6</span> Gradient Boosted Machine</h2>
<p>Boosting models were developed in the 1980s <span class="citation">(L <a href="#ref-Valiant1984">1984</a>; M and L <a href="#ref-KV1989">1989</a>)</span> and were originally for classification problems. Due to the excellent model performance, they were widely used for a variety of applications, such as gene expression <span class="citation">(Dudoit S and T <a href="#ref-dudoit2002">2002</a>; al <a href="#ref-bendor2000">2000</a>)</span>, chemical substructure classification <span class="citation">(Varmuza K and K <a href="#ref-Varmuza2003">2003</a>)</span>, music classification <span class="citation">(al <a href="#ref-Bergstra2006">2006</a>)</span>, etc. The first effective implementation of boosting is Adaptive Boosting (AdaBoost) algorithm came up by Yoav Freund and Robert Schapire in 1996 <span class="citation">(YFR <a href="#ref-Schapire1999">1999</a>)</span>. After that, some researchers <span class="citation">(Friedman J and R <a href="#ref-Friedman2000">2000</a>)</span> started to connect the boosting algorithm with some statistical concepts, such as loss function, additive model, logistic regression. Friedman pointed out that boosting can be considered as a forward stagewise additive model that minimizes exponential loss. The new view of boosting in a statistical framework enabled the method to be extended to regression problems.</p>
<p>The idea is to combine a group of weak learners (a classifier that is marginally better than random guess) to produce a strong learner. Like bagging, boosting is a general approach that can be applied to different learners. Here we focus on the decision tree. Recall that both bagging and random forest create multiple copies of the original training data using the bootstrap, fitting a separate decision tree to each copy and combining all the results to create a single prediction. Boosting also creates different trees but the trees are grown sequentially and each tree is a weak learner. Any modeling technique with tuning parameters can produce a range of learners, from weak to strong. You can easily make a weak learner by restricting the depth of the tree. There are different types of boosting. Here we introduce two main types: adaptive boosting and stochastic gradient boosting.</p>
<div id="adaptive-boosting" class="section level3">
<h3><span class="header-section-number">11.6.1</span> Adaptive Boosting</h3>
<p>Yoav Freund and Robert Schapire <span class="citation">(Freund and Schapire <a href="#ref-Freund1997">1997</a>)</span> came up the AdaBoost.M1 algorithm. Consider a binary classification problem where the response variable has two categories <span class="math inline">\(Y \in \{-1, 1\}\)</span>. Given predictor matrix, <span class="math inline">\(X\)</span>, construct a classifier <span class="math inline">\(G(X)\)</span> that predicts <span class="math inline">\(1\)</span> or <span class="math inline">\(-1\)</span>. The corresponding error rate in the training set is:</p>
<p><span class="math display">\[\bar{err}=\frac{1}{N}\Sigma_{i=1}^NI(y_i\neq G(x_i))\]</span></p>
<p>The algorithm produces a series of classifiers <span class="math inline">\(G_m(x),\ m=1,2,...,M\)</span> from different iterations. In each iteration, it finds the best classifier based on the current weights. The misclassified samples in the <span class="math inline">\(m^{th}\)</span> iteration will have higher weights in the <span class="math inline">\(m+1^{th}\)</span> iteration and the correctly classified samples will have lower weights. As it moves on, the algorithm will put more effort into the “difficult” samples until it can correctly classify them. So it requires the algorithm to change focus at each iteration. At each iteration, the algorithm will calculate a stage weight based on the error rate. The final prediction is a weighted average of all those weak classifiers using stage weights from all the iterations:</p>
<p><span class="math display">\[G(x)=sign ( \Sigma_{m=1}^M \alpha_{m}G_m(x))\]</span> where <span class="math inline">\(\alpha_1,\alpha_2,...,\alpha_M\)</span> are the weights from different iterations.</p>
<p><strong>AdaBoost.M1</strong></p>
<ol style="list-style-type: decimal">
<li>Response variables have two values: +1 and -1</li>
<li>Initialize the observation to have the same weights: <span class="math inline">\(w_i=\frac{1}{N},i=1,...,N\)</span></li>
<li><p>for m = 1 to M:</p>
<ol style="list-style-type: lower-alpha">
<li><p>Fit a classifier <span class="math inline">\(G_m(x)\)</span> to the training data using weights <span class="math inline">\(w_i\)</span></p></li>
<li><p>Compute the error rate for the <span class="math inline">\(m^th\)</span> classifier: <span class="math inline">\(err_m=\frac{\Sigma_{i=1}^Nw_i I(y_i\neq G_m(x_i))}{\Sigma_{i=1}^Nw_i}\)</span></p></li>
<li><p>Compute the stage weight for <span class="math inline">\(m^{th}\)</span> iteration: <span class="math inline">\(\alpha_m=log\frac{1-err_m}{err_m}\)</span></p></li>
<li><p>Update <span class="math inline">\(w_i = w_i\cdot exp[\alpha_m\cdot I(y_i \neq G_m(x_i))],\ i=1,2,\dots,N\)</span></p></li>
</ol></li>
<li><p>Calculate the prediction：<span class="math inline">\(G(x)=sign[\Sigma_{m=1}^M\alpha_mG_m(x)]\)</span>，where <span class="math inline">\(sign(\cdot)\)</span> means if <span class="math inline">\(\cdot\)</span> is positive, then the sample is classified as +1, -1 otherwise.</p></li>
</ol>
<p>Since the classifier <span class="math inline">\(G_m(x)\)</span> returns discrete value, the AdaBoost.M1 algorithm is known as “Discrete AdaBoost” <span class="citation">(Friedman J and R <a href="#ref-Friedman2000">2000</a>)</span>. You can revise the above algorithm if it returns continuous value, for example, a probability<span class="citation">(Friedman J and R <a href="#ref-Friedman2000">2000</a>)</span>. As mentioned before, boosting is a general approach that can be applied to different learners. Since you can easily create weak learners by limiting the depth of the tree, the boosting tree is a common method. Since the classification tree is a low bias/high variance technique, ensemble decreases model variance and lead to low bias/low variance model. See Breinman<span class="citation">(Breiman <a href="#ref-Breiman1998">1998</a>)</span> for more explanation about why the boosting tree performs well in general. However, boosting can not significantly improve the low variance model. So applying boosting to Latent Dirichlet Allocation (LDA) or K-Nearest Neighbor (KNN) doesn’t lead to as good improvement as applying boosting to statistical learning methods like naive Bayes <span class="citation">(E and R <a href="#ref-Bauer1999">1999</a>)</span>.</p>
</div>
<div id="stochastic-gradient-boosting" class="section level3">
<h3><span class="header-section-number">11.6.2</span> Stochastic Gradient Boosting</h3>
<p>As mentioned before, Friedman <span class="citation">(Friedman J and R <a href="#ref-Friedman2000">2000</a>)</span> provided a statistical framework for the AdaBoost algorithm and pointed out that boosting can be considered as a forward stagewise additive model that minimizes exponential loss. The framework led to some generalized algorithms such as Real AdaBoost, Gentle AdaBoost, and LogitBoost. Those algorithms later were unified under a framework called gradient boosting machine. The last section of the chapter illustrates how boosting can be considered as an additive model.</p>
<p>Consider a 2-class classification problem. You have the response <span class="math inline">\(y \in \{0, 1\}\)</span> and the sample proportion of class 1 from the training set is <span class="math inline">\(p\)</span>. <span class="math inline">\(f(x)\)</span> is the model prediction in the range of <span class="math inline">\([-\infty, +\infty]\)</span> and the predicted event probability is <span class="math inline">\(\hat{p}=\frac{1}{1+exp[-f(x)]}\)</span>. The gradient boosting for this problem is as follows:</p>
<ol style="list-style-type: decimal">
<li>Initialize all predictions to the sample log-odds: <span class="math inline">\(f_{i} = log \frac{\hat{p}}{1- \hat{p}}\)</span></li>
<li><strong>for</strong> j=1 … M <strong>do</strong>
<ul>
<li>Compute predicted event probability: <span class="math inline">\(\hat{p}_i=\frac{1}{1+exp[-f_{i}(x)]}\)</span>.</li>
<li>Compute the residual (i.e. gradient): <span class="math inline">\(z_i=y_i-\hat{p}_i\)</span></li>
<li>Randomly sample the training data</li>
<li>Train a tree model on the random subset using the residuals as the outcome</li>
<li>Compute the terminal node estimates of the Pearson residuals: <span class="math inline">\(r_i=\frac{1/n\Sigma_i^n(y_i-\hat{p}_i)}{1/n\Sigma_i^n\hat{p}_i(1-\hat{p}_i)}\)</span></li>
<li>Update f：<span class="math inline">\(f_i=f_i+\lambda f_i^{(j)}\)</span></li>
</ul></li>
<li>end</li>
</ol>
<p>When using the tree as the base learner, basic gradient boosting has two tuning parameters: tree depth and the number of iterations. You can further customize the algorithm by selecting a different loss function and gradient<span class="citation">(Hastie T <a href="#ref-Hastie2008">2008</a>)</span>. The final line of the loop includes a regularization strategy. Instead of adding <span class="math inline">\(f_i^{(j)}\)</span> to the previous iteration’s <span class="math inline">\(f_i\)</span>, only a fraction of the value is added. This fraction is called learning rate which is <span class="math inline">\(\lambda\)</span> in the algorithm. It can take values between 0 and 1 which is another tuning parameter of the model.</p>
<p>The way to calculate variable importance in boosting is similar to a bagging model. You get variable importance by combining measures of importance across the ensemble. For example, we can calculate the Gini index improvement for each variable across all trees and use the average as the measurement of the importance.</p>
<p>Boosting is a very popular method for classification. It is one of the methods that can be directly applied to the data without requiring a great deal of time-consuming data preprocessing. Applying boosting on tree models significantly improves predictive accuracy. Some advantages of trees that are sacrificed by boosting are speed and interpretability.</p>
<p>Let’s look at the R implementation.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">gbmGrid &lt;-<span class="st"> </span><span class="kw">expand.grid</span>(<span class="dt">interaction.depth =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">5</span>, <span class="dv">7</span>, <span class="dv">9</span>),
                       <span class="dt">n.trees =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">5</span>,
                       <span class="dt">shrinkage =</span> <span class="kw">c</span>(.<span class="dv">01</span>, .<span class="dv">1</span>),
                       <span class="dt">n.minobsinnode =</span> <span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>))

<span class="kw">set.seed</span>(<span class="dv">100</span>)
gbmTune &lt;-<span class="st"> </span><span class="kw">train</span>(<span class="dt">x =</span> trainx, 
                <span class="dt">y =</span> trainy,
                <span class="dt">method =</span> <span class="st">&quot;gbm&quot;</span>,
                <span class="dt">tuneGrid =</span> gbmGrid,
                <span class="dt">metric =</span> <span class="st">&quot;ROC&quot;</span>,
                <span class="dt">verbose =</span> <span class="ot">FALSE</span>,
                <span class="dt">trControl =</span> <span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>,
                           <span class="dt">summaryFunction =</span> twoClassSummary,
                           <span class="dt">classProbs =</span> <span class="ot">TRUE</span>,
                           <span class="dt">savePredictions =</span> <span class="ot">TRUE</span>))</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># only show part of the output</span>
gbmTune</code></pre></div>
<div class="sourceCode"><pre class="sourceCode html"><code class="sourceCode html">Stochastic Gradient Boosting 

1000 samples
  11 predictor
   2 classes: &#39;Female&#39;, &#39;Male&#39; 

No pre-processing
Resampling: Cross-Validated (10 fold) 
Summary of sample sizes: 899, 900, 900, 899, 899, 901, ... 
Resampling results across tuning parameters:

  shrinkage  interaction.depth  n.minobsinnode  n.trees  ROC     Sens    Spec    
  0.01       1                   1              1        0.6821  1.0000  0.000000
  0.01       1                   1              2        0.6882  1.0000  0.000000
  0.01       1                   1              3        0.6936  1.0000  0.000000
  .
  .
  .
  0.01       5                   8              2        0.7127  1.0000  0.000000
  0.01       5                   8              3        0.7148  1.0000  0.000000
  0.01       5                   8              4        0.7096  1.0000  0.000000
  0.01       5                   8              5        0.7100  1.0000  0.000000
  0.01       5                   9              1        0.7006  1.0000  0.000000
  0.01       5                   9              2        0.7055  1.0000  0.000000
 [ reached getOption(&quot;max.print&quot;) -- omitted 358 rows ]

ROC was used to select the optimal model using the largest value.
The final values used for the model were n.trees = 4, interaction.depth = 3, shrinkage
 = 0.01 and n.minobsinnode = 6.</code></pre></div>
<p>The results show that the tuning parameter settings that lead to the best ROC are <code>n.trees = 4</code> (number of trees), <code>interaction.depth = 3</code> (depth of tree), <code>shrinkage = 0.01</code> (learning rate) and <code>n.minobsinnode = 6</code> (minimum number of observations in each node).</p>
<p>Now, let’s compare the results from the three tree models.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">treebagRoc &lt;-<span class="st"> </span><span class="kw">roc</span>(<span class="dt">response =</span> bagTune<span class="op">$</span>pred<span class="op">$</span>obs,
                        <span class="dt">predictor =</span> bagTune<span class="op">$</span>pred<span class="op">$</span>Female,
                        <span class="dt">levels =</span> <span class="kw">rev</span>(<span class="kw">levels</span>(bagTune<span class="op">$</span>pred<span class="op">$</span>obs)))

rfRoc &lt;-<span class="st"> </span><span class="kw">roc</span>(<span class="dt">response =</span> rfTune<span class="op">$</span>pred<span class="op">$</span>obs,
             <span class="dt">predictor =</span> rfTune<span class="op">$</span>pred<span class="op">$</span>Female,
             <span class="dt">levels =</span> <span class="kw">rev</span>(<span class="kw">levels</span>(rfTune<span class="op">$</span>pred<span class="op">$</span>obs)))

gbmRoc &lt;-<span class="st"> </span><span class="kw">roc</span>(<span class="dt">response =</span> gbmTune<span class="op">$</span>pred<span class="op">$</span>obs,
              <span class="dt">predictor =</span> gbmTune<span class="op">$</span>pred<span class="op">$</span>Female,
              <span class="dt">levels =</span> <span class="kw">rev</span>(<span class="kw">levels</span>(gbmTune<span class="op">$</span>pred<span class="op">$</span>obs)))

<span class="kw">plot</span>(rpartRoc, <span class="dt">type =</span> <span class="st">&quot;s&quot;</span>, <span class="dt">print.thres =</span> <span class="kw">c</span>(.<span class="dv">5</span>),
     <span class="dt">print.thres.pch =</span> <span class="dv">16</span>,
     <span class="dt">print.thres.pattern =</span> <span class="st">&quot;&quot;</span>,
     <span class="dt">print.thres.cex =</span> <span class="fl">1.2</span>,
     <span class="dt">col =</span> <span class="st">&quot;black&quot;</span>, <span class="dt">legacy.axes =</span> <span class="ot">TRUE</span>,
     <span class="dt">print.thres.col =</span> <span class="st">&quot;black&quot;</span>)

<span class="kw">plot</span>(treebagRoc, <span class="dt">type =</span> <span class="st">&quot;s&quot;</span>, <span class="dt">add =</span> <span class="ot">TRUE</span>, <span class="dt">print.thres =</span> <span class="kw">c</span>(.<span class="dv">5</span>), 
     <span class="dt">print.thres.pch =</span> <span class="dv">3</span>, <span class="dt">legacy.axes =</span> <span class="ot">TRUE</span>, <span class="dt">print.thres.pattern =</span> <span class="st">&quot;&quot;</span>, 
     <span class="dt">print.thres.cex =</span> <span class="fl">1.2</span>,
     <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>, <span class="dt">print.thres.col =</span> <span class="st">&quot;red&quot;</span>)

<span class="kw">plot</span>(rfRoc, <span class="dt">type =</span> <span class="st">&quot;s&quot;</span>, <span class="dt">add =</span> <span class="ot">TRUE</span>, <span class="dt">print.thres =</span> <span class="kw">c</span>(.<span class="dv">5</span>), 
     <span class="dt">print.thres.pch =</span> <span class="dv">1</span>, <span class="dt">legacy.axes =</span> <span class="ot">TRUE</span>, <span class="dt">print.thres.pattern =</span> <span class="st">&quot;&quot;</span>, 
     <span class="dt">print.thres.cex =</span> <span class="fl">1.2</span>,
     <span class="dt">col =</span> <span class="st">&quot;green&quot;</span>, <span class="dt">print.thres.col =</span> <span class="st">&quot;green&quot;</span>)

<span class="kw">plot</span>(gbmRoc, <span class="dt">type =</span> <span class="st">&quot;s&quot;</span>, <span class="dt">add =</span> <span class="ot">TRUE</span>, <span class="dt">print.thres =</span> <span class="kw">c</span>(.<span class="dv">5</span>), 
     <span class="dt">print.thres.pch =</span> <span class="dv">10</span>, <span class="dt">legacy.axes =</span> <span class="ot">TRUE</span>, <span class="dt">print.thres.pattern =</span> <span class="st">&quot;&quot;</span>, 
     <span class="dt">print.thres.cex =</span> <span class="fl">1.2</span>,
     <span class="dt">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="dt">print.thres.col =</span> <span class="st">&quot;blue&quot;</span>)

<span class="kw">legend</span>(<span class="fl">0.2</span>, <span class="fl">0.5</span>, <span class="dt">cex =</span> <span class="fl">0.8</span>,
       <span class="kw">c</span>(<span class="st">&quot;Single Tree&quot;</span>, <span class="st">&quot;Bagged Tree&quot;</span>, <span class="st">&quot;Random Forest&quot;</span>, <span class="st">&quot;Boosted Tree&quot;</span>),
       <span class="dt">lwd =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>),
       <span class="dt">col =</span> <span class="kw">c</span>(<span class="st">&quot;black&quot;</span>, <span class="st">&quot;red&quot;</span>, <span class="st">&quot;green&quot;</span>, <span class="st">&quot;blue&quot;</span>),
       <span class="dt">pch =</span> <span class="kw">c</span>(<span class="dv">16</span>, <span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">10</span>))</code></pre></div>
<p><img src="IDS_files/figure-html/unnamed-chunk-103-1.svg" width="672" /></p>
<p>Since the data here doesn’t have many variables, we don’t see a significant difference among the models. But you can still see those ensemble methods are better than a single tree. In most of the real applications, ensemble methods perform much better. Random forest and boosting trees can be a baseline model. Before exploring different models, you can quickly run a random forest to see the performance and then try to improve that performance. If the performance you got from the random forest is not too much better than guessing, you should consider collecting more data or reviewing the problem to frame it a different way instead of trying other models. Because it usually means the current data is not enough to solve the problem.</p>

</div>
</div>
<!-- </div> -->
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Valiant1984">
<p>L, Valiant. 1984. “A Theory of the Learnable.” <em>Communications of the ACM</em> 27: 1134–42.</p>
</div>
<div id="ref-KV1989">
<p>M, Kearns, and Valiant L. 1989. “Cryptographic Limitations on Learning Boolean Formulae and Finite Automata.”</p>
</div>
<div id="ref-dudoit2002">
<p>Dudoit S, Fridlyand J, and Speed T. 2002. “Comparison of Discrimination Meth- Ods for the Classification of Tumors Using Gene Expression Data.” <em>Journal of the American Statistical Association</em> 97 (457): 77–87.</p>
</div>
<div id="ref-bendor2000">
<p>al, BenDor A et. 2000. “Tissue Classification with Gene Expression Profiles.” <em>Journal of Computational Biology</em> 7 (3): 559–83.</p>
</div>
<div id="ref-Varmuza2003">
<p>Varmuza K, He P, and Fang K. 2003. “Boosting Applied to Classification of Mass Spectral Data.” <em>Journal of Data Science</em> 1 (391–404).</p>
</div>
<div id="ref-Bergstra2006">
<p>al, Bergstra J et. 2006. “Aggregate Features and Adaboost for Music Classification.” <em>Machine Learning</em> 65: 473–84.</p>
</div>
<div id="ref-Schapire1999">
<p>YFR, Schapire. 1999. “Adaptive Game Playing Using Multiplicative Weights.” <em>Games and Economic Behavior</em> 29: 79–103.</p>
</div>
<div id="ref-Friedman2000">
<p>Friedman J, Hastie T, and Tibshirani R. 2000. “Additive Logistic Regression: A Statistical View of Boosting.” <em>Annals of Statistics</em> 38: 337–74.</p>
</div>
<div id="ref-Freund1997">
<p>Freund, Y., and R. Schapire. 1997. “A Decision-Theoretic Generalization of Online Learning and an Application to Boosting.” <em>Journal of Computer and System Sciences</em> 55: 119–39.</p>
</div>
<div id="ref-Breiman1998">
<p>Breiman, Leo. 1998. “Arcing Classifiers.” <em>The Annals of Statistics</em> 26: 123–40.</p>
</div>
<div id="ref-Bauer1999">
<p>E, Bauer, and Kohavi R. 1999. “An Empirical Comparison of Voting Classifica- Tion Algorithms: Bagging, Boosting, and Variants.” <em>Machine Learning</em> 36: 105–42.</p>
</div>
<div id="ref-Hastie2008">
<p>Hastie T, Friedman J, Tibshirani R. 2008. <em>The Elements of Statistical Learning: Data Mining, Inference and Prediction</em>. 2nd ed. Springer.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="random-forest.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="neural-network.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/lunr.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/happyrabbit/IntroDataScience/11-tree.Rmd",
"text": "Edit"
},
"download": ["IDS.pdf", "IDS.epub", "IDS.mobi"],
"toc": {
"collapse": "section",
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
