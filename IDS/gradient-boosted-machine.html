<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>11.7 Gradient Boosted Machine | Practitioner’s Guide to Data Science</title>
  <meta name="description" content="Introduction to Data Science" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="11.7 Gradient Boosted Machine | Practitioner’s Guide to Data Science" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="http://scientistcafe.com/IDS/" />
  
  <meta property="og:description" content="Introduction to Data Science" />
  <meta name="github-repo" content="happyrabbit/IntroDataScience" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="11.7 Gradient Boosted Machine | Practitioner’s Guide to Data Science" />
  
  <meta name="twitter:description" content="Introduction to Data Science" />
  

<meta name="author" content="Hui Lin and Ming Li" />


<meta name="date" content="2023-04-18" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="random-forest.html"/>
<link rel="next" href="deeplearning.html"/>
<script src="libs/header-attrs/header-attrs.js"></script>
<script src="libs/jquery/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="http://scientistcafe.com">Homepage</a></li>

<li class="divider"></li>
<li><a href="index.html#preface" id="toc-preface">Preface</a>
<ul>
<li><a href="goal-of-the-book.html#goal-of-the-book" id="toc-goal-of-the-book">Goal of the Book</a></li>
<li><a href="what-this-book-covers.html#what-this-book-covers" id="toc-what-this-book-covers">What This Book Covers</a></li>
<li><a href="who-this-book-is-for.html#who-this-book-is-for" id="toc-who-this-book-is-for">Who This Book Is For</a></li>
<li><a href="how-to-use-this-book.html#how-to-use-this-book" id="toc-how-to-use-this-book">How to Use This Book</a>
<ul>
<li><a href="how-to-use-this-book.html#what-the-book-assumes" id="toc-what-the-book-assumes">What the Book Assumes</a></li>
<li><a href="how-to-use-this-book.html#how-to-run-r-and-python-code" id="toc-how-to-run-r-and-python-code">How to Run R and Python Code</a></li>
</ul></li>
<li><a href="complementary-reading.html#complementary-reading" id="toc-complementary-reading">Complementary Reading</a></li>
</ul></li>
<li><a href="about-the-authors.html#about-the-authors" id="toc-about-the-authors">About the Authors</a></li>
<li><a href="introduction.html#introduction" id="toc-introduction"><span class="toc-section-number">1</span> Introduction</a>
<ul>
<li><a href="a-brief-history-of-data-science.html#a-brief-history-of-data-science" id="toc-a-brief-history-of-data-science"><span class="toc-section-number">1.1</span> A Brief History of Data Science</a></li>
<li><a href="data-science-role-and-skill-tracks.html#data-science-role-and-skill-tracks" id="toc-data-science-role-and-skill-tracks"><span class="toc-section-number">1.2</span> Data Science Role and Skill Tracks</a>
<ul>
<li><a href="data-science-role-and-skill-tracks.html#engineering" id="toc-engineering"><span class="toc-section-number">1.2.1</span> Engineering</a></li>
<li><a href="data-science-role-and-skill-tracks.html#analysis" id="toc-analysis"><span class="toc-section-number">1.2.2</span> Analysis</a></li>
<li><a href="data-science-role-and-skill-tracks.html#modelinginference" id="toc-modelinginference"><span class="toc-section-number">1.2.3</span> Modeling/Inference</a></li>
</ul></li>
<li><a href="what-kind-of-questions-can-data-science-solve.html#what-kind-of-questions-can-data-science-solve" id="toc-what-kind-of-questions-can-data-science-solve"><span class="toc-section-number">1.3</span> What Kind of Questions Can Data Science Solve?</a>
<ul>
<li><a href="what-kind-of-questions-can-data-science-solve.html#prerequisites" id="toc-prerequisites"><span class="toc-section-number">1.3.1</span> Prerequisites</a></li>
<li><a href="what-kind-of-questions-can-data-science-solve.html#problem-type" id="toc-problem-type"><span class="toc-section-number">1.3.2</span> Problem Type</a></li>
</ul></li>
<li><a href="structure-of-data-science-team.html#structure-of-data-science-team" id="toc-structure-of-data-science-team"><span class="toc-section-number">1.4</span> Structure of Data Science Team</a></li>
<li><a href="data-science-roles.html#data-science-roles" id="toc-data-science-roles"><span class="toc-section-number">1.5</span> Data Science Roles</a></li>
</ul></li>
<li><a href="SoftSkillsforDataScientists.html#SoftSkillsforDataScientists" id="toc-SoftSkillsforDataScientists"><span class="toc-section-number">2</span> Soft Skills for Data Scientists</a>
<ul>
<li><a href="comparison-between-statistician-and-data-scientist.html#comparison-between-statistician-and-data-scientist" id="toc-comparison-between-statistician-and-data-scientist"><span class="toc-section-number">2.1</span> Comparison between Statistician and Data Scientist</a></li>
<li><a href="beyond-data-and-analytics.html#beyond-data-and-analytics" id="toc-beyond-data-and-analytics"><span class="toc-section-number">2.2</span> Beyond Data and Analytics</a></li>
<li><a href="three-pillars-of-knowledge.html#three-pillars-of-knowledge" id="toc-three-pillars-of-knowledge"><span class="toc-section-number">2.3</span> Three Pillars of Knowledge</a></li>
<li><a href="data-science-project-cycle.html#data-science-project-cycle" id="toc-data-science-project-cycle"><span class="toc-section-number">2.4</span> Data Science Project Cycle</a>
<ul>
<li><a href="data-science-project-cycle.html#types-of-data-science-projects" id="toc-types-of-data-science-projects"><span class="toc-section-number">2.4.1</span> Types of Data Science Projects</a></li>
<li><a href="data-science-project-cycle.html#ProblemFormulationandProjectPlanningStage" id="toc-ProblemFormulationandProjectPlanningStage"><span class="toc-section-number">2.4.2</span> Problem Formulation and Project Planning Stage</a></li>
<li><a href="data-science-project-cycle.html#project-modeling-stage" id="toc-project-modeling-stage"><span class="toc-section-number">2.4.3</span> Project Modeling Stage</a></li>
<li><a href="data-science-project-cycle.html#ModelImplementationandPostProductionStage" id="toc-ModelImplementationandPostProductionStage"><span class="toc-section-number">2.4.4</span> Model Implementation and Post Production Stage</a></li>
<li><a href="data-science-project-cycle.html#ProjectCycleSummary" id="toc-ProjectCycleSummary"><span class="toc-section-number">2.4.5</span> Project Cycle Summary</a></li>
</ul></li>
<li><a href="common-mistakes-in-data-science.html#common-mistakes-in-data-science" id="toc-common-mistakes-in-data-science"><span class="toc-section-number">2.5</span> Common Mistakes in Data Science</a>
<ul>
<li><a href="common-mistakes-in-data-science.html#problem-formulation-stage" id="toc-problem-formulation-stage"><span class="toc-section-number">2.5.1</span> Problem Formulation Stage</a></li>
<li><a href="common-mistakes-in-data-science.html#project-planning-stage" id="toc-project-planning-stage"><span class="toc-section-number">2.5.2</span> Project Planning Stage</a></li>
<li><a href="common-mistakes-in-data-science.html#project-modeling-stage-1" id="toc-project-modeling-stage-1"><span class="toc-section-number">2.5.3</span> Project Modeling Stage</a></li>
<li><a href="common-mistakes-in-data-science.html#model-implementation-and-post-production-stage" id="toc-model-implementation-and-post-production-stage"><span class="toc-section-number">2.5.4</span> Model Implementation and Post Production Stage</a></li>
<li><a href="common-mistakes-in-data-science.html#summary-of-common-mistakes" id="toc-summary-of-common-mistakes"><span class="toc-section-number">2.5.5</span> Summary of Common Mistakes</a></li>
</ul></li>
</ul></li>
<li><a href="introduction-to-the-data.html#introduction-to-the-data" id="toc-introduction-to-the-data"><span class="toc-section-number">3</span> Introduction to the Data</a>
<ul>
<li><a href="customer-data-for-a-clothing-company.html#customer-data-for-a-clothing-company" id="toc-customer-data-for-a-clothing-company"><span class="toc-section-number">3.1</span> Customer Data for a Clothing Company</a></li>
<li><a href="swinediseasedata.html#swinediseasedata" id="toc-swinediseasedata"><span class="toc-section-number">3.2</span> Swine Disease Breakout Data</a></li>
<li><a href="mnist-dataset.html#mnist-dataset" id="toc-mnist-dataset"><span class="toc-section-number">3.3</span> MNIST Dataset</a></li>
<li><a href="imdb-dataset.html#imdb-dataset" id="toc-imdb-dataset"><span class="toc-section-number">3.4</span> IMDB Dataset</a></li>
</ul></li>
<li><a href="bigdatacloudplatform.html#bigdatacloudplatform" id="toc-bigdatacloudplatform"><span class="toc-section-number">4</span> Big Data Cloud Platform</a>
<ul>
<li><a href="power-of-cluster-of-computers.html#power-of-cluster-of-computers" id="toc-power-of-cluster-of-computers"><span class="toc-section-number">4.1</span> Power of Cluster of Computers</a></li>
<li><a href="evolution-of-cluster-computing.html#evolution-of-cluster-computing" id="toc-evolution-of-cluster-computing"><span class="toc-section-number">4.2</span> Evolution of Cluster Computing</a>
<ul>
<li><a href="evolution-of-cluster-computing.html#hadoop" id="toc-hadoop"><span class="toc-section-number">4.2.1</span> Hadoop</a></li>
<li><a href="evolution-of-cluster-computing.html#spark" id="toc-spark"><span class="toc-section-number">4.2.2</span> Spark</a></li>
</ul></li>
<li><a href="CloudEnvironment.html#CloudEnvironment" id="toc-CloudEnvironment"><span class="toc-section-number">4.3</span> Introduction of Cloud Environment</a>
<ul>
<li><a href="CloudEnvironment.html#open-account-and-create-a-cluster" id="toc-open-account-and-create-a-cluster"><span class="toc-section-number">4.3.1</span> Open Account and Create a Cluster</a></li>
<li><a href="CloudEnvironment.html#r-notebook" id="toc-r-notebook"><span class="toc-section-number">4.3.2</span> R Notebook</a></li>
<li><a href="CloudEnvironment.html#markdown-cells" id="toc-markdown-cells"><span class="toc-section-number">4.3.3</span> Markdown Cells</a></li>
</ul></li>
<li><a href="leveragesparkr.html#leveragesparkr" id="toc-leveragesparkr"><span class="toc-section-number">4.4</span> Leverage Spark Using R Notebook</a></li>
<li><a href="databases-and-sql.html#databases-and-sql" id="toc-databases-and-sql"><span class="toc-section-number">4.5</span> Databases and SQL</a>
<ul>
<li><a href="databases-and-sql.html#history" id="toc-history"><span class="toc-section-number">4.5.1</span> History</a></li>
<li><a href="databases-and-sql.html#database-table-and-view" id="toc-database-table-and-view"><span class="toc-section-number">4.5.2</span> Database, Table and View</a></li>
<li><a href="databases-and-sql.html#basic-sql-statement" id="toc-basic-sql-statement"><span class="toc-section-number">4.5.3</span> Basic SQL Statement</a></li>
<li><a href="databases-and-sql.html#advanced-topics-in-database" id="toc-advanced-topics-in-database"><span class="toc-section-number">4.5.4</span> Advanced Topics in Database</a></li>
</ul></li>
</ul></li>
<li><a href="datapreprocessing.html#datapreprocessing" id="toc-datapreprocessing"><span class="toc-section-number">5</span> Data Pre-processing</a>
<ul>
<li><a href="data-cleaning.html#data-cleaning" id="toc-data-cleaning"><span class="toc-section-number">5.1</span> Data Cleaning</a></li>
<li><a href="missing-values.html#missing-values" id="toc-missing-values"><span class="toc-section-number">5.2</span> Missing Values</a>
<ul>
<li><a href="missing-values.html#impute-missing-values-with-medianmode" id="toc-impute-missing-values-with-medianmode"><span class="toc-section-number">5.2.1</span> Impute missing values with median/mode</a></li>
<li><a href="missing-values.html#k-nearest-neighbors" id="toc-k-nearest-neighbors"><span class="toc-section-number">5.2.2</span> K-nearest neighbors</a></li>
<li><a href="missing-values.html#bagging-tree" id="toc-bagging-tree"><span class="toc-section-number">5.2.3</span> Bagging Tree</a></li>
</ul></li>
<li><a href="centering-and-scaling.html#centering-and-scaling" id="toc-centering-and-scaling"><span class="toc-section-number">5.3</span> Centering and Scaling</a></li>
<li><a href="resolve-skewness.html#resolve-skewness" id="toc-resolve-skewness"><span class="toc-section-number">5.4</span> Resolve Skewness</a></li>
<li><a href="outliers.html#outliers" id="toc-outliers"><span class="toc-section-number">5.5</span> Resolve Outliers</a></li>
<li><a href="collinearity.html#collinearity" id="toc-collinearity"><span class="toc-section-number">5.6</span> Collinearity</a></li>
<li><a href="sparse-variables.html#sparse-variables" id="toc-sparse-variables"><span class="toc-section-number">5.7</span> Sparse Variables</a></li>
<li><a href="re-encode-dummy-variables.html#re-encode-dummy-variables" id="toc-re-encode-dummy-variables"><span class="toc-section-number">5.8</span> Re-encode Dummy Variables</a></li>
</ul></li>
<li><a href="datawrangline.html#datawrangline" id="toc-datawrangline"><span class="toc-section-number">6</span> Data Wrangling</a>
<ul>
<li><a href="summarize-data.html#summarize-data" id="toc-summarize-data"><span class="toc-section-number">6.1</span> Summarize Data</a>
<ul>
<li><a href="summarize-data.html#dplyr-package" id="toc-dplyr-package"><span class="toc-section-number">6.1.1</span> <code>dplyr</code> package</a></li>
<li><a href="summarize-data.html#applyfamilyinbaser" id="toc-applyfamilyinbaser"><span class="toc-section-number">6.1.2</span> <code>apply()</code>, <code>lapply()</code> and <code>sapply()</code> in base R</a></li>
</ul></li>
<li><a href="tidy-and-reshape-data.html#tidy-and-reshape-data" id="toc-tidy-and-reshape-data"><span class="toc-section-number">6.2</span> Tidy and Reshape Data</a></li>
</ul></li>
<li><a href="modeltuningstrategy.html#modeltuningstrategy" id="toc-modeltuningstrategy"><span class="toc-section-number">7</span> Model Tuning Strategy</a>
<ul>
<li><a href="vbtradeoff.html#vbtradeoff" id="toc-vbtradeoff"><span class="toc-section-number">7.1</span> Variance-Bias Trade-Off</a></li>
<li><a href="datasplittingresampling.html#datasplittingresampling" id="toc-datasplittingresampling"><span class="toc-section-number">7.2</span> Data Splitting and Resampling</a>
<ul>
<li><a href="datasplittingresampling.html#datasplitting" id="toc-datasplitting"><span class="toc-section-number">7.2.1</span> Data Splitting</a></li>
<li><a href="datasplittingresampling.html#resampling" id="toc-resampling"><span class="toc-section-number">7.2.2</span> Resampling</a></li>
</ul></li>
</ul></li>
<li><a href="measuring-performance.html#measuring-performance" id="toc-measuring-performance"><span class="toc-section-number">8</span> Measuring Performance</a>
<ul>
<li><a href="regression-model-performance.html#regression-model-performance" id="toc-regression-model-performance"><span class="toc-section-number">8.1</span> Regression Model Performance</a></li>
<li><a href="classification-model-performance.html#classification-model-performance" id="toc-classification-model-performance"><span class="toc-section-number">8.2</span> Classification Model Performance</a>
<ul>
<li><a href="classification-model-performance.html#confusion-matrix" id="toc-confusion-matrix"><span class="toc-section-number">8.2.1</span> Confusion Matrix</a></li>
<li><a href="classification-model-performance.html#kappa-statistic" id="toc-kappa-statistic"><span class="toc-section-number">8.2.2</span> Kappa Statistic</a></li>
<li><a href="classification-model-performance.html#roc" id="toc-roc"><span class="toc-section-number">8.2.3</span> ROC</a></li>
<li><a href="classification-model-performance.html#gain-and-lift-charts" id="toc-gain-and-lift-charts"><span class="toc-section-number">8.2.4</span> Gain and Lift Charts</a></li>
</ul></li>
</ul></li>
<li><a href="regression-models.html#regression-models" id="toc-regression-models"><span class="toc-section-number">9</span> Regression Models</a>
<ul>
<li><a href="ordinary-least-square.html#ordinary-least-square" id="toc-ordinary-least-square"><span class="toc-section-number">9.1</span> Ordinary Least Square</a>
<ul>
<li><a href="ordinary-least-square.html#the-magic-p-value" id="toc-the-magic-p-value"><span class="toc-section-number">9.1.1</span> The Magic P-value</a></li>
<li><a href="ordinary-least-square.html#diagnostics-for-linear-regression" id="toc-diagnostics-for-linear-regression"><span class="toc-section-number">9.1.2</span> Diagnostics for Linear Regression</a></li>
</ul></li>
<li><a href="principal-component-regression-and-partial-least-square.html#principal-component-regression-and-partial-least-square" id="toc-principal-component-regression-and-partial-least-square"><span class="toc-section-number">9.2</span> Principal Component Regression and Partial Least Square</a></li>
</ul></li>
<li><a href="regularization-methods.html#regularization-methods" id="toc-regularization-methods"><span class="toc-section-number">10</span> Regularization Methods</a>
<ul>
<li><a href="ridge-regression.html#ridge-regression" id="toc-ridge-regression"><span class="toc-section-number">10.1</span> Ridge Regression</a></li>
<li><a href="lasso.html#lasso" id="toc-lasso"><span class="toc-section-number">10.2</span> LASSO</a></li>
<li><a href="elastic-net.html#elastic-net" id="toc-elastic-net"><span class="toc-section-number">10.3</span> Elastic Net</a></li>
<li><a href="penalized-generalized-linear-model.html#penalized-generalized-linear-model" id="toc-penalized-generalized-linear-model"><span class="toc-section-number">10.4</span> Penalized Generalized Linear Model</a>
<ul>
<li><a href="penalized-generalized-linear-model.html#introduction-to-glmnet-package" id="toc-introduction-to-glmnet-package"><span class="toc-section-number">10.4.1</span> Introduction to <code>glmnet</code> package</a></li>
<li><a href="penalized-generalized-linear-model.html#penalized-logistic-regression" id="toc-penalized-logistic-regression"><span class="toc-section-number">10.4.2</span> Penalized logistic regression</a></li>
</ul></li>
</ul></li>
<li><a href="treemodel.html#treemodel" id="toc-treemodel"><span class="toc-section-number">11</span> Tree-Based Methods</a>
<ul>
<li><a href="tree-basics.html#tree-basics" id="toc-tree-basics"><span class="toc-section-number">11.1</span> Tree Basics</a></li>
<li><a href="splitting-criteria.html#splitting-criteria" id="toc-splitting-criteria"><span class="toc-section-number">11.2</span> Splitting Criteria</a>
<ul>
<li><a href="splitting-criteria.html#gini-impurity" id="toc-gini-impurity"><span class="toc-section-number">11.2.1</span> Gini impurity</a></li>
<li><a href="splitting-criteria.html#information-gain-ig" id="toc-information-gain-ig"><span class="toc-section-number">11.2.2</span> Information Gain (IG)</a></li>
<li><a href="splitting-criteria.html#information-gain-ratio-igr" id="toc-information-gain-ratio-igr"><span class="toc-section-number">11.2.3</span> Information Gain Ratio (IGR)</a></li>
<li><a href="splitting-criteria.html#sum-of-squared-error-sse" id="toc-sum-of-squared-error-sse"><span class="toc-section-number">11.2.4</span> Sum of Squared Error (SSE)</a></li>
</ul></li>
<li><a href="tree-pruning.html#tree-pruning" id="toc-tree-pruning"><span class="toc-section-number">11.3</span> Tree Pruning</a></li>
<li><a href="regression-and-decision-tree-basic.html#regression-and-decision-tree-basic" id="toc-regression-and-decision-tree-basic"><span class="toc-section-number">11.4</span> Regression and Decision Tree Basic</a>
<ul>
<li><a href="regression-and-decision-tree-basic.html#regression-tree" id="toc-regression-tree"><span class="toc-section-number">11.4.1</span> Regression Tree</a></li>
<li><a href="regression-and-decision-tree-basic.html#decision-tree" id="toc-decision-tree"><span class="toc-section-number">11.4.2</span> Decision Tree</a></li>
</ul></li>
<li><a href="bagging-tree-1.html#bagging-tree-1" id="toc-bagging-tree-1"><span class="toc-section-number">11.5</span> Bagging Tree</a></li>
<li><a href="random-forest.html#random-forest" id="toc-random-forest"><span class="toc-section-number">11.6</span> Random Forest</a></li>
<li><a href="gradient-boosted-machine.html#gradient-boosted-machine" id="toc-gradient-boosted-machine"><span class="toc-section-number">11.7</span> Gradient Boosted Machine</a>
<ul>
<li><a href="gradient-boosted-machine.html#adaptive-boosting" id="toc-adaptive-boosting"><span class="toc-section-number">11.7.1</span> Adaptive Boosting</a></li>
<li><a href="gradient-boosted-machine.html#stochastic-gradient-boosting" id="toc-stochastic-gradient-boosting"><span class="toc-section-number">11.7.2</span> Stochastic Gradient Boosting</a></li>
</ul></li>
</ul></li>
<li><a href="deeplearning.html#deeplearning" id="toc-deeplearning"><span class="toc-section-number">12</span> Deep Learning</a>
<ul>
<li><a href="feedforward-neural-network.html#feedforward-neural-network" id="toc-feedforward-neural-network"><span class="toc-section-number">12.1</span> Feedforward Neural Network</a>
<ul>
<li><a href="feedforward-neural-network.html#logisticregasneuralnetwork" id="toc-logisticregasneuralnetwork"><span class="toc-section-number">12.1.1</span> Logistic Regression as Neural Network</a></li>
<li><a href="feedforward-neural-network.html#stochastic-gradient-descent" id="toc-stochastic-gradient-descent"><span class="toc-section-number">12.1.2</span> Stochastic Gradient Descent</a></li>
<li><a href="feedforward-neural-network.html#deepneuralnetwork" id="toc-deepneuralnetwork"><span class="toc-section-number">12.1.3</span> Deep Neural Network</a></li>
<li><a href="feedforward-neural-network.html#activationfunction" id="toc-activationfunction"><span class="toc-section-number">12.1.4</span> Activation Function</a></li>
<li><a href="feedforward-neural-network.html#optimization" id="toc-optimization"><span class="toc-section-number">12.1.5</span> Optimization</a></li>
<li><a href="feedforward-neural-network.html#deal-with-overfitting" id="toc-deal-with-overfitting"><span class="toc-section-number">12.1.6</span> Deal with Overfitting</a></li>
<li><a href="feedforward-neural-network.html#ffnnexample" id="toc-ffnnexample"><span class="toc-section-number">12.1.7</span> Image Recognition Using FFNN</a></li>
</ul></li>
<li><a href="convolutional-neural-network.html#convolutional-neural-network" id="toc-convolutional-neural-network"><span class="toc-section-number">12.2</span> Convolutional Neural Network</a>
<ul>
<li><a href="convolutional-neural-network.html#convolution-layer" id="toc-convolution-layer"><span class="toc-section-number">12.2.1</span> Convolution Layer</a></li>
<li><a href="convolutional-neural-network.html#padding-layer" id="toc-padding-layer"><span class="toc-section-number">12.2.2</span> Padding Layer</a></li>
<li><a href="convolutional-neural-network.html#pooling-layer" id="toc-pooling-layer"><span class="toc-section-number">12.2.3</span> Pooling Layer</a></li>
<li><a href="convolutional-neural-network.html#convolution-over-volume" id="toc-convolution-over-volume"><span class="toc-section-number">12.2.4</span> Convolution Over Volume</a></li>
<li><a href="convolutional-neural-network.html#cnnexample" id="toc-cnnexample"><span class="toc-section-number">12.2.5</span> Image Recognition Using CNN</a></li>
</ul></li>
<li><a href="recurrent-neural-network.html#recurrent-neural-network" id="toc-recurrent-neural-network"><span class="toc-section-number">12.3</span> Recurrent Neural Network</a>
<ul>
<li><a href="recurrent-neural-network.html#rnn-model" id="toc-rnn-model"><span class="toc-section-number">12.3.1</span> RNN Model</a></li>
<li><a href="recurrent-neural-network.html#lstm" id="toc-lstm"><span class="toc-section-number">12.3.2</span> Long Short Term Memory</a></li>
<li><a href="recurrent-neural-network.html#embedding" id="toc-embedding"><span class="toc-section-number">12.3.3</span> Word Embedding</a></li>
<li><a href="recurrent-neural-network.html#rnnexample" id="toc-rnnexample"><span class="toc-section-number">12.3.4</span> Sentiment Analysis Using RNN</a></li>
</ul></li>
</ul></li>
<li><a href="appendix.html#appendix" id="toc-appendix">Appendix</a></li>
<li><a href="largelocaldata.html#largelocaldata" id="toc-largelocaldata"><span class="toc-section-number">13</span> Handling Large Local Data</a>
<ul>
<li><a href="readr.html#readr" id="toc-readr"><span class="toc-section-number">13.1</span> <code>readr</code></a></li>
<li><a href="data.table-enhanced-data.html#data.table-enhanced-data.frame" id="toc-data.table-enhanced-data.frame"><span class="toc-section-number">13.2</span> <code>data.table</code>— enhanced <code>data.frame</code></a></li>
</ul></li>
<li><a href="r-code-for-data-simulation.html#r-code-for-data-simulation" id="toc-r-code-for-data-simulation"><span class="toc-section-number">14</span> R code for data simulation</a>
<ul>
<li><a href="appendixdata1.html#appendixdata1" id="toc-appendixdata1"><span class="toc-section-number">14.1</span> Customer Data for Clothing Company</a></li>
<li><a href="appendixdata3.html#appendixdata3" id="toc-appendixdata3"><span class="toc-section-number">14.2</span> Swine Disease Breakout Data</a></li>
</ul></li>
<li><a href="references.html#references" id="toc-references">References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Practitioner’s Guide to Data Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="gradient-boosted-machine" class="section level2 hasAnchor" number="11.7">
<h2><span class="header-section-number">11.7</span> Gradient Boosted Machine<a href="gradient-boosted-machine.html#gradient-boosted-machine" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Boosting models were developed in the 1980s <span class="citation">(<a href="#ref-Valiant1984" role="doc-biblioref">V. L 1984</a>; <a href="#ref-KV1989" role="doc-biblioref">K. M and L 1989</a>)</span> and were originally for classification problems. Due to the excellent model performance, they were widely used for a variety of applications, such as gene expression <span class="citation">(<a href="#ref-dudoit2002" role="doc-biblioref">Dudoit S and T 2002</a>; <a href="#ref-bendor2000" role="doc-biblioref">al 2000</a>)</span>, chemical substructure classification <span class="citation">(<a href="#ref-Varmuza2003" role="doc-biblioref">Varmuza K and K 2003</a>)</span>, music classification <span class="citation">(<a href="#ref-Bergstra2006" role="doc-biblioref">Bergstra et al. 2006</a>)</span>, etc. The first effective implementation of boosting is Adaptive Boosting (AdaBoost) algorithm came up by Yoav Freund and Robert Schapire in 1996 <span class="citation">(<a href="#ref-Schapire1999" role="doc-biblioref">YFR 1999</a>)</span>. After that, some researchers <span class="citation">(<a href="#ref-Friedman2000" role="doc-biblioref">Friedman, Hastie, and Tibshirani 2000</a>)</span> started to connect the boosting algorithm with some statistical concepts, such as loss function, additive model, logistic regression. Friedman pointed out that boosting can be considered as a forward stagewise additive model that minimizes exponential loss. The new view of boosting in a statistical framework enabled the method to be extended to regression problems.</p>
<p>The idea is to combine a group of weak learners (a classifier that is marginally better than random guess) to produce a strong learner. Like bagging, boosting is a general approach that can be applied to different learners. Here we focus on the decision tree. Recall that both bagging and random forest create multiple copies of the original training data using the bootstrap, fitting a separate decision tree to each copy and combining all the results to create a single prediction. Boosting also creates different trees but the trees are grown sequentially and each tree is a weak learner. Any modeling technique with tuning parameters can produce a range of learners, from weak to strong. You can easily make a weak learner by restricting the depth of the tree. There are different types of boosting. Here we introduce two main types: adaptive boosting and stochastic gradient boosting.</p>
<div id="adaptive-boosting" class="section level3 hasAnchor" number="11.7.1">
<h3><span class="header-section-number">11.7.1</span> Adaptive Boosting<a href="gradient-boosted-machine.html#adaptive-boosting" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Yoav Freund and Robert Schapire <span class="citation">(<a href="#ref-Freund1997" role="doc-biblioref">Freund and Schapire 1997</a>)</span> came up the AdaBoost.M1 algorithm. Consider a binary classification problem where the response variable has two categories <span class="math inline">\(Y \in \{-1, 1\}\)</span>. Given predictor matrix, <span class="math inline">\(X\)</span>, construct a classifier <span class="math inline">\(G(X)\)</span> that predicts <span class="math inline">\(1\)</span> or <span class="math inline">\(-1\)</span>. The corresponding error rate in the training set is:</p>
<p><span class="math display">\[\bar{err}=\frac{1}{N}\Sigma_{i=1}^NI(y_i\neq G(x_i))\]</span></p>
<p>The algorithm produces a series of classifiers <span class="math inline">\(G_m(x),\ m=1,2,...,M\)</span> from different iterations. In each iteration, it finds the best classifier based on the current weights. The misclassified samples in the <span class="math inline">\(m^{th}\)</span> iteration will have higher weights in the <span class="math inline">\((m+1)^{th}\)</span> iteration and the correctly classified samples will have lower weights. As it moves on, the algorithm will put more effort into the “difficult” samples until it can correctly classify them. So it requires the algorithm to change focus at each iteration. At each iteration, the algorithm will calculate a stage weight based on the error rate. The final prediction is a weighted average of all those weak classifiers using stage weights from all the iterations:</p>
<p><span class="math display">\[G(x)=sign ( \Sigma_{m=1}^M \alpha_{m}G_m(x))\]</span>
where <span class="math inline">\(\alpha_1,\alpha_2,...,\alpha_M\)</span> are the weights from different iterations.</p>
<p>Since the classifier <span class="math inline">\(G_m(x)\)</span> returns discrete value, the AdaBoost.M1 algorithm is known as “Discrete AdaBoost” <span class="citation">(<a href="#ref-Friedman2000" role="doc-biblioref">Friedman, Hastie, and Tibshirani 2000</a>)</span>. You can revise the above algorithm if it returns continuousf value, for example, a probability <span class="citation">(<a href="#ref-Friedman2000" role="doc-biblioref">Friedman, Hastie, and Tibshirani 2000</a>)</span>. As mentioned before, boosting is a general approach that can be applied to different learners. Since you can easily create weak learners by limiting the depth of the tree, the boosting tree is a common method. Since the classification tree is a low bias/high variance technique, ensemble decreases model variance and lead to low bias/low variance model. See Breinman <span class="citation">(<a href="#ref-Breiman1998" role="doc-biblioref">Leo Breiman 1998</a>)</span> for more explanation about why the boosting tree performs well in general. However, boosting can not significantly improve the low variance model. So applying boosting to K-Nearest Neighbor (KNN) doesn’t lead to as good improvement as applying boosting to statistical learning methods like naive Bayes <span class="citation">(<a href="#ref-Bauer1999" role="doc-biblioref">Bauer and Kohavi 1999</a>)</span>.</p>
</div>
<div id="stochastic-gradient-boosting" class="section level3 hasAnchor" number="11.7.2">
<h3><span class="header-section-number">11.7.2</span> Stochastic Gradient Boosting<a href="gradient-boosted-machine.html#stochastic-gradient-boosting" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>As mentioned before, Friedman <span class="citation">(<a href="#ref-Friedman2000" role="doc-biblioref">Friedman, Hastie, and Tibshirani 2000</a>)</span> provided a statistical framework for the AdaBoost algorithm and pointed out that boosting can be considered as a forward stagewise additive model that minimizes exponential loss. The framework led to some generalized algorithms such as Real AdaBoost, Gentle AdaBoost, and LogitBoost. Those algorithms later were unified under a framework called gradient boosting machine. The last section of the chapter illustrates how boosting can be considered as an additive model.</p>
<p>Consider a 2-class classification problem. You have the response <span class="math inline">\(y \in \{-1, 1\}\)</span> and the sample proportion of class 1 from the training set is <span class="math inline">\(p\)</span>. <span class="math inline">\(f(x)\)</span> is the model prediction in the range of <span class="math inline">\([-\infty, +\infty]\)</span> and the predicted event probability is <span class="math inline">\(\hat{p}=\frac{1}{1+exp[-f(x)]}\)</span>. The gradient boosting for this problem is as follows:</p>
<p>When using the tree as the base learner, basic gradient boosting has two tuning parameters: tree depth and the number of iterations. You can further customize the algorithm by selecting a different loss function and gradient <span class="citation">(<a href="#ref-Hastie2008" role="doc-biblioref">Hastie T 2008</a>)</span>. The final line of the loop includes a regularization strategy. Instead of adding <span class="math inline">\(f_i^{(j)}\)</span> to the previous iteration’s <span class="math inline">\(f_i\)</span>, only a fraction of the value is added. This fraction is called learning rate which is <span class="math inline">\(\lambda\)</span> in the algorithm. It can take values between 0 and 1 which is another tuning parameter of the model.</p>
<p>The way to calculate variable importance in boosting is similar to a bagging model. You get variable importance by combining measures of importance across the ensemble. For example, we can calculate the Gini index improvement for each variable across all trees and use the average as the measurement of the importance.</p>
<p>Boosting is a very popular method for classification. It is one of the methods that can be directly applied to the data without requiring a great deal of time-consuming data preprocessing. Applying boosting on tree models significantly improves predictive accuracy. Some advantages of trees that are sacrificed by boosting are speed and interpretability.</p>
<p>Let’s look at the R implementation.</p>
<div class="sourceCode" id="cb317"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb317-1"><a href="gradient-boosted-machine.html#cb317-1" aria-hidden="true" tabindex="-1"></a>gbmGrid <span class="ot">&lt;-</span> <span class="fu">expand.grid</span>(<span class="at">interaction.depth =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">5</span>, <span class="dv">7</span>, <span class="dv">9</span>),</span>
<span id="cb317-2"><a href="gradient-boosted-machine.html#cb317-2" aria-hidden="true" tabindex="-1"></a>                       <span class="at">n.trees =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>,</span>
<span id="cb317-3"><a href="gradient-boosted-machine.html#cb317-3" aria-hidden="true" tabindex="-1"></a>                       <span class="at">shrinkage =</span> <span class="fu">c</span>(.<span class="dv">01</span>, .<span class="dv">1</span>),</span>
<span id="cb317-4"><a href="gradient-boosted-machine.html#cb317-4" aria-hidden="true" tabindex="-1"></a>                       <span class="at">n.minobsinnode =</span> <span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>))</span>
<span id="cb317-5"><a href="gradient-boosted-machine.html#cb317-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb317-6"><a href="gradient-boosted-machine.html#cb317-6" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">100</span>)</span>
<span id="cb317-7"><a href="gradient-boosted-machine.html#cb317-7" aria-hidden="true" tabindex="-1"></a>gbmTune <span class="ot">&lt;-</span> caret<span class="sc">::</span><span class="fu">train</span>(<span class="at">x =</span> trainx, </span>
<span id="cb317-8"><a href="gradient-boosted-machine.html#cb317-8" aria-hidden="true" tabindex="-1"></a>                <span class="at">y =</span> trainy,</span>
<span id="cb317-9"><a href="gradient-boosted-machine.html#cb317-9" aria-hidden="true" tabindex="-1"></a>                <span class="at">method =</span> <span class="st">&quot;gbm&quot;</span>,</span>
<span id="cb317-10"><a href="gradient-boosted-machine.html#cb317-10" aria-hidden="true" tabindex="-1"></a>                <span class="at">tuneGrid =</span> gbmGrid,</span>
<span id="cb317-11"><a href="gradient-boosted-machine.html#cb317-11" aria-hidden="true" tabindex="-1"></a>                <span class="at">metric =</span> <span class="st">&quot;ROC&quot;</span>,</span>
<span id="cb317-12"><a href="gradient-boosted-machine.html#cb317-12" aria-hidden="true" tabindex="-1"></a>                <span class="at">verbose =</span> <span class="cn">FALSE</span>,</span>
<span id="cb317-13"><a href="gradient-boosted-machine.html#cb317-13" aria-hidden="true" tabindex="-1"></a>                <span class="at">trControl =</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">&quot;cv&quot;</span>,</span>
<span id="cb317-14"><a href="gradient-boosted-machine.html#cb317-14" aria-hidden="true" tabindex="-1"></a>                           <span class="at">classProbs =</span> <span class="cn">TRUE</span>,</span>
<span id="cb317-15"><a href="gradient-boosted-machine.html#cb317-15" aria-hidden="true" tabindex="-1"></a>                           <span class="at">savePredictions =</span> <span class="cn">TRUE</span>))</span></code></pre></div>
<div class="sourceCode" id="cb318"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb318-1"><a href="gradient-boosted-machine.html#cb318-1" aria-hidden="true" tabindex="-1"></a><span class="co"># only show part of the output</span></span>
<span id="cb318-2"><a href="gradient-boosted-machine.html#cb318-2" aria-hidden="true" tabindex="-1"></a>gbmTune</span></code></pre></div>
<div class="sourceCode" id="cb319"><pre class="sourceCode html"><code class="sourceCode html"><span id="cb319-1"><a href="gradient-boosted-machine.html#cb319-1" aria-hidden="true" tabindex="-1"></a>Stochastic Gradient Boosting </span>
<span id="cb319-2"><a href="gradient-boosted-machine.html#cb319-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb319-3"><a href="gradient-boosted-machine.html#cb319-3" aria-hidden="true" tabindex="-1"></a>1000 samples</span>
<span id="cb319-4"><a href="gradient-boosted-machine.html#cb319-4" aria-hidden="true" tabindex="-1"></a>  11 predictor</span>
<span id="cb319-5"><a href="gradient-boosted-machine.html#cb319-5" aria-hidden="true" tabindex="-1"></a>   2 classes: &#39;Female&#39;, &#39;Male&#39; </span>
<span id="cb319-6"><a href="gradient-boosted-machine.html#cb319-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb319-7"><a href="gradient-boosted-machine.html#cb319-7" aria-hidden="true" tabindex="-1"></a>No pre-processing</span>
<span id="cb319-8"><a href="gradient-boosted-machine.html#cb319-8" aria-hidden="true" tabindex="-1"></a>Resampling: Cross-Validated (10 fold) </span>
<span id="cb319-9"><a href="gradient-boosted-machine.html#cb319-9" aria-hidden="true" tabindex="-1"></a>Summary of sample sizes: 899, 900, 900, 899, 899, 901, ... </span>
<span id="cb319-10"><a href="gradient-boosted-machine.html#cb319-10" aria-hidden="true" tabindex="-1"></a>Resampling results across tuning parameters:</span>
<span id="cb319-11"><a href="gradient-boosted-machine.html#cb319-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb319-12"><a href="gradient-boosted-machine.html#cb319-12" aria-hidden="true" tabindex="-1"></a>shrinkage interaction.depth n.minobsinnode n.trees ROC    Sens Spec    </span>
<span id="cb319-13"><a href="gradient-boosted-machine.html#cb319-13" aria-hidden="true" tabindex="-1"></a>0.01      1                 1              1       0.6821 1.00 0.00</span>
<span id="cb319-14"><a href="gradient-boosted-machine.html#cb319-14" aria-hidden="true" tabindex="-1"></a>0.01      1                 1              2       0.6882 1.00 0.00</span>
<span id="cb319-15"><a href="gradient-boosted-machine.html#cb319-15" aria-hidden="true" tabindex="-1"></a>  .</span>
<span id="cb319-16"><a href="gradient-boosted-machine.html#cb319-16" aria-hidden="true" tabindex="-1"></a>  .</span>
<span id="cb319-17"><a href="gradient-boosted-machine.html#cb319-17" aria-hidden="true" tabindex="-1"></a>  .</span>
<span id="cb319-18"><a href="gradient-boosted-machine.html#cb319-18" aria-hidden="true" tabindex="-1"></a>0.01      5                 8              4       0.7096 1.00 0.00</span>
<span id="cb319-19"><a href="gradient-boosted-machine.html#cb319-19" aria-hidden="true" tabindex="-1"></a>0.01      5                 8              5       0.7100 1.00 0.00</span>
<span id="cb319-20"><a href="gradient-boosted-machine.html#cb319-20" aria-hidden="true" tabindex="-1"></a>0.01      5                 9              1       0.7006 1.00 0.00</span>
<span id="cb319-21"><a href="gradient-boosted-machine.html#cb319-21" aria-hidden="true" tabindex="-1"></a>0.01      5                 9              2       0.7055 1.00 0.00</span>
<span id="cb319-22"><a href="gradient-boosted-machine.html#cb319-22" aria-hidden="true" tabindex="-1"></a> [ reached getOption(&quot;max.print&quot;) -- omitted 358 rows ]</span>
<span id="cb319-23"><a href="gradient-boosted-machine.html#cb319-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb319-24"><a href="gradient-boosted-machine.html#cb319-24" aria-hidden="true" tabindex="-1"></a>ROC was used to select the optimal model using the largest value.</span>
<span id="cb319-25"><a href="gradient-boosted-machine.html#cb319-25" aria-hidden="true" tabindex="-1"></a>The final values used for the model were n.trees = 4, </span>
<span id="cb319-26"><a href="gradient-boosted-machine.html#cb319-26" aria-hidden="true" tabindex="-1"></a>interaction.depth = 3, shrinkage = 0.01 and n.minobsinnode = 6.</span></code></pre></div>
<p>The results show that the tuning parameter settings that lead to the best ROC are <code>n.trees = 4</code> (number of trees), <code>interaction.depth = 3</code> (depth of tree), <code>shrinkage = 0.01</code> (learning rate) and <code>n.minobsinnode = 6</code> (minimum number of observations in each node).</p>
<p>Now, let’s compare the results from the three tree models.</p>
<div class="sourceCode" id="cb320"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb320-1"><a href="gradient-boosted-machine.html#cb320-1" aria-hidden="true" tabindex="-1"></a>treebagRoc <span class="ot">&lt;-</span> pROC<span class="sc">::</span><span class="fu">roc</span>(<span class="at">response =</span> bagTune<span class="sc">$</span>pred<span class="sc">$</span>obs,</span>
<span id="cb320-2"><a href="gradient-boosted-machine.html#cb320-2" aria-hidden="true" tabindex="-1"></a>                        <span class="at">predictor =</span> bagTune<span class="sc">$</span>pred<span class="sc">$</span>Female,</span>
<span id="cb320-3"><a href="gradient-boosted-machine.html#cb320-3" aria-hidden="true" tabindex="-1"></a>                        <span class="at">levels =</span> <span class="fu">rev</span>(<span class="fu">levels</span>(bagTune<span class="sc">$</span>pred<span class="sc">$</span>obs)))</span>
<span id="cb320-4"><a href="gradient-boosted-machine.html#cb320-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb320-5"><a href="gradient-boosted-machine.html#cb320-5" aria-hidden="true" tabindex="-1"></a>rfRoc <span class="ot">&lt;-</span> pROC<span class="sc">::</span><span class="fu">roc</span>(<span class="at">response =</span> rfTune<span class="sc">$</span>pred<span class="sc">$</span>obs,</span>
<span id="cb320-6"><a href="gradient-boosted-machine.html#cb320-6" aria-hidden="true" tabindex="-1"></a>             <span class="at">predictor =</span> rfTune<span class="sc">$</span>pred<span class="sc">$</span>Female,</span>
<span id="cb320-7"><a href="gradient-boosted-machine.html#cb320-7" aria-hidden="true" tabindex="-1"></a>             <span class="at">levels =</span> <span class="fu">rev</span>(<span class="fu">levels</span>(rfTune<span class="sc">$</span>pred<span class="sc">$</span>obs)))</span>
<span id="cb320-8"><a href="gradient-boosted-machine.html#cb320-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb320-9"><a href="gradient-boosted-machine.html#cb320-9" aria-hidden="true" tabindex="-1"></a>gbmRoc <span class="ot">&lt;-</span> pROC<span class="sc">::</span><span class="fu">roc</span>(<span class="at">response =</span> gbmTune<span class="sc">$</span>pred<span class="sc">$</span>obs,</span>
<span id="cb320-10"><a href="gradient-boosted-machine.html#cb320-10" aria-hidden="true" tabindex="-1"></a>              <span class="at">predictor =</span> gbmTune<span class="sc">$</span>pred<span class="sc">$</span>Female,</span>
<span id="cb320-11"><a href="gradient-boosted-machine.html#cb320-11" aria-hidden="true" tabindex="-1"></a>              <span class="at">levels =</span> <span class="fu">rev</span>(<span class="fu">levels</span>(gbmTune<span class="sc">$</span>pred<span class="sc">$</span>obs)))</span>
<span id="cb320-12"><a href="gradient-boosted-machine.html#cb320-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb320-13"><a href="gradient-boosted-machine.html#cb320-13" aria-hidden="true" tabindex="-1"></a><span class="fu">plot.roc</span>(rpartRoc, </span>
<span id="cb320-14"><a href="gradient-boosted-machine.html#cb320-14" aria-hidden="true" tabindex="-1"></a>     <span class="at">type =</span> <span class="st">&quot;s&quot;</span>, </span>
<span id="cb320-15"><a href="gradient-boosted-machine.html#cb320-15" aria-hidden="true" tabindex="-1"></a>     <span class="at">print.thres =</span> <span class="fu">c</span>(.<span class="dv">5</span>), <span class="at">print.thres.pch =</span> <span class="dv">16</span>,</span>
<span id="cb320-16"><a href="gradient-boosted-machine.html#cb320-16" aria-hidden="true" tabindex="-1"></a>     <span class="at">print.thres.pattern =</span> <span class="st">&quot;&quot;</span>, <span class="at">print.thres.cex =</span> <span class="fl">1.2</span>,</span>
<span id="cb320-17"><a href="gradient-boosted-machine.html#cb320-17" aria-hidden="true" tabindex="-1"></a>     <span class="at">col =</span> <span class="st">&quot;black&quot;</span>, <span class="at">legacy.axes =</span> <span class="cn">TRUE</span>,</span>
<span id="cb320-18"><a href="gradient-boosted-machine.html#cb320-18" aria-hidden="true" tabindex="-1"></a>     <span class="at">print.thres.col =</span> <span class="st">&quot;black&quot;</span>)</span>
<span id="cb320-19"><a href="gradient-boosted-machine.html#cb320-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb320-20"><a href="gradient-boosted-machine.html#cb320-20" aria-hidden="true" tabindex="-1"></a><span class="fu">plot.roc</span>(treebagRoc, </span>
<span id="cb320-21"><a href="gradient-boosted-machine.html#cb320-21" aria-hidden="true" tabindex="-1"></a>     <span class="at">type =</span> <span class="st">&quot;s&quot;</span>, </span>
<span id="cb320-22"><a href="gradient-boosted-machine.html#cb320-22" aria-hidden="true" tabindex="-1"></a>     <span class="at">add =</span> <span class="cn">TRUE</span>, </span>
<span id="cb320-23"><a href="gradient-boosted-machine.html#cb320-23" aria-hidden="true" tabindex="-1"></a>     <span class="at">print.thres =</span> <span class="fu">c</span>(.<span class="dv">5</span>), <span class="at">print.thres.pch =</span> <span class="dv">3</span>, </span>
<span id="cb320-24"><a href="gradient-boosted-machine.html#cb320-24" aria-hidden="true" tabindex="-1"></a>     <span class="at">legacy.axes =</span> <span class="cn">TRUE</span>, <span class="at">print.thres.pattern =</span> <span class="st">&quot;&quot;</span>, </span>
<span id="cb320-25"><a href="gradient-boosted-machine.html#cb320-25" aria-hidden="true" tabindex="-1"></a>     <span class="at">print.thres.cex =</span> <span class="fl">1.2</span>,</span>
<span id="cb320-26"><a href="gradient-boosted-machine.html#cb320-26" aria-hidden="true" tabindex="-1"></a>     <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">print.thres.col =</span> <span class="st">&quot;red&quot;</span>)</span>
<span id="cb320-27"><a href="gradient-boosted-machine.html#cb320-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb320-28"><a href="gradient-boosted-machine.html#cb320-28" aria-hidden="true" tabindex="-1"></a><span class="fu">plot.roc</span>(rfRoc, </span>
<span id="cb320-29"><a href="gradient-boosted-machine.html#cb320-29" aria-hidden="true" tabindex="-1"></a>     <span class="at">type =</span> <span class="st">&quot;s&quot;</span>, </span>
<span id="cb320-30"><a href="gradient-boosted-machine.html#cb320-30" aria-hidden="true" tabindex="-1"></a>     <span class="at">add =</span> <span class="cn">TRUE</span>, </span>
<span id="cb320-31"><a href="gradient-boosted-machine.html#cb320-31" aria-hidden="true" tabindex="-1"></a>     <span class="at">print.thres =</span> <span class="fu">c</span>(.<span class="dv">5</span>), <span class="at">print.thres.pch =</span> <span class="dv">1</span>, </span>
<span id="cb320-32"><a href="gradient-boosted-machine.html#cb320-32" aria-hidden="true" tabindex="-1"></a>     <span class="at">legacy.axes =</span> <span class="cn">TRUE</span>, <span class="at">print.thres.pattern =</span> <span class="st">&quot;&quot;</span>, </span>
<span id="cb320-33"><a href="gradient-boosted-machine.html#cb320-33" aria-hidden="true" tabindex="-1"></a>     <span class="at">print.thres.cex =</span> <span class="fl">1.2</span>,</span>
<span id="cb320-34"><a href="gradient-boosted-machine.html#cb320-34" aria-hidden="true" tabindex="-1"></a>     <span class="at">col =</span> <span class="st">&quot;green&quot;</span>, <span class="at">print.thres.col =</span> <span class="st">&quot;green&quot;</span>)</span>
<span id="cb320-35"><a href="gradient-boosted-machine.html#cb320-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb320-36"><a href="gradient-boosted-machine.html#cb320-36" aria-hidden="true" tabindex="-1"></a><span class="fu">plot.roc</span>(gbmRoc, </span>
<span id="cb320-37"><a href="gradient-boosted-machine.html#cb320-37" aria-hidden="true" tabindex="-1"></a>     <span class="at">type =</span> <span class="st">&quot;s&quot;</span>, </span>
<span id="cb320-38"><a href="gradient-boosted-machine.html#cb320-38" aria-hidden="true" tabindex="-1"></a>     <span class="at">add =</span> <span class="cn">TRUE</span>, </span>
<span id="cb320-39"><a href="gradient-boosted-machine.html#cb320-39" aria-hidden="true" tabindex="-1"></a>     <span class="at">print.thres =</span> <span class="fu">c</span>(.<span class="dv">5</span>), <span class="at">print.thres.pch =</span> <span class="dv">10</span>, </span>
<span id="cb320-40"><a href="gradient-boosted-machine.html#cb320-40" aria-hidden="true" tabindex="-1"></a>     <span class="at">legacy.axes =</span> <span class="cn">TRUE</span>, <span class="at">print.thres.pattern =</span> <span class="st">&quot;&quot;</span>, </span>
<span id="cb320-41"><a href="gradient-boosted-machine.html#cb320-41" aria-hidden="true" tabindex="-1"></a>     <span class="at">print.thres.cex =</span> <span class="fl">1.2</span>,</span>
<span id="cb320-42"><a href="gradient-boosted-machine.html#cb320-42" aria-hidden="true" tabindex="-1"></a>     <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">print.thres.col =</span> <span class="st">&quot;blue&quot;</span>)</span>
<span id="cb320-43"><a href="gradient-boosted-machine.html#cb320-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb320-44"><a href="gradient-boosted-machine.html#cb320-44" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="fl">0.2</span>, <span class="fl">0.5</span>, <span class="at">cex =</span> <span class="fl">0.8</span>,</span>
<span id="cb320-45"><a href="gradient-boosted-machine.html#cb320-45" aria-hidden="true" tabindex="-1"></a>       <span class="fu">c</span>(<span class="st">&quot;Single Tree&quot;</span>, <span class="st">&quot;Bagged Tree&quot;</span>, </span>
<span id="cb320-46"><a href="gradient-boosted-machine.html#cb320-46" aria-hidden="true" tabindex="-1"></a>         <span class="st">&quot;Random Forest&quot;</span>, <span class="st">&quot;Boosted Tree&quot;</span>),</span>
<span id="cb320-47"><a href="gradient-boosted-machine.html#cb320-47" aria-hidden="true" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>),</span>
<span id="cb320-48"><a href="gradient-boosted-machine.html#cb320-48" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;black&quot;</span>, <span class="st">&quot;red&quot;</span>, <span class="st">&quot;green&quot;</span>, <span class="st">&quot;blue&quot;</span>),</span>
<span id="cb320-49"><a href="gradient-boosted-machine.html#cb320-49" aria-hidden="true" tabindex="-1"></a>       <span class="at">pch =</span> <span class="fu">c</span>(<span class="dv">16</span>, <span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">10</span>))</span></code></pre></div>
<p><img src="IDS_files/figure-html/unnamed-chunk-157-1.svg" width="672" /></p>
<p>Since the data here doesn’t have many variables, we don’t see a significant difference among the models. But you can still see those ensemble methods are better than a single tree. In most of the real applications, ensemble methods perform much better. Random forest and boosting trees can be a baseline model. Before exploring different models, you can quickly run a random forest to see the performance and then try to improve that performance. If the performance you got from the random forest is not too much better than guessing, you should consider collecting more data or reviewing the problem to frame it a different way instead of trying other models. Because it usually means the current data is not enough to solve the problem.</p>
<!--
### Boosting as Additive Model

This section illustrates how boosting is a forward stagewise additive model that minimizes exponential loss [@Friedman2000]. Many seemingly different models can be represented as a **basis expansion model**. Recall the classifier  obtained by the AdaBoost.M1 algorithm:

$$G(x)=sign ( \Sigma_{m=1}^M \alpha_{m}G_m(x))$$

The above expression fits in the framework of basis expansion which is as following:

\begin{equation}
f(x)=\Sigma_{m=1}^M \beta_m b(x,\gamma_m)
(\#eq:basisexp)
\end{equation}

where $\beta_m,\ m=1,\dots,M$ is expansion coefficient and $b(x,\gamma)\in \mathbb{R}$ is basis function.  Many of the learning methods fit into this additive framework. Hastie et al discuss basis expansion in detail in Chapter 5 of @Hastie2008 and cover the additive expansion for different learning techniques, such as single-hidden-layer neural networks (Chapter 11) and MARS (Section 9.4). These models are fitted by minimizing a designated loss function averaged over the training data:

\begin{equation}
\underset{\{\beta_m,\gamma_m\}_i^M}{min}\Sigma_{i=1}^N L\left(y_i,\Sigma_{m=1}^M\beta_{m}b(x_i;\gamma_m)\right)
(\#eq:additiveloss)
\end{equation}

Different models have different basis function $b(x_i;\gamma_m)$. You can also customize loss function $L(\cdot)$, such as squared-error loss, or likelihood-based loss. Optimizing the loss function across the whole training set is usually computation costly no matter the choice of basis and loss. The good news is that the problem can be simplified as fitting a single basis function. 

$$\underset{\beta,\gamma}{min}=\Sigma_{i=1}^N L(y_i,\beta b(x_i;\gamma))$$

The forward stagewise algorithm approximates the optimal solution of equation \@ref(eq:additiveloss). It adds new basis functions to the expansion without adjusting the previous ones. The forward stagewise additive algorithm is as following (Section 10.2 of @Hastie2008):

\begin{algorithm}
\caption{Forward stagewise algorithm}\label{forwardstagewisealgorithm} 
\begin{algorithmic}[1] 
\State Initialize $f_0(x)=0$
\For {$m=1,\dots,M$}
    \State Compute
    $$(\beta_m,\gamma_m)=\underset{\beta,\gamma}{argmin}\Sigma_{i=1}^NL(y_i,f_{m-1}(x_i)+\beta b(x_i;\gamma))$$
    \State Set $f_m(x)=f_{m-1}(x)+\beta_m b(x;\gamma_m)$
\EndFor
\end{algorithmic}
\end{algorithm}

At iteration m, it will search for the optimal $b(x;\gamma_m)$ and $\beta_m$ based on the previous basis function $f_{m-1}(x)$. And then add the new basis $b(x;\gamma_m)\beta_m$ to the previous basis function to get a new basis function $f_m(x)$ without changing any parameters from previous steps. Assume we use squared-error loss:

$$L(y,f(x))=(y-f(x))^2$$

Then we have: 

$$L(y_i,f_{m-1}(x_i)+\beta b(x_i;\gamma))=(y_i-f_{m-1}(x_i)-\beta b(x_i;\gamma))^2$$

where $y_i-f_{m-1}(x_i)$ is the residual for sample i based on the previous model. That is to say, it is fitting the new basis using the residual of the previous step. 

However, the squared-error loss is generally not a good choice. For a regression problem, it is susceptible to outliers. It doesn't fit the classification problem since the response is categorical. Hence we often consider other loss functions.

Now let us go back to the AdaBoost.M1 algorithm. It is actually a special case of the above forward stagewise model when the loss function is: 

$$L(y,f(x))=exp(-yf(x))$$

In AdaBoost.M1, the basis function is the classifier from each iteration $G_m(x)\in \{-1,1\}$. If we use the exponential loss, the optimization problem is 

\begin{equation}
\begin{array}{ccc}
(\beta_m,G_m) & = & \underset{\beta,G}{argmin}\Sigma_{i=1}^N exp[-y_i(f_{m-1}(x_i)+\beta G(x_i))]\\
& = & \underset{\beta, G}{argmin}\Sigma_{i=1}^N exp[-y_i \beta G(x_i)]\cdot exp[-y_if_{m-1}(x_i)]\\
& = & \underset{\beta, G}{argmin}\Sigma_{i=1}^N w_i^m exp[-y_i\beta G(x_i)]
\end{array}
(\#eq:explossadaboost1)
\end{equation}


where  $w_i^m= exp[-y_if_{m-1}(x_i)]$. It does not depend on $\beta$ and $G(x)$. So we can consider it as the weight for each sample. Since the weight is related to $f_{m-1}(x_i)$, it changes each iteration. We can further decompose  equation \@ref(eq:explossadaboost1): 

\begin{equation}
\begin{array}{ccc}
 &  & \underset{\beta,G}{argmin}\Sigma_{i=1}^{N}w_{i}^{m}exp[-y_{i}\beta G(x_{i})]\\
 & = & \underset{\beta,G}{argmin}\Sigma_{i=1}^{N}\left\{ w_{i}^{m}e^{-\beta}I(y_{i}=G(x))+w_{i}^{m}e^{\beta}I(y_{i}\neq G(x))\right\} \\
 & = & \underset{\beta,G}{argmin}\Sigma_{i=1}^{N}\left\{ w_{i}^{m}e^{-\beta}[1-I(y_{i}\neq G(x))]+w_{i}^{m}e^{\beta}I(y_{i}\neq G(x))\right\} \\
 & = & \underset{\beta,G}{argmin}\left\{ (e^{\beta}-e^{-\beta})\cdot\Sigma_{i=1}^{N}w_{i}^{m}I(y_{i}\neq G(x_{i}))+e^{-\beta}\cdot\Sigma_{i=1}^{N}w_{i}^{m}\right\} 
\end{array}
(\#eq:explossadaboost2)
\end{equation}

when $\beta >0$, the solution for equation \@ref(eq:explossadaboost2) is:

$$G_{m} = \underset{G}{argmin}\Sigma_{i=1}^{N}w_{i}^{m}I(y_{i}\neq G(x))$$

It is the classifier that minimizes the weighted error.  Plug the above $G_m$ into equation \@ref(eq:explossadaboost2). Take the derivative with respect to $\beta$ and set it to be 0 to solve the optimal $\beta_m$: 

$$\beta_m =\frac{1}{2}ln\frac{1-err_m}{err_m}$$

where

$$err_m = \frac{\Sigma_{i=1}^N w_i^{m}I(y_i \neq G_m(x_i))}{\Sigma_{i=1}^N w_i^{m}}$$

According to the forward stagewise algorithm, the result is updated as:

$$f_m(x)=f_{m-1}(x)+\beta_m G_m(x)$$

We can go ahead and get the weight for the next iteration:

\begin{equation}
\begin{array}{ccc}
w_i^{m+1} & = & exp[-y_if_m (x_i)]\\
& = & exp[-y_if_{m-1}(x)-y_i \beta_m G_m(x)]\\
& = & w_{i}^{m}\cdot exp[-\beta_m y_i G_m(x_i)]
\end{array}
(\#eq:explossadaboost3)
\end{equation}

Since $-y_i G_m(x_i)=2\cdot I(y_i \neq G_m(x_i))-1$, equation \@ref(eq:explossadaboost3) can be written as:

$$w_i^{m+1}=w_i^m \cdot exp[\alpha_mI(y_i\neq G_m(x_i))] \cdot exp[-\beta_m]$$

where  $\alpha_m=2\beta_m=log\frac{1-err_m}{err_m}$ is the same with the $\alpha_m$ in AdaBoost.M1 algorithm we showed before. So AdaBoost.M1 is a special case of a forward stagewise additive model using exponential loss. For comparing and selecting different loss functions, you can refer to section 10.5 and 10.6 in [@Hastie2008].
-->

</div>
</div>
<!-- </div> -->
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-bendor2000" class="csl-entry">
al, BenDor A et. 2000. <span>“Tissue Classification with Gene Expression Profiles.”</span> <em>Journal of Computational Biology</em> 7 (3): 559–83.
</div>
<div id="ref-Bauer1999" class="csl-entry">
Bauer, Eric, and Ron Kohavi. 1999. <span>“An Empirical Comparison of Voting Classification Algorithms: Bagging, Boosting, and Variants.”</span> <em>Machine Learning</em> 36: 105–42.
</div>
<div id="ref-Bergstra2006" class="csl-entry">
Bergstra, James, Norman Casagrande, Dumitru Erhan, Douglas Eck, and Balazs Kegl. 2006. <span>“Aggregate Features and AdaBoost for Music Classification.”</span> <em>Machine Learning</em> 65: 473–84.
</div>
<div id="ref-Breiman1998" class="csl-entry">
Breiman, Leo. 1998. <span>“Arcing Classifiers.”</span> <em>The Annals of Statistics</em> 26: 123–40.
</div>
<div id="ref-dudoit2002" class="csl-entry">
Dudoit S, Fridlyand J, and Speed T. 2002. <span>“Comparison of Discrimination Meth- Ods for the Classification of Tumors Using Gene Expression Data.”</span> <em>Journal of the American Statistical Association</em> 97 (457): 77–87.
</div>
<div id="ref-Freund1997" class="csl-entry">
Freund, Y., and R. Schapire. 1997. <span>“A Decision-Theoretic Generalization of Online Learning and an Application to Boosting.”</span> <em>Journal of Computer and System Sciences</em> 55: 119–39.
</div>
<div id="ref-Friedman2000" class="csl-entry">
Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. 2000. <span>“Additive Logistic Regression: A Statistical View of Boosting.”</span> <em>Annals of Statistics</em> 38: 337–74.
</div>
<div id="ref-Hastie2008" class="csl-entry">
Hastie T, Friedman J, Tibshirani R. 2008. <em>The Elements of Statistical Learning: Data Mining, Inference and Prediction</em>. 2nd ed. Springer.
</div>
<div id="ref-Valiant1984" class="csl-entry">
L, Valiant. 1984. <span>“A Theory of the Learnable.”</span> <em>Communications of the ACM</em> 27: 1134–42.
</div>
<div id="ref-KV1989" class="csl-entry">
M, Kearns, and Valiant L. 1989. <span>“Cryptographic Limitations on Learning Boolean Formulae and Finite Automata.”</span>
</div>
<div id="ref-Varmuza2003" class="csl-entry">
Varmuza K, He P, and Fang K. 2003. <span>“Boosting Applied to Classification of Mass Spectral Data.”</span> <em>Journal of Data Science</em> 1 (391–404).
</div>
<div id="ref-Schapire1999" class="csl-entry">
YFR, Schapire. 1999. <span>“Adaptive Game Playing Using Multiplicative Weights.”</span> <em>Games and Economic Behavior</em> 29: 79–103.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="random-forest.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="deeplearning.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": true,
"twitter": true,
"linkedin": true,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/happyrabbit/IntroDataScience/11-tree.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["IDS.pdf", "IDS.epub", "IDS.mobi"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section",
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
