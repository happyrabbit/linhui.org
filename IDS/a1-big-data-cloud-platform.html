<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Introduction to Data Science</title>
  <meta name="description" content="Introduction to Data Science">
  <meta name="generator" content="bookdown 0.4 and GitBook 2.6.7">

  <meta property="og:title" content="Introduction to Data Science" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="http://scientistcafe.com/IDS/" />
  
  <meta property="og:description" content="Introduction to Data Science" />
  <meta name="github-repo" content="happyrabbit/IntroDataScience" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Introduction to Data Science" />
  
  <meta name="twitter:description" content="Introduction to Data Science" />
  

<meta name="author" content="Hui Lin and Ming Li">


<meta name="date" content="2017-12-03">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="appendix.html">
<link rel="next" href="databases-and-sql.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-0.8/htmlwidgets.js"></script>
<script src="libs/datatables-binding-0.2/datatables.js"></script>
<link href="libs/dt-core-1.10.12/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.10.12/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.10.12/js/jquery.dataTables.min.js"></script>
<link href="libs/leaflet-0.7.3/leaflet.css" rel="stylesheet" />
<script src="libs/leaflet-0.7.3/leaflet.js"></script>
<link href="libs/leafletfix-1.0.0/leafletfix.css" rel="stylesheet" />
<script src="libs/leaflet-binding-1.0.1/leaflet.js"></script>
<link href="libs/dygraphs-1.1.1/dygraph.css" rel="stylesheet" />
<script src="libs/dygraphs-1.1.1/dygraph-combined.js"></script>
<script src="libs/moment-2.8.4/moment.js"></script>
<script src="libs/moment-timezone-0.2.5/moment-timezone-with-data.js"></script>
<script src="libs/moment-fquarter-1.0.0/moment-fquarter.min.js"></script>
<script src="libs/dygraphs-binding-1.1.1.4/dygraphs.js"></script>
<script src="libs/proj4js-2.3.15/proj4.js"></script>
<link href="libs/highcharts-5.0.6/css/motion.css" rel="stylesheet" />
<script src="libs/highcharts-5.0.6/highstock.js"></script>
<script src="libs/highcharts-5.0.6/highcharts-3d.js"></script>
<script src="libs/highcharts-5.0.6/highcharts-more.js"></script>
<script src="libs/highcharts-5.0.6/modules/annotations.js"></script>
<script src="libs/highcharts-5.0.6/modules/broken-axis.js"></script>
<script src="libs/highcharts-5.0.6/modules/data.js"></script>
<script src="libs/highcharts-5.0.6/modules/drilldown.js"></script>
<script src="libs/highcharts-5.0.6/modules/exporting.js"></script>
<script src="libs/highcharts-5.0.6/modules/funnel.js"></script>
<script src="libs/highcharts-5.0.6/modules/heatmap.js"></script>
<script src="libs/highcharts-5.0.6/modules/map.js"></script>
<script src="libs/highcharts-5.0.6/modules/no-data-to-display.js"></script>
<script src="libs/highcharts-5.0.6/modules/offline-exporting.js"></script>
<script src="libs/highcharts-5.0.6/modules/solid-gauge.js"></script>
<script src="libs/highcharts-5.0.6/modules/treemap.js"></script>
<script src="libs/highcharts-5.0.6/plugins/annotations.js"></script>
<script src="libs/highcharts-5.0.6/plugins/draggable-legend.js"></script>
<script src="libs/highcharts-5.0.6/plugins/draggable-points.js"></script>
<script src="libs/highcharts-5.0.6/plugins/export-csv.js"></script>
<script src="libs/highcharts-5.0.6/plugins/grouped-categories.js"></script>
<script src="libs/highcharts-5.0.6/plugins/motion.js"></script>
<script src="libs/highcharts-5.0.6/plugins/pattern-fill-v2.js"></script>
<script src="libs/highcharts-5.0.6/plugins/tooltip-delay.js"></script>
<script src="libs/highcharts-5.0.6/custom/reset.js"></script>
<script src="libs/highcharts-5.0.6/custom/symbols-extra.js"></script>
<script src="libs/highcharts-5.0.6/custom/text-symbols.js"></script>
<link href="libs/fontawesome-4.5.0/font-awesome.min.css" rel="stylesheet" />
<link href="libs/htmlwdgtgrid-1/htmlwdgtgrid.css" rel="stylesheet" />
<script src="libs/highchart-binding-0.5.0/highchart.js"></script>
<link href="libs/bokehjs-0.11.1/bokeh.min.css" rel="stylesheet" />
<script src="libs/bokehjs-0.11.1/bokeh.min.js"></script>
<script src="libs/rbokeh-binding-0.4.2/rbokeh.js"></script>
<script src="libs/d3-3.5.12/d3.min.js"></script>
<link href="libs/metrics-graphics-2.7.0/dist/metricsgraphics.css" rel="stylesheet" />
<link href="libs/metrics-graphics-2.7.0/dist/mg_regions.css" rel="stylesheet" />
<script src="libs/metrics-graphics-2.7.0/dist/metricsgraphics.min.js"></script>
<script src="libs/metrics-graphics-2.7.0/dist/mg_regions.js"></script>
<script src="libs/metricsgraphics-binding-0.9.0/metricsgraphics.js"></script>
<script src="libs/forceNetwork-binding-0.2.11/forceNetwork.js"></script>
<script src="libs/threejs-70/three.min.js"></script>
<script src="libs/threejs-70/Detector.js"></script>
<script src="libs/threejs-70/Projector.js"></script>
<script src="libs/threejs-70/CanvasRenderer.js"></script>
<script src="libs/globe-binding-0.2.2/globe.js"></script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="http://scientistcafe.com">Homepage</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Copyright Statement</a></li>
<li class="chapter" data-level="" data-path="about-the-authors.html"><a href="about-the-authors.html"><i class="fa fa-check"></i>About the Authors</a></li>
<li class="chapter" data-level="" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="what-is-data-science.html"><a href="what-is-data-science.html"><i class="fa fa-check"></i><b>1.1</b> What is data science?</a></li>
<li class="chapter" data-level="1.2" data-path="is-it-science-totally.html"><a href="is-it-science-totally.html"><i class="fa fa-check"></i><b>1.2</b> Is it science? Totally?</a></li>
<li class="chapter" data-level="1.3" data-path="what-kind-of-questions-can-data-science-solve.html"><a href="what-kind-of-questions-can-data-science-solve.html"><i class="fa fa-check"></i><b>1.3</b> What kind of questions can data science solve?</a><ul>
<li class="chapter" data-level="1.3.1" data-path="what-kind-of-questions-can-data-science-solve.html"><a href="what-kind-of-questions-can-data-science-solve.html#prerequisites"><i class="fa fa-check"></i><b>1.3.1</b> Prerequisites</a></li>
<li class="chapter" data-level="1.3.2" data-path="what-kind-of-questions-can-data-science-solve.html"><a href="what-kind-of-questions-can-data-science-solve.html#problem-type"><i class="fa fa-check"></i><b>1.3.2</b> Problem type</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="what-are-the-required-skills-for-data-scientist.html"><a href="what-are-the-required-skills-for-data-scientist.html"><i class="fa fa-check"></i><b>1.4</b> What are the required skills for data scientist?</a></li>
<li class="chapter" data-level="1.5" data-path="types-of-learning.html"><a href="types-of-learning.html"><i class="fa fa-check"></i><b>1.5</b> Types of Learning</a></li>
<li class="chapter" data-level="1.6" data-path="types-of-algorithm.html"><a href="types-of-algorithm.html"><i class="fa fa-check"></i><b>1.6</b> Types of Algorithm</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="soft-skills-for-data-scientists.html"><a href="soft-skills-for-data-scientists.html"><i class="fa fa-check"></i><b>2</b> Soft Skills for Data Scientists</a><ul>
<li class="chapter" data-level="2.1" data-path="comparison-between-statistician-and-data-scientist.html"><a href="comparison-between-statistician-and-data-scientist.html"><i class="fa fa-check"></i><b>2.1</b> Comparison between Statistician and Data Scientist</a></li>
<li class="chapter" data-level="2.2" data-path="where-data-science-team-fits.html"><a href="where-data-science-team-fits.html"><i class="fa fa-check"></i><b>2.2</b> Where Data Science Team Fits?</a></li>
<li class="chapter" data-level="2.3" data-path="beyond-data-and-analytics.html"><a href="beyond-data-and-analytics.html"><i class="fa fa-check"></i><b>2.3</b> Beyond Data and Analytics</a></li>
<li class="chapter" data-level="2.4" data-path="data-scientist-as-a-leader.html"><a href="data-scientist-as-a-leader.html"><i class="fa fa-check"></i><b>2.4</b> Data Scientist as a Leader</a></li>
<li class="chapter" data-level="2.5" data-path="three-pillars-of-knowledge.html"><a href="three-pillars-of-knowledge.html"><i class="fa fa-check"></i><b>2.5</b> Three Pillars of Knowledge</a></li>
<li class="chapter" data-level="2.6" data-path="common-pitfalls-of-data-science-projects.html"><a href="common-pitfalls-of-data-science-projects.html"><i class="fa fa-check"></i><b>2.6</b> Common Pitfalls of Data Science Projects</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="introduction-to-the-data.html"><a href="introduction-to-the-data.html"><i class="fa fa-check"></i><b>3</b> Introduction to the data</a><ul>
<li class="chapter" data-level="3.1" data-path="customer-data-for-clothing-company.html"><a href="customer-data-for-clothing-company.html"><i class="fa fa-check"></i><b>3.1</b> Customer Data for Clothing Company</a></li>
<li class="chapter" data-level="3.2" data-path="customer-satisfaction-survey-data-from-airline-company.html"><a href="customer-satisfaction-survey-data-from-airline-company.html"><i class="fa fa-check"></i><b>3.2</b> Customer Satisfaction Survey Data from Airline Company</a></li>
<li class="chapter" data-level="3.3" data-path="swine-disease-breakout-data.html"><a href="swine-disease-breakout-data.html"><i class="fa fa-check"></i><b>3.3</b> Swine Disease Breakout Data</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="data-pre-processing.html"><a href="data-pre-processing.html"><i class="fa fa-check"></i><b>4</b> Data Pre-processing</a><ul>
<li class="chapter" data-level="4.1" data-path="data-cleaning.html"><a href="data-cleaning.html"><i class="fa fa-check"></i><b>4.1</b> Data Cleaning</a></li>
<li class="chapter" data-level="4.2" data-path="missing-values.html"><a href="missing-values.html"><i class="fa fa-check"></i><b>4.2</b> Missing Values</a><ul>
<li class="chapter" data-level="4.2.1" data-path="missing-values.html"><a href="missing-values.html#impute-missing-values-with-medianmode"><i class="fa fa-check"></i><b>4.2.1</b> Impute missing values with median/mode</a></li>
<li class="chapter" data-level="4.2.2" data-path="missing-values.html"><a href="missing-values.html#k-nearest-neighbors"><i class="fa fa-check"></i><b>4.2.2</b> K-nearest neighbors</a></li>
<li class="chapter" data-level="4.2.3" data-path="missing-values.html"><a href="missing-values.html#bagging-tree"><i class="fa fa-check"></i><b>4.2.3</b> Bagging Tree</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="centering-and-scaling.html"><a href="centering-and-scaling.html"><i class="fa fa-check"></i><b>4.3</b> Centering and Scaling</a></li>
<li class="chapter" data-level="4.4" data-path="resolve-skewness.html"><a href="resolve-skewness.html"><i class="fa fa-check"></i><b>4.4</b> Resolve Skewness</a></li>
<li class="chapter" data-level="4.5" data-path="resolve-outliers.html"><a href="resolve-outliers.html"><i class="fa fa-check"></i><b>4.5</b> Resolve Outliers</a></li>
<li class="chapter" data-level="4.6" data-path="collinearity.html"><a href="collinearity.html"><i class="fa fa-check"></i><b>4.6</b> Collinearity</a></li>
<li class="chapter" data-level="4.7" data-path="sparse-variables.html"><a href="sparse-variables.html"><i class="fa fa-check"></i><b>4.7</b> Sparse Variables</a></li>
<li class="chapter" data-level="4.8" data-path="re-encode-dummy-variables.html"><a href="re-encode-dummy-variables.html"><i class="fa fa-check"></i><b>4.8</b> Re-encode Dummy Variables</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="data-wrangling.html"><a href="data-wrangling.html"><i class="fa fa-check"></i><b>5</b> Data Wrangling</a><ul>
<li class="chapter" data-level="5.1" data-path="read-and-write-data.html"><a href="read-and-write-data.html"><i class="fa fa-check"></i><b>5.1</b> Read and write data</a><ul>
<li class="chapter" data-level="5.1.1" data-path="read-and-write-data.html"><a href="read-and-write-data.html#readr"><i class="fa fa-check"></i><b>5.1.1</b> <code>readr</code></a></li>
<li class="chapter" data-level="5.1.2" data-path="read-and-write-data.html"><a href="read-and-write-data.html#data.table-enhanced-data.frame"><i class="fa fa-check"></i><b>5.1.2</b> <code>data.table</code>— enhanced <code>data.frame</code></a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="summarize-data.html"><a href="summarize-data.html"><i class="fa fa-check"></i><b>5.2</b> Summarize data</a><ul>
<li class="chapter" data-level="5.2.1" data-path="summarize-data.html"><a href="summarize-data.html#apply-lapply-and-sapply-in-base-r"><i class="fa fa-check"></i><b>5.2.1</b> <code>apply()</code>, <code>lapply()</code> and <code>sapply()</code> in base R</a></li>
<li class="chapter" data-level="5.2.2" data-path="summarize-data.html"><a href="summarize-data.html#ddply-in-plyr-package"><i class="fa fa-check"></i><b>5.2.2</b> <code>ddply()</code> in <code>plyr</code> package</a></li>
<li class="chapter" data-level="5.2.3" data-path="summarize-data.html"><a href="summarize-data.html#dplyr-package"><i class="fa fa-check"></i><b>5.2.3</b> <code>dplyr</code> package</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="tidy-and-reshape-data.html"><a href="tidy-and-reshape-data.html"><i class="fa fa-check"></i><b>5.3</b> Tidy and Reshape Data</a><ul>
<li class="chapter" data-level="5.3.1" data-path="tidy-and-reshape-data.html"><a href="tidy-and-reshape-data.html#reshape2-package"><i class="fa fa-check"></i><b>5.3.1</b> <code>reshape2</code> package</a></li>
<li class="chapter" data-level="5.3.2" data-path="tidy-and-reshape-data.html"><a href="tidy-and-reshape-data.html#tidyr-package"><i class="fa fa-check"></i><b>5.3.2</b> <code>tidyr</code> package</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="measuring-performance.html"><a href="measuring-performance.html"><i class="fa fa-check"></i><b>6</b> Measuring Performance</a><ul>
<li class="chapter" data-level="6.1" data-path="regression-model-performance.html"><a href="regression-model-performance.html"><i class="fa fa-check"></i><b>6.1</b> Regression Model Performance</a></li>
<li class="chapter" data-level="6.2" data-path="classification-model-performance.html"><a href="classification-model-performance.html"><i class="fa fa-check"></i><b>6.2</b> Classification Model Performance</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="model-tuning-technique.html"><a href="model-tuning-technique.html"><i class="fa fa-check"></i><b>7</b> Model Tuning Technique</a><ul>
<li class="chapter" data-level="7.1" data-path="systematic-error-and-random-error.html"><a href="systematic-error-and-random-error.html"><i class="fa fa-check"></i><b>7.1</b> Systematic Error and Random Error</a><ul>
<li class="chapter" data-level="7.1.1" data-path="systematic-error-and-random-error.html"><a href="systematic-error-and-random-error.html#measurement-error-in-the-response"><i class="fa fa-check"></i><b>7.1.1</b> Measurement Error in the Response</a></li>
<li class="chapter" data-level="7.1.2" data-path="systematic-error-and-random-error.html"><a href="systematic-error-and-random-error.html#measurement-error-in-the-independent-variables"><i class="fa fa-check"></i><b>7.1.2</b> Measurement Error in the Independent Variables</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="data-splitting-and-resampling.html"><a href="data-splitting-and-resampling.html"><i class="fa fa-check"></i><b>7.2</b> Data Splitting and Resampling</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="feature-engineering.html"><a href="feature-engineering.html"><i class="fa fa-check"></i><b>8</b> Feature Engineering</a><ul>
<li class="chapter" data-level="8.1" data-path="feature-construction.html"><a href="feature-construction.html"><i class="fa fa-check"></i><b>8.1</b> Feature Construction</a></li>
<li class="chapter" data-level="8.2" data-path="feature-extraction.html"><a href="feature-extraction.html"><i class="fa fa-check"></i><b>8.2</b> Feature Extraction</a></li>
<li class="chapter" data-level="8.3" data-path="feature-selection.html"><a href="feature-selection.html"><i class="fa fa-check"></i><b>8.3</b> Feature Selection</a><ul>
<li class="chapter" data-level="8.3.1" data-path="feature-selection.html"><a href="feature-selection.html#filter-method"><i class="fa fa-check"></i><b>8.3.1</b> Filter Method</a></li>
<li class="chapter" data-level="8.3.2" data-path="feature-selection.html"><a href="feature-selection.html#wrapper-method"><i class="fa fa-check"></i><b>8.3.2</b> Wrapper Method</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="regression-models.html"><a href="regression-models.html"><i class="fa fa-check"></i><b>9</b> Regression Models</a><ul>
<li class="chapter" data-level="9.1" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html"><i class="fa fa-check"></i><b>9.1</b> Ordinary Least Squares</a></li>
<li class="chapter" data-level="9.2" data-path="multivariate-adaptive-regression-splines.html"><a href="multivariate-adaptive-regression-splines.html"><i class="fa fa-check"></i><b>9.2</b> Multivariate Adaptive Regression Splines</a></li>
<li class="chapter" data-level="9.3" data-path="generalized-linear-model.html"><a href="generalized-linear-model.html"><i class="fa fa-check"></i><b>9.3</b> Generalized Linear Model</a></li>
<li class="chapter" data-level="9.4" data-path="pcr-and-pls.html"><a href="pcr-and-pls.html"><i class="fa fa-check"></i><b>9.4</b> PCR and PLS</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="regularization-methods.html"><a href="regularization-methods.html"><i class="fa fa-check"></i><b>10</b> Regularization Methods</a><ul>
<li class="chapter" data-level="10.1" data-path="ridge-regression.html"><a href="ridge-regression.html"><i class="fa fa-check"></i><b>10.1</b> Ridge Regression</a></li>
<li class="chapter" data-level="10.2" data-path="lasso.html"><a href="lasso.html"><i class="fa fa-check"></i><b>10.2</b> LASSO</a></li>
<li class="chapter" data-level="10.3" data-path="elastic-net.html"><a href="elastic-net.html"><i class="fa fa-check"></i><b>10.3</b> Elastic Net</a></li>
<li class="chapter" data-level="10.4" data-path="lasso-generalized-linear-model.html"><a href="lasso-generalized-linear-model.html"><i class="fa fa-check"></i><b>10.4</b> LASSO Generalized Linear Model</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="tree-based-methods.html"><a href="tree-based-methods.html"><i class="fa fa-check"></i><b>11</b> Tree-Based Methods</a><ul>
<li class="chapter" data-level="11.1" data-path="splitting-criteria.html"><a href="splitting-criteria.html"><i class="fa fa-check"></i><b>11.1</b> Splitting Criteria</a></li>
<li class="chapter" data-level="11.2" data-path="tree-pruning.html"><a href="tree-pruning.html"><i class="fa fa-check"></i><b>11.2</b> Tree Pruning</a></li>
<li class="chapter" data-level="11.3" data-path="regression-and-decision-tree-basic.html"><a href="regression-and-decision-tree-basic.html"><i class="fa fa-check"></i><b>11.3</b> Regression and Decision Tree Basic</a></li>
<li class="chapter" data-level="11.4" data-path="bagging-tree-1.html"><a href="bagging-tree-1.html"><i class="fa fa-check"></i><b>11.4</b> Bagging Tree</a></li>
<li class="chapter" data-level="11.5" data-path="random-forest.html"><a href="random-forest.html"><i class="fa fa-check"></i><b>11.5</b> Random Forest</a></li>
<li class="chapter" data-level="11.6" data-path="gradient-boosted-machine.html"><a href="gradient-boosted-machine.html"><i class="fa fa-check"></i><b>11.6</b> Gradient Boosted Machine</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="neural-network.html"><a href="neural-network.html"><i class="fa fa-check"></i><b>12</b> Neural Network</a><ul>
<li class="chapter" data-level="12.1" data-path="projection-pursuit-regression.html"><a href="projection-pursuit-regression.html"><i class="fa fa-check"></i><b>12.1</b> Projection Pursuit Regression</a></li>
<li class="chapter" data-level="12.2" data-path="neural-networks.html"><a href="neural-networks.html"><i class="fa fa-check"></i><b>12.2</b> Neural Networks</a></li>
<li class="chapter" data-level="12.3" data-path="fitting-neural-network.html"><a href="fitting-neural-network.html"><i class="fa fa-check"></i><b>12.3</b> Fitting Neural Network</a></li>
<li class="chapter" data-level="12.4" data-path="model-training.html"><a href="model-training.html"><i class="fa fa-check"></i><b>12.4</b> Model Training</a></li>
<li class="chapter" data-level="12.5" data-path="computation-in-r.html"><a href="computation-in-r.html"><i class="fa fa-check"></i><b>12.5</b> Computation in R</a><ul>
<li class="chapter" data-level="12.5.1" data-path="computation-in-r.html"><a href="computation-in-r.html#general-neural-network"><i class="fa fa-check"></i><b>12.5.1</b> General Neural Network</a></li>
<li class="chapter" data-level="12.5.2" data-path="computation-in-r.html"><a href="computation-in-r.html#averaged-neural-network"><i class="fa fa-check"></i><b>12.5.2</b> Averaged Neural Network</a></li>
<li class="chapter" data-level="12.5.3" data-path="computation-in-r.html"><a href="computation-in-r.html#model-comparison"><i class="fa fa-check"></i><b>12.5.3</b> Model Comparison</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="dynamicreproducible-report.html"><a href="dynamicreproducible-report.html"><i class="fa fa-check"></i><b>13</b> Dynamic/Reproducible Report</a><ul>
<li class="chapter" data-level="13.1" data-path="what-is-r-markdown.html"><a href="what-is-r-markdown.html"><i class="fa fa-check"></i><b>13.1</b> What is R Markdown?</a></li>
<li class="chapter" data-level="13.2" data-path="how-to-start.html"><a href="how-to-start.html"><i class="fa fa-check"></i><b>13.2</b> How to Start?</a><ul>
<li class="chapter" data-level="13.2.1" data-path="how-to-start.html"><a href="how-to-start.html#how-it-works"><i class="fa fa-check"></i><b>13.2.1</b> How It Works?</a></li>
<li class="chapter" data-level="13.2.2" data-path="how-to-start.html"><a href="how-to-start.html#get-started"><i class="fa fa-check"></i><b>13.2.2</b> Get Started</a></li>
<li class="chapter" data-level="13.2.3" data-path="how-to-start.html"><a href="how-to-start.html#markdown-basic"><i class="fa fa-check"></i><b>13.2.3</b> Markdown Basic</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="html.html"><a href="html.html"><i class="fa fa-check"></i><b>13.3</b> HTML</a><ul>
<li class="chapter" data-level="13.3.1" data-path="html.html"><a href="html.html#create-an-html-document"><i class="fa fa-check"></i><b>13.3.1</b> Create an HTML document</a></li>
<li class="chapter" data-level="13.3.2" data-path="html.html"><a href="html.html#floating-toc"><i class="fa fa-check"></i><b>13.3.2</b> Floating TOC</a></li>
<li class="chapter" data-level="13.3.3" data-path="html.html"><a href="html.html#code-chunks"><i class="fa fa-check"></i><b>13.3.3</b> Code Chunks</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="html5-slides.html"><a href="html5-slides.html"><i class="fa fa-check"></i><b>13.4</b> HTML5 Slides</a><ul>
<li class="chapter" data-level="13.4.1" data-path="html5-slides.html"><a href="html5-slides.html#ioslides-presentation"><i class="fa fa-check"></i><b>13.4.1</b> <code>ioslides</code> presentation</a></li>
<li class="chapter" data-level="13.4.2" data-path="html5-slides.html"><a href="html5-slides.html#slidy-presentation"><i class="fa fa-check"></i><b>13.4.2</b> <code>slidy</code> presentation</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="dashboards.html"><a href="dashboards.html"><i class="fa fa-check"></i><b>13.5</b> Dashboards</a><ul>
<li class="chapter" data-level="13.5.1" data-path="dashboards.html"><a href="dashboards.html#layouts"><i class="fa fa-check"></i><b>13.5.1</b> Layouts</a></li>
<li class="chapter" data-level="13.5.2" data-path="dashboards.html"><a href="dashboards.html#components"><i class="fa fa-check"></i><b>13.5.2</b> Components</a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="shiny-dashboard.html"><a href="shiny-dashboard.html"><i class="fa fa-check"></i><b>13.6</b> Shiny Dashboard</a><ul>
<li class="chapter" data-level="13.6.1" data-path="shiny-dashboard.html"><a href="shiny-dashboard.html#brief-introduction-to-shiny"><i class="fa fa-check"></i><b>13.6.1</b> Brief Introduction to Shiny</a></li>
<li class="chapter" data-level="13.6.2" data-path="shiny-dashboard.html"><a href="shiny-dashboard.html#using-shiny-with-flexdashboard"><i class="fa fa-check"></i><b>13.6.2</b> Using <code>shiny</code> with <code>flexdashboard</code></a></li>
</ul></li>
<li class="chapter" data-level="13.7" data-path="html-widgets-1.html"><a href="html-widgets-1.html"><i class="fa fa-check"></i><b>13.7</b> HTML Widgets</a><ul>
<li class="chapter" data-level="13.7.1" data-path="html-widgets-1.html"><a href="html-widgets-1.html#dt-a-wrapper-of-the-javascript-library-datatables"><i class="fa fa-check"></i><b>13.7.1</b> <code>DT</code>: A Wrapper of the JavaScript Library DataTables</a></li>
<li class="chapter" data-level="13.7.2" data-path="html-widgets-1.html"><a href="html-widgets-1.html#leafletinteractive-web-maps-based-on-the-leaflet-javascript-library"><i class="fa fa-check"></i><b>13.7.2</b> <code>leaflet</code>:Interactive Web-Maps Based on the Leaflet JavaScript Library</a></li>
<li class="chapter" data-level="13.7.3" data-path="html-widgets-1.html"><a href="html-widgets-1.html#dygraphs-interactive-plot-for-time-series-data"><i class="fa fa-check"></i><b>13.7.3</b> <code>dygraphs</code>: interactive plot for time series data</a></li>
<li class="chapter" data-level="13.7.4" data-path="html-widgets-1.html"><a href="html-widgets-1.html#highcharter"><i class="fa fa-check"></i><b>13.7.4</b> <code>highcharter</code></a></li>
<li class="chapter" data-level="13.7.5" data-path="html-widgets-1.html"><a href="html-widgets-1.html#rbokeh-is-a-visualization-library-that-provides-a-flexible-and-powerful-declarative-framework-for-creating-web-based-plots"><i class="fa fa-check"></i><b>13.7.5</b> <code>rbokeh</code> is a visualization library that provides a flexible and powerful declarative framework for creating web-based plots</a></li>
<li class="chapter" data-level="13.7.6" data-path="html-widgets-1.html"><a href="html-widgets-1.html#metricsgraphics-enables-easy-creation-of-d3-scatterplots-line-charts-and-histograms."><i class="fa fa-check"></i><b>13.7.6</b> <code>metricsgraphics</code> enables easy creation of D3 scatterplots, line charts, and histograms.</a></li>
<li class="chapter" data-level="13.7.7" data-path="html-widgets-1.html"><a href="html-widgets-1.html#networkd3-d3-javascript-network-graphs-from-r"><i class="fa fa-check"></i><b>13.7.7</b> <code>networkD3</code>: D3 JavaScript Network Graphs from R</a></li>
<li class="chapter" data-level="13.7.8" data-path="html-widgets-1.html"><a href="html-widgets-1.html#threejs-interactive-3d-scatter-plots-and-globes"><i class="fa fa-check"></i><b>13.7.8</b> <code>threejs</code>: Interactive 3D Scatter Plots and Globes</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i>Appendix</a><ul>
<li class="chapter" data-level="13.8" data-path="a1-big-data-cloud-platform.html"><a href="a1-big-data-cloud-platform.html"><i class="fa fa-check"></i><b>13.8</b> A1. Big Data Cloud Platform</a><ul>
<li class="chapter" data-level="13.8.1" data-path="a1-big-data-cloud-platform.html"><a href="a1-big-data-cloud-platform.html#how-data-becomes-science"><i class="fa fa-check"></i><b>13.8.1</b> How Data becomes Science?</a></li>
<li class="chapter" data-level="13.8.2" data-path="a1-big-data-cloud-platform.html"><a href="a1-big-data-cloud-platform.html#power-of-cluster-of-computers"><i class="fa fa-check"></i><b>13.8.2</b> Power of Cluster of Computers</a></li>
<li class="chapter" data-level="13.8.3" data-path="a1-big-data-cloud-platform.html"><a href="a1-big-data-cloud-platform.html#introduction-of-cloud-environment"><i class="fa fa-check"></i><b>13.8.3</b> Introduction of Cloud Environment</a></li>
<li class="chapter" data-level="13.8.4" data-path="a1-big-data-cloud-platform.html"><a href="a1-big-data-cloud-platform.html#summary"><i class="fa fa-check"></i><b>13.8.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="13.9" data-path="databases-and-sql.html"><a href="databases-and-sql.html"><i class="fa fa-check"></i><b>13.9</b> Databases and SQL</a><ul>
<li class="chapter" data-level="13.9.1" data-path="databases-and-sql.html"><a href="databases-and-sql.html#database-table-and-view"><i class="fa fa-check"></i><b>13.9.1</b> Database, Table and View</a></li>
<li class="chapter" data-level="13.9.2" data-path="databases-and-sql.html"><a href="databases-and-sql.html#sample-tables"><i class="fa fa-check"></i><b>13.9.2</b> Sample Tables</a></li>
<li class="chapter" data-level="13.9.3" data-path="databases-and-sql.html"><a href="databases-and-sql.html#basic-sql-statement"><i class="fa fa-check"></i><b>13.9.3</b> Basic SQL Statement</a></li>
<li class="chapter" data-level="13.9.4" data-path="databases-and-sql.html"><a href="databases-and-sql.html#simple-select-statement"><i class="fa fa-check"></i><b>13.9.4</b> Simple SELECT Statement</a></li>
<li class="chapter" data-level="13.9.5" data-path="databases-and-sql.html"><a href="databases-and-sql.html#aggregation-functions-and-group-by"><i class="fa fa-check"></i><b>13.9.5</b> Aggregation Functions and GROUP BY</a></li>
<li class="chapter" data-level="13.9.6" data-path="databases-and-sql.html"><a href="databases-and-sql.html#join-multiple-tables"><i class="fa fa-check"></i><b>13.9.6</b> Join Multiple Tables</a></li>
<li class="chapter" data-level="13.9.7" data-path="databases-and-sql.html"><a href="databases-and-sql.html#add-more-content-into-a-table"><i class="fa fa-check"></i><b>13.9.7</b> Add More Content into a Table</a></li>
<li class="chapter" data-level="13.9.8" data-path="databases-and-sql.html"><a href="databases-and-sql.html#advanced-topics-in-database"><i class="fa fa-check"></i><b>13.9.8</b> Advanced Topics in Database</a></li>
</ul></li>
<li class="chapter" data-level="13.10" data-path="other-useful-topics.html"><a href="other-useful-topics.html"><i class="fa fa-check"></i><b>13.10</b> Other Useful Topics</a><ul>
<li class="chapter" data-level="13.10.1" data-path="other-useful-topics.html"><a href="other-useful-topics.html#linux-operation-system"><i class="fa fa-check"></i><b>13.10.1</b> Linux Operation System</a></li>
<li class="chapter" data-level="13.10.2" data-path="other-useful-topics.html"><a href="other-useful-topics.html#visualization"><i class="fa fa-check"></i><b>13.10.2</b> Visualization</a></li>
<li class="chapter" data-level="13.10.3" data-path="other-useful-topics.html"><a href="other-useful-topics.html#gpu"><i class="fa fa-check"></i><b>13.10.3</b> GPU</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>14</b> References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Data Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="a1.-big-data-cloud-platform" class="section level2">
<h2><span class="header-section-number">13.8</span> A1. Big Data Cloud Platform</h2>
<div id="how-data-becomes-science" class="section level3">
<h3><span class="header-section-number">13.8.1</span> How Data becomes Science?</h3>
<p>Data has been a friend of statistician for hundreds of years. Tabulated data are the most familiar format that we use daily. People used to store data on papers, tapes, diskettes, or hard drives. Only recently, with the development of the computer, hardware, software, and algorithms, the volume, variety, and speed of the data suddenly beyond the capacity of a traditional statistician. And data becomes a special science with the very first focus on a fundamental question: with a huge amount of data, how can we store the data and quick access and process the data. In the past a few years, by utilizing commodity hardware and open source software, a big data ecosystem was created for data storage, data retrieval, and parallel computation. Hadoop and Spark have become a popular platform that enables data scientist, statistician, and business analyst to access the data and to build models. Programming skills in the big data platform have been the largest gap for a statistician to become a successful data scientist. However, with the recent wave of cloud computing, this gap is significantly reduced. Many of the technical details have been pushed to the background, and the user interface becomes much easier to learn. Cloud systems also enable quick implementation to the production environment. Now data science is emphasis more on the data itself as well as models and algorithms on top of the data instead of platform and infrastructure.</p>
</div>
<div id="power-of-cluster-of-computers" class="section level3">
<h3><span class="header-section-number">13.8.2</span> Power of Cluster of Computers</h3>
<p>We are all familiar with our laptop/desktop computers which contain mainly three components to finish computation with data: (1) Hard disk, (2) Memory, and (3) CPU as shown in Figure 41 left. The data and codes are stored in the hard disk which has certain features such as relatively slow for reading and writes and relatively large capacity of around a few TB in today’s market. Memory is relatively fast for reading and writing but relatively small in capacity in the order of a few dozens of GB in today’s market. CPU is where all the computation is done.</p>
<center>
<img src="http://scientistcafe.com/CE_JSM2017/images/cluster.png" alt="Single computer (left) and a cluster of computers (right)" />
</center>
<p>For statistical software such as R, the amount of data that it can process is limited by the computer’s memory. For a typical computer before the year 2000, the memory is less than 1 GB. The memory capacity grows far slower than the availability of the data to analyze. Now it is quite often that we need to analyze data far beyond the capacity of a single computer’s memory, especially in an enterprise environment. Meanwhile, the computation time is growing faster than linear to solve the same problem (such as regressions) as the data size increases. Using a cluster of computers become a common way to solve big data problem. In Figure 41 (right), a cluster of computers can be viewed as one powerful machine with total memory, hard disk and CPU equivale to the sum of individual computers. It is common to have thousands of nodes for a cluster.</p>
<p>In the past, to use a cluster of computers, users must write code (such as MPI) to take care of how data is distributed and how the computation is done in a parallel fashion. Luckily with the recent new development, the cloud environment for big data analysis is more user-friendly. As data is typically beyond the size of one hard disk, the dataset itself is stored across different nodes’ hard disk (i.e. the Hadoop system mentioned below). When we perform analysis, we can assume the needed data is already distributed across many node’s memories in the cluster and algorithm are parallel in nature to leverage corresponding nodes’ CPUs to compute (i.e. the Spark system mentioned below).</p>
<div id="evolution-of-clustering-computing" class="section level4">
<h4><span class="header-section-number">13.8.2.1</span> Evolution of Clustering Computing</h4>
<p>Using computer clusters to solve general purpose data and analytics problems needs a lot of efforts if we have to specifically control every element and steps such as data storage, memory allocation, and parallel computation. Fortunately, high tech IT companies and open source communities have developed the entire ecosystem based on Hadoop and Spark. Users need only to know high-level scripting language such as Python and R to leverage computer clusters’ storage, memory and computation power.</p>
</div>
<div id="hadoop" class="section level4">
<h4><span class="header-section-number">13.8.2.2</span> Hadoop</h4>
<p>The very first problem internet companies face is that a lot of data has been collected and how to better store these data for future analysis. Google developed its own file system to provide efficient, reliable access to data using large clusters of commodity hardware. The open source version is known as Hadoop Distributed File System (HDFS). Both systems use Map-Reduce to allocate computation across computation nodes on top of the file system. Hadoop in written in Java and writing map-reduce job using Java is a direct way to interact with Hadoop which is not familiar to many in the data and analytics community. To help better use Hadoop system, an SQL-like data warehouse system called Hive, and a scripting language for analytics interface called Pig were introduced for people with analytics background to interact with Hadoop system. Within Hive, we can create user defined function through R or Python to leverage the distributed and parallel computing infrastructure. Map-reduce on top of HDFS is the main concept of the Hadoop ecosystem. Each map-reduce operation requires retrieving data from hard disk, computation time, and then storing the result onto disk again. So, jobs on top of Hadoop require a lot of disk operation which may slow down the computation process.</p>
</div>
<div id="spark" class="section level4">
<h4><span class="header-section-number">13.8.2.3</span> Spark</h4>
<p>Spark works on top of distributed file system including HDFS with better data and analytics efficiency by leveraging in-memory operations and is more tailored for data processing and analytics. The spark system includes an SQL-like framework called Spark SQL and a parallel machine learning library called MLib. Fortunately for many in the analytics community, Spark also supports R and Python. We can interact with data stored in distributed file system using parallel computing across nodes easily with R and Python through the Spark API and do not need to worry about lower level details of distributed computing. We will introduce how to use R notebook to drive Spark computations.</p>
</div>
</div>
<div id="introduction-of-cloud-environment" class="section level3">
<h3><span class="header-section-number">13.8.3</span> Introduction of Cloud Environment</h3>
<p>There are many cloud computing environments such as Amazon’s AWS which provides a complete list of functions for heavy-duty enterprise applications. For example, Netflix runs its business entirely on AWS and Netflix does not own any data centers. For beginners, Databricks provides an easy to use cloud system for learning purpose. Databricks is a company founded by the creators of Apache Spark and it provides a user-friendly web-based notebook environment that can create Hadoop/Spark/GPU cluster on the fly and run R/Python/Scala/SQL. We will use Databricks’ community edition to run demos in this book. Please note the content of this section is adopted from the following web pages:</p>
<ul>
<li><a href="https://docs.databricks.com/user-guide/faq/sparklyr.html" class="uri">https://docs.databricks.com/user-guide/faq/sparklyr.html</a></li>
<li><a href="http://spark.rstudio.com/index.html" class="uri">http://spark.rstudio.com/index.html</a></li>
</ul>
<p><strong>Open Account and Create a Cluster</strong></p>
<p>Anyone can apply for a community edition for free through <a href="https://databricks.com/try-databricks" class="uri">https://databricks.com/try-databricks</a> and a short YouTube video illustrates the application process can be found <a href="https://youtu.be/vx-3-htFvrg" class="uri">https://youtu.be/vx-3-htFvrg</a>. Another short YouTube video shows how to create a cluster for a cloud computing environment and create an R notebook to run R codes which can be found at <a href="https://youtu.be/0HFujX3t6TU" class="uri">https://youtu.be/0HFujX3t6TU</a>. In fact, you can run Python/R/Scala/SQL cells, as well as markdown cells, in the same notebook by including a keyword at the beginning of each cell that we will discuss later.</p>
<p><strong>R Notebook</strong></p>
<p>In last section of the video, we created an R notebook. For an R notebook, it contains multiple cells and by default, the content within each cell are R scripts. Usually, each cell is a well-managed a few lines of codes that accomplish a specific task. For example, Figure 42 shows the default cell for an R notebook for cell 1. We can type in R scripts and comments same as we are using R console. By default, only the result from the last line will be shown following the cell. However, you can use <code>print()</code> function to output results for any lines. If we move the mouse to the middle of the lower edge of the cell below the results, a “+” symbol will show up and click on the symbol will insert a new cell below. When you click any area within a cell, it will make it editable and you will see a few icons on the top right corn of the cell where you can run the cell, as well as add a cell below or above, copy the cell, cut cell, high cell etc. One quick way to run the cell is Shift+Enter when the cell is chosen. You will become familiar with the notebook environment quickly.</p>
<center>
<img src="http://scientistcafe.com/CE_JSM2017/images/rnotebook.png" alt="R notebook default cell with R scripts" />
</center>
<p><strong>Markdown Cells</strong></p>
<p>For an R notebook, every cell by default will contain R scripts. But if we put %md, %sql or %python at the first line of a cell, that cell becomes Markdown cell, SQL script cell, and Python script cell accordingly. For example, Figure 43 shows a markdown cell with scripts and the actual appearance when exits editing mode. Markdown cell provides a straightforward way to descript what each cell is doing as well as what the entire notebook is about. It is a better way than simple comment within in the code.</p>
<center>
<img src="http://scientistcafe.com/CE_JSM2017/images/markdown_databrick.png" alt="An example of Markdown cell with scripts at top and actual appearance at bottom" />
</center>
<p><strong>Leverage Hadoop and Spark Parallel using R Notebook</strong></p>
<p>R is a powerful tool for data analysis given the data can be fit into memory. Because of the memory bounded dataset limit, R itself cannot be used directly for big data analysis where the data is likely stored in Hadoop and Spark system. By leverage <code>sparklyr</code> package created by RStudio, we can use Databricks’ R notebook to analyze data stored in Spark system where the data are stored across different nodes and computation are parallel in nature to use the collection of memory units across all nodes. And the process is relatively simple. In this section, we will illustrate how to use Databricks’ R notebook for big data analysis on top of Spark environment through <code>sparklyr</code> package.</p>
<p><strong>Library Installation</strong></p>
<p>First, we need to install <code>sparklyr</code> package which enables the connection between master or local node to Spark cluster environments. As it will install more than 10 dependencies, it may take more than 5 minutes to finish. Be patient while it is installing! Once the installation finishes, load the <code>sparklyr</code> package as illustrated by the following code:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Installing sparklyr takes a few minutes, </span>
<span class="co"># because it installs +10 dependencies.</span>

if (!<span class="kw">require</span>(<span class="st">&quot;sparklyr&quot;</span>)) {
  <span class="kw">install.packages</span>(<span class="st">&quot;sparklyr&quot;</span>)  
}

<span class="co"># Load sparklyr package.</span>
<span class="kw">library</span>(sparklyr)</code></pre></div>
<p><strong>Create Connection</strong></p>
<p>Once the library is loaded, we need to create a Spark Connection to link master / local node to Spark environment. Here we use the “databricks” option for parameter method which is specific for databricks’ system. In the enterprise environment, please consult your administrator for details. The created Spark Connection (i.e. sc) will be the pipe that connects master/local/terminal to the Spark Cluster. We can think of the web interface/terminal is running on a master node which has its local memory and CPU. The Spark Connection can be established with:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># create a sparklyr connection </span>
sc &lt;-<span class="st"> </span><span class="kw">spark_connect</span>(<span class="dt">method =</span> <span class="st">&quot;databricks&quot;</span>)</code></pre></div>
<p><strong>Sample Dataset</strong></p>
<p>To simplify the learning process, let us use a very familiar dataset: the iris dataset. It is part of the <code>dplyr</code> library and let’s load that library to use the iris data frame. Here the iris dataset is still on the local node where the R notebook is running on. And we can see that the first a few lines of the iris dataset below the code after running:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(dplyr)
<span class="kw">head</span>(iris)</code></pre></div>
<p><strong>IMPORTANT - Copy Data to Spark Environment</strong></p>
<p>In real applications, your data maybe massive and cannot fit onto a single hard disk. If the data is already in Hadoop/Spark ecosystem, you can use SparkDataFrame to analyze it in the Spark system directly. Here, we illustrate how to copy a local dataset to Spark environment and then work on that dataset in the Spark system. As we have already created the Spark Connection sc, it is easy to copy data to spark system by sdf_copy_to() function as below:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">iris_tbl &lt;-<span class="st"> </span><span class="kw">sdf_copy_to</span>(<span class="dt">sc =</span> sc, <span class="dt">x =</span> iris, <span class="dt">overwrite =</span> T)</code></pre></div>
<p>The above one line code copies iris dataset from the local node to Spark cluster environment. “<code>sc</code>” is the Spark Connection we just created; “<code>x</code>” is the data frame that we want to copy; “<code>overwrite</code>” is the option whether we want to overwrite the target object if the same name SparkDataFrame exists in the Spark environment. Finally, sdf_copy_to() function will return an R object wrapping the copied SparkDataFrame. So irir_tbl can be used to refer to the iris SparkDataFrame.</p>
<p>To check whether the iris data was copied to Spark environment successfully or not, we can use <code>src_tbls( )</code> function to the Spark Connection (sc):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">src_tbls</span>(sc) ## code to return all the data frames associated with sc</code></pre></div>
<p><strong>Analyzing the Data</strong></p>
<p>Now we have successfully copied the iris dataset to the Spark environment as a SparkDataFrame. This means that <code>iris_tbl</code> is an R object wrapping the iris SparkDataFrame and we can use <code>iris_tbl</code> to refer the iris dataset in the Spark system (i.e. the iris SparkDataFrame). With the <code>sparklyr</code> packages, we can use many functions in <code>dplyr</code> to SparkDataFrame directly through <code>iris_tbl</code>, same as we are applying <code>dplyr</code> functions to a local R data frame in our laptop. For example, we can use the <code>%&gt;%</code> operator to pass <code>iris_tbl</code> to the <code>count( )</code> function:</p>
<pre><code>iris_tbl %&gt;% count</code></pre>
<p>or using the <code>head( )</code> function to return the first few rows in iris_tbl:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">head</span>(iris_tbl)</code></pre></div>
<p>or more advanced data manipulation directly to <code>iris_tbl</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">iris_tbl %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">Sepal_Width =</span> <span class="kw">ROUND</span>(Sepal_Width *<span class="st"> </span><span class="dv">2</span>) /<span class="st"> </span><span class="dv">2</span>) %&gt;%<span class="st"> </span><span class="co"># Bucketizing Sepal_Width</span>
<span class="st">  </span><span class="kw">group_by</span>(Species, Sepal_Width) %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">count =</span> <span class="kw">n</span>(), <span class="dt">Sepal_Length =</span> <span class="kw">mean</span>(Sepal_Length), <span class="dt">stdev =</span> <span class="kw">sd</span>(Sepal_Length))</code></pre></div>
<p><strong>Collect Results Back to Master Node</strong></p>
<p>Even though we can run many of the <code>dplyr</code> functions on SparkDataFrame, we cannot apply functions from other packages to SparkDataFrame direction (such as <code>ggplot()</code>). For functions that can only work on local R data frames, we must copy the SparkDataFrame back to the local node. To copy SparkDataFrame back to the local node, we use the <code>collect()</code> function where the argument to it is the name of the SparkDataFrame. The following code <code>collect()</code> the results of a few operations and assign the collected data to iris_summary variable:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">iris_summary &lt;-<span class="st"> </span>
<span class="st">  </span>iris_tbl %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">Sepal_Width =</span> <span class="kw">ROUND</span>(Sepal_Width *<span class="st"> </span><span class="dv">2</span>) /<span class="st"> </span><span class="dv">2</span>) %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>(Species, Sepal_Width) %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">count =</span> <span class="kw">n</span>(), <span class="dt">Sepal_Length =</span> <span class="kw">mean</span>(Sepal_Length), <span class="dt">stdev =</span> <span class="kw">sd</span>(Sepal_Length)) %&gt;%<span class="st">  </span>
<span class="st">  </span>collect</code></pre></div>
<p>Now, <code>iris_summary</code> is a local variable to the R notebook and we can use all R packages and functions to it. In the following code, we will apply <code>ggplot()</code> to it, exactly the same as a stand along R console:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(ggplot2)
<span class="kw">ggplot</span>(iris_summary, <span class="kw">aes</span>(Sepal_Width, Sepal_Length, <span class="dt">color =</span> Species)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="dt">size =</span> <span class="fl">1.2</span>) +
<span class="st">  </span><span class="kw">geom_errorbar</span>(<span class="kw">aes</span>(<span class="dt">ymin =</span> Sepal_Length -<span class="st"> </span>stdev, <span class="dt">ymax =</span> Sepal_Length +<span class="st"> </span>stdev), <span class="dt">width =</span> <span class="fl">0.05</span>) +
<span class="st">  </span><span class="kw">geom_text</span>(<span class="kw">aes</span>(<span class="dt">label =</span> count), <span class="dt">vjust =</span> -<span class="fl">0.2</span>, <span class="dt">hjust =</span> <span class="fl">1.2</span>, <span class="dt">color =</span> <span class="st">&quot;black&quot;</span>) +
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">legend.position=</span><span class="st">&quot;top&quot;</span>)</code></pre></div>
<p><strong>Fit Regression to SparkDataFrame</strong></p>
<p>One of the largest advantages is that, within Spark system, there are already many statistical and machine learning algorithms developed to run parallelly across many CPUs with data across many memory units. In this example, we have already uploaded the data to Spark system, and the data in the Spark system can be referred through iris_tbl. The linear regression implemented in Spark system can be called through <code>ml_linear_regression()</code> function. The syntax to call the function is to define the Spark Data Frame (i.e. iris_tbl), response variable (i.e. y-variable in linear regression in the Spark data frame iris_tbl) and features (i.e. the x-variable in linear regression in the Spark data frame iris_tbl). So, we can easily fit a linear regression for large dataset far beyond the memory limit of one single computer, and it is truly scalable and only constrained by the resource of the Spark cluster. Below is an illustration of how to fit a linear regression to SparkDataFrame using R notebook:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit1 &lt;-<span class="st">  </span><span class="kw">ml_linear_regression</span>(<span class="dt">x =</span> iris_tbl, <span class="dt">response =</span> <span class="st">&quot;Sepal_Length&quot;</span>, 
                              <span class="dt">features =</span> <span class="kw">c</span>(<span class="st">&quot;Sepal_Width&quot;</span>, <span class="st">&quot;Petal_Length&quot;</span>, <span class="st">&quot;Petal_Width&quot;</span>))
<span class="kw">summary</span>(fit1)</code></pre></div>
<p>In the above code, x is the R object wrapping the SparkDataFrame; the response is y-variable, features are the collection of explanatory variables. For this function, both the data and computation are in the Spark cluster which leverages multiple CPUs and distributed memories.</p>
<p><strong>Fit a K-means Cluster</strong></p>
<p>Through the <code>sparklyr</code> package, we can use an R notebook to access many Spark Machine Learning Library (MLlib) algorithms such as linear regression, logistic regression, Survival Regression, Generalized Linear Regression, Decision Trees, Random Forests, Gradient-Boosted Trees, Principal Components Analysis, Naive-Bayes, K-Means Clustering and a few other methods. Below codes fit a k-means cluster algorithm:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## Now fit a k-means clustering using iris_tbl data 
## with only two out of four features in iris_tbl
fit2 &lt;-<span class="st"> </span><span class="kw">ml_kmeans</span>(<span class="dt">x =</span> iris_tbl, <span class="dt">centers =</span> <span class="dv">3</span>, 
                  <span class="dt">features =</span> <span class="kw">c</span>(<span class="st">&quot;Petal_Length&quot;</span>, <span class="st">&quot;Petal_Width&quot;</span>))

<span class="co"># print our model fit</span>
<span class="kw">print</span>(fit2)</code></pre></div>
<p>After the k-means model is fit, we can apply the model to predict other datasets through <code>sdf_predict()</code> function. Below code apply the model to <code>iris_tbl</code> again to predict and then the results are collected back to local variable prediction through <code>collect()</code> function:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">prediction =<span class="st"> </span><span class="kw">collect</span>(<span class="kw">sdf_predict</span>(fit2, iris_tbl)) </code></pre></div>
<p>As prediction is a local variable, we can apply any R functions from any libraries to it. For example:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">prediction  %&gt;%
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(Petal_Length, Petal_Width)) +
<span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(Petal_Width, Petal_Length, <span class="dt">col =</span> <span class="kw">factor</span>(prediction +<span class="st"> </span><span class="dv">1</span>)),
             <span class="dt">size =</span> <span class="dv">2</span>, <span class="dt">alpha =</span> <span class="fl">0.5</span>) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">data =</span> fit2$centers, <span class="kw">aes</span>(Petal_Width, Petal_Length),
             <span class="dt">col =</span> scales::<span class="kw">muted</span>(<span class="kw">c</span>(<span class="st">&quot;red&quot;</span>, <span class="st">&quot;green&quot;</span>, <span class="st">&quot;blue&quot;</span>)),
             <span class="dt">pch =</span> <span class="st">&#39;x&#39;</span>, <span class="dt">size =</span> <span class="dv">12</span>) +
<span class="st">  </span><span class="kw">scale_color_discrete</span>(<span class="dt">name =</span> <span class="st">&quot;Predicted Cluster&quot;</span>,
                       <span class="dt">labels =</span> <span class="kw">paste</span>(<span class="st">&quot;Cluster&quot;</span>, <span class="dv">1</span>:<span class="dv">3</span>)) +
<span class="st">  </span><span class="kw">labs</span>(
    <span class="dt">x =</span> <span class="st">&quot;Petal Length&quot;</span>,
    <span class="dt">y =</span> <span class="st">&quot;Petal Width&quot;</span>,
    <span class="dt">title =</span> <span class="st">&quot;K-Means Clustering&quot;</span>,
    <span class="dt">subtitle =</span> <span class="st">&quot;Use Spark.ML to predict cluster membership with the iris dataset.&quot;</span>
  )</code></pre></div>
</div>
<div id="summary" class="section level3">
<h3><span class="header-section-number">13.8.4</span> Summary</h3>
<p>In the above a few sub-sections, we illustrated</p>
<ol style="list-style-type: decimal">
<li><p>the relationship between master / local node and Spark Clusters;</p></li>
<li><p>how to copy a local data frame to a SparkDataFrame (please note if your data is already in Spark environment, there is no need to copy. This is likely to be the case for enterprise environment);</p></li>
<li><p>how to manipulate SparkDataFrame through <code>dplyr</code> functions with the installation of <code>sparklyr</code> package;</p></li>
<li><p>how to fit statistical and machine learning models to SparkDataFrame;</p></li>
<li><p>how to collect information from SparkDataFrame back to a local data frame for future analysis.</p></li>
</ol>
<p>These procedures are pretty much covered the basics of big data analysis that a data scientist needs to know. The above steps are published as an R notebook: <a href="https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/2961012104553482/3725396058299890/1806228006848429/latest.html" class="uri">https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/2961012104553482/3725396058299890/1806228006848429/latest.html</a></p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="appendix.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="databases-and-sql.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/happyrabbit/IntroDataScience/15-Appendix.Rmd",
"text": "Edit"
},
"download": ["IDS.pdf", "IDS.epub", "IDS.mobi"],
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
