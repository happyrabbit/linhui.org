<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>12.3 Recurrent Neural Network | Practitioner’s Guide to Data Science</title>
  <meta name="description" content="Introduction to Data Science" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="12.3 Recurrent Neural Network | Practitioner’s Guide to Data Science" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="http://scientistcafe.com/IDS/" />
  
  <meta property="og:description" content="Introduction to Data Science" />
  <meta name="github-repo" content="happyrabbit/IntroDataScience" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="12.3 Recurrent Neural Network | Practitioner’s Guide to Data Science" />
  
  <meta name="twitter:description" content="Introduction to Data Science" />
  

<meta name="author" content="Hui Lin and Ming Li" />


<meta name="date" content="2023-04-18" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="convolutional-neural-network.html"/>
<link rel="next" href="appendix.html"/>
<script src="libs/header-attrs/header-attrs.js"></script>
<script src="libs/jquery/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="http://scientistcafe.com">Homepage</a></li>

<li class="divider"></li>
<li><a href="index.html#preface" id="toc-preface">Preface</a>
<ul>
<li><a href="goal-of-the-book.html#goal-of-the-book" id="toc-goal-of-the-book">Goal of the Book</a></li>
<li><a href="what-this-book-covers.html#what-this-book-covers" id="toc-what-this-book-covers">What This Book Covers</a></li>
<li><a href="who-this-book-is-for.html#who-this-book-is-for" id="toc-who-this-book-is-for">Who This Book Is For</a></li>
<li><a href="how-to-use-this-book.html#how-to-use-this-book" id="toc-how-to-use-this-book">How to Use This Book</a>
<ul>
<li><a href="how-to-use-this-book.html#what-the-book-assumes" id="toc-what-the-book-assumes">What the Book Assumes</a></li>
<li><a href="how-to-use-this-book.html#how-to-run-r-and-python-code" id="toc-how-to-run-r-and-python-code">How to Run R and Python Code</a></li>
</ul></li>
<li><a href="complementary-reading.html#complementary-reading" id="toc-complementary-reading">Complementary Reading</a></li>
</ul></li>
<li><a href="about-the-authors.html#about-the-authors" id="toc-about-the-authors">About the Authors</a></li>
<li><a href="introduction.html#introduction" id="toc-introduction"><span class="toc-section-number">1</span> Introduction</a>
<ul>
<li><a href="a-brief-history-of-data-science.html#a-brief-history-of-data-science" id="toc-a-brief-history-of-data-science"><span class="toc-section-number">1.1</span> A Brief History of Data Science</a></li>
<li><a href="data-science-role-and-skill-tracks.html#data-science-role-and-skill-tracks" id="toc-data-science-role-and-skill-tracks"><span class="toc-section-number">1.2</span> Data Science Role and Skill Tracks</a>
<ul>
<li><a href="data-science-role-and-skill-tracks.html#engineering" id="toc-engineering"><span class="toc-section-number">1.2.1</span> Engineering</a></li>
<li><a href="data-science-role-and-skill-tracks.html#analysis" id="toc-analysis"><span class="toc-section-number">1.2.2</span> Analysis</a></li>
<li><a href="data-science-role-and-skill-tracks.html#modelinginference" id="toc-modelinginference"><span class="toc-section-number">1.2.3</span> Modeling/Inference</a></li>
</ul></li>
<li><a href="what-kind-of-questions-can-data-science-solve.html#what-kind-of-questions-can-data-science-solve" id="toc-what-kind-of-questions-can-data-science-solve"><span class="toc-section-number">1.3</span> What Kind of Questions Can Data Science Solve?</a>
<ul>
<li><a href="what-kind-of-questions-can-data-science-solve.html#prerequisites" id="toc-prerequisites"><span class="toc-section-number">1.3.1</span> Prerequisites</a></li>
<li><a href="what-kind-of-questions-can-data-science-solve.html#problem-type" id="toc-problem-type"><span class="toc-section-number">1.3.2</span> Problem Type</a></li>
</ul></li>
<li><a href="structure-of-data-science-team.html#structure-of-data-science-team" id="toc-structure-of-data-science-team"><span class="toc-section-number">1.4</span> Structure of Data Science Team</a></li>
<li><a href="data-science-roles.html#data-science-roles" id="toc-data-science-roles"><span class="toc-section-number">1.5</span> Data Science Roles</a></li>
</ul></li>
<li><a href="SoftSkillsforDataScientists.html#SoftSkillsforDataScientists" id="toc-SoftSkillsforDataScientists"><span class="toc-section-number">2</span> Soft Skills for Data Scientists</a>
<ul>
<li><a href="comparison-between-statistician-and-data-scientist.html#comparison-between-statistician-and-data-scientist" id="toc-comparison-between-statistician-and-data-scientist"><span class="toc-section-number">2.1</span> Comparison between Statistician and Data Scientist</a></li>
<li><a href="beyond-data-and-analytics.html#beyond-data-and-analytics" id="toc-beyond-data-and-analytics"><span class="toc-section-number">2.2</span> Beyond Data and Analytics</a></li>
<li><a href="three-pillars-of-knowledge.html#three-pillars-of-knowledge" id="toc-three-pillars-of-knowledge"><span class="toc-section-number">2.3</span> Three Pillars of Knowledge</a></li>
<li><a href="data-science-project-cycle.html#data-science-project-cycle" id="toc-data-science-project-cycle"><span class="toc-section-number">2.4</span> Data Science Project Cycle</a>
<ul>
<li><a href="data-science-project-cycle.html#types-of-data-science-projects" id="toc-types-of-data-science-projects"><span class="toc-section-number">2.4.1</span> Types of Data Science Projects</a></li>
<li><a href="data-science-project-cycle.html#ProblemFormulationandProjectPlanningStage" id="toc-ProblemFormulationandProjectPlanningStage"><span class="toc-section-number">2.4.2</span> Problem Formulation and Project Planning Stage</a></li>
<li><a href="data-science-project-cycle.html#project-modeling-stage" id="toc-project-modeling-stage"><span class="toc-section-number">2.4.3</span> Project Modeling Stage</a></li>
<li><a href="data-science-project-cycle.html#ModelImplementationandPostProductionStage" id="toc-ModelImplementationandPostProductionStage"><span class="toc-section-number">2.4.4</span> Model Implementation and Post Production Stage</a></li>
<li><a href="data-science-project-cycle.html#ProjectCycleSummary" id="toc-ProjectCycleSummary"><span class="toc-section-number">2.4.5</span> Project Cycle Summary</a></li>
</ul></li>
<li><a href="common-mistakes-in-data-science.html#common-mistakes-in-data-science" id="toc-common-mistakes-in-data-science"><span class="toc-section-number">2.5</span> Common Mistakes in Data Science</a>
<ul>
<li><a href="common-mistakes-in-data-science.html#problem-formulation-stage" id="toc-problem-formulation-stage"><span class="toc-section-number">2.5.1</span> Problem Formulation Stage</a></li>
<li><a href="common-mistakes-in-data-science.html#project-planning-stage" id="toc-project-planning-stage"><span class="toc-section-number">2.5.2</span> Project Planning Stage</a></li>
<li><a href="common-mistakes-in-data-science.html#project-modeling-stage-1" id="toc-project-modeling-stage-1"><span class="toc-section-number">2.5.3</span> Project Modeling Stage</a></li>
<li><a href="common-mistakes-in-data-science.html#model-implementation-and-post-production-stage" id="toc-model-implementation-and-post-production-stage"><span class="toc-section-number">2.5.4</span> Model Implementation and Post Production Stage</a></li>
<li><a href="common-mistakes-in-data-science.html#summary-of-common-mistakes" id="toc-summary-of-common-mistakes"><span class="toc-section-number">2.5.5</span> Summary of Common Mistakes</a></li>
</ul></li>
</ul></li>
<li><a href="introduction-to-the-data.html#introduction-to-the-data" id="toc-introduction-to-the-data"><span class="toc-section-number">3</span> Introduction to the Data</a>
<ul>
<li><a href="customer-data-for-a-clothing-company.html#customer-data-for-a-clothing-company" id="toc-customer-data-for-a-clothing-company"><span class="toc-section-number">3.1</span> Customer Data for a Clothing Company</a></li>
<li><a href="swinediseasedata.html#swinediseasedata" id="toc-swinediseasedata"><span class="toc-section-number">3.2</span> Swine Disease Breakout Data</a></li>
<li><a href="mnist-dataset.html#mnist-dataset" id="toc-mnist-dataset"><span class="toc-section-number">3.3</span> MNIST Dataset</a></li>
<li><a href="imdb-dataset.html#imdb-dataset" id="toc-imdb-dataset"><span class="toc-section-number">3.4</span> IMDB Dataset</a></li>
</ul></li>
<li><a href="bigdatacloudplatform.html#bigdatacloudplatform" id="toc-bigdatacloudplatform"><span class="toc-section-number">4</span> Big Data Cloud Platform</a>
<ul>
<li><a href="power-of-cluster-of-computers.html#power-of-cluster-of-computers" id="toc-power-of-cluster-of-computers"><span class="toc-section-number">4.1</span> Power of Cluster of Computers</a></li>
<li><a href="evolution-of-cluster-computing.html#evolution-of-cluster-computing" id="toc-evolution-of-cluster-computing"><span class="toc-section-number">4.2</span> Evolution of Cluster Computing</a>
<ul>
<li><a href="evolution-of-cluster-computing.html#hadoop" id="toc-hadoop"><span class="toc-section-number">4.2.1</span> Hadoop</a></li>
<li><a href="evolution-of-cluster-computing.html#spark" id="toc-spark"><span class="toc-section-number">4.2.2</span> Spark</a></li>
</ul></li>
<li><a href="CloudEnvironment.html#CloudEnvironment" id="toc-CloudEnvironment"><span class="toc-section-number">4.3</span> Introduction of Cloud Environment</a>
<ul>
<li><a href="CloudEnvironment.html#open-account-and-create-a-cluster" id="toc-open-account-and-create-a-cluster"><span class="toc-section-number">4.3.1</span> Open Account and Create a Cluster</a></li>
<li><a href="CloudEnvironment.html#r-notebook" id="toc-r-notebook"><span class="toc-section-number">4.3.2</span> R Notebook</a></li>
<li><a href="CloudEnvironment.html#markdown-cells" id="toc-markdown-cells"><span class="toc-section-number">4.3.3</span> Markdown Cells</a></li>
</ul></li>
<li><a href="leveragesparkr.html#leveragesparkr" id="toc-leveragesparkr"><span class="toc-section-number">4.4</span> Leverage Spark Using R Notebook</a></li>
<li><a href="databases-and-sql.html#databases-and-sql" id="toc-databases-and-sql"><span class="toc-section-number">4.5</span> Databases and SQL</a>
<ul>
<li><a href="databases-and-sql.html#history" id="toc-history"><span class="toc-section-number">4.5.1</span> History</a></li>
<li><a href="databases-and-sql.html#database-table-and-view" id="toc-database-table-and-view"><span class="toc-section-number">4.5.2</span> Database, Table and View</a></li>
<li><a href="databases-and-sql.html#basic-sql-statement" id="toc-basic-sql-statement"><span class="toc-section-number">4.5.3</span> Basic SQL Statement</a></li>
<li><a href="databases-and-sql.html#advanced-topics-in-database" id="toc-advanced-topics-in-database"><span class="toc-section-number">4.5.4</span> Advanced Topics in Database</a></li>
</ul></li>
</ul></li>
<li><a href="datapreprocessing.html#datapreprocessing" id="toc-datapreprocessing"><span class="toc-section-number">5</span> Data Pre-processing</a>
<ul>
<li><a href="data-cleaning.html#data-cleaning" id="toc-data-cleaning"><span class="toc-section-number">5.1</span> Data Cleaning</a></li>
<li><a href="missing-values.html#missing-values" id="toc-missing-values"><span class="toc-section-number">5.2</span> Missing Values</a>
<ul>
<li><a href="missing-values.html#impute-missing-values-with-medianmode" id="toc-impute-missing-values-with-medianmode"><span class="toc-section-number">5.2.1</span> Impute missing values with median/mode</a></li>
<li><a href="missing-values.html#k-nearest-neighbors" id="toc-k-nearest-neighbors"><span class="toc-section-number">5.2.2</span> K-nearest neighbors</a></li>
<li><a href="missing-values.html#bagging-tree" id="toc-bagging-tree"><span class="toc-section-number">5.2.3</span> Bagging Tree</a></li>
</ul></li>
<li><a href="centering-and-scaling.html#centering-and-scaling" id="toc-centering-and-scaling"><span class="toc-section-number">5.3</span> Centering and Scaling</a></li>
<li><a href="resolve-skewness.html#resolve-skewness" id="toc-resolve-skewness"><span class="toc-section-number">5.4</span> Resolve Skewness</a></li>
<li><a href="outliers.html#outliers" id="toc-outliers"><span class="toc-section-number">5.5</span> Resolve Outliers</a></li>
<li><a href="collinearity.html#collinearity" id="toc-collinearity"><span class="toc-section-number">5.6</span> Collinearity</a></li>
<li><a href="sparse-variables.html#sparse-variables" id="toc-sparse-variables"><span class="toc-section-number">5.7</span> Sparse Variables</a></li>
<li><a href="re-encode-dummy-variables.html#re-encode-dummy-variables" id="toc-re-encode-dummy-variables"><span class="toc-section-number">5.8</span> Re-encode Dummy Variables</a></li>
</ul></li>
<li><a href="datawrangline.html#datawrangline" id="toc-datawrangline"><span class="toc-section-number">6</span> Data Wrangling</a>
<ul>
<li><a href="summarize-data.html#summarize-data" id="toc-summarize-data"><span class="toc-section-number">6.1</span> Summarize Data</a>
<ul>
<li><a href="summarize-data.html#dplyr-package" id="toc-dplyr-package"><span class="toc-section-number">6.1.1</span> <code>dplyr</code> package</a></li>
<li><a href="summarize-data.html#applyfamilyinbaser" id="toc-applyfamilyinbaser"><span class="toc-section-number">6.1.2</span> <code>apply()</code>, <code>lapply()</code> and <code>sapply()</code> in base R</a></li>
</ul></li>
<li><a href="tidy-and-reshape-data.html#tidy-and-reshape-data" id="toc-tidy-and-reshape-data"><span class="toc-section-number">6.2</span> Tidy and Reshape Data</a></li>
</ul></li>
<li><a href="modeltuningstrategy.html#modeltuningstrategy" id="toc-modeltuningstrategy"><span class="toc-section-number">7</span> Model Tuning Strategy</a>
<ul>
<li><a href="vbtradeoff.html#vbtradeoff" id="toc-vbtradeoff"><span class="toc-section-number">7.1</span> Variance-Bias Trade-Off</a></li>
<li><a href="datasplittingresampling.html#datasplittingresampling" id="toc-datasplittingresampling"><span class="toc-section-number">7.2</span> Data Splitting and Resampling</a>
<ul>
<li><a href="datasplittingresampling.html#datasplitting" id="toc-datasplitting"><span class="toc-section-number">7.2.1</span> Data Splitting</a></li>
<li><a href="datasplittingresampling.html#resampling" id="toc-resampling"><span class="toc-section-number">7.2.2</span> Resampling</a></li>
</ul></li>
</ul></li>
<li><a href="measuring-performance.html#measuring-performance" id="toc-measuring-performance"><span class="toc-section-number">8</span> Measuring Performance</a>
<ul>
<li><a href="regression-model-performance.html#regression-model-performance" id="toc-regression-model-performance"><span class="toc-section-number">8.1</span> Regression Model Performance</a></li>
<li><a href="classification-model-performance.html#classification-model-performance" id="toc-classification-model-performance"><span class="toc-section-number">8.2</span> Classification Model Performance</a>
<ul>
<li><a href="classification-model-performance.html#confusion-matrix" id="toc-confusion-matrix"><span class="toc-section-number">8.2.1</span> Confusion Matrix</a></li>
<li><a href="classification-model-performance.html#kappa-statistic" id="toc-kappa-statistic"><span class="toc-section-number">8.2.2</span> Kappa Statistic</a></li>
<li><a href="classification-model-performance.html#roc" id="toc-roc"><span class="toc-section-number">8.2.3</span> ROC</a></li>
<li><a href="classification-model-performance.html#gain-and-lift-charts" id="toc-gain-and-lift-charts"><span class="toc-section-number">8.2.4</span> Gain and Lift Charts</a></li>
</ul></li>
</ul></li>
<li><a href="regression-models.html#regression-models" id="toc-regression-models"><span class="toc-section-number">9</span> Regression Models</a>
<ul>
<li><a href="ordinary-least-square.html#ordinary-least-square" id="toc-ordinary-least-square"><span class="toc-section-number">9.1</span> Ordinary Least Square</a>
<ul>
<li><a href="ordinary-least-square.html#the-magic-p-value" id="toc-the-magic-p-value"><span class="toc-section-number">9.1.1</span> The Magic P-value</a></li>
<li><a href="ordinary-least-square.html#diagnostics-for-linear-regression" id="toc-diagnostics-for-linear-regression"><span class="toc-section-number">9.1.2</span> Diagnostics for Linear Regression</a></li>
</ul></li>
<li><a href="principal-component-regression-and-partial-least-square.html#principal-component-regression-and-partial-least-square" id="toc-principal-component-regression-and-partial-least-square"><span class="toc-section-number">9.2</span> Principal Component Regression and Partial Least Square</a></li>
</ul></li>
<li><a href="regularization-methods.html#regularization-methods" id="toc-regularization-methods"><span class="toc-section-number">10</span> Regularization Methods</a>
<ul>
<li><a href="ridge-regression.html#ridge-regression" id="toc-ridge-regression"><span class="toc-section-number">10.1</span> Ridge Regression</a></li>
<li><a href="lasso.html#lasso" id="toc-lasso"><span class="toc-section-number">10.2</span> LASSO</a></li>
<li><a href="elastic-net.html#elastic-net" id="toc-elastic-net"><span class="toc-section-number">10.3</span> Elastic Net</a></li>
<li><a href="penalized-generalized-linear-model.html#penalized-generalized-linear-model" id="toc-penalized-generalized-linear-model"><span class="toc-section-number">10.4</span> Penalized Generalized Linear Model</a>
<ul>
<li><a href="penalized-generalized-linear-model.html#introduction-to-glmnet-package" id="toc-introduction-to-glmnet-package"><span class="toc-section-number">10.4.1</span> Introduction to <code>glmnet</code> package</a></li>
<li><a href="penalized-generalized-linear-model.html#penalized-logistic-regression" id="toc-penalized-logistic-regression"><span class="toc-section-number">10.4.2</span> Penalized logistic regression</a></li>
</ul></li>
</ul></li>
<li><a href="treemodel.html#treemodel" id="toc-treemodel"><span class="toc-section-number">11</span> Tree-Based Methods</a>
<ul>
<li><a href="tree-basics.html#tree-basics" id="toc-tree-basics"><span class="toc-section-number">11.1</span> Tree Basics</a></li>
<li><a href="splitting-criteria.html#splitting-criteria" id="toc-splitting-criteria"><span class="toc-section-number">11.2</span> Splitting Criteria</a>
<ul>
<li><a href="splitting-criteria.html#gini-impurity" id="toc-gini-impurity"><span class="toc-section-number">11.2.1</span> Gini impurity</a></li>
<li><a href="splitting-criteria.html#information-gain-ig" id="toc-information-gain-ig"><span class="toc-section-number">11.2.2</span> Information Gain (IG)</a></li>
<li><a href="splitting-criteria.html#information-gain-ratio-igr" id="toc-information-gain-ratio-igr"><span class="toc-section-number">11.2.3</span> Information Gain Ratio (IGR)</a></li>
<li><a href="splitting-criteria.html#sum-of-squared-error-sse" id="toc-sum-of-squared-error-sse"><span class="toc-section-number">11.2.4</span> Sum of Squared Error (SSE)</a></li>
</ul></li>
<li><a href="tree-pruning.html#tree-pruning" id="toc-tree-pruning"><span class="toc-section-number">11.3</span> Tree Pruning</a></li>
<li><a href="regression-and-decision-tree-basic.html#regression-and-decision-tree-basic" id="toc-regression-and-decision-tree-basic"><span class="toc-section-number">11.4</span> Regression and Decision Tree Basic</a>
<ul>
<li><a href="regression-and-decision-tree-basic.html#regression-tree" id="toc-regression-tree"><span class="toc-section-number">11.4.1</span> Regression Tree</a></li>
<li><a href="regression-and-decision-tree-basic.html#decision-tree" id="toc-decision-tree"><span class="toc-section-number">11.4.2</span> Decision Tree</a></li>
</ul></li>
<li><a href="bagging-tree-1.html#bagging-tree-1" id="toc-bagging-tree-1"><span class="toc-section-number">11.5</span> Bagging Tree</a></li>
<li><a href="random-forest.html#random-forest" id="toc-random-forest"><span class="toc-section-number">11.6</span> Random Forest</a></li>
<li><a href="gradient-boosted-machine.html#gradient-boosted-machine" id="toc-gradient-boosted-machine"><span class="toc-section-number">11.7</span> Gradient Boosted Machine</a>
<ul>
<li><a href="gradient-boosted-machine.html#adaptive-boosting" id="toc-adaptive-boosting"><span class="toc-section-number">11.7.1</span> Adaptive Boosting</a></li>
<li><a href="gradient-boosted-machine.html#stochastic-gradient-boosting" id="toc-stochastic-gradient-boosting"><span class="toc-section-number">11.7.2</span> Stochastic Gradient Boosting</a></li>
</ul></li>
</ul></li>
<li><a href="deeplearning.html#deeplearning" id="toc-deeplearning"><span class="toc-section-number">12</span> Deep Learning</a>
<ul>
<li><a href="feedforward-neural-network.html#feedforward-neural-network" id="toc-feedforward-neural-network"><span class="toc-section-number">12.1</span> Feedforward Neural Network</a>
<ul>
<li><a href="feedforward-neural-network.html#logisticregasneuralnetwork" id="toc-logisticregasneuralnetwork"><span class="toc-section-number">12.1.1</span> Logistic Regression as Neural Network</a></li>
<li><a href="feedforward-neural-network.html#stochastic-gradient-descent" id="toc-stochastic-gradient-descent"><span class="toc-section-number">12.1.2</span> Stochastic Gradient Descent</a></li>
<li><a href="feedforward-neural-network.html#deepneuralnetwork" id="toc-deepneuralnetwork"><span class="toc-section-number">12.1.3</span> Deep Neural Network</a></li>
<li><a href="feedforward-neural-network.html#activationfunction" id="toc-activationfunction"><span class="toc-section-number">12.1.4</span> Activation Function</a></li>
<li><a href="feedforward-neural-network.html#optimization" id="toc-optimization"><span class="toc-section-number">12.1.5</span> Optimization</a></li>
<li><a href="feedforward-neural-network.html#deal-with-overfitting" id="toc-deal-with-overfitting"><span class="toc-section-number">12.1.6</span> Deal with Overfitting</a></li>
<li><a href="feedforward-neural-network.html#ffnnexample" id="toc-ffnnexample"><span class="toc-section-number">12.1.7</span> Image Recognition Using FFNN</a></li>
</ul></li>
<li><a href="convolutional-neural-network.html#convolutional-neural-network" id="toc-convolutional-neural-network"><span class="toc-section-number">12.2</span> Convolutional Neural Network</a>
<ul>
<li><a href="convolutional-neural-network.html#convolution-layer" id="toc-convolution-layer"><span class="toc-section-number">12.2.1</span> Convolution Layer</a></li>
<li><a href="convolutional-neural-network.html#padding-layer" id="toc-padding-layer"><span class="toc-section-number">12.2.2</span> Padding Layer</a></li>
<li><a href="convolutional-neural-network.html#pooling-layer" id="toc-pooling-layer"><span class="toc-section-number">12.2.3</span> Pooling Layer</a></li>
<li><a href="convolutional-neural-network.html#convolution-over-volume" id="toc-convolution-over-volume"><span class="toc-section-number">12.2.4</span> Convolution Over Volume</a></li>
<li><a href="convolutional-neural-network.html#cnnexample" id="toc-cnnexample"><span class="toc-section-number">12.2.5</span> Image Recognition Using CNN</a></li>
</ul></li>
<li><a href="recurrent-neural-network.html#recurrent-neural-network" id="toc-recurrent-neural-network"><span class="toc-section-number">12.3</span> Recurrent Neural Network</a>
<ul>
<li><a href="recurrent-neural-network.html#rnn-model" id="toc-rnn-model"><span class="toc-section-number">12.3.1</span> RNN Model</a></li>
<li><a href="recurrent-neural-network.html#lstm" id="toc-lstm"><span class="toc-section-number">12.3.2</span> Long Short Term Memory</a></li>
<li><a href="recurrent-neural-network.html#embedding" id="toc-embedding"><span class="toc-section-number">12.3.3</span> Word Embedding</a></li>
<li><a href="recurrent-neural-network.html#rnnexample" id="toc-rnnexample"><span class="toc-section-number">12.3.4</span> Sentiment Analysis Using RNN</a></li>
</ul></li>
</ul></li>
<li><a href="appendix.html#appendix" id="toc-appendix">Appendix</a></li>
<li><a href="largelocaldata.html#largelocaldata" id="toc-largelocaldata"><span class="toc-section-number">13</span> Handling Large Local Data</a>
<ul>
<li><a href="readr.html#readr" id="toc-readr"><span class="toc-section-number">13.1</span> <code>readr</code></a></li>
<li><a href="data.table-enhanced-data.html#data.table-enhanced-data.frame" id="toc-data.table-enhanced-data.frame"><span class="toc-section-number">13.2</span> <code>data.table</code>— enhanced <code>data.frame</code></a></li>
</ul></li>
<li><a href="r-code-for-data-simulation.html#r-code-for-data-simulation" id="toc-r-code-for-data-simulation"><span class="toc-section-number">14</span> R code for data simulation</a>
<ul>
<li><a href="appendixdata1.html#appendixdata1" id="toc-appendixdata1"><span class="toc-section-number">14.1</span> Customer Data for Clothing Company</a></li>
<li><a href="appendixdata3.html#appendixdata3" id="toc-appendixdata3"><span class="toc-section-number">14.2</span> Swine Disease Breakout Data</a></li>
</ul></li>
<li><a href="references.html#references" id="toc-references">References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Practitioner’s Guide to Data Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="recurrent-neural-network" class="section level2 hasAnchor" number="12.3">
<h2><span class="header-section-number">12.3</span> Recurrent Neural Network<a href="recurrent-neural-network.html#recurrent-neural-network" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Traditional neural networks don’t have a framework that can handle sequential events where the later events are based on the previous ones. For example, map an input audio clip to a text transcript where the input is voice over time, and the output is the corresponding sequence of words over time. Recurrent Neural Network is a deep-learning model that can process this type of sequential data.</p>
<p>The recurrent neural network allows information to flow from one step to the next with a repetitive structure. Figure <a href="recurrent-neural-network.html#fig:rnnunit">12.20</a> shows the basic chunk of an RNN network. You combine the activated neuro from the previous step with the current input <span class="math inline">\(x^{&lt;t&gt;}\)</span> to produce an output <span class="math inline">\(\hat{y}^{&lt;t&gt;}\)</span> and an updated activated neuro to support the next input at <span class="math inline">\(t+1\)</span>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:rnnunit"></span>
<img src="images/rnnunit.png" alt="Recurrent Neural Network Unit" width="40%" />
<p class="caption">
FIGURE 12.20: Recurrent Neural Network Unit
</p>
</div>
<p>So the whole process repeats a similar pattern. If we unroll the loop (figure <a href="recurrent-neural-network.html#fig:unrolledrnn">12.21</a>, the chain-like recurrent nature makes it the natural architecture for sequential data.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unrolledrnn"></span>
<img src="images/rnnrollout.png" alt="An Unrolled Recurrent Neural Network" width="90%" />
<p class="caption">
FIGURE 12.21: An Unrolled Recurrent Neural Network
</p>
</div>
<p>There is incredible success applying RNNs to this type of problems:</p>
<ul>
<li>Machine translation</li>
<li>Voice recognition</li>
<li>Music generation</li>
<li>Sentiment analysis</li>
</ul>
<p>A trained CNN accepts a fixed-sized vector as input (such as <span class="math inline">\(28 \times 28\)</span> image) and produces a fixed-sized vector as output (such as the probabilities of being one the ten digits). RNN has a much more flexible structure. It can operate over sequences of vectors and produces sequences of outputs and they can vary in size. To understand what it means, let’s look at some RNN structure examples.</p>
<center>
<img src="images/rnnstrs.png" style="width:100.0%" />
</center>
<p>The rectangle represents a vector and the arrow represents matrix multiplications. The input vector is in green and the output vector is in blue. The red rectangle holds the intermediate state. From left to right:</p>
<ul>
<li>one-to-one: model takes a fixed size input and produces a fixed size output, such as CNN. it is not sequential.</li>
<li>one-to-many: model takes one input and generate a sequence of output, such as the music generation.</li>
<li>many-to-one: model takes a sequence of input and produces a single output, such as sentiment analysis.</li>
<li>many-to-many: model takes a sequence of input and produces a sequence of output. The input size can be the same with the output size (such as name entity recognition) or it can be different (such as machine translation).</li>
</ul>
<div id="rnn-model" class="section level3 hasAnchor" number="12.3.1">
<h3><span class="header-section-number">12.3.1</span> RNN Model<a href="recurrent-neural-network.html#rnn-model" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>To further understand the RNN model, let’s look at an entity recognition example. Assume you want to build a sequence model to recognize the company or computer language names in a sentence like this: “Use Netlify and Hugo”. It is a name recognition problem which is used by the research company to index different company names in the articles. For tasks like this, we need a model that can learn and share the learning across different texts. The position of the word has important information about the word. For example, the word before “to” is more likely to be a verb than a noun. It is also used in material science to tag chemicals mentioned in the most recent journals to find any indication of the next research topic.</p>
<p>Given input sentence x, you want a model to produce one output for each word in x that tells you if that word is the name for something. So in this example, the input is a sequence of 5 words including the period in the end. The output is a sequence of 0/1 with the same length that indicates whether the input word is a name (1) or not (0). We use superscript <span class="math inline">\(&lt;t&gt;\)</span> to denote the element position of input and output; use superscript <span class="math inline">\((i)\)</span> to denote the <span class="math inline">\(i^{th}\)</span> sample (you will have different sentences in the training data); Use <span class="math inline">\(T_x^{(i)}\)</span> to denote the length of <span class="math inline">\(i^{th}\)</span> input, <span class="math inline">\(T_y^{(i)}\)</span> for output. In this case, <span class="math inline">\(T_x^{(i)}\)</span> is equal to <span class="math inline">\(T_y^{(i)}\)</span>.</p>
<center>
<img src="images/rnn_notation.png" style="width:70.0%" />
</center>
<p>Before we build a neural network, we need to decide a way to represent individual words in numbers. What should <span class="math inline">\(x^{&lt;1&gt;}\)</span> be? In practice, people use word embedding which we will discuss in the later section. Here, for illustration, we use the one-hot encoding word representation. Assume we have a dictionary of 10,000 unique words. You can build the dictionary by finding the top 10,000 occurring words in your training set. Each word in your training set will have a position in the dictionary sequence. For example, “use” is the 8320th element of the dictionary sequence. So <span class="math inline">\(x^{&lt;1&gt;}\)</span> is a vector with all zeros except for a one on position 8320. Using this one-hot representation, each input <span class="math inline">\(x^{&lt;t&gt;}\)</span> is a vector with all zeros except for one element.</p>
<center>
<img src="images/nn_ohe.png" style="width:70.0%" />
</center>
<p>Given this representation of input words, the goal is to learn a sequence model that maps the input words to output y, indicating if the word is an entity (1) or not (0). Let us build a one-layer recurrent neural network. The model starts from the first word “use” (<span class="math inline">\(x^{&lt;1&gt;}\)</span>) and build a neural network to predict the output. To start the process, we also need to initialize the activation at time 0, <span class="math inline">\(a_0\)</span>. The most common choice is to use a vector of zeros. The common activation function for the intermediate layer is the Hyperbolic Tangent Function (tanh). RNN uses other methods to prevent the vanishing gradient problem discussed in section <a href="recurrent-neural-network.html#lstm">12.3.2</a>. Similar to FFNN, The output layer activation function depends on the output type. The current example is a binary classification, so we use the sigmoid function (<span class="math inline">\(\sigma\)</span>).</p>
<p><span class="math display">\[a^{&lt;0&gt;} = \mathbf{0};\ a^{&lt;1&gt;} = tanh(W_{aa}a^{&lt;0&gt;} + W_{ax}x^{&lt;1&gt;}+b_a)\]</span>
<span class="math display">\[\hat{y}^{&lt;1&gt;} = \sigma(W_{ya}a^{&lt;1&gt;}+b_y)\]</span>
And when it takes the second word <span class="math inline">\(x^{&lt;2&gt;}\)</span>, it also gets information from the previous step using the non-activated neurons.</p>
<p><span class="math display">\[a^{&lt;2&gt;} = tanh(W_{aa}a^{&lt;1&gt;}+W_{ax}x^{&lt;2&gt;}+b_a)\]</span>
<span class="math display">\[\hat{y}^{&lt;2&gt;} = \sigma(W_{ya}a^{&lt;2&gt;}+b_y)\]</span></p>
<p>For the <span class="math inline">\(t^{th}\)</span> word:</p>
<p><span class="math display">\[a^{&lt;t&gt;} = tanh(W_{aa}a^{&lt;t-1&gt;}+W_{ax}x^{&lt;t&gt;}+b_a)\]</span>
<span class="math display">\[\hat{y}^{&lt;t&gt;} = \sigma(W_{ya}a^{&lt;t&gt;}+b_y)\]</span></p>
<p>The information flows from one step to the next with a repetitive structure until the last time step input <span class="math inline">\(x^{&lt;T_x&gt;}\)</span> and then it outputs <span class="math inline">\(\hat{y}^{&lt;T_y&gt;}\)</span>. In this example, <span class="math inline">\(T_x = T_y\)</span>. The architecture changes when <span class="math inline">\(T_x\)</span> and <span class="math inline">\(T_y\)</span> are not the same. The model shares parameters, <span class="math inline">\(W_{ya}, W_{aa}, W_{ax},b_a, b_y\)</span>, for all time steps of the input.</p>
<center>
<img src="images/RNN_pro.png" style="width:80.0%" />
</center>
<p>Calculate the loss function:</p>
<p><span class="math display">\[L^{&lt;t&gt;}(\hat{y}^{&lt;t&gt;}) = -y^{&lt;t&gt;}log(\hat{y}^{&lt;t&gt;})-(1-y^{&lt;t&gt;})log(1-\hat{y}^{&lt;t&gt;})\]</span>
<span class="math display">\[L(\hat{y},y)=\Sigma_{t=1}^{T_y}L^{&lt;t&gt;}(\hat{y},y)\]</span>
The above defines the forward process. Same as before, the backward propagation computes the gradient descent for the parameters by the chain rule for differentiation.</p>
<p>In this RNN structure, the information only flows from the left to the right. So at any position, it only uses data from earlier in the sequence to make a prediction. It does not work when predicting the current word needs information from later words. For example, consider the following two sentences:</p>
<ol style="list-style-type: decimal">
<li>Do you like April Kepner in Grey’s Anatomy?</li>
<li>Do you like April in Los Angeles? It is not too hot.</li>
</ol>
<p>Given just the first three words is not enough to know if the word “April” is part of a person’s name. It is a person’s name in 1 but not 2. The two sentences have the same first three words. In this case, we need a model that allows the information to flow in both directions. A bidirectional RNN takes data from both earlier and later in the sequence. The disadvantage is that it needs the entire word sequence to predict at any position. For a speech recognition application that requires capturing the speech in real-time, we need a more complex method called the attention model. We will not get into those models here. Deep Learning with R <span class="citation">(<a href="#ref-deeplearningr18" role="doc-biblioref">Chollet and Allaire 2018</a>)</span> provides a high-level introduction of bidirectional RNN with applicable codes. It teaches both intuition and practical, computational usage of deep learning models. For python users, refer to Deep Learning with Python <span class="citation">(<a href="#ref-deeplearningpy17" role="doc-biblioref">Chollet 2017</a>)</span>. A standard text with heavy mathematics is Deep Learning <span class="citation">(<a href="#ref-Goodfellow-et-al-2016" role="doc-biblioref">Goodfellow, Bengio, and Courville 2016</a>)</span>.</p>
</div>
<div id="lstm" class="section level3 hasAnchor" number="12.3.2">
<h3><span class="header-section-number">12.3.2</span> Long Short Term Memory<a href="recurrent-neural-network.html#lstm" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The sequence in RNN can be very long, which leads to the vanishing gradient problem even when the RNN network is not deep. Think about the following examples:</p>
<ol style="list-style-type: decimal">
<li>The <strong>girl</strong> walked away, sat down in the shade of a tree, and began to read a new book which <strong>she</strong> bought the day before.</li>
<li>The <strong>boy</strong> walked away, sat down in the shade of a tree, and began to read a new book which <strong>he</strong> bought the day before.</li>
</ol>
<p>For sentence 1, you need to use “she” in the adjective clause after “which” because it is a girl. For sentence 2, you need to use “he” because it is a boy. This is a long-term dependency example where the information at the beginning can affect what needs to come much later in the sentence. RNN needs to forward propagate information from left to right and then backpropagate from right to left. It can be difficult for the error associated with the later sequence to affect the optimization earlier. So in practice, it means the model might fail to do the task mentioned above. People came up with different methods to mitigate this issue, such as the Greater Recurrent Units (GRU) <span class="citation">(<a href="#ref-chung2014empirical" role="doc-biblioref">Chung et al. 2014</a>)</span> and Long Short Term Memory Units (LSTM) <span class="citation">(<a href="#ref-lstm1997" role="doc-biblioref">Hochreiter and Schmidhuber 1997</a>)</span>. The goal is to help the model memorize information in the earlier sequence. We are going to walk through LSTM step by step.</p>
<p>The first step of LSTM is to decide what information to <strong>forget</strong>. This decision is made by “forget gate”, a sigmoid function (<span class="math inline">\(\Gamma_{f}\)</span>). It looks at <span class="math inline">\(a^{&lt;t-1&gt;}\)</span> and <span class="math inline">\(x^{t}\)</span> and outputs a number between 0 and 1 for each number in the cell state <span class="math inline">\(c^{t-1}\)</span>. A value 1 means “completely remember the state”, while 0 means “completely forget the state”.</p>
<center>
<img src="images/lstm1.png" style="width:80.0%" />
</center>
<p>The next step is to decide what new information we’re going to add to the cell state. This step includes two parts:</p>
<ol style="list-style-type: decimal">
<li>input gate (<span class="math inline">\(\Gamma_{u}\)</span>): a sigmoid function that decides how much we want to update</li>
<li>a vector of new candidate value (<span class="math inline">\(\tilde{c}^{&lt;t&gt;}\)</span>)</li>
</ol>
<center>
<img src="images/lstm2.png" style="width:80.0%" />
</center>
<p>The multiplication of the above two parts <span class="math inline">\(\Gamma_{u}*\tilde{c}^{&lt;t&gt;}\)</span> is the new candidate scaled by the input gate. We then combine the results we get so far to get new cell state <span class="math inline">\(c^{&lt;t&gt;}\)</span>.</p>
<center>
<img src="images/lstm3.png" style="width:80.0%" />
</center>
<p>Finally, we need to decide what we are going to output. The output is a filtered version of the new cell state <span class="math inline">\(c^{&lt;t&gt;}\)</span>.</p>
<center>
<img src="images/lstm4.png" style="width:80.0%" />
</center>
</div>
<div id="embedding" class="section level3 hasAnchor" number="12.3.3">
<h3><span class="header-section-number">12.3.3</span> Word Embedding<a href="recurrent-neural-network.html#embedding" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>So far, we have been using one-hot encoding to represent the words. This representation is sparse and doesn’t capture the relationship between the words. For example, if your model learns from the training data that the word after “pumpkin” in the first sentence is “pie,” can it fill the second sentence’s blank with “pie”?</p>
<ol style="list-style-type: decimal">
<li>[training data] My favorite Christmas dessert is pumpkin pie.</li>
<li>[testing data] My favorite Christmas dessert is apple ____.</li>
</ol>
<p>The algorithm can not learn the relationship between “pumpkin” and “apple” by the distance between the one-hot encoding for the two words. If we can find a way to create features to represent the words, we may teach the model to learn that pumpkin and apple is food. And the distance between the feature representations of these two words is closer than, for example, “apple” and “nurse.” So when the model sees “apple” in a new sentence and needs to predict the word after, it is more likely to choose “pie” since it sees “pumpkin pie” in the training data. The idea of word embedding is to learn a set of features to represent words. For example, you can score each word in the dictionary according to a group of features like this:</p>
<center>
<img src="images/rnn_embedding1.png" style="width:90.0%" />
</center>
<p>The word “male” has a score of -1 for the “gender” feature, “female” has a score of 1. Both “Apple” and “pumpkin” have a high score for the “food” feature and much lower scores for the rest. You can set the number of features to learn, usually more than what we list in the above figure. If you use 200 features to represent the words, then the learned embedding for each word is a vector with a length of 200.</p>
<p>For language-related applications, text embedding is the most critical step. It converts raw text into a meaningful vector representation. Once we have a vector representation, it is easy to calculate typical numerical metrics such as cosine similarity. There are many pre-trained text embeddings available for us to use. We will briefly introduce some of these popular embeddings.</p>
<p>The first widely used embedding is word2vec. It was first introduced in 2013 and was trained by a large collection of text in an unsupervised way. Training the word2vec embedding vector uses bag-of-words or skip-gram. In the bag-of-words architecture, the model predicts the current word based on a window of surrounding context words. In skip-gram architecture, the model uses the current word to predict the surrounding window of context words. There are pre-trained word2vec embeddings based on a large amount of text (such as wiki pages, news reports, etc.) for general applications.</p>
<p>GloVe (Global Vectors) embedding is an extension of word2vec and performs better. It uses a unique version of the square loss function. However, words are composite of meaningful components such as radicals.<br />
“eat” and “eaten” are different forms of the same word. Both word2vec and GloVe use word-level information, and they treat each word uniquely based on its context.</p>
<p>The fastText embedding is introduced to use the word’s internal structure to make the process more efficient. It uses morphological information to extend the skip-gram model. New words that are not in the training data can be repressed well. It also supports 150+ different languages. The above-mentioned embeddings (word2vec, GloVe, and fastText) do not consider the words’ context (i.e., the same word has the same embedding vector). However, the same word may have different meanings in a different context.</p>
<p>More recently transformer based networks, such as BERT (Bidirectional Encoder Representations from Transformers), were introduced to add context-level information in text-related applications. These models use a new mechanism, attention, to read a sequence simultaneously instead of the one-input-at-a-time process of RNNs. These networks combine positional embeddings along with embeddings for each token in the sequence giving it the ability to differentiate different uses of the same word based on surrounding words.</p>
</div>
<div id="rnnexample" class="section level3 hasAnchor" number="12.3.4">
<h3><span class="header-section-number">12.3.4</span> Sentiment Analysis Using RNN<a href="recurrent-neural-network.html#rnnexample" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In this section, we will walk through an example of text sentiment analysis using RNN. Refer to section <a href="CloudEnvironment.html#CloudEnvironment">4.3</a> to set up an account, create a notebook (R or Python) and start a cluster. Refer to section <a href="feedforward-neural-network.html#ffnnexample">12.1.7</a> for package installation.</p>
<p>We will use the IMDB movie review data. It is one of the most used datasets for text-related machine learning methods. The datasets’ inputs are movie reviews published at IMDB in its raw text format, and the output is a binary sentiment indicator( “1” for positive and “0” for negative) created through human evaluation. The training and testing data have 25,000 records each. Each review varies in length.</p>
<div id="data-preprocessing-2" class="section level4 hasAnchor" number="12.3.4.1">
<h4><span class="header-section-number">12.3.4.1</span> Data preprocessing<a href="recurrent-neural-network.html#data-preprocessing-2" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Machine learning algorithms can not deal with raw text, and we have to convert text into numbers before feeding it into an algorithm. Tokenization is one way to convert text data into a numerical representation. For example, suppose we have 500 unique words for all reviews in the training dataset. We can label each word by the rank (i.e., from 1 to 500) of their frequency in the training data. Then each word is replaced by an integer between 1 to 500. This way, we can map each movie review from its raw text format to a sequence of integers.</p>
<p>As reviews can have different lengths, sequences of integers will have different sizes too. So another important step is to make sure each input has the same length by padding or truncating. For example, we can set a length of 50 words, and for any reviews less than 50 words, we can pad 0 to make it 50 in length; and for reviews with more than 50 words, we can truncate the sequence to 50 by keeping only the first 50 words. After padding and truncating, we have a typical data frame, each row is an observation, and each column is a feature. The number of features is the number of words designed for each review (i.e., 50 in this example).</p>
<p>After tokenization, the numerical input is just a naive mapping to the original words, and the integers do not have their usual numerical meanings. We need to use embedding to convert these categorical integers to more meaningful representations. The word embedding captures the inherited relationship of words and dramatically reduces the input dimension (see section <a href="recurrent-neural-network.html#embedding">12.3.3</a>). The dimension is a vector space representing the entire vocabulary. It can be 128 or 256, and the vector space dimension is the same when the vocabulary changes. It has a lower dimension, and each vector is filled with real numbers. The embedding vectors can be learned from the training data, or we can use pre-trained embedding models. There are many pre-trained embeddings for us to use, such as Word2Vec, BIRD.</p>
<img src="images/TokenizingPadding.png" style="width:80.0%" />
<center>
</div>
<div id="r-code-for-imdb-dataset" class="section level4 hasAnchor" number="12.3.4.2">
<h4><span class="header-section-number">12.3.4.2</span> R code for IMDB dataset<a href="recurrent-neural-network.html#r-code-for-imdb-dataset" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The IMDB dataset is preloaded for <code>keras</code> and we can call <code>dataset_imdb()</code> to load a partially pre-processed dataset into a data frame. We can define a few parameters in that function. <code>num_words</code> is the number of words in each review to be used. All the unique words are ranked by their frequency counts in the training dataset. The <code>dataset_imdb()</code> function keeps the top <code>num_words</code> words and replaces other words with a default value of 2, and using integers to represent text (i.e., top frequency word will be replaced by 3 and 0, 1, 2 are reserved for “padding,” “start of the sequence,” and “unknown.” ).</p>
<div class="sourceCode" id="cb384"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb384-1"><a href="recurrent-neural-network.html#cb384-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load `keras` package</span></span>
<span id="cb384-2"><a href="recurrent-neural-network.html#cb384-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(keras)</span>
<span id="cb384-3"><a href="recurrent-neural-network.html#cb384-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb384-4"><a href="recurrent-neural-network.html#cb384-4" aria-hidden="true" tabindex="-1"></a><span class="co"># consider only the top 10,000 words in the dataset</span></span>
<span id="cb384-5"><a href="recurrent-neural-network.html#cb384-5" aria-hidden="true" tabindex="-1"></a>max_unique_word <span class="ot">&lt;-</span> <span class="dv">2500</span></span>
<span id="cb384-6"><a href="recurrent-neural-network.html#cb384-6" aria-hidden="true" tabindex="-1"></a><span class="co"># cut off reviews after 100 words</span></span>
<span id="cb384-7"><a href="recurrent-neural-network.html#cb384-7" aria-hidden="true" tabindex="-1"></a>max_review_len <span class="ot">&lt;-</span> <span class="dv">100</span></span></code></pre></div>
<p>Now we load the IMDB dataset, and we can check the structure of the loaded object by using <code>str()</code> command.</p>
<div class="sourceCode" id="cb385"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb385-1"><a href="recurrent-neural-network.html#cb385-1" aria-hidden="true" tabindex="-1"></a>my_imdb <span class="ot">&lt;-</span> <span class="fu">dataset_imdb</span>(<span class="at">num_words =</span> max_unique_word)</span>
<span id="cb385-2"><a href="recurrent-neural-network.html#cb385-2" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(my_imdb)</span></code></pre></div>
<div class="sourceCode" id="cb386"><pre class="sourceCode html"><code class="sourceCode html"><span id="cb386-1"><a href="recurrent-neural-network.html#cb386-1" aria-hidden="true" tabindex="-1"></a>Downloading data from</span>
<span id="cb386-2"><a href="recurrent-neural-network.html#cb386-2" aria-hidden="true" tabindex="-1"></a>https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz</span>
<span id="cb386-3"><a href="recurrent-neural-network.html#cb386-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb386-4"><a href="recurrent-neural-network.html#cb386-4" aria-hidden="true" tabindex="-1"></a>    8192/17464789 [..............................] - ETA: 0s</span>
<span id="cb386-5"><a href="recurrent-neural-network.html#cb386-5" aria-hidden="true" tabindex="-1"></a>  811008/17464789 [&gt;.............................] - ETA: 1s</span>
<span id="cb386-6"><a href="recurrent-neural-network.html#cb386-6" aria-hidden="true" tabindex="-1"></a> 4202496/17464789 [======&gt;.......................] - ETA: 0s</span>
<span id="cb386-7"><a href="recurrent-neural-network.html#cb386-7" aria-hidden="true" tabindex="-1"></a>11476992/17464789 [==================&gt;...........] - ETA: 0s</span>
<span id="cb386-8"><a href="recurrent-neural-network.html#cb386-8" aria-hidden="true" tabindex="-1"></a>17465344/17464789 [==============================] - 0s 0us/step</span>
<span id="cb386-9"><a href="recurrent-neural-network.html#cb386-9" aria-hidden="true" tabindex="-1"></a>List of 2</span>
<span id="cb386-10"><a href="recurrent-neural-network.html#cb386-10" aria-hidden="true" tabindex="-1"></a> $ train:List of 2</span>
<span id="cb386-11"><a href="recurrent-neural-network.html#cb386-11" aria-hidden="true" tabindex="-1"></a>  ..$ x:List of 25000</span>
<span id="cb386-12"><a href="recurrent-neural-network.html#cb386-12" aria-hidden="true" tabindex="-1"></a>  .. ..$ : int [1:218] 1 14 22 16 43 530 973 1622 1385 65 ...</span>
<span id="cb386-13"><a href="recurrent-neural-network.html#cb386-13" aria-hidden="true" tabindex="-1"></a>  .. ..$ : int [1:189] 1 194 1153 194 2 78 228 5 6 1463 ...</span>
<span id="cb386-14"><a href="recurrent-neural-network.html#cb386-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb386-15"><a href="recurrent-neural-network.html#cb386-15" aria-hidden="true" tabindex="-1"></a>*** skipped some output ***</span></code></pre></div>
<div class="sourceCode" id="cb387"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb387-1"><a href="recurrent-neural-network.html#cb387-1" aria-hidden="true" tabindex="-1"></a>x_train <span class="ot">&lt;-</span> my_imdb<span class="sc">$</span>train<span class="sc">$</span>x</span>
<span id="cb387-2"><a href="recurrent-neural-network.html#cb387-2" aria-hidden="true" tabindex="-1"></a>y_train <span class="ot">&lt;-</span> my_imdb<span class="sc">$</span>train<span class="sc">$</span>y</span>
<span id="cb387-3"><a href="recurrent-neural-network.html#cb387-3" aria-hidden="true" tabindex="-1"></a>x_test  <span class="ot">&lt;-</span> my_imdb<span class="sc">$</span>test<span class="sc">$</span>x</span>
<span id="cb387-4"><a href="recurrent-neural-network.html#cb387-4" aria-hidden="true" tabindex="-1"></a>y_test  <span class="ot">&lt;-</span> my_imdb<span class="sc">$</span>test<span class="sc">$</span>y</span></code></pre></div>
<p>Next, we do the padding and truncating process.</p>
<div class="sourceCode" id="cb388"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb388-1"><a href="recurrent-neural-network.html#cb388-1" aria-hidden="true" tabindex="-1"></a>x_train <span class="ot">&lt;-</span> <span class="fu">pad_sequences</span>(x_train, <span class="at">maxlen =</span> max_review_len)</span>
<span id="cb388-2"><a href="recurrent-neural-network.html#cb388-2" aria-hidden="true" tabindex="-1"></a>x_test <span class="ot">&lt;-</span> <span class="fu">pad_sequences</span>(x_test, <span class="at">maxlen =</span> max_review_len)</span></code></pre></div>
<p>The <code>x_train</code> and <code>x_test</code> are numerical data frames ready to be used for recurrent neural network models.</p>
<p><strong>Simple Recurrent Neural Network</strong></p>
<p>Like DNN and CNN models we trained in the past, RNN models are relatively easy to train using <code>keras</code> after the pre-processing stage. In the following example, we use <code>layer_embedding()</code> to fit an embedding layer based on the training dataset, which has two parameters: <code>input_dim</code> (the number of unique words) and <code>output_dim</code> (the length of dense vectors). Then, we add a simple RNN layer by calling <code>layer_simple_rnn()</code> and followed by a dense layer <code>layer_dense()</code> to connect to the response binary variable.</p>
<div class="sourceCode" id="cb389"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb389-1"><a href="recurrent-neural-network.html#cb389-1" aria-hidden="true" tabindex="-1"></a>rnn_model <span class="ot">&lt;-</span> <span class="fu">keras_model_sequential</span>()</span>
<span id="cb389-2"><a href="recurrent-neural-network.html#cb389-2" aria-hidden="true" tabindex="-1"></a>rnn_model <span class="sc">%&gt;%</span></span>
<span id="cb389-3"><a href="recurrent-neural-network.html#cb389-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_embedding</span>(<span class="at">input_dim =</span> max_unique_word, <span class="at">output_dim =</span> <span class="dv">128</span>) <span class="sc">%&gt;%</span></span>
<span id="cb389-4"><a href="recurrent-neural-network.html#cb389-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_simple_rnn</span>(<span class="at">units =</span> <span class="dv">64</span>, <span class="at">dropout =</span> <span class="fl">0.2</span>,</span>
<span id="cb389-5"><a href="recurrent-neural-network.html#cb389-5" aria-hidden="true" tabindex="-1"></a>                   <span class="at">recurrent_dropout =</span> <span class="fl">0.2</span>) <span class="sc">%&gt;%</span></span>
<span id="cb389-6"><a href="recurrent-neural-network.html#cb389-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> <span class="dv">1</span>, <span class="at">activation =</span> <span class="st">&#39;sigmoid&#39;</span>)</span></code></pre></div>
<p>We compile the RNN model by defining the loss function, optimizer to use, and metrics to track the same way as DNN and CNN models.</p>
<div class="sourceCode" id="cb390"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb390-1"><a href="recurrent-neural-network.html#cb390-1" aria-hidden="true" tabindex="-1"></a>rnn_model <span class="sc">%&gt;%</span> <span class="fu">compile</span>(</span>
<span id="cb390-2"><a href="recurrent-neural-network.html#cb390-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">loss =</span> <span class="st">&#39;binary_crossentropy&#39;</span>,</span>
<span id="cb390-3"><a href="recurrent-neural-network.html#cb390-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">optimizer =</span> <span class="st">&#39;adam&#39;</span>,</span>
<span id="cb390-4"><a href="recurrent-neural-network.html#cb390-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">metrics =</span> <span class="fu">c</span>(<span class="st">&#39;accuracy&#39;</span>)</span>
<span id="cb390-5"><a href="recurrent-neural-network.html#cb390-5" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<p>Let us define a few more variables before fitting the model: <code>batch_size</code>, <code>epochs</code>, and <code>validation_split</code>. These variables have the same meaning as DNN and CNN models we see in the past.</p>
<div class="sourceCode" id="cb391"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb391-1"><a href="recurrent-neural-network.html#cb391-1" aria-hidden="true" tabindex="-1"></a>batch_size <span class="ot">=</span> <span class="dv">128</span></span>
<span id="cb391-2"><a href="recurrent-neural-network.html#cb391-2" aria-hidden="true" tabindex="-1"></a>epochs <span class="ot">=</span> <span class="dv">5</span></span>
<span id="cb391-3"><a href="recurrent-neural-network.html#cb391-3" aria-hidden="true" tabindex="-1"></a>validation_split <span class="ot">=</span> <span class="fl">0.2</span></span>
<span id="cb391-4"><a href="recurrent-neural-network.html#cb391-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb391-5"><a href="recurrent-neural-network.html#cb391-5" aria-hidden="true" tabindex="-1"></a>rnn_history <span class="ot">&lt;-</span> rnn_model <span class="sc">%&gt;%</span> <span class="fu">fit</span>(</span>
<span id="cb391-6"><a href="recurrent-neural-network.html#cb391-6" aria-hidden="true" tabindex="-1"></a>  x_train, y_train,</span>
<span id="cb391-7"><a href="recurrent-neural-network.html#cb391-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">batch_size =</span> batch_size,</span>
<span id="cb391-8"><a href="recurrent-neural-network.html#cb391-8" aria-hidden="true" tabindex="-1"></a>  <span class="at">epochs =</span> epochs,</span>
<span id="cb391-9"><a href="recurrent-neural-network.html#cb391-9" aria-hidden="true" tabindex="-1"></a>  <span class="at">validation_split =</span> validation_split</span>
<span id="cb391-10"><a href="recurrent-neural-network.html#cb391-10" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<div class="sourceCode" id="cb392"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb392-1"><a href="recurrent-neural-network.html#cb392-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(rnn_history)</span></code></pre></div>
<center>
<img src="images/rnnhandson1.png" style="width:90.0%" />
</center>
<div class="sourceCode" id="cb393"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb393-1"><a href="recurrent-neural-network.html#cb393-1" aria-hidden="true" tabindex="-1"></a>rnn_model <span class="sc">%&gt;%</span></span>
<span id="cb393-2"><a href="recurrent-neural-network.html#cb393-2" aria-hidden="true" tabindex="-1"></a>   <span class="fu">evaluate</span>(x_test, y_test)</span></code></pre></div>
<div class="sourceCode" id="cb394"><pre class="sourceCode html"><code class="sourceCode html"><span id="cb394-1"><a href="recurrent-neural-network.html#cb394-1" aria-hidden="true" tabindex="-1"></a>##      loss  accuracy </span>
<span id="cb394-2"><a href="recurrent-neural-network.html#cb394-2" aria-hidden="true" tabindex="-1"></a>## 0.5441073 0.7216800 </span></code></pre></div>
<p><strong>LSTM RNN Model</strong></p>
<p>A simple RNN layer is a good starting point for learning RNN, but the performance is usually not that good because these long-term dependencies are impossible to learn due to vanishing gradient. Long Short Term Memory RNN model (LSTM) can carry useful information from the earlier words to later words. In <code>keras</code>, it is easy to replace a simple RNN layer with an LSTM layer by using <code>layer_lstm()</code>.</p>
<div class="sourceCode" id="cb395"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb395-1"><a href="recurrent-neural-network.html#cb395-1" aria-hidden="true" tabindex="-1"></a>lstm_model <span class="ot">&lt;-</span> <span class="fu">keras_model_sequential</span>()</span>
<span id="cb395-2"><a href="recurrent-neural-network.html#cb395-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb395-3"><a href="recurrent-neural-network.html#cb395-3" aria-hidden="true" tabindex="-1"></a>lstm_model <span class="sc">%&gt;%</span></span>
<span id="cb395-4"><a href="recurrent-neural-network.html#cb395-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_embedding</span>(<span class="at">input_dim =</span> max_unique_word, <span class="at">output_dim =</span> <span class="dv">128</span>) <span class="sc">%&gt;%</span></span>
<span id="cb395-5"><a href="recurrent-neural-network.html#cb395-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_lstm</span>(<span class="at">units =</span> <span class="dv">64</span>, <span class="at">dropout =</span> <span class="fl">0.2</span>, <span class="at">recurrent_dropout =</span> <span class="fl">0.2</span>) <span class="sc">%&gt;%</span></span>
<span id="cb395-6"><a href="recurrent-neural-network.html#cb395-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">layer_dense</span>(<span class="at">units =</span> <span class="dv">1</span>, <span class="at">activation =</span> <span class="st">&#39;sigmoid&#39;</span>)</span>
<span id="cb395-7"><a href="recurrent-neural-network.html#cb395-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb395-8"><a href="recurrent-neural-network.html#cb395-8" aria-hidden="true" tabindex="-1"></a>lstm_model <span class="sc">%&gt;%</span> <span class="fu">compile</span>(</span>
<span id="cb395-9"><a href="recurrent-neural-network.html#cb395-9" aria-hidden="true" tabindex="-1"></a>  <span class="at">loss =</span> <span class="st">&#39;binary_crossentropy&#39;</span>,</span>
<span id="cb395-10"><a href="recurrent-neural-network.html#cb395-10" aria-hidden="true" tabindex="-1"></a>  <span class="at">optimizer =</span> <span class="st">&#39;adam&#39;</span>,</span>
<span id="cb395-11"><a href="recurrent-neural-network.html#cb395-11" aria-hidden="true" tabindex="-1"></a>  <span class="at">metrics =</span> <span class="fu">c</span>(<span class="st">&#39;accuracy&#39;</span>)</span>
<span id="cb395-12"><a href="recurrent-neural-network.html#cb395-12" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb395-13"><a href="recurrent-neural-network.html#cb395-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb395-14"><a href="recurrent-neural-network.html#cb395-14" aria-hidden="true" tabindex="-1"></a>batch_size <span class="ot">=</span> <span class="dv">128</span></span>
<span id="cb395-15"><a href="recurrent-neural-network.html#cb395-15" aria-hidden="true" tabindex="-1"></a>epochs <span class="ot">=</span> <span class="dv">5</span></span>
<span id="cb395-16"><a href="recurrent-neural-network.html#cb395-16" aria-hidden="true" tabindex="-1"></a>validation_split <span class="ot">=</span> <span class="fl">0.2</span></span>
<span id="cb395-17"><a href="recurrent-neural-network.html#cb395-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb395-18"><a href="recurrent-neural-network.html#cb395-18" aria-hidden="true" tabindex="-1"></a>lstm_history <span class="ot">&lt;-</span> lstm_model <span class="sc">%&gt;%</span> <span class="fu">fit</span>(</span>
<span id="cb395-19"><a href="recurrent-neural-network.html#cb395-19" aria-hidden="true" tabindex="-1"></a>  x_train, y_train,</span>
<span id="cb395-20"><a href="recurrent-neural-network.html#cb395-20" aria-hidden="true" tabindex="-1"></a>  <span class="at">batch_size =</span> batch_size,</span>
<span id="cb395-21"><a href="recurrent-neural-network.html#cb395-21" aria-hidden="true" tabindex="-1"></a>  <span class="at">epochs =</span> epochs,</span>
<span id="cb395-22"><a href="recurrent-neural-network.html#cb395-22" aria-hidden="true" tabindex="-1"></a>  <span class="at">validation_split =</span> validation_split</span>
<span id="cb395-23"><a href="recurrent-neural-network.html#cb395-23" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<div class="sourceCode" id="cb396"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb396-1"><a href="recurrent-neural-network.html#cb396-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(lstm_history)</span></code></pre></div>
<center>
<img src="images/rnnhandson2.png" style="width:90.0%" />
</center>
<div class="sourceCode" id="cb397"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb397-1"><a href="recurrent-neural-network.html#cb397-1" aria-hidden="true" tabindex="-1"></a>lstm_model <span class="sc">%&gt;%</span></span>
<span id="cb397-2"><a href="recurrent-neural-network.html#cb397-2" aria-hidden="true" tabindex="-1"></a>   <span class="fu">evaluate</span>(x_test, y_test)</span></code></pre></div>
<div class="sourceCode" id="cb398"><pre class="sourceCode html"><code class="sourceCode html"><span id="cb398-1"><a href="recurrent-neural-network.html#cb398-1" aria-hidden="true" tabindex="-1"></a>##     loss accuracy </span>
<span id="cb398-2"><a href="recurrent-neural-network.html#cb398-2" aria-hidden="true" tabindex="-1"></a>## 0.361364 0.844080</span></code></pre></div>
<p>This simple example shows that LSTM’s performance has improved dramatically from the simple RNN model. The computation time for LSTM is roughly doubled when compared with the simple RNN model for this small dataset.</p>

</div>
</div>
</div>
<!-- </div> -->
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-deeplearningpy17" class="csl-entry">
Chollet, François. 2017. <em>Deep Learning with Python</em>. Manning.
</div>
<div id="ref-deeplearningr18" class="csl-entry">
Chollet, François, and J. J. Allaire. 2018. <em>Deep Learning with r</em>. Manning.
</div>
<div id="ref-chung2014empirical" class="csl-entry">
Chung, Junyoung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. 2014. <span>“Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling.”</span> <a href="https://arxiv.org/abs/1412.3555">https://arxiv.org/abs/1412.3555</a>.
</div>
<div id="ref-Goodfellow-et-al-2016" class="csl-entry">
Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. <em>Deep Learning</em>. MIT Press.
</div>
<div id="ref-lstm1997" class="csl-entry">
Hochreiter, Sepp, and JÃŒrgen Schmidhuber. 1997. <span>“Long Short-Term Memory.”</span> <em>Neural Computation</em> 9 (8): 1735–80.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="convolutional-neural-network.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="appendix.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": true,
"twitter": true,
"linkedin": true,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/happyrabbit/IntroDataScience/12-DeepLearning.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["IDS.pdf", "IDS.epub", "IDS.mobi"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section",
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
