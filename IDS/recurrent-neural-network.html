<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>12.4 Recurrent Neural Network | Introduction to Data Science</title>
  <meta name="description" content="12.4 Recurrent Neural Network | Introduction to Data Science" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="12.4 Recurrent Neural Network | Introduction to Data Science" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="http://scientistcafe.com/IDS/" />
  
  <meta property="og:description" content="12.4 Recurrent Neural Network | Introduction to Data Science" />
  <meta name="github-repo" content="happyrabbit/IntroDataScience" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="12.4 Recurrent Neural Network | Introduction to Data Science" />
  
  <meta name="twitter:description" content="12.4 Recurrent Neural Network | Introduction to Data Science" />
  

<meta name="author" content="Hui Lin and Ming Li" />


<meta name="date" content="2021-02-09" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="convolutional-neural-network.html"/>
<link rel="next" href="r-code-for-data-simulation.html"/>
<script src="libs/jquery/jquery.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="http://scientistcafe.com">Homepage</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="goal-of-the-book.html"><a href="goal-of-the-book.html"><i class="fa fa-check"></i>Goal of the Book</a></li>
<li class="chapter" data-level="" data-path="who-this-book-is-for.html"><a href="who-this-book-is-for.html"><i class="fa fa-check"></i>Who This Book Is For</a></li>
<li class="chapter" data-level="" data-path="what-this-book-covers.html"><a href="what-this-book-covers.html"><i class="fa fa-check"></i>What This Book Covers</a></li>
<li class="chapter" data-level="" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="about-the-authors.html"><a href="about-the-authors.html"><i class="fa fa-check"></i>About the Authors</a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="a-brief-history-of-data-science.html"><a href="a-brief-history-of-data-science.html"><i class="fa fa-check"></i><b>1.1</b> A brief history of data science</a></li>
<li class="chapter" data-level="1.2" data-path="data-science-role-and-skill-tracks.html"><a href="data-science-role-and-skill-tracks.html"><i class="fa fa-check"></i><b>1.2</b> Data science role and skill tracks</a><ul>
<li class="chapter" data-level="1.2.1" data-path="data-science-role-and-skill-tracks.html"><a href="data-science-role-and-skill-tracks.html#engineering"><i class="fa fa-check"></i><b>1.2.1</b> Engineering</a></li>
<li class="chapter" data-level="1.2.2" data-path="data-science-role-and-skill-tracks.html"><a href="data-science-role-and-skill-tracks.html#analysis"><i class="fa fa-check"></i><b>1.2.2</b> Analysis</a></li>
<li class="chapter" data-level="1.2.3" data-path="data-science-role-and-skill-tracks.html"><a href="data-science-role-and-skill-tracks.html#modeling"><i class="fa fa-check"></i><b>1.2.3</b> Modeling</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="what-kind-of-questions-can-data-science-solve.html"><a href="what-kind-of-questions-can-data-science-solve.html"><i class="fa fa-check"></i><b>1.3</b> What kind of questions can data science solve?</a><ul>
<li class="chapter" data-level="1.3.1" data-path="what-kind-of-questions-can-data-science-solve.html"><a href="what-kind-of-questions-can-data-science-solve.html#prerequisites"><i class="fa fa-check"></i><b>1.3.1</b> Prerequisites</a></li>
<li class="chapter" data-level="1.3.2" data-path="what-kind-of-questions-can-data-science-solve.html"><a href="what-kind-of-questions-can-data-science-solve.html#problem-type"><i class="fa fa-check"></i><b>1.3.2</b> Problem type</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="structure-data-science-team.html"><a href="structure-data-science-team.html"><i class="fa fa-check"></i><b>1.4</b> Structure data science team</a></li>
<li class="chapter" data-level="1.5" data-path="list-of-potential-data-science-careers.html"><a href="list-of-potential-data-science-careers.html"><i class="fa fa-check"></i><b>1.5</b> List of potential data science careers</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="soft-skills-for-data-scientists.html"><a href="soft-skills-for-data-scientists.html"><i class="fa fa-check"></i><b>2</b> Soft Skills for Data Scientists</a><ul>
<li class="chapter" data-level="2.1" data-path="comparison-between-statistician-and-data-scientist.html"><a href="comparison-between-statistician-and-data-scientist.html"><i class="fa fa-check"></i><b>2.1</b> Comparison between Statistician and Data Scientist</a></li>
<li class="chapter" data-level="2.2" data-path="beyond-data-and-analytics.html"><a href="beyond-data-and-analytics.html"><i class="fa fa-check"></i><b>2.2</b> Beyond Data and Analytics</a></li>
<li class="chapter" data-level="2.3" data-path="three-pillars-of-knowledge.html"><a href="three-pillars-of-knowledge.html"><i class="fa fa-check"></i><b>2.3</b> Three Pillars of Knowledge</a></li>
<li class="chapter" data-level="2.4" data-path="data-science-project-cycle.html"><a href="data-science-project-cycle.html"><i class="fa fa-check"></i><b>2.4</b> Data Science Project Cycle</a><ul>
<li class="chapter" data-level="2.4.1" data-path="data-science-project-cycle.html"><a href="data-science-project-cycle.html#types-of-data-science-projects"><i class="fa fa-check"></i><b>2.4.1</b> Types of Data Science Projects</a></li>
<li class="chapter" data-level="2.4.2" data-path="data-science-project-cycle.html"><a href="data-science-project-cycle.html#problem-formulation-and-project-planning-stage"><i class="fa fa-check"></i><b>2.4.2</b> Problem Formulation and Project Planning Stage</a></li>
<li class="chapter" data-level="2.4.3" data-path="data-science-project-cycle.html"><a href="data-science-project-cycle.html#project-modeling-stage"><i class="fa fa-check"></i><b>2.4.3</b> Project Modeling Stage</a></li>
<li class="chapter" data-level="2.4.4" data-path="data-science-project-cycle.html"><a href="data-science-project-cycle.html#model-implementation-and-post-production-stage"><i class="fa fa-check"></i><b>2.4.4</b> Model Implementation and Post Production Stage</a></li>
<li class="chapter" data-level="2.4.5" data-path="data-science-project-cycle.html"><a href="data-science-project-cycle.html#project-cycle-summary"><i class="fa fa-check"></i><b>2.4.5</b> Project Cycle Summary</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="common-mistakes-in-data-science.html"><a href="common-mistakes-in-data-science.html"><i class="fa fa-check"></i><b>2.5</b> Common Mistakes in Data Science</a><ul>
<li class="chapter" data-level="2.5.1" data-path="common-mistakes-in-data-science.html"><a href="common-mistakes-in-data-science.html#problem-formulation-stage"><i class="fa fa-check"></i><b>2.5.1</b> Problem Formulation Stage</a></li>
<li class="chapter" data-level="2.5.2" data-path="common-mistakes-in-data-science.html"><a href="common-mistakes-in-data-science.html#project-planning-stage"><i class="fa fa-check"></i><b>2.5.2</b> Project Planning Stage</a></li>
<li class="chapter" data-level="2.5.3" data-path="common-mistakes-in-data-science.html"><a href="common-mistakes-in-data-science.html#project-modeling-stage-1"><i class="fa fa-check"></i><b>2.5.3</b> Project Modeling Stage</a></li>
<li class="chapter" data-level="2.5.4" data-path="common-mistakes-in-data-science.html"><a href="common-mistakes-in-data-science.html#model-implementation-and-post-production-stage-1"><i class="fa fa-check"></i><b>2.5.4</b> Model Implementation and Post Production Stage</a></li>
<li class="chapter" data-level="2.5.5" data-path="common-mistakes-in-data-science.html"><a href="common-mistakes-in-data-science.html#summary-of-common-mistakes"><i class="fa fa-check"></i><b>2.5.5</b> Summary of Common Mistakes</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="introduction-to-the-data.html"><a href="introduction-to-the-data.html"><i class="fa fa-check"></i><b>3</b> Introduction to The Data</a><ul>
<li class="chapter" data-level="3.1" data-path="customer-data-for-a-clothing-company.html"><a href="customer-data-for-a-clothing-company.html"><i class="fa fa-check"></i><b>3.1</b> Customer Data for A Clothing Company</a></li>
<li class="chapter" data-level="3.2" data-path="swinediseasedata.html"><a href="swinediseasedata.html"><i class="fa fa-check"></i><b>3.2</b> Swine Disease Breakout Data</a></li>
<li class="chapter" data-level="3.3" data-path="mnist-dataset.html"><a href="mnist-dataset.html"><i class="fa fa-check"></i><b>3.3</b> MNIST Dataset</a></li>
<li class="chapter" data-level="3.4" data-path="imdb-dataset.html"><a href="imdb-dataset.html"><i class="fa fa-check"></i><b>3.4</b> IMDB Dataset</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="big-data-cloud-platform.html"><a href="big-data-cloud-platform.html"><i class="fa fa-check"></i><b>4</b> Big Data Cloud Platform</a><ul>
<li class="chapter" data-level="4.1" data-path="power-of-cluster-of-computers.html"><a href="power-of-cluster-of-computers.html"><i class="fa fa-check"></i><b>4.1</b> Power of Cluster of Computers</a></li>
<li class="chapter" data-level="4.2" data-path="evolution-of-cluster-computing.html"><a href="evolution-of-cluster-computing.html"><i class="fa fa-check"></i><b>4.2</b> Evolution of Cluster Computing</a><ul>
<li class="chapter" data-level="4.2.1" data-path="evolution-of-cluster-computing.html"><a href="evolution-of-cluster-computing.html#hadoop"><i class="fa fa-check"></i><b>4.2.1</b> Hadoop</a></li>
<li class="chapter" data-level="4.2.2" data-path="evolution-of-cluster-computing.html"><a href="evolution-of-cluster-computing.html#spark"><i class="fa fa-check"></i><b>4.2.2</b> Spark</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="CloudEnvironment.html"><a href="CloudEnvironment.html"><i class="fa fa-check"></i><b>4.3</b> Introduction of Cloud Environment</a><ul>
<li class="chapter" data-level="4.3.1" data-path="CloudEnvironment.html"><a href="CloudEnvironment.html#open-account-and-create-a-cluster"><i class="fa fa-check"></i><b>4.3.1</b> Open Account and Create a Cluster</a></li>
<li class="chapter" data-level="4.3.2" data-path="CloudEnvironment.html"><a href="CloudEnvironment.html#r-notebook"><i class="fa fa-check"></i><b>4.3.2</b> R Notebook</a></li>
<li class="chapter" data-level="4.3.3" data-path="CloudEnvironment.html"><a href="CloudEnvironment.html#markdown-cells"><i class="fa fa-check"></i><b>4.3.3</b> Markdown Cells</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="leverage-spark-using-r-notebook.html"><a href="leverage-spark-using-r-notebook.html"><i class="fa fa-check"></i><b>4.4</b> Leverage Spark Using R Notebook</a></li>
<li class="chapter" data-level="4.5" data-path="databases-and-sql.html"><a href="databases-and-sql.html"><i class="fa fa-check"></i><b>4.5</b> Databases and SQL</a><ul>
<li class="chapter" data-level="4.5.1" data-path="databases-and-sql.html"><a href="databases-and-sql.html#history"><i class="fa fa-check"></i><b>4.5.1</b> History</a></li>
<li class="chapter" data-level="4.5.2" data-path="databases-and-sql.html"><a href="databases-and-sql.html#database-table-and-view"><i class="fa fa-check"></i><b>4.5.2</b> Database, Table and View</a></li>
<li class="chapter" data-level="4.5.3" data-path="databases-and-sql.html"><a href="databases-and-sql.html#basic-sql-statement"><i class="fa fa-check"></i><b>4.5.3</b> Basic SQL Statement</a></li>
<li class="chapter" data-level="4.5.4" data-path="databases-and-sql.html"><a href="databases-and-sql.html#advanced-topics-in-database"><i class="fa fa-check"></i><b>4.5.4</b> Advanced Topics in Database</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="data-pre-processing.html"><a href="data-pre-processing.html"><i class="fa fa-check"></i><b>5</b> Data Pre-processing</a><ul>
<li class="chapter" data-level="5.1" data-path="data-cleaning.html"><a href="data-cleaning.html"><i class="fa fa-check"></i><b>5.1</b> Data Cleaning</a></li>
<li class="chapter" data-level="5.2" data-path="missing-values.html"><a href="missing-values.html"><i class="fa fa-check"></i><b>5.2</b> Missing Values</a><ul>
<li class="chapter" data-level="5.2.1" data-path="missing-values.html"><a href="missing-values.html#impute-missing-values-with-medianmode"><i class="fa fa-check"></i><b>5.2.1</b> Impute missing values with median/mode</a></li>
<li class="chapter" data-level="5.2.2" data-path="missing-values.html"><a href="missing-values.html#k-nearest-neighbors"><i class="fa fa-check"></i><b>5.2.2</b> K-nearest neighbors</a></li>
<li class="chapter" data-level="5.2.3" data-path="missing-values.html"><a href="missing-values.html#bagging-tree"><i class="fa fa-check"></i><b>5.2.3</b> Bagging Tree</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="centering-and-scaling.html"><a href="centering-and-scaling.html"><i class="fa fa-check"></i><b>5.3</b> Centering and Scaling</a></li>
<li class="chapter" data-level="5.4" data-path="resolve-skewness.html"><a href="resolve-skewness.html"><i class="fa fa-check"></i><b>5.4</b> Resolve Skewness</a></li>
<li class="chapter" data-level="5.5" data-path="outliers.html"><a href="outliers.html"><i class="fa fa-check"></i><b>5.5</b> Resolve Outliers</a></li>
<li class="chapter" data-level="5.6" data-path="collinearity.html"><a href="collinearity.html"><i class="fa fa-check"></i><b>5.6</b> Collinearity</a></li>
<li class="chapter" data-level="5.7" data-path="sparse-variables.html"><a href="sparse-variables.html"><i class="fa fa-check"></i><b>5.7</b> Sparse Variables</a></li>
<li class="chapter" data-level="5.8" data-path="re-encode-dummy-variables.html"><a href="re-encode-dummy-variables.html"><i class="fa fa-check"></i><b>5.8</b> Re-encode Dummy Variables</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="data-wrangling.html"><a href="data-wrangling.html"><i class="fa fa-check"></i><b>6</b> Data Wrangling</a><ul>
<li class="chapter" data-level="6.1" data-path="read-and-write-data.html"><a href="read-and-write-data.html"><i class="fa fa-check"></i><b>6.1</b> Read and write data</a><ul>
<li class="chapter" data-level="6.1.1" data-path="read-and-write-data.html"><a href="read-and-write-data.html#readr"><i class="fa fa-check"></i><b>6.1.1</b> <code>readr</code></a></li>
<li class="chapter" data-level="6.1.2" data-path="read-and-write-data.html"><a href="read-and-write-data.html#data.table-enhanced-data.frame"><i class="fa fa-check"></i><b>6.1.2</b> <code>data.table</code>— enhanced <code>data.frame</code></a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="summarize-data.html"><a href="summarize-data.html"><i class="fa fa-check"></i><b>6.2</b> Summarize data</a><ul>
<li class="chapter" data-level="6.2.1" data-path="summarize-data.html"><a href="summarize-data.html#apply-lapply-and-sapply-in-base-r"><i class="fa fa-check"></i><b>6.2.1</b> <code>apply()</code>, <code>lapply()</code> and <code>sapply()</code> in base R</a></li>
<li class="chapter" data-level="6.2.2" data-path="summarize-data.html"><a href="summarize-data.html#dplyr-package"><i class="fa fa-check"></i><b>6.2.2</b> <code>dplyr</code> package</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="tidy-and-reshape-data.html"><a href="tidy-and-reshape-data.html"><i class="fa fa-check"></i><b>6.3</b> Tidy and Reshape Data</a><ul>
<li class="chapter" data-level="6.3.1" data-path="tidy-and-reshape-data.html"><a href="tidy-and-reshape-data.html#reshape2-package"><i class="fa fa-check"></i><b>6.3.1</b> <code>reshape2</code> package</a></li>
<li class="chapter" data-level="6.3.2" data-path="tidy-and-reshape-data.html"><a href="tidy-and-reshape-data.html#tidyr-package"><i class="fa fa-check"></i><b>6.3.2</b> <code>tidyr</code> package</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="modeltuningstrategy.html"><a href="modeltuningstrategy.html"><i class="fa fa-check"></i><b>7</b> Model Tuning Strategy</a><ul>
<li class="chapter" data-level="7.1" data-path="vbtradeoff.html"><a href="vbtradeoff.html"><i class="fa fa-check"></i><b>7.1</b> Variance-Bias Trade-Off</a></li>
<li class="chapter" data-level="7.2" data-path="datasplittingresampling.html"><a href="datasplittingresampling.html"><i class="fa fa-check"></i><b>7.2</b> Data Splitting and Resampling</a><ul>
<li class="chapter" data-level="7.2.1" data-path="datasplittingresampling.html"><a href="datasplittingresampling.html#datasplitting"><i class="fa fa-check"></i><b>7.2.1</b> Data Splitting</a></li>
<li class="chapter" data-level="7.2.2" data-path="datasplittingresampling.html"><a href="datasplittingresampling.html#resampling"><i class="fa fa-check"></i><b>7.2.2</b> Resampling</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="measuring-performance.html"><a href="measuring-performance.html"><i class="fa fa-check"></i><b>8</b> Measuring Performance</a><ul>
<li class="chapter" data-level="8.1" data-path="regression-model-performance.html"><a href="regression-model-performance.html"><i class="fa fa-check"></i><b>8.1</b> Regression Model Performance</a></li>
<li class="chapter" data-level="8.2" data-path="classification-model-performance.html"><a href="classification-model-performance.html"><i class="fa fa-check"></i><b>8.2</b> Classification Model Performance</a><ul>
<li class="chapter" data-level="8.2.1" data-path="classification-model-performance.html"><a href="classification-model-performance.html#confusion-matrix"><i class="fa fa-check"></i><b>8.2.1</b> Confusion Matrix</a></li>
<li class="chapter" data-level="8.2.2" data-path="classification-model-performance.html"><a href="classification-model-performance.html#kappa-statistic"><i class="fa fa-check"></i><b>8.2.2</b> Kappa Statistic</a></li>
<li class="chapter" data-level="8.2.3" data-path="classification-model-performance.html"><a href="classification-model-performance.html#roc"><i class="fa fa-check"></i><b>8.2.3</b> ROC</a></li>
<li class="chapter" data-level="8.2.4" data-path="classification-model-performance.html"><a href="classification-model-performance.html#gain-and-lift-charts"><i class="fa fa-check"></i><b>8.2.4</b> Gain and Lift Charts</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="regression-models.html"><a href="regression-models.html"><i class="fa fa-check"></i><b>9</b> Regression Models</a><ul>
<li class="chapter" data-level="9.1" data-path="ordinary-least-square.html"><a href="ordinary-least-square.html"><i class="fa fa-check"></i><b>9.1</b> Ordinary Least Square</a><ul>
<li class="chapter" data-level="9.1.1" data-path="ordinary-least-square.html"><a href="ordinary-least-square.html#the-magic-p-value"><i class="fa fa-check"></i><b>9.1.1</b> The Magic P-value</a></li>
<li class="chapter" data-level="9.1.2" data-path="ordinary-least-square.html"><a href="ordinary-least-square.html#diagnostics-for-linear-regression"><i class="fa fa-check"></i><b>9.1.2</b> Diagnostics for Linear Regression</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="principal-component-regression-and-partial-least-square.html"><a href="principal-component-regression-and-partial-least-square.html"><i class="fa fa-check"></i><b>9.2</b> Principal Component Regression and Partial Least Square</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="regularization-methods.html"><a href="regularization-methods.html"><i class="fa fa-check"></i><b>10</b> Regularization Methods</a><ul>
<li class="chapter" data-level="10.1" data-path="ridge-regression.html"><a href="ridge-regression.html"><i class="fa fa-check"></i><b>10.1</b> Ridge Regression</a></li>
<li class="chapter" data-level="10.2" data-path="lasso.html"><a href="lasso.html"><i class="fa fa-check"></i><b>10.2</b> LASSO</a></li>
<li class="chapter" data-level="10.3" data-path="variable-selection-property-of-the-lasso.html"><a href="variable-selection-property-of-the-lasso.html"><i class="fa fa-check"></i><b>10.3</b> Variable selection property of the lasso</a></li>
<li class="chapter" data-level="10.4" data-path="elastic-net.html"><a href="elastic-net.html"><i class="fa fa-check"></i><b>10.4</b> Elastic Net</a></li>
<li class="chapter" data-level="10.5" data-path="penalized-generalized-linear-model.html"><a href="penalized-generalized-linear-model.html"><i class="fa fa-check"></i><b>10.5</b> Penalized Generalized Linear Model</a><ul>
<li class="chapter" data-level="10.5.1" data-path="penalized-generalized-linear-model.html"><a href="penalized-generalized-linear-model.html#introduction-to-glmnet-package"><i class="fa fa-check"></i><b>10.5.1</b> Introduction to <code>glmnet</code> package</a></li>
<li class="chapter" data-level="10.5.2" data-path="penalized-generalized-linear-model.html"><a href="penalized-generalized-linear-model.html#penalized-logistic-regression"><i class="fa fa-check"></i><b>10.5.2</b> Penalized logistic regression</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="treemodel.html"><a href="treemodel.html"><i class="fa fa-check"></i><b>11</b> Tree-Based Methods</a><ul>
<li class="chapter" data-level="11.1" data-path="splitting-criteria.html"><a href="splitting-criteria.html"><i class="fa fa-check"></i><b>11.1</b> Splitting Criteria</a></li>
<li class="chapter" data-level="11.2" data-path="tree-pruning.html"><a href="tree-pruning.html"><i class="fa fa-check"></i><b>11.2</b> Tree Pruning</a></li>
<li class="chapter" data-level="11.3" data-path="regression-and-decision-tree-basic.html"><a href="regression-and-decision-tree-basic.html"><i class="fa fa-check"></i><b>11.3</b> Regression and Decision Tree Basic</a><ul>
<li class="chapter" data-level="11.3.1" data-path="regression-and-decision-tree-basic.html"><a href="regression-and-decision-tree-basic.html#regression-tree"><i class="fa fa-check"></i><b>11.3.1</b> Regression Tree</a></li>
<li class="chapter" data-level="11.3.2" data-path="regression-and-decision-tree-basic.html"><a href="regression-and-decision-tree-basic.html#decision-tree"><i class="fa fa-check"></i><b>11.3.2</b> Decision Tree</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="missing-values.html"><a href="missing-values.html#bagging-tree"><i class="fa fa-check"></i><b>11.4</b> Bagging Tree</a></li>
<li class="chapter" data-level="11.5" data-path="random-forest.html"><a href="random-forest.html"><i class="fa fa-check"></i><b>11.5</b> Random Forest</a></li>
<li class="chapter" data-level="11.6" data-path="gradient-boosted-machine.html"><a href="gradient-boosted-machine.html"><i class="fa fa-check"></i><b>11.6</b> Gradient Boosted Machine</a><ul>
<li class="chapter" data-level="11.6.1" data-path="gradient-boosted-machine.html"><a href="gradient-boosted-machine.html#adaptive-boosting"><i class="fa fa-check"></i><b>11.6.1</b> Adaptive Boosting</a></li>
<li class="chapter" data-level="11.6.2" data-path="gradient-boosted-machine.html"><a href="gradient-boosted-machine.html#stochastic-gradient-boosting"><i class="fa fa-check"></i><b>11.6.2</b> Stochastic Gradient Boosting</a></li>
<li class="chapter" data-level="11.6.3" data-path="gradient-boosted-machine.html"><a href="gradient-boosted-machine.html#boosting-as-additive-model"><i class="fa fa-check"></i><b>11.6.3</b> Boosting as Additive Model</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="deep-learning.html"><a href="deep-learning.html"><i class="fa fa-check"></i><b>12</b> Deep Learning</a><ul>
<li class="chapter" data-level="12.1" data-path="projection-pursuit-regression.html"><a href="projection-pursuit-regression.html"><i class="fa fa-check"></i><b>12.1</b> Projection Pursuit Regression</a></li>
<li class="chapter" data-level="12.2" data-path="feedforward-neural-network.html"><a href="feedforward-neural-network.html"><i class="fa fa-check"></i><b>12.2</b> Feedforward Neural Network</a><ul>
<li class="chapter" data-level="12.2.1" data-path="feedforward-neural-network.html"><a href="feedforward-neural-network.html#logistic_reg_as_neural_network"><i class="fa fa-check"></i><b>12.2.1</b> Logistic Regression as Neural Network</a></li>
<li class="chapter" data-level="12.2.2" data-path="feedforward-neural-network.html"><a href="feedforward-neural-network.html#stochastic-gradient-descent"><i class="fa fa-check"></i><b>12.2.2</b> Stochastic Gradient Descent</a></li>
<li class="chapter" data-level="12.2.3" data-path="feedforward-neural-network.html"><a href="feedforward-neural-network.html#deepneuralnetwork"><i class="fa fa-check"></i><b>12.2.3</b> Deep Neural Network</a></li>
<li class="chapter" data-level="12.2.4" data-path="feedforward-neural-network.html"><a href="feedforward-neural-network.html#activation-function"><i class="fa fa-check"></i><b>12.2.4</b> Activation Function</a></li>
<li class="chapter" data-level="12.2.5" data-path="feedforward-neural-network.html"><a href="feedforward-neural-network.html#optimization"><i class="fa fa-check"></i><b>12.2.5</b> Optimization</a></li>
<li class="chapter" data-level="12.2.6" data-path="feedforward-neural-network.html"><a href="feedforward-neural-network.html#deal-with-overfitting"><i class="fa fa-check"></i><b>12.2.6</b> Deal with Overfitting</a></li>
<li class="chapter" data-level="12.2.7" data-path="feedforward-neural-network.html"><a href="feedforward-neural-network.html#ffnnexample"><i class="fa fa-check"></i><b>12.2.7</b> Image Recognition Using FFNN</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="convolutional-neural-network.html"><a href="convolutional-neural-network.html"><i class="fa fa-check"></i><b>12.3</b> Convolutional Neural Network</a><ul>
<li class="chapter" data-level="12.3.1" data-path="convolutional-neural-network.html"><a href="convolutional-neural-network.html#convolution-layer"><i class="fa fa-check"></i><b>12.3.1</b> Convolution Layer</a></li>
<li class="chapter" data-level="12.3.2" data-path="convolutional-neural-network.html"><a href="convolutional-neural-network.html#padding-layer"><i class="fa fa-check"></i><b>12.3.2</b> Padding Layer</a></li>
<li class="chapter" data-level="12.3.3" data-path="convolutional-neural-network.html"><a href="convolutional-neural-network.html#pooling-layer"><i class="fa fa-check"></i><b>12.3.3</b> Pooling Layer</a></li>
<li class="chapter" data-level="12.3.4" data-path="convolutional-neural-network.html"><a href="convolutional-neural-network.html#convolution-over-volume"><i class="fa fa-check"></i><b>12.3.4</b> Convolution Over Volume</a></li>
<li class="chapter" data-level="12.3.5" data-path="convolutional-neural-network.html"><a href="convolutional-neural-network.html#cnnexample"><i class="fa fa-check"></i><b>12.3.5</b> Image Recognition Using CNN</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="recurrent-neural-network.html"><a href="recurrent-neural-network.html"><i class="fa fa-check"></i><b>12.4</b> Recurrent Neural Network</a><ul>
<li class="chapter" data-level="12.4.1" data-path="recurrent-neural-network.html"><a href="recurrent-neural-network.html#rnn-model"><i class="fa fa-check"></i><b>12.4.1</b> RNN Model</a></li>
<li class="chapter" data-level="12.4.2" data-path="recurrent-neural-network.html"><a href="recurrent-neural-network.html#embedding"><i class="fa fa-check"></i><b>12.4.2</b> Word Embedding</a></li>
<li class="chapter" data-level="12.4.3" data-path="recurrent-neural-network.html"><a href="recurrent-neural-network.html#lstm"><i class="fa fa-check"></i><b>12.4.3</b> Long Short Term Memory</a></li>
<li class="chapter" data-level="12.4.4" data-path="recurrent-neural-network.html"><a href="recurrent-neural-network.html#rnnexample"><i class="fa fa-check"></i><b>12.4.4</b> Sentiment Analysis Using RNN</a></li>
</ul></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="r-code-for-data-simulation.html"><a href="r-code-for-data-simulation.html"><i class="fa fa-check"></i><b>A</b> R code for data simulation</a><ul>
<li class="chapter" data-level="A.1" data-path="appendixdata1.html"><a href="appendixdata1.html"><i class="fa fa-check"></i><b>A.1</b> Customer Data for Clothing Company</a></li>
<li class="chapter" data-level="A.2" data-path="appendixdata3.html"><a href="appendixdata3.html"><i class="fa fa-check"></i><b>A.2</b> Swine Disease Breakout Data</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Data Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="recurrent-neural-network" class="section level2">
<h2><span class="header-section-number">12.4</span> Recurrent Neural Network</h2>
<p>Traditional neural networks don’t have a framework that can handle sequential events where the later events are based on the previous ones. For example, map an input audio clip to a text transcript where the input is voice over time, and the output is the corresponding sequence of words over time. Recurrent Neural Network is a deep-learning model that can process this type of sequential data.</p>
<p>The recurrent neural network allows information to flow from one step to the next with a repetitive structure. Figure <a href="recurrent-neural-network.html#fig:rnnunit">12.20</a> shows the basic chunk of an RNN network. You combine the activated neuro from the previous step with the current input <span class="math inline">\(x^{&lt;t&gt;}\)</span> to produce an output <span class="math inline">\(\hat{y}^{&lt;t&gt;}\)</span> and an updated activated neuro to support the next input at <span class="math inline">\(t+1\)</span>.</p>
<div class="figure" style="text-align: center"><span id="fig:rnnunit"></span>
<img src="images/rnnunit.png" alt="Recurrent Neural Network Unit"  />
<p class="caption">
FIGURE 12.20: Recurrent Neural Network Unit
</p>
</div>
<p>So the whole process repeats a similar pattern. If we unroll the loop:</p>
<div class="figure" style="text-align: center"><span id="fig:unrolledrnn"></span>
<img src="images/rnnrollout.png" alt="An Unrolled Recurrent Neural Network" width="70%" />
<p class="caption">
FIGURE 12.21: An Unrolled Recurrent Neural Network
</p>
</div>
<p>The chain-like recurrent nature makes it the natural architecture for sequential data. There is incredible success applying RNNs to this type of problems:</p>
<ul>
<li>Machine translation</li>
<li>Voice recognition</li>
<li>Music generation</li>
<li>Sentiment analysis</li>
</ul>
<center>
<img src="images/rnnapplication.png" style="width:70.0%" />
</center>
<p>A trained CNN accepts a fixed-sized vector as input (such as <span class="math inline">\(28 \times 28\)</span> image) and produces a fixed-sized vector as output (such as the probabilities of being one the ten digits). RNN has a much more flexible structure. It can operate over sequences of vectors and produces sequences of outputs and they can vary in size. To understand what it means, let’s look at some RNN structure examples.</p>
<center>
<img src="images/rnnstrs.png" style="width:100.0%" />
</center>
<p>The rectangle represents a vector and the arrow represents matrix multiplications. The input vector is in green and the output vector is in blue. The red rectangle holds the intermediate state. From left to right:</p>
<ul>
<li>one-to-one: model takes a fixed size input and produces a fixed size output, such as CNN. it is not sequential.</li>
<li>one-to-many: model takes one input and generate a sequence of output, such as the music generation.</li>
<li>many-to-one: model takes a sequence of input and produces a single output, such as sentiment analysis.</li>
<li>many-to-many: model takes a sequence of input and produces a sequence of output. The input size can be the same with the output size (such as name entity recognition) or it can be different (such as machine translation).</li>
</ul>
<div id="rnn-model" class="section level3">
<h3><span class="header-section-number">12.4.1</span> RNN Model</h3>
<p>To further understand the RNN model, let’s look at an entity recognition example. Assume you want to build a sequence model to recognize the company or computer language names in a sentence like this: “Use Netlify and Hugo”. It is a name recognition problem which is used by the research company to index different company names in the articles. For tasks like this, we need a model that can learn and share the learning across different texts. The position of the word has important information about the word. For example, the word before “to” is more likely to be a verb than a noun. It is also used in material science to tag chemicals mentioned in the most recent journals to find any indication of the next research topic.</p>
<p>Given input sentence x, you want a model to produce one output for each word in x that tells you if that word is the name for something. So in this example, the input is a sequence of 5 words including the period in the end. The output is a sequence of 0/1 with the same length that indicates whether the input word is a name (1) or not (0). We use superscript <span class="math inline">\(&lt;t&gt;\)</span> to denote the element position of input and output; use superscript <span class="math inline">\((i)\)</span> to denote the <span class="math inline">\(i^{th}\)</span> sample (you will have different sentences in the training data); Use <span class="math inline">\(T_x^{(i)}\)</span> to denote the length of <span class="math inline">\(i^{th}\)</span> input, <span class="math inline">\(T_y^{(i)}\)</span> for output. In this case, <span class="math inline">\(T_x^{(i)}\)</span> is equal to <span class="math inline">\(T_y^{(i)}\)</span>.</p>
<center>
<img src="images/rnn_notation.png" style="width:50.0%" />
</center>
<p>Before we build a neural network, we need to decide a way to represent individual words in numbers. What should <span class="math inline">\(x^{&lt;1&gt;}\)</span> be? In practice, people use word embedding which we will discuss in the later section. Here, for illustration, we use the one-hot encoding word representation. Assume we have a dictionary of 10,000 unique words. You can build the dictionary by finding the top 10,000 occurring words in your training set. Each word in your training set will have a position in the dictionary sequence. For example, “use” is the 8320th element of the dictionary sequence. So <span class="math inline">\(x^{&lt;1&gt;}\)</span> is a vector with all zeros except for a one on position 8320. Using this one-hot representation, each input <span class="math inline">\(x^{&lt;t&gt;}\)</span> is a vector with all zeros except for one element.</p>
<center>
<img src="images/nn_ohe.png" style="width:50.0%" />
</center>
<p>Given this representation of input words, the goal is to learn a sequence model that maps the input words to output y, indicating if the word is an entity (1) or not (0). Let us build a one-layer recurrent neural network. The model starts from the first word “use” (<span class="math inline">\(x^{&lt;1&gt;}\)</span>) and build a neural network to predict the output. To start the process, we also need to initialize the activation at time 0, <span class="math inline">\(a_0\)</span>. The most common choice is to use a vector of zeros. The common activation function for the intermediate layer is the Hyperbolic Tangent Function (tanh). RNN uses other methods to prevent the vanishing gradient problem discussed in section <a href="recurrent-neural-network.html#lstm">12.4.3</a>. Similar to FFNN, The output layer activation function depends on the output type. The current example is a binary classification, so we use the sigmoid function (<span class="math inline">\(\sigma\)</span>).</p>
<p><span class="math display">\[a^{&lt;0&gt;} = \mathbf{0};\ a^{&lt;1&gt;} = tanh(W_{aa}a^{&lt;0&gt;} + W_{ax}x^{&lt;1&gt;}+b_a)\]</span>
<span class="math display">\[\hat{y}^{&lt;1&gt;} = \sigma(W_{ya}a^{&lt;1&gt;}+b_y)\]</span>
And when it takes the second word <span class="math inline">\(x^{&lt;2&gt;}\)</span>, it also gets information from the previous step using the non-activated neurons.</p>
<p><span class="math display">\[a^{&lt;2&gt;} = tanh(W_{aa}a^{&lt;1&gt;}+W_{ax}x^{&lt;2&gt;}+b_a)\]</span>
<span class="math display">\[\hat{y}^{&lt;2&gt;} = \sigma(W_{ya}a^{&lt;2&gt;}+b_y)\]</span></p>
<p>For the <span class="math inline">\(t^{th}\)</span> word:</p>
<p><span class="math display">\[a^{&lt;t&gt;} = tanh(W_{aa}a^{&lt;t-1&gt;}+W_{ax}x^{&lt;t&gt;}+b_a)\]</span>
<span class="math display">\[\hat{y}^{&lt;t&gt;} = \sigma(W_{ya}a^{&lt;t&gt;}+b_y)\]</span></p>
<p>The information flows from one step to the next with a repetitive structure until the last time step input <span class="math inline">\(x^{&lt;T_x&gt;}\)</span> and then it outputs <span class="math inline">\(\hat{y}^{&lt;T_y&gt;}\)</span>. In this example, <span class="math inline">\(T_x = T_y\)</span>. The architecture changes when <span class="math inline">\(T_x\)</span> and <span class="math inline">\(T_y\)</span> are not the same. The model shares parameters, <span class="math inline">\(W_{ya}, W_{aa}, W_{ax},b_a, b_y\)</span>, for all time steps of the input.</p>
<center>
<img src="images/RNN_pro.png" style="width:80.0%" />
</center>
<p>Calculate the loss function:</p>
<p><span class="math display">\[L^{&lt;t&gt;}(\hat{y}^{&lt;t&gt;}) = -y^{&lt;t&gt;}log(\hat{y}^{&lt;t&gt;})-(1-y^{&lt;t&gt;})log(1-\hat{y}^{&lt;t&gt;})\]</span>
<span class="math display">\[L(\hat{y},y)=\Sigma_{t=1}^{T_y}L^{&lt;t&gt;}(\hat{y},y)\]</span>
The above defines the forward process. Same as before, the backward propagation computes the gradient descent for the parameters by the chain rule for differentiation.</p>
<p>In this RNN structure, the information only flows from the left to the right. So at any position, it only uses data from earlier in the sequence to make a prediction. It does not work when predicting the current word needs information from later words. For example, consider the following two sentences:</p>
<ol style="list-style-type: decimal">
<li>Do you like April Kepner in Grey’s Anatomy?</li>
<li>Do you like April in Los Angeles? It is not too hot.</li>
</ol>
<p>Given just the first three words is not enough to know if the word “April” is part of a person’s name. It is a person’s name in 1 but not 2. The two sentences have the same first three words. In this case, we need a model that allows the information to flow in both directions. A bidirectional RNN takes data from both earlier and later in the sequence. The disadvantage is that it needs the entire word sequence to predict at any position. For a speech recognition application that requires capturing the speech in real-time, we need a more complex method called the attention model. We will not get into those models here. Deep Learning with R <span class="citation">(Chollet and Allaire <a href="#ref-deeplearningr18">2018</a>)</span> provides a high-level introduction of bidirectional RNN with applicable codes. It teaches both intuition and practical, computational usage of deep learning models. For python users, refer to Deep Learning with Python <span class="citation">(Chollet <a href="#ref-deeplearningpy17">2017</a>)</span>. A standard text with heavy mathematics is Deep Learning <span class="citation">(Goodfellow, Bengio, and Courville <a href="#ref-Goodfellow-et-al-2016">2016</a>)</span>.</p>
</div>
<div id="embedding" class="section level3">
<h3><span class="header-section-number">12.4.2</span> Word Embedding</h3>
<p>So far, we have been using one-hot encoding to represent the words. This representation is sparse and doesn’t capture the relationship between the words. For example, if your model learns from the training data that the word after “pumpkin” in the first sentence is “pie,” can it fill the second sentence’s blank with “pie”?</p>
<ol style="list-style-type: decimal">
<li>[training data] My favorite Christmas dessert is pumpkin pie.</li>
<li>[testing data] My favorite Christmas dessert is apple ____.</li>
</ol>
<p>The algorithm can not learn the relationship between “pumpkin” and “apple” by the distance between the one-hot encoding for the two words. If we can find a way to create features to represent the words, we may teach the model to learn that pumpkin and apple is food. And the distance between the feature representations of these two words is closer than, for example, “apple” and “nurse.” So when the model sees “apple” in a new sentence and needs to predict the word after, it is more likely to choose “pie” since it sees “pumpkin pie” in the training data. The idea of word embedding is to learn a set of features to represent words. For example, you can score each word in the dictionary according to a group of features like this:</p>
<center>
<img src="images/rnn_embedding1.png" style="width:80.0%" />
</center>
<p>The word “male” has a score of -1 for the “gender” feature, “female” has a score of 1. Both “Apple” and “pumpkin” have a high score for the “food” feature and much lower scores for the rest. You can set the number of features to learn, usually more than what we list in the above figure. If you use 200 features to represent the words, then the learned embedding for each word is a vector with a length of 200.</p>
<p>For language-related applications, text embedding is the most critical step. It converts raw text into a meaningful vector representation. Once we have a vector representation, it is easy to calculate typical numerical metrics such as cosine similarity. There are many pre-trained text embeddings available for us to use. We will briefly introduce some of these popular embeddings.</p>
<p>The first widely used embedding is word2vec. It was first introduced in 2013 and was trained by a large collection of text in an unsupervised way. Training the word2vec embedding vector uses bag-of-words or skip-gram. In the bag-of-words architecture, the model predicts the current word based on a window of surrounding context words. In skip-gram architecture, the model uses the current word to predict the surrounding window of context words. There are pre-trained word2vec embeddings based on a large amount of text (such as wiki pages, news reports, etc.) for general applications.</p>
<p>GloVe (Global Vectors) embedding is an extension of word2vec and performs better. It uses a unique version of the square loss function. However, words are composite of meaningful components such as radicals.<br />
“eat” and “eaten” are different forms of the same word. Both word2vec and GloVe use word-level information, and they treat each word uniquely based on its context.</p>
<p>The fastText embedding is introduced to use the word’s internal structure to make the process more efficient. It uses morphological information to extend the skip-gram model. New words that are not in the training data can be repressed well. It also supports 150+ different languages. The above-mentioned embeddings (word2vec, GloVe, and fastText) do not consider the words’ context (i.e., the same word has the same embedding vector). However, the same word may have different meanings in a different context. BERT (Bidirectional Encoder Representations from Transformers) is introduced to add context-level information in text-related applications. As of early 2021, BERT is generally considered the best language model for common application tasks.</p>
</div>
<div id="lstm" class="section level3">
<h3><span class="header-section-number">12.4.3</span> Long Short Term Memory</h3>
<p>The sequence in RNN can be very long, which leads to the vanishing gradient problem even when the RNN network is not deep. Think about the following examples:</p>
<ol style="list-style-type: decimal">
<li>The <strong>girl</strong> walked away, sat down in the shade of a tree, and began to read a new book which <strong>she</strong> bought the day before.</li>
<li>The <strong>boy</strong> walked away, sat down in the shade of a tree, and began to read a new book which <strong>he</strong> bought the day before.</li>
</ol>
<p>For sentence 1, you need to use “she” in the adjective clause after “which” because it is a girl. For sentence 2, you need to use “he” because it is a boy. This is a long-term dependency example where the information at the beginning can affect what needs to come much later in the sentence. RNN needs to forward propagate information from left to right and then backpropagate from right to left. It can be difficult for the error associated with the later sequence to affect the optimization earlier. So in practice, it means the model might fail to do the task mentioned above. People came up with different methods to mitigate this issue, such as the Greater Recurrent Units (GRU) <span class="citation">(Chung et al. <a href="#ref-chung2014empirical">2014</a>)</span> and Long Short Term Memory Units (LSTM) <span class="citation">(Hochreiter and Schmidhuber <a href="#ref-lstm1997">1997</a>)</span>. The goal is to help the model memorize information in the earlier sequence. We are going to walk through LSTM step by step.</p>
<p>The first step of LSTM is to decide what information to <strong>forget</strong>. This decision is made by “forget gate”, a sigmoid function (<span class="math inline">\(\Gamma_{f}\)</span>). It looks at <span class="math inline">\(a^{&lt;t-1&gt;}\)</span> and <span class="math inline">\(x^{t}\)</span> and outputs a number between 0 and 1 for each number in the cell state <span class="math inline">\(c^{t-1}\)</span>. A value 1 means “completely remember the state”, while 0 means “completely forget the state”.</p>
<center>
<img src="images/lstm1.png" style="width:80.0%" />
</center>
<p>The next step is to decide what new information we’re going to add to the cell state. This step includes two parts:</p>
<ol style="list-style-type: decimal">
<li>input gate (<span class="math inline">\(\Gamma_{u}\)</span>): a sigmoid function that decides how much we want to update</li>
<li>a vector of new candidate value (<span class="math inline">\(\tilde{c}^{&lt;t&gt;}\)</span>)</li>
</ol>
<center>
<img src="images/lstm2.png" style="width:80.0%" />
</center>
<p>The multiplication of the above two parts <span class="math inline">\(\Gamma_{u}*\tilde{c}^{&lt;t&gt;}\)</span> is the new candidate scaled by the input gate. We then combine the results we get so far to get new cell state <span class="math inline">\(c^{&lt;t&gt;}\)</span>.</p>
<center>
<img src="images/lstm3.png" style="width:80.0%" />
</center>
<p>Finally, we need to decide what we are going to output. The output is a filtered version of the new cell state <span class="math inline">\(c^{&lt;t&gt;}\)</span>.</p>
<center>
<img src="images/lstm4.png" style="width:80.0%" />
</center>
</div>
<div id="rnnexample" class="section level3">
<h3><span class="header-section-number">12.4.4</span> Sentiment Analysis Using RNN</h3>
<p>In this section, we will walk through an example of text sentiment analysis using RNN. Refer to section <a href="CloudEnvironment.html#CloudEnvironment">4.3</a> to set up an account, create a notebook (R or Python) and start a cluster. Refer to section <a href="feedforward-neural-network.html#ffnnexample">12.2.7</a> for package installation.</p>
<p>We will use the IMDB movie review data. It is one of the most used datasets for text-related machine learning methods. The datasets’ inputs are movie reviews published at IMDB in its raw text format, and the output is a binary sentiment indicator( “1” for positive and “0” for negative) created through human evaluation. The training and testing data have 25,000 records each. Each review varies in length.</p>
<div id="data-preprocessing-2" class="section level4">
<h4><span class="header-section-number">12.4.4.1</span> Data preprocessing</h4>
<p>Machine learning algorithms can not deal with raw text, and we have to convert text into numbers before feeding it into an algorithm. Tokenization is one way to convert text data into a numerical representation. For example, suppose we have 500 unique words for all reviews in the training dataset. We can label each word by the rank (i.e., from 1 to 500) of their frequency in the training data. Then each word is replaced by an integer between 1 to 500. This way, we can map each movie review from its raw text format to a sequence of integers.</p>
<p>As reviews can have different lengths, sequences of integers will have different sizes too. So another important step is to make sure each input has the same length by padding or truncating. For example, we can set a length of 50 words, and for any reviews less than 50 words, we can pad 0 to make it 50 in length; and for reviews with more than 50 words, we can truncate the sequence to 50 by keeping only the first 50 words. After padding and truncating, we have a typical data frame, each row is an observation, and each column is a feature. The number of features is the number of words designed for each review (i.e., 50 in this example).</p>
<p>After tokenization, the numerical input is just a naive mapping to the original words, and the integers do not have their usual numerical meanings. We need to use embedding to convert these categorical integers to more meaningful representations. The word embedding captures the inherited relationship of words and dramatically reduces the input dimension (see section <a href="recurrent-neural-network.html#embedding">12.4.2</a>). The dimension is a vector space representing the entire vocabulary. It can be 128 or 256, and the vector space dimension is the same when the vocabulary changes. It has a lower dimension, and each vector is filled with real numbers. The embedding vectors can be learned from the training data, or we can use pre-trained embedding models. There are many pre-trained embeddings for us to use, such as Word2Vec, BIRD.</p>
<img src="images/TokenizingPadding.png" style="width:80.0%" />
<center>
</div>
<div id="r-code-for-imdb-dataset" class="section level4">
<h4><span class="header-section-number">12.4.4.2</span> R code for IMDB dataset</h4>
<p>The IMDB dataset is preloaded for <code>keras</code> and we can call <code>dataset_imdb()</code> to load a partially pre-processed dataset into a data frame. We can define a few parameters in that function. <code>num_words</code> is the number of words in each review to be used. All the unique words are ranked by their frequency counts in the training dataset. The <code>dataset_imdb()</code> function keeps the top <code>num_words</code> words and replaces other words with a default value of 2, and using integers to represent text (i.e., top frequency word will be replaced by 3 and 0, 1, 2 are reserved for “padding,” “start of the sequence,” and “unknown.” ).</p>
<div class="sourceCode" id="cb458"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb458-1" data-line-number="1"><span class="co"># Load `keras` package</span></a>
<a class="sourceLine" id="cb458-2" data-line-number="2"><span class="kw">library</span>(keras)</a>
<a class="sourceLine" id="cb458-3" data-line-number="3"></a>
<a class="sourceLine" id="cb458-4" data-line-number="4"><span class="co"># consider only the top 10,000 words in the dataset</span></a>
<a class="sourceLine" id="cb458-5" data-line-number="5">max_unique_word &lt;-<span class="st"> </span><span class="dv">2500</span></a>
<a class="sourceLine" id="cb458-6" data-line-number="6"><span class="co"># cut off reviews after 100 words</span></a>
<a class="sourceLine" id="cb458-7" data-line-number="7">max_review_len &lt;-<span class="st"> </span><span class="dv">100</span></a></code></pre></div>
<p>Now we load the IMDB dataset, and we can check the structure of the loaded object by using <code>str()</code> command.</p>
<div class="sourceCode" id="cb459"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb459-1" data-line-number="1">my_imdb &lt;-<span class="st"> </span><span class="kw">dataset_imdb</span>(<span class="dt">num_words =</span> max_unique_word)</a>
<a class="sourceLine" id="cb459-2" data-line-number="2"><span class="kw">str</span>(my_imdb)</a></code></pre></div>
<div class="sourceCode" id="cb460"><pre class="sourceCode html"><code class="sourceCode html"><a class="sourceLine" id="cb460-1" data-line-number="1">Downloading data from</a>
<a class="sourceLine" id="cb460-2" data-line-number="2">https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz</a>
<a class="sourceLine" id="cb460-3" data-line-number="3"></a>
<a class="sourceLine" id="cb460-4" data-line-number="4">    8192/17464789 [..............................] - ETA: 0s</a>
<a class="sourceLine" id="cb460-5" data-line-number="5">  811008/17464789 [&gt;.............................] - ETA: 1s</a>
<a class="sourceLine" id="cb460-6" data-line-number="6"> 4202496/17464789 [======&gt;.......................] - ETA: 0s</a>
<a class="sourceLine" id="cb460-7" data-line-number="7">11476992/17464789 [==================&gt;...........] - ETA: 0s</a>
<a class="sourceLine" id="cb460-8" data-line-number="8">17465344/17464789 [==============================] - 0s 0us/step</a>
<a class="sourceLine" id="cb460-9" data-line-number="9">List of 2</a>
<a class="sourceLine" id="cb460-10" data-line-number="10"> $ train:List of 2</a>
<a class="sourceLine" id="cb460-11" data-line-number="11">  ..$ x:List of 25000</a>
<a class="sourceLine" id="cb460-12" data-line-number="12">  .. ..$ : int [1:218] 1 14 22 16 43 530 973 1622 1385 65 ...</a>
<a class="sourceLine" id="cb460-13" data-line-number="13">  .. ..$ : int [1:189] 1 194 1153 194 2 78 228 5 6 1463 ...</a>
<a class="sourceLine" id="cb460-14" data-line-number="14">  </a>
<a class="sourceLine" id="cb460-15" data-line-number="15">*** skipped some output ***</a></code></pre></div>
<div class="sourceCode" id="cb461"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb461-1" data-line-number="1">x_train &lt;-<span class="st"> </span>my_imdb<span class="op">$</span>train<span class="op">$</span>x</a>
<a class="sourceLine" id="cb461-2" data-line-number="2">y_train &lt;-<span class="st"> </span>my_imdb<span class="op">$</span>train<span class="op">$</span>y</a>
<a class="sourceLine" id="cb461-3" data-line-number="3">x_test  &lt;-<span class="st"> </span>my_imdb<span class="op">$</span>test<span class="op">$</span>x</a>
<a class="sourceLine" id="cb461-4" data-line-number="4">y_test  &lt;-<span class="st"> </span>my_imdb<span class="op">$</span>test<span class="op">$</span>y</a></code></pre></div>
<p>Next, we do the padding and truncating process.</p>
<div class="sourceCode" id="cb462"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb462-1" data-line-number="1">x_train &lt;-<span class="st"> </span><span class="kw">pad_sequences</span>(x_train, <span class="dt">maxlen =</span> max_review_len)</a>
<a class="sourceLine" id="cb462-2" data-line-number="2">x_test &lt;-<span class="st"> </span><span class="kw">pad_sequences</span>(x_test, <span class="dt">maxlen =</span> max_review_len)</a></code></pre></div>
<p>The <code>x_train</code> and <code>x_test</code> are numerical data frames ready to be used for recurrent neural network models.</p>
<p><strong>Simple Recurrent Neurel Network</strong></p>
<p>Like DNN and CNN models we trained in the past, RNN models are relatively easy to train using <code>keras</code> after the pre-processing stage. In the following example, we use <code>layer_embedding()</code> to fit an embedding layer based on the training dataset, which has two parameters: <code>input_dim</code> (the number of unique words) and <code>output_dim</code> (the length of dense vectors). Then, we add a simple RNN layer by calling <code>layer_simple_rnn()</code> and followed by a dense layer <code>layer_dense()</code> to connect to the response binary variable.</p>
<div class="sourceCode" id="cb463"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb463-1" data-line-number="1">rnn_model &lt;-<span class="st"> </span><span class="kw">keras_model_sequential</span>()</a>
<a class="sourceLine" id="cb463-2" data-line-number="2">rnn_model <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb463-3" data-line-number="3"><span class="st">  </span><span class="kw">layer_embedding</span>(<span class="dt">input_dim =</span> max_unique_word, <span class="dt">output_dim =</span> <span class="dv">128</span>) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb463-4" data-line-number="4"><span class="st">  </span><span class="kw">layer_simple_rnn</span>(<span class="dt">units =</span> <span class="dv">64</span>, <span class="dt">dropout =</span> <span class="fl">0.2</span>, <span class="dt">recurrent_dropout =</span> <span class="fl">0.2</span>) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb463-5" data-line-number="5"><span class="st">  </span><span class="kw">layer_dense</span>(<span class="dt">units =</span> <span class="dv">1</span>, <span class="dt">activation =</span> <span class="st">&#39;sigmoid&#39;</span>)</a></code></pre></div>
<p>We compile the RNN model by defining the loss function, optimizer to use, and metrics to track the same way as DNN and CNN models.</p>
<div class="sourceCode" id="cb464"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb464-1" data-line-number="1">rnn_model <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">compile</span>(</a>
<a class="sourceLine" id="cb464-2" data-line-number="2">  <span class="dt">loss =</span> <span class="st">&#39;binary_crossentropy&#39;</span>,</a>
<a class="sourceLine" id="cb464-3" data-line-number="3">  <span class="dt">optimizer =</span> <span class="st">&#39;adam&#39;</span>,</a>
<a class="sourceLine" id="cb464-4" data-line-number="4">  <span class="dt">metrics =</span> <span class="kw">c</span>(<span class="st">&#39;accuracy&#39;</span>)</a>
<a class="sourceLine" id="cb464-5" data-line-number="5">)</a></code></pre></div>
<p>Let us define a few more variables before fitting the model: <code>batch_size</code>, <code>epochs</code>, and <code>validation_split</code>. These variables have the same meaning as DNN and CNN models we see in the past.</p>
<div class="sourceCode" id="cb465"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb465-1" data-line-number="1">batch_size =<span class="st"> </span><span class="dv">128</span></a>
<a class="sourceLine" id="cb465-2" data-line-number="2">epochs =<span class="st"> </span><span class="dv">5</span></a>
<a class="sourceLine" id="cb465-3" data-line-number="3">validation_split =<span class="st"> </span><span class="fl">0.2</span></a>
<a class="sourceLine" id="cb465-4" data-line-number="4"></a>
<a class="sourceLine" id="cb465-5" data-line-number="5">rnn_history &lt;-<span class="st"> </span>rnn_model <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">fit</span>(</a>
<a class="sourceLine" id="cb465-6" data-line-number="6">  x_train, y_train,</a>
<a class="sourceLine" id="cb465-7" data-line-number="7">  <span class="dt">batch_size =</span> batch_size,</a>
<a class="sourceLine" id="cb465-8" data-line-number="8">  <span class="dt">epochs =</span> epochs,</a>
<a class="sourceLine" id="cb465-9" data-line-number="9">  <span class="dt">validation_split =</span> validation_split</a>
<a class="sourceLine" id="cb465-10" data-line-number="10">)</a></code></pre></div>
<div class="sourceCode" id="cb466"><pre class="sourceCode html"><code class="sourceCode html"><a class="sourceLine" id="cb466-1" data-line-number="1">Epoch 1/5</a>
<a class="sourceLine" id="cb466-2" data-line-number="2"></a>
<a class="sourceLine" id="cb466-3" data-line-number="3">  1/157 [...............] - ETA: 0s - loss: 0.7348 - accuracy: 0.4766</a>
<a class="sourceLine" id="cb466-4" data-line-number="4">  2/157 [...............] - ETA: 7s - loss: 0.7279 - accuracy: 0.4961</a>
<a class="sourceLine" id="cb466-5" data-line-number="5">  3/157 [...............] - ETA: 10s - loss: 0.7290 - accuracy: 0.4896</a>
<a class="sourceLine" id="cb466-6" data-line-number="6"></a>
<a class="sourceLine" id="cb466-7" data-line-number="7">*** skipped some output ***</a>
<a class="sourceLine" id="cb466-8" data-line-number="8"></a>
<a class="sourceLine" id="cb466-9" data-line-number="9">154/157 [=============&gt;.] - ETA: 0s - loss: 0.4456 - accuracy: 0.7991</a>
<a class="sourceLine" id="cb466-10" data-line-number="10">155/157 [=============&gt;.] - ETA: 0s - loss: 0.4460 - accuracy: 0.7991</a>
<a class="sourceLine" id="cb466-11" data-line-number="11">156/157 [=============&gt;.] - ETA: 0s - loss: 0.4457 - accuracy: 0.7995</a>
<a class="sourceLine" id="cb466-12" data-line-number="12">157/157 [===============] - 17s 109ms/step </a>
<a class="sourceLine" id="cb466-13" data-line-number="13">loss: 0.4457 - accuracy: 0.7995 - val_loss: 0.4444 - val_accuracy: 0.7908</a></code></pre></div>
<div class="sourceCode" id="cb467"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb467-1" data-line-number="1"><span class="kw">plot</span>(rnn_history)</a></code></pre></div>
<center>
<img src="images/rnnhandson1.png" style="width:90.0%" />
</center>
<div class="sourceCode" id="cb468"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb468-1" data-line-number="1">rnn_model <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb468-2" data-line-number="2"><span class="st">   </span><span class="kw">evaluate</span>(x_test, y_test)</a></code></pre></div>
<div class="sourceCode" id="cb469"><pre class="sourceCode html"><code class="sourceCode html"><a class="sourceLine" id="cb469-1" data-line-number="1">  1/782 [...............] - ETA: 0s - loss: 0.3019 - accuracy: 0.8438</a>
<a class="sourceLine" id="cb469-2" data-line-number="2">  8/782 [...............] - ETA: 5s - loss: 0.4328 - accuracy: 0.8008</a>
<a class="sourceLine" id="cb469-3" data-line-number="3"> 15/782 [...............] - ETA: 5s - loss: 0.4415 - accuracy: 0.7937</a>
<a class="sourceLine" id="cb469-4" data-line-number="4"> 23/782 [...............] - ETA: 5s - loss: 0.4247 - accuracy: 0.8043</a>
<a class="sourceLine" id="cb469-5" data-line-number="5"></a>
<a class="sourceLine" id="cb469-6" data-line-number="6">*** skipped some output ***</a>
<a class="sourceLine" id="cb469-7" data-line-number="7"></a>
<a class="sourceLine" id="cb469-8" data-line-number="8">775/782 [=============&gt;.] - ETA: 0s - loss: 0.4371 - accuracy: 0.8007</a>
<a class="sourceLine" id="cb469-9" data-line-number="9">782/782 [===============] - ETA: 0s - loss: 0.4365 - accuracy: 0.8010</a>
<a class="sourceLine" id="cb469-10" data-line-number="10">782/782 [===============] - 6s 8ms/step - loss: 0.4365 - accuracy: 0.8010</a>
<a class="sourceLine" id="cb469-11" data-line-number="11">     loss  accuracy </a>
<a class="sourceLine" id="cb469-12" data-line-number="12">0.4365373 0.8010000 </a></code></pre></div>
<p><strong>LSTM RNN Model</strong></p>
<p>A simple RNN layer is a good starting point for learning RNN, but the performance is usually not that good because these long-term dependencies are impossible to learn due to vanishing gradient. Long Short Term Memory RNN model (LSTM) can carry useful information from the earlier words to later words. In <code>keras</code>, it is easy to replace a simple RNN layer with an LSTM layer by using <code>layer_lstm()</code>.</p>
<div class="sourceCode" id="cb470"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb470-1" data-line-number="1">lstm_model &lt;-<span class="st"> </span><span class="kw">keras_model_sequential</span>()</a>
<a class="sourceLine" id="cb470-2" data-line-number="2"></a>
<a class="sourceLine" id="cb470-3" data-line-number="3">lstm_model <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb470-4" data-line-number="4"><span class="st">  </span><span class="kw">layer_embedding</span>(<span class="dt">input_dim =</span> max_unique_word, <span class="dt">output_dim =</span> <span class="dv">128</span>) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb470-5" data-line-number="5"><span class="st">  </span><span class="kw">layer_lstm</span>(<span class="dt">units =</span> <span class="dv">64</span>, <span class="dt">dropout =</span> <span class="fl">0.2</span>, <span class="dt">recurrent_dropout =</span> <span class="fl">0.2</span>) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb470-6" data-line-number="6"><span class="st">  </span><span class="kw">layer_dense</span>(<span class="dt">units =</span> <span class="dv">1</span>, <span class="dt">activation =</span> <span class="st">&#39;sigmoid&#39;</span>)</a>
<a class="sourceLine" id="cb470-7" data-line-number="7"></a>
<a class="sourceLine" id="cb470-8" data-line-number="8">lstm_model <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">compile</span>(</a>
<a class="sourceLine" id="cb470-9" data-line-number="9">  <span class="dt">loss =</span> <span class="st">&#39;binary_crossentropy&#39;</span>,</a>
<a class="sourceLine" id="cb470-10" data-line-number="10">  <span class="dt">optimizer =</span> <span class="st">&#39;adam&#39;</span>,</a>
<a class="sourceLine" id="cb470-11" data-line-number="11">  <span class="dt">metrics =</span> <span class="kw">c</span>(<span class="st">&#39;accuracy&#39;</span>)</a>
<a class="sourceLine" id="cb470-12" data-line-number="12">)</a>
<a class="sourceLine" id="cb470-13" data-line-number="13"></a>
<a class="sourceLine" id="cb470-14" data-line-number="14">batch_size =<span class="st"> </span><span class="dv">128</span></a>
<a class="sourceLine" id="cb470-15" data-line-number="15">epochs =<span class="st"> </span><span class="dv">5</span></a>
<a class="sourceLine" id="cb470-16" data-line-number="16">validation_split =<span class="st"> </span><span class="fl">0.2</span></a>
<a class="sourceLine" id="cb470-17" data-line-number="17"></a>
<a class="sourceLine" id="cb470-18" data-line-number="18">lstm_history &lt;-<span class="st"> </span>lstm_model <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">fit</span>(</a>
<a class="sourceLine" id="cb470-19" data-line-number="19">  x_train, y_train,</a>
<a class="sourceLine" id="cb470-20" data-line-number="20">  <span class="dt">batch_size =</span> batch_size,</a>
<a class="sourceLine" id="cb470-21" data-line-number="21">  <span class="dt">epochs =</span> epochs,</a>
<a class="sourceLine" id="cb470-22" data-line-number="22">  <span class="dt">validation_split =</span> validation_split</a>
<a class="sourceLine" id="cb470-23" data-line-number="23">)</a></code></pre></div>
<div class="sourceCode" id="cb471"><pre class="sourceCode html"><code class="sourceCode html"><a class="sourceLine" id="cb471-1" data-line-number="1">Epoch 1/5</a>
<a class="sourceLine" id="cb471-2" data-line-number="2"></a>
<a class="sourceLine" id="cb471-3" data-line-number="3">  1/157 [................] - ETA: 0s - loss: 0.6939 - accuracy: 0.4766</a>
<a class="sourceLine" id="cb471-4" data-line-number="4">  2/157 [................] - ETA: 31s - loss: 0.6940 - accuracy: 0.4766</a>
<a class="sourceLine" id="cb471-5" data-line-number="5">  3/157 [................] - ETA: 43s - loss: 0.6937 - accuracy: 0.4896</a>
<a class="sourceLine" id="cb471-6" data-line-number="6"></a>
<a class="sourceLine" id="cb471-7" data-line-number="7">*** skipped some output ***</a>
<a class="sourceLine" id="cb471-8" data-line-number="8"></a>
<a class="sourceLine" id="cb471-9" data-line-number="9">155/157 [===============&gt;.] - ETA: 0s - loss: 0.2610 - accuracy: 0.8918</a>
<a class="sourceLine" id="cb471-10" data-line-number="10">156/157 [===============&gt;.] - ETA: 0s - loss: 0.2607 - accuracy: 0.8920</a>
<a class="sourceLine" id="cb471-11" data-line-number="11">157/157 [=================] - ETA: 0s - loss: 0.2609 - accuracy: 0.8917</a>
<a class="sourceLine" id="cb471-12" data-line-number="12">157/157 [=================] - 67s 424ms/step </a>
<a class="sourceLine" id="cb471-13" data-line-number="13">loss: 0.2609 - accuracy: 0.8917 - val_loss: 0.3754 - val_accuracy: 0.8328</a></code></pre></div>
<div class="sourceCode" id="cb472"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb472-1" data-line-number="1"><span class="kw">plot</span>(lstm_history)</a></code></pre></div>
<center>
<img src="images/rnnhandson2.png" style="width:90.0%" />
</center>
<div class="sourceCode" id="cb473"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb473-1" data-line-number="1">lstm_model <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb473-2" data-line-number="2"><span class="st">   </span><span class="kw">evaluate</span>(x_test, y_test)</a></code></pre></div>
<div class="sourceCode" id="cb474"><pre class="sourceCode html"><code class="sourceCode html"><a class="sourceLine" id="cb474-1" data-line-number="1">  1/782 [................] - ETA: 0s - loss: 0.2332 - accuracy: 0.9062</a>
<a class="sourceLine" id="cb474-2" data-line-number="2">  4/782 [................] - ETA: 12s - loss: 0.3536 - accuracy: 0.8750</a>
<a class="sourceLine" id="cb474-3" data-line-number="3">  7/782 [................] - ETA: 14s - loss: 0.3409 - accuracy: 0.8705</a>
<a class="sourceLine" id="cb474-4" data-line-number="4"> 10/782 [................] - ETA: 14s - loss: 0.3508 - accuracy: 0.8625</a>
<a class="sourceLine" id="cb474-5" data-line-number="5"></a>
<a class="sourceLine" id="cb474-6" data-line-number="6">*** skipped some output ***</a>
<a class="sourceLine" id="cb474-7" data-line-number="7"></a>
<a class="sourceLine" id="cb474-8" data-line-number="8">775/782 [=============&gt;.] - ETA: 0s - loss: 0.3640 - accuracy: 0.8415</a>
<a class="sourceLine" id="cb474-9" data-line-number="9">778/782 [==============&gt;.] - ETA: 0s - loss: 0.3637 - accuracy: 0.8417</a>
<a class="sourceLine" id="cb474-10" data-line-number="10">780/782 [==============&gt;.] - ETA: 0s - loss: 0.3632 - accuracy: 0.8419</a>
<a class="sourceLine" id="cb474-11" data-line-number="11">782/782 [================] - ETA: 0s - loss: 0.3631 - accuracy: 0.8420</a>
<a class="sourceLine" id="cb474-12" data-line-number="12">782/782 [================] - 18s 22ms/step - loss: 0.3631 - accuracy: 0.8420</a>
<a class="sourceLine" id="cb474-13" data-line-number="13">     loss  accuracy </a>
<a class="sourceLine" id="cb474-14" data-line-number="14">0.3631141 0.8420000 </a></code></pre></div>
<p>This simple example shows that LSTM’s performance has improved dramatically from the simple RNN model. The computation time for LSTM is roughly doubled when compared with the simple RNN model for this small dataset.</p>

</div>
</div>
</div>
<!-- </div> -->



<h3>References</h3>
<div id="refs" class="references">
<div id="ref-deeplearningpy17">
<p>Chollet, François. 2017. <em>Deep Learning with Python</em>. Manning.</p>
</div>
<div id="ref-deeplearningr18">
<p>Chollet, François, and J. J. Allaire. 2018. <em>Deep Learning with R</em>. Manning.</p>
</div>
<div id="ref-chung2014empirical">
<p>Chung, Junyoung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. 2014. “Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling.”</p>
</div>
<div id="ref-Goodfellow-et-al-2016">
<p>Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. <em>Deep Learning</em>. MIT Press.</p>
</div>
<div id="ref-lstm1997">
<p>Hochreiter, Sepp, and JÃŒrgen Schmidhuber. 1997. “Long Short-Term Memory.” <em>Neural Computation</em> 9 (8): 1735–80.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="convolutional-neural-network.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="r-code-for-data-simulation.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/lunr.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": true,
"twitter": true,
"linkedin": true,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/happyrabbit/IntroDataScience/12-DeepLearning.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["IDS.pdf", "IDS.epub", "IDS.mobi"],
"toc": {
"collapse": "section",
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
