<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>8.2 Classification Model Performance | Practitioner’s Guide to Data Science</title>
  <meta name="description" content="Introduction to Data Science" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="8.2 Classification Model Performance | Practitioner’s Guide to Data Science" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="http://scientistcafe.com/IDS/" />
  
  <meta property="og:description" content="Introduction to Data Science" />
  <meta name="github-repo" content="happyrabbit/IntroDataScience" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="8.2 Classification Model Performance | Practitioner’s Guide to Data Science" />
  
  <meta name="twitter:description" content="Introduction to Data Science" />
  

<meta name="author" content="Hui Lin and Ming Li" />


<meta name="date" content="2023-02-19" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="regression-model-performance.html"/>
<link rel="next" href="regression-models.html"/>
<script src="libs/header-attrs/header-attrs.js"></script>
<script src="libs/jquery/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="http://scientistcafe.com">Homepage</a></li>

<li class="divider"></li>
<li><a href="index.html#preface" id="toc-preface">Preface</a>
<ul>
<li><a href="goal-of-the-book.html#goal-of-the-book" id="toc-goal-of-the-book">Goal of the Book</a></li>
<li><a href="what-this-book-covers.html#what-this-book-covers" id="toc-what-this-book-covers">What This Book Covers</a></li>
<li><a href="who-this-book-is-for.html#who-this-book-is-for" id="toc-who-this-book-is-for">Who This Book Is For</a></li>
<li><a href="how-to-use-this-book.html#how-to-use-this-book" id="toc-how-to-use-this-book">How to Use This Book</a>
<ul>
<li><a href="how-to-use-this-book.html#what-the-book-assumes" id="toc-what-the-book-assumes">What the Book Assumes</a></li>
<li><a href="how-to-use-this-book.html#how-to-run-r-and-python-code" id="toc-how-to-run-r-and-python-code">How to Run R and Python Code</a></li>
</ul></li>
<li><a href="complementary-reading.html#complementary-reading" id="toc-complementary-reading">Complementary Reading</a></li>
</ul></li>
<li><a href="about-the-authors.html#about-the-authors" id="toc-about-the-authors">About the Authors</a></li>
<li><a href="introduction.html#introduction" id="toc-introduction"><span class="toc-section-number">1</span> Introduction</a>
<ul>
<li><a href="a-brief-history-of-data-science.html#a-brief-history-of-data-science" id="toc-a-brief-history-of-data-science"><span class="toc-section-number">1.1</span> A brief history of data science</a></li>
<li><a href="data-science-role-and-skill-tracks.html#data-science-role-and-skill-tracks" id="toc-data-science-role-and-skill-tracks"><span class="toc-section-number">1.2</span> Data science role and skill tracks</a>
<ul>
<li><a href="data-science-role-and-skill-tracks.html#engineering" id="toc-engineering"><span class="toc-section-number">1.2.1</span> Engineering</a></li>
<li><a href="data-science-role-and-skill-tracks.html#analysis" id="toc-analysis"><span class="toc-section-number">1.2.2</span> Analysis</a></li>
<li><a href="data-science-role-and-skill-tracks.html#modelinginference" id="toc-modelinginference"><span class="toc-section-number">1.2.3</span> Modeling/inference</a></li>
</ul></li>
<li><a href="what-kind-of-questions-can-data-science-solve.html#what-kind-of-questions-can-data-science-solve" id="toc-what-kind-of-questions-can-data-science-solve"><span class="toc-section-number">1.3</span> What kind of questions can data science solve?</a>
<ul>
<li><a href="what-kind-of-questions-can-data-science-solve.html#prerequisites" id="toc-prerequisites"><span class="toc-section-number">1.3.1</span> Prerequisites</a></li>
<li><a href="what-kind-of-questions-can-data-science-solve.html#problem-type" id="toc-problem-type"><span class="toc-section-number">1.3.2</span> Problem type</a></li>
</ul></li>
<li><a href="structure-of-data-science-team.html#structure-of-data-science-team" id="toc-structure-of-data-science-team"><span class="toc-section-number">1.4</span> Structure of data science team</a></li>
<li><a href="data-science-roles.html#data-science-roles" id="toc-data-science-roles"><span class="toc-section-number">1.5</span> Data science roles</a></li>
</ul></li>
<li><a href="SoftSkillsforDataScientists.html#SoftSkillsforDataScientists" id="toc-SoftSkillsforDataScientists"><span class="toc-section-number">2</span> Soft Skills for Data Scientists</a>
<ul>
<li><a href="comparison-between-statistician-and-data-scientist.html#comparison-between-statistician-and-data-scientist" id="toc-comparison-between-statistician-and-data-scientist"><span class="toc-section-number">2.1</span> Comparison between Statistician and Data Scientist</a></li>
<li><a href="beyond-data-and-analytics.html#beyond-data-and-analytics" id="toc-beyond-data-and-analytics"><span class="toc-section-number">2.2</span> Beyond Data and Analytics</a></li>
<li><a href="three-pillars-of-knowledge.html#three-pillars-of-knowledge" id="toc-three-pillars-of-knowledge"><span class="toc-section-number">2.3</span> Three Pillars of Knowledge</a></li>
<li><a href="data-science-project-cycle.html#data-science-project-cycle" id="toc-data-science-project-cycle"><span class="toc-section-number">2.4</span> Data Science Project Cycle</a>
<ul>
<li><a href="data-science-project-cycle.html#types-of-data-science-projects" id="toc-types-of-data-science-projects"><span class="toc-section-number">2.4.1</span> Types of Data Science Projects</a></li>
<li><a href="data-science-project-cycle.html#ProblemFormulationandProjectPlanningStage" id="toc-ProblemFormulationandProjectPlanningStage"><span class="toc-section-number">2.4.2</span> Problem Formulation and Project Planning Stage</a></li>
<li><a href="data-science-project-cycle.html#project-modeling-stage" id="toc-project-modeling-stage"><span class="toc-section-number">2.4.3</span> Project Modeling Stage</a></li>
<li><a href="data-science-project-cycle.html#ModelImplementationandPostProductionStage" id="toc-ModelImplementationandPostProductionStage"><span class="toc-section-number">2.4.4</span> Model Implementation and Post Production Stage</a></li>
<li><a href="data-science-project-cycle.html#ProjectCycleSummary" id="toc-ProjectCycleSummary"><span class="toc-section-number">2.4.5</span> Project Cycle Summary</a></li>
</ul></li>
<li><a href="common-mistakes-in-data-science.html#common-mistakes-in-data-science" id="toc-common-mistakes-in-data-science"><span class="toc-section-number">2.5</span> Common Mistakes in Data Science</a>
<ul>
<li><a href="common-mistakes-in-data-science.html#problem-formulation-stage" id="toc-problem-formulation-stage"><span class="toc-section-number">2.5.1</span> Problem Formulation Stage</a></li>
<li><a href="common-mistakes-in-data-science.html#project-planning-stage" id="toc-project-planning-stage"><span class="toc-section-number">2.5.2</span> Project Planning Stage</a></li>
<li><a href="common-mistakes-in-data-science.html#project-modeling-stage-1" id="toc-project-modeling-stage-1"><span class="toc-section-number">2.5.3</span> Project Modeling Stage</a></li>
<li><a href="common-mistakes-in-data-science.html#model-implementation-and-post-production-stage" id="toc-model-implementation-and-post-production-stage"><span class="toc-section-number">2.5.4</span> Model Implementation and Post Production Stage</a></li>
<li><a href="common-mistakes-in-data-science.html#summary-of-common-mistakes" id="toc-summary-of-common-mistakes"><span class="toc-section-number">2.5.5</span> Summary of Common Mistakes</a></li>
</ul></li>
</ul></li>
<li><a href="introduction-to-the-data.html#introduction-to-the-data" id="toc-introduction-to-the-data"><span class="toc-section-number">3</span> Introduction to The Data</a>
<ul>
<li><a href="customer-data-for-a-clothing-company.html#customer-data-for-a-clothing-company" id="toc-customer-data-for-a-clothing-company"><span class="toc-section-number">3.1</span> Customer Data for A Clothing Company</a></li>
<li><a href="swinediseasedata.html#swinediseasedata" id="toc-swinediseasedata"><span class="toc-section-number">3.2</span> Swine Disease Breakout Data</a></li>
<li><a href="mnist-dataset.html#mnist-dataset" id="toc-mnist-dataset"><span class="toc-section-number">3.3</span> MNIST Dataset</a></li>
<li><a href="imdb-dataset.html#imdb-dataset" id="toc-imdb-dataset"><span class="toc-section-number">3.4</span> IMDB Dataset</a></li>
</ul></li>
<li><a href="bigdatacloudplatform.html#bigdatacloudplatform" id="toc-bigdatacloudplatform"><span class="toc-section-number">4</span> Big Data Cloud Platform</a>
<ul>
<li><a href="power-of-cluster-of-computers.html#power-of-cluster-of-computers" id="toc-power-of-cluster-of-computers"><span class="toc-section-number">4.1</span> Power of Cluster of Computers</a></li>
<li><a href="evolution-of-cluster-computing.html#evolution-of-cluster-computing" id="toc-evolution-of-cluster-computing"><span class="toc-section-number">4.2</span> Evolution of Cluster Computing</a>
<ul>
<li><a href="evolution-of-cluster-computing.html#hadoop" id="toc-hadoop"><span class="toc-section-number">4.2.1</span> Hadoop</a></li>
<li><a href="evolution-of-cluster-computing.html#spark" id="toc-spark"><span class="toc-section-number">4.2.2</span> Spark</a></li>
</ul></li>
<li><a href="CloudEnvironment.html#CloudEnvironment" id="toc-CloudEnvironment"><span class="toc-section-number">4.3</span> Introduction of Cloud Environment</a>
<ul>
<li><a href="CloudEnvironment.html#open-account-and-create-a-cluster" id="toc-open-account-and-create-a-cluster"><span class="toc-section-number">4.3.1</span> Open Account and Create a Cluster</a></li>
<li><a href="CloudEnvironment.html#r-notebook" id="toc-r-notebook"><span class="toc-section-number">4.3.2</span> R Notebook</a></li>
<li><a href="CloudEnvironment.html#markdown-cells" id="toc-markdown-cells"><span class="toc-section-number">4.3.3</span> Markdown Cells</a></li>
</ul></li>
<li><a href="leveragesparkr.html#leveragesparkr" id="toc-leveragesparkr"><span class="toc-section-number">4.4</span> Leverage Spark Using R Notebook</a></li>
<li><a href="databases-and-sql.html#databases-and-sql" id="toc-databases-and-sql"><span class="toc-section-number">4.5</span> Databases and SQL</a>
<ul>
<li><a href="databases-and-sql.html#history" id="toc-history"><span class="toc-section-number">4.5.1</span> History</a></li>
<li><a href="databases-and-sql.html#database-table-and-view" id="toc-database-table-and-view"><span class="toc-section-number">4.5.2</span> Database, Table and View</a></li>
<li><a href="databases-and-sql.html#basic-sql-statement" id="toc-basic-sql-statement"><span class="toc-section-number">4.5.3</span> Basic SQL Statement</a></li>
<li><a href="databases-and-sql.html#advanced-topics-in-database" id="toc-advanced-topics-in-database"><span class="toc-section-number">4.5.4</span> Advanced Topics in Database</a></li>
</ul></li>
</ul></li>
<li><a href="datapreprocessing.html#datapreprocessing" id="toc-datapreprocessing"><span class="toc-section-number">5</span> Data Pre-processing</a>
<ul>
<li><a href="data-cleaning.html#data-cleaning" id="toc-data-cleaning"><span class="toc-section-number">5.1</span> Data Cleaning</a></li>
<li><a href="missing-values.html#missing-values" id="toc-missing-values"><span class="toc-section-number">5.2</span> Missing Values</a>
<ul>
<li><a href="missing-values.html#impute-missing-values-with-medianmode" id="toc-impute-missing-values-with-medianmode"><span class="toc-section-number">5.2.1</span> Impute missing values with median/mode</a></li>
<li><a href="missing-values.html#k-nearest-neighbors" id="toc-k-nearest-neighbors"><span class="toc-section-number">5.2.2</span> K-nearest neighbors</a></li>
<li><a href="missing-values.html#bagging-tree" id="toc-bagging-tree"><span class="toc-section-number">5.2.3</span> Bagging Tree</a></li>
</ul></li>
<li><a href="centering-and-scaling.html#centering-and-scaling" id="toc-centering-and-scaling"><span class="toc-section-number">5.3</span> Centering and Scaling</a></li>
<li><a href="resolve-skewness.html#resolve-skewness" id="toc-resolve-skewness"><span class="toc-section-number">5.4</span> Resolve Skewness</a></li>
<li><a href="outliers.html#outliers" id="toc-outliers"><span class="toc-section-number">5.5</span> Resolve Outliers</a></li>
<li><a href="collinearity.html#collinearity" id="toc-collinearity"><span class="toc-section-number">5.6</span> Collinearity</a></li>
<li><a href="sparse-variables.html#sparse-variables" id="toc-sparse-variables"><span class="toc-section-number">5.7</span> Sparse Variables</a></li>
<li><a href="re-encode-dummy-variables.html#re-encode-dummy-variables" id="toc-re-encode-dummy-variables"><span class="toc-section-number">5.8</span> Re-encode Dummy Variables</a></li>
</ul></li>
<li><a href="datawrangline.html#datawrangline" id="toc-datawrangline"><span class="toc-section-number">6</span> Data Wrangling</a>
<ul>
<li><a href="summarize-data.html#summarize-data" id="toc-summarize-data"><span class="toc-section-number">6.1</span> Summarize Data</a>
<ul>
<li><a href="summarize-data.html#dplyr-package" id="toc-dplyr-package"><span class="toc-section-number">6.1.1</span> <code>dplyr</code> package</a></li>
<li><a href="summarize-data.html#applyfamilyinbaser" id="toc-applyfamilyinbaser"><span class="toc-section-number">6.1.2</span> <code>apply()</code>, <code>lapply()</code> and <code>sapply()</code> in base R</a></li>
</ul></li>
<li><a href="tidy-and-reshape-data.html#tidy-and-reshape-data" id="toc-tidy-and-reshape-data"><span class="toc-section-number">6.2</span> Tidy and Reshape Data</a></li>
</ul></li>
<li><a href="modeltuningstrategy.html#modeltuningstrategy" id="toc-modeltuningstrategy"><span class="toc-section-number">7</span> Model Tuning Strategy</a>
<ul>
<li><a href="vbtradeoff.html#vbtradeoff" id="toc-vbtradeoff"><span class="toc-section-number">7.1</span> Variance-Bias Trade-Off</a></li>
<li><a href="datasplittingresampling.html#datasplittingresampling" id="toc-datasplittingresampling"><span class="toc-section-number">7.2</span> Data Splitting and Resampling</a>
<ul>
<li><a href="datasplittingresampling.html#datasplitting" id="toc-datasplitting"><span class="toc-section-number">7.2.1</span> Data Splitting</a></li>
<li><a href="datasplittingresampling.html#resampling" id="toc-resampling"><span class="toc-section-number">7.2.2</span> Resampling</a></li>
</ul></li>
</ul></li>
<li><a href="measuring-performance.html#measuring-performance" id="toc-measuring-performance"><span class="toc-section-number">8</span> Measuring Performance</a>
<ul>
<li><a href="regression-model-performance.html#regression-model-performance" id="toc-regression-model-performance"><span class="toc-section-number">8.1</span> Regression Model Performance</a></li>
<li><a href="classification-model-performance.html#classification-model-performance" id="toc-classification-model-performance"><span class="toc-section-number">8.2</span> Classification Model Performance</a>
<ul>
<li><a href="classification-model-performance.html#confusion-matrix" id="toc-confusion-matrix"><span class="toc-section-number">8.2.1</span> Confusion Matrix</a></li>
<li><a href="classification-model-performance.html#kappa-statistic" id="toc-kappa-statistic"><span class="toc-section-number">8.2.2</span> Kappa Statistic</a></li>
<li><a href="classification-model-performance.html#roc" id="toc-roc"><span class="toc-section-number">8.2.3</span> ROC</a></li>
<li><a href="classification-model-performance.html#gain-and-lift-charts" id="toc-gain-and-lift-charts"><span class="toc-section-number">8.2.4</span> Gain and Lift Charts</a></li>
</ul></li>
</ul></li>
<li><a href="regression-models.html#regression-models" id="toc-regression-models"><span class="toc-section-number">9</span> Regression Models</a>
<ul>
<li><a href="ordinary-least-square.html#ordinary-least-square" id="toc-ordinary-least-square"><span class="toc-section-number">9.1</span> Ordinary Least Square</a>
<ul>
<li><a href="ordinary-least-square.html#the-magic-p-value" id="toc-the-magic-p-value"><span class="toc-section-number">9.1.1</span> The Magic P-value</a></li>
<li><a href="ordinary-least-square.html#diagnostics-for-linear-regression" id="toc-diagnostics-for-linear-regression"><span class="toc-section-number">9.1.2</span> Diagnostics for Linear Regression</a></li>
</ul></li>
<li><a href="principal-component-regression-and-partial-least-square.html#principal-component-regression-and-partial-least-square" id="toc-principal-component-regression-and-partial-least-square"><span class="toc-section-number">9.2</span> Principal Component Regression and Partial Least Square</a></li>
</ul></li>
<li><a href="regularization-methods.html#regularization-methods" id="toc-regularization-methods"><span class="toc-section-number">10</span> Regularization Methods</a>
<ul>
<li><a href="ridge-regression.html#ridge-regression" id="toc-ridge-regression"><span class="toc-section-number">10.1</span> Ridge Regression</a></li>
<li><a href="lasso.html#lasso" id="toc-lasso"><span class="toc-section-number">10.2</span> LASSO</a></li>
<li><a href="elastic-net.html#elastic-net" id="toc-elastic-net"><span class="toc-section-number">10.3</span> Elastic Net</a></li>
<li><a href="penalized-generalized-linear-model.html#penalized-generalized-linear-model" id="toc-penalized-generalized-linear-model"><span class="toc-section-number">10.4</span> Penalized Generalized Linear Model</a>
<ul>
<li><a href="penalized-generalized-linear-model.html#introduction-to-glmnet-package" id="toc-introduction-to-glmnet-package"><span class="toc-section-number">10.4.1</span> Introduction to <code>glmnet</code> package</a></li>
<li><a href="penalized-generalized-linear-model.html#penalized-logistic-regression" id="toc-penalized-logistic-regression"><span class="toc-section-number">10.4.2</span> Penalized logistic regression</a></li>
</ul></li>
</ul></li>
<li><a href="treemodel.html#treemodel" id="toc-treemodel"><span class="toc-section-number">11</span> Tree-Based Methods</a>
<ul>
<li><a href="tree-basics.html#tree-basics" id="toc-tree-basics"><span class="toc-section-number">11.1</span> Tree Basics</a></li>
<li><a href="splitting-criteria.html#splitting-criteria" id="toc-splitting-criteria"><span class="toc-section-number">11.2</span> Splitting Criteria</a>
<ul>
<li><a href="splitting-criteria.html#gini-impurity" id="toc-gini-impurity"><span class="toc-section-number">11.2.1</span> Gini impurity</a></li>
<li><a href="splitting-criteria.html#information-gain-ig" id="toc-information-gain-ig"><span class="toc-section-number">11.2.2</span> Information Gain (IG)</a></li>
<li><a href="splitting-criteria.html#information-gain-ratio-igr" id="toc-information-gain-ratio-igr"><span class="toc-section-number">11.2.3</span> Information Gain Ratio (IGR)</a></li>
<li><a href="splitting-criteria.html#sum-of-squared-error-sse" id="toc-sum-of-squared-error-sse"><span class="toc-section-number">11.2.4</span> Sum of Squared Error (SSE)</a></li>
</ul></li>
<li><a href="tree-pruning.html#tree-pruning" id="toc-tree-pruning"><span class="toc-section-number">11.3</span> Tree Pruning</a></li>
<li><a href="regression-and-decision-tree-basic.html#regression-and-decision-tree-basic" id="toc-regression-and-decision-tree-basic"><span class="toc-section-number">11.4</span> Regression and Decision Tree Basic</a>
<ul>
<li><a href="regression-and-decision-tree-basic.html#regression-tree" id="toc-regression-tree"><span class="toc-section-number">11.4.1</span> Regression Tree</a></li>
<li><a href="regression-and-decision-tree-basic.html#decision-tree" id="toc-decision-tree"><span class="toc-section-number">11.4.2</span> Decision Tree</a></li>
</ul></li>
<li><a href="bagging-tree-1.html#bagging-tree-1" id="toc-bagging-tree-1"><span class="toc-section-number">11.5</span> Bagging Tree</a></li>
<li><a href="random-forest.html#random-forest" id="toc-random-forest"><span class="toc-section-number">11.6</span> Random Forest</a></li>
<li><a href="gradient-boosted-machine.html#gradient-boosted-machine" id="toc-gradient-boosted-machine"><span class="toc-section-number">11.7</span> Gradient Boosted Machine</a>
<ul>
<li><a href="gradient-boosted-machine.html#adaptive-boosting" id="toc-adaptive-boosting"><span class="toc-section-number">11.7.1</span> Adaptive Boosting</a></li>
<li><a href="gradient-boosted-machine.html#stochastic-gradient-boosting" id="toc-stochastic-gradient-boosting"><span class="toc-section-number">11.7.2</span> Stochastic Gradient Boosting</a></li>
</ul></li>
</ul></li>
<li><a href="deeplearning.html#deeplearning" id="toc-deeplearning"><span class="toc-section-number">12</span> Deep Learning</a>
<ul>
<li><a href="feedforward-neural-network.html#feedforward-neural-network" id="toc-feedforward-neural-network"><span class="toc-section-number">12.1</span> Feedforward Neural Network</a>
<ul>
<li><a href="feedforward-neural-network.html#logisticregasneuralnetwork" id="toc-logisticregasneuralnetwork"><span class="toc-section-number">12.1.1</span> Logistic Regression as Neural Network</a></li>
<li><a href="feedforward-neural-network.html#stochastic-gradient-descent" id="toc-stochastic-gradient-descent"><span class="toc-section-number">12.1.2</span> Stochastic Gradient Descent</a></li>
<li><a href="feedforward-neural-network.html#deepneuralnetwork" id="toc-deepneuralnetwork"><span class="toc-section-number">12.1.3</span> Deep Neural Network</a></li>
<li><a href="feedforward-neural-network.html#activationfunction" id="toc-activationfunction"><span class="toc-section-number">12.1.4</span> Activation Function</a></li>
<li><a href="feedforward-neural-network.html#optimization" id="toc-optimization"><span class="toc-section-number">12.1.5</span> Optimization</a></li>
<li><a href="feedforward-neural-network.html#deal-with-overfitting" id="toc-deal-with-overfitting"><span class="toc-section-number">12.1.6</span> Deal with Overfitting</a></li>
<li><a href="feedforward-neural-network.html#ffnnexample" id="toc-ffnnexample"><span class="toc-section-number">12.1.7</span> Image Recognition Using FFNN</a></li>
</ul></li>
<li><a href="convolutional-neural-network.html#convolutional-neural-network" id="toc-convolutional-neural-network"><span class="toc-section-number">12.2</span> Convolutional Neural Network</a>
<ul>
<li><a href="convolutional-neural-network.html#convolution-layer" id="toc-convolution-layer"><span class="toc-section-number">12.2.1</span> Convolution Layer</a></li>
<li><a href="convolutional-neural-network.html#padding-layer" id="toc-padding-layer"><span class="toc-section-number">12.2.2</span> Padding Layer</a></li>
<li><a href="convolutional-neural-network.html#pooling-layer" id="toc-pooling-layer"><span class="toc-section-number">12.2.3</span> Pooling Layer</a></li>
<li><a href="convolutional-neural-network.html#convolution-over-volume" id="toc-convolution-over-volume"><span class="toc-section-number">12.2.4</span> Convolution Over Volume</a></li>
<li><a href="convolutional-neural-network.html#cnnexample" id="toc-cnnexample"><span class="toc-section-number">12.2.5</span> Image Recognition Using CNN</a></li>
</ul></li>
<li><a href="recurrent-neural-network.html#recurrent-neural-network" id="toc-recurrent-neural-network"><span class="toc-section-number">12.3</span> Recurrent Neural Network</a>
<ul>
<li><a href="recurrent-neural-network.html#rnn-model" id="toc-rnn-model"><span class="toc-section-number">12.3.1</span> RNN Model</a></li>
<li><a href="recurrent-neural-network.html#lstm" id="toc-lstm"><span class="toc-section-number">12.3.2</span> Long Short Term Memory</a></li>
<li><a href="recurrent-neural-network.html#embedding" id="toc-embedding"><span class="toc-section-number">12.3.3</span> Word Embedding</a></li>
<li><a href="recurrent-neural-network.html#rnnexample" id="toc-rnnexample"><span class="toc-section-number">12.3.4</span> Sentiment Analysis Using RNN</a></li>
</ul></li>
</ul></li>
<li><a href="#appendix-appendix" id="toc-appendix-appendix">(APPENDIX) Appendix</a></li>
<li><a href="largelocaldata.html#largelocaldata" id="toc-largelocaldata"><span class="toc-section-number">13</span> Handling Large Local Data</a>
<ul>
<li><a href="readr.html#readr" id="toc-readr"><span class="toc-section-number">13.1</span> <code>readr</code></a></li>
<li><a href="data.table-enhanced-data.html#data.table-enhanced-data.frame" id="toc-data.table-enhanced-data.frame"><span class="toc-section-number">13.2</span> <code>data.table</code>— enhanced <code>data.frame</code></a></li>
</ul></li>
<li><a href="r-code-for-data-simulation.html#r-code-for-data-simulation" id="toc-r-code-for-data-simulation"><span class="toc-section-number">14</span> R code for data simulation</a>
<ul>
<li><a href="appendixdata1.html#appendixdata1" id="toc-appendixdata1"><span class="toc-section-number">14.1</span> Customer Data for Clothing Company</a></li>
<li><a href="appendixdata3.html#appendixdata3" id="toc-appendixdata3"><span class="toc-section-number">14.2</span> Swine Disease Breakout Data</a></li>
</ul></li>
<li><a href="references.html#references" id="toc-references">References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Practitioner’s Guide to Data Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="classification-model-performance" class="section level2 hasAnchor" number="8.2">
<h2><span class="header-section-number">8.2</span> Classification Model Performance<a href="classification-model-performance.html#classification-model-performance" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>This section focuses on performance measurement for models with a categorical response. The metrics in the previous section are for models with a continuous response and they are not appropriate in the context of classification. Most of the classification problems are dichotomous, such as an outbreak of disease, spam email, etc. There are also cases with more than two categories as the segments in the clothing company data. We use swine disease data to illustrate different metrics. Let’s train a random forest model as an example. We will discuss the model in Chapter <a href="treemodel.html#treemodel">11</a>.</p>
<div class="sourceCode" id="cb177"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb177-1"><a href="classification-model-performance.html#cb177-1" aria-hidden="true" tabindex="-1"></a>disease_dat <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;http://bit.ly/2KXb1Qi&quot;</span>)</span>
<span id="cb177-2"><a href="classification-model-performance.html#cb177-2" aria-hidden="true" tabindex="-1"></a><span class="co"># you can check the data using glimpse()</span></span>
<span id="cb177-3"><a href="classification-model-performance.html#cb177-3" aria-hidden="true" tabindex="-1"></a><span class="co"># glimpse(disease_dat)</span></span></code></pre></div>
<p>The process includes (1) separate the data to be training and testing sets, (2) fit model using training data (<code>xTrain</code> and <code>yTrain</code>), and (3) applied the trained model on testing data (<code>xTest</code> and <code>yTest</code>) to evaluate model performance.</p>
<p>We use 70% of the sample as training and the rest 30% as testing.</p>
<div class="sourceCode" id="cb178"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb178-1"><a href="classification-model-performance.html#cb178-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">100</span>)</span>
<span id="cb178-2"><a href="classification-model-performance.html#cb178-2" aria-hidden="true" tabindex="-1"></a><span class="co"># separate the data to be training and testing</span></span>
<span id="cb178-3"><a href="classification-model-performance.html#cb178-3" aria-hidden="true" tabindex="-1"></a>trainIndex <span class="ot">&lt;-</span> <span class="fu">createDataPartition</span>(disease_dat<span class="sc">$</span>y, <span class="at">p =</span> <span class="fl">0.8</span>, </span>
<span id="cb178-4"><a href="classification-model-performance.html#cb178-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">list =</span> F, <span class="at">times =</span> <span class="dv">1</span>)</span>
<span id="cb178-5"><a href="classification-model-performance.html#cb178-5" aria-hidden="true" tabindex="-1"></a>xTrain <span class="ot">&lt;-</span> disease_dat[trainIndex, ] <span class="sc">%&gt;%</span> dplyr<span class="sc">::</span><span class="fu">select</span>(<span class="sc">-</span>y)</span>
<span id="cb178-6"><a href="classification-model-performance.html#cb178-6" aria-hidden="true" tabindex="-1"></a>xTest <span class="ot">&lt;-</span> disease_dat[<span class="sc">-</span>trainIndex, ] <span class="sc">%&gt;%</span> dplyr<span class="sc">::</span><span class="fu">select</span>(<span class="sc">-</span>y)</span>
<span id="cb178-7"><a href="classification-model-performance.html#cb178-7" aria-hidden="true" tabindex="-1"></a><span class="co"># the response variable need to be factor</span></span>
<span id="cb178-8"><a href="classification-model-performance.html#cb178-8" aria-hidden="true" tabindex="-1"></a>yTrain <span class="ot">&lt;-</span> disease_dat<span class="sc">$</span>y[trainIndex] <span class="sc">%&gt;%</span> <span class="fu">as.factor</span>()</span>
<span id="cb178-9"><a href="classification-model-performance.html#cb178-9" aria-hidden="true" tabindex="-1"></a>yTest <span class="ot">&lt;-</span> disease_dat<span class="sc">$</span>y[<span class="sc">-</span>trainIndex] <span class="sc">%&gt;%</span> <span class="fu">as.factor</span>()</span></code></pre></div>
<p>Train a random forest model:</p>
<div class="sourceCode" id="cb179"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb179-1"><a href="classification-model-performance.html#cb179-1" aria-hidden="true" tabindex="-1"></a>train_rf <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(yTrain <span class="sc">~</span> ., </span>
<span id="cb179-2"><a href="classification-model-performance.html#cb179-2" aria-hidden="true" tabindex="-1"></a>                         <span class="at">data =</span> xTrain, </span>
<span id="cb179-3"><a href="classification-model-performance.html#cb179-3" aria-hidden="true" tabindex="-1"></a>                         <span class="at">mtry =</span> <span class="fu">trunc</span>(<span class="fu">sqrt</span>(<span class="fu">ncol</span>(xTrain) <span class="sc">-</span> <span class="dv">1</span>)),</span>
<span id="cb179-4"><a href="classification-model-performance.html#cb179-4" aria-hidden="true" tabindex="-1"></a>                         <span class="at">ntree =</span> <span class="dv">1000</span>, </span>
<span id="cb179-5"><a href="classification-model-performance.html#cb179-5" aria-hidden="true" tabindex="-1"></a>                         <span class="at">importance =</span> T)</span></code></pre></div>
<p>Apply the trained random forest model to the testing data to get two types of predictions:</p>
<ul>
<li>probability (a value between 0 to 1)</li>
</ul>
<div class="sourceCode" id="cb180"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb180-1"><a href="classification-model-performance.html#cb180-1" aria-hidden="true" tabindex="-1"></a>yhatprob <span class="ot">&lt;-</span> <span class="fu">predict</span>(train_rf, xTest, <span class="st">&quot;prob&quot;</span>)</span>
<span id="cb180-2"><a href="classification-model-performance.html#cb180-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">100</span>)</span>
<span id="cb180-3"><a href="classification-model-performance.html#cb180-3" aria-hidden="true" tabindex="-1"></a>car<span class="sc">::</span><span class="fu">some</span>(yhatprob)</span></code></pre></div>
<pre><code>##         0     1
## 47  0.831 0.169
## 101 0.177 0.823
## 196 0.543 0.457
## 258 0.858 0.142
## 274 0.534 0.466
## 369 0.827 0.173
## 389 0.852 0.148
## 416 0.183 0.817
## 440 0.523 0.477
## 642 0.836 0.164</code></pre>
<ul>
<li>category prediction (0 or 1)</li>
</ul>
<div class="sourceCode" id="cb182"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb182-1"><a href="classification-model-performance.html#cb182-1" aria-hidden="true" tabindex="-1"></a>yhat <span class="ot">&lt;-</span> <span class="fu">predict</span>(train_rf, xTest)</span>
<span id="cb182-2"><a href="classification-model-performance.html#cb182-2" aria-hidden="true" tabindex="-1"></a>car<span class="sc">::</span><span class="fu">some</span>(yhat)</span></code></pre></div>
<pre><code>## 146 232 269 302 500 520 521 575 738 781 
##   0   0   1   0   0   0   1   0   0   0 
## Levels: 0 1</code></pre>
<p>We will use the above two types of predictions to show different performance metrics.</p>
<div id="confusion-matrix" class="section level3 hasAnchor" number="8.2.1">
<h3><span class="header-section-number">8.2.1</span> Confusion Matrix<a href="classification-model-performance.html#confusion-matrix" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Confusion Matrix</strong> is a counting table to describe the performance of a classification model. For the true response <code>yTest</code> and prediction <code>yhat</code>, the confusion matrix is:</p>
<div class="sourceCode" id="cb184"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb184-1"><a href="classification-model-performance.html#cb184-1" aria-hidden="true" tabindex="-1"></a>yhat <span class="ot">=</span> <span class="fu">as.factor</span>(yhat) <span class="sc">%&gt;%</span> <span class="fu">relevel</span>(<span class="st">&quot;1&quot;</span>)</span>
<span id="cb184-2"><a href="classification-model-performance.html#cb184-2" aria-hidden="true" tabindex="-1"></a>yTest <span class="ot">=</span> <span class="fu">as.factor</span>(yTest) <span class="sc">%&gt;%</span> <span class="fu">relevel</span>(<span class="st">&quot;1&quot;</span>)</span>
<span id="cb184-3"><a href="classification-model-performance.html#cb184-3" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(yhat, yTest)</span></code></pre></div>
<pre><code>##     yTest
## yhat  1  0
##    1 56  1
##    0 15 88</code></pre>
<p>The top-left and bottom-right are the numbers of correctly classified samples. The top-right and bottom-left are the numbers of wrongly classified samples. A general confusion matrix for a binary classifier is following:</p>
<table>
<thead>
<tr class="header">
<th align="center"></th>
<th align="center">Predicted Yes</th>
<th align="center">Predicted No</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">Actual Yes</td>
<td align="center">TP</td>
<td align="center">FN</td>
</tr>
<tr class="even">
<td align="center">Actual No</td>
<td align="center">FP</td>
<td align="center">TN</td>
</tr>
</tbody>
</table>
<p>where TP is true positive, FP is false positive, TN is true negative, FN is false negative. The cells along the diagonal line from top-left to bottom-right contain the counts of correctly classified samples. The cells along the other diagonal line contain the counts of wrongly classified samples. The most straightforward performance measure is the <strong>total accuracy</strong> which is the percentage of correctly classified samples:</p>
<p><span class="math display">\[Total\ accuracy = \frac{TP+TN}{TP+TN+FP+FN}\]</span></p>
<p>You can calculate the total accuracy when there are more than two categories. This statistic is straightforward but has some disadvantages. First, it doesn’t differentiate different error types. In a real application, different types of error may have different impacts. For example, it is much worse to tag an important email as spam and miss it than failing to filter out a spam email. Provost et al. <span class="citation">(<a href="#ref-Provost1998" role="doc-biblioref">Provost F 1998</a>)</span> discussed in detail about the problem of using total accuracy on different classifiers. There are some other metrics based on the confusion matrix that measure different types of error.</p>
<p><strong>Precision</strong> is a metric to measure how accurate positive predictions are (i.e. among those emails predicted as spam, how many percentages of them are spam emails?):</p>
<p><span class="math display">\[precision = \frac{TP}{TP+FP}\]</span></p>
<p><strong>Sensitivity</strong> is to measure the coverage of actual positive samples (i.e. among those spam emails, how many percentages of them are predicted as spam) :</p>
<p><span class="math display">\[Sensitivity = \frac{TP}{TP+FN}\]</span></p>
<p><strong>Specificity</strong> is to measure the coverage of actual negative samples (i.e. among those non-spam emails, how many percentages of them pass the filter):</p>
<p><span class="math display">\[Specificity = \frac{TN}{TN+FP}\]</span></p>
<p>Since wrongly tagging an important email as spam has a bigger impact, in the spam email case, we want to make sure the model specificity is high enough.</p>
<p>Second, total accuracy doesn’t reflect the natural frequencies of each class. For example, the percentage of fraud cases for insurance may be very low, like 0.1%. A model can achieve nearly perfect accuracy (99.9%) by predicting all samples to be negative. The percentage of the largest class in the training set is also called the no-information rate. In this example, the no-information rate is 99.9%. You need to get a model that at least beats this rate.</p>
</div>
<div id="kappa-statistic" class="section level3 hasAnchor" number="8.2.2">
<h3><span class="header-section-number">8.2.2</span> Kappa Statistic<a href="classification-model-performance.html#kappa-statistic" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Another metric is the Kappa statistic. It measures the agreement between the observed and predicted classes. It was originally come up by Cohen etc. <span class="citation">(<a href="#ref-Cohen1960" role="doc-biblioref">J 1960</a>)</span>. Kappa takes into account the accuracy generated simply by chance. It is defined as:</p>
<p><span class="math display">\[Kappa=\frac{P_{0}-P_{e}}{1-P_{e}}\]</span></p>
<p>Let <span class="math inline">\(n=TP+TN+FP+FN\)</span> be the total number of samples, where <span class="math inline">\(P_{0}=\frac{TP+TN}{n}\)</span> is the observed accuracy, <span class="math inline">\(P_{e}=\frac{(TP+FP)(TP+FN)+(FN+TN)(FP+TN)}{n^{2}}\)</span> is the expected accuracy based on the marginal totals of the confusion matrix. Kappa can take on a value from -1 to 1. The higher the value, the higher the agreement. A value of 0 means there is no agreement between the observed and predicted classes, while a value of 1 indicates perfect agreement. A negative value indicates that the prediction is in the opposite direction of the observed value. The following table may help you “visualize” the interpretation of kappa <span class="citation">(<a href="#ref-landis1977" role="doc-biblioref">Landis JR 1977</a>)</span>:</p>
<table>
<thead>
<tr class="header">
<th align="center">Kappa</th>
<th align="center">Agreement</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">&lt; 0</td>
<td align="center">Less than chance agreement</td>
</tr>
<tr class="even">
<td align="center">0.01–0.20</td>
<td align="center">Slight agreement</td>
</tr>
<tr class="odd">
<td align="center">0.21– 0.40</td>
<td align="center">Fair agreement</td>
</tr>
<tr class="even">
<td align="center">0.41–0.60</td>
<td align="center">Moderate agreement</td>
</tr>
<tr class="odd">
<td align="center">0.61–0.80</td>
<td align="center">Substantial agreement</td>
</tr>
<tr class="even">
<td align="center">0.81–0.99</td>
<td align="center">Almost perfect agreement</td>
</tr>
</tbody>
</table>
<p>In general, a value between 0.3 to 0.5 indicates a reasonable agreement. If a model has a high accuracy of 90%, while the expected accuracy is also high, say 85%. The Kappa statistics is <span class="math inline">\(\frac{1}{3}\)</span>. It means the prediction and the observation have a fair agreement. You can calculate Kappa when the number of categories is larger than 2. The package <code>fmsb</code> has a function <code>Kappa.test()</code> to calculate Cohen’s Kappa statistics. The function can also return the hypothesis test result and a confidence interval. Use the above observation vector <code>yTest</code> and prediction vector <code>yhat</code> as an example, you can calculate the statistics:</p>
<div class="sourceCode" id="cb186"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb186-1"><a href="classification-model-performance.html#cb186-1" aria-hidden="true" tabindex="-1"></a><span class="co"># install.packages(&quot;fmsb&quot;)</span></span>
<span id="cb186-2"><a href="classification-model-performance.html#cb186-2" aria-hidden="true" tabindex="-1"></a>kt<span class="ot">&lt;-</span>fmsb<span class="sc">::</span><span class="fu">Kappa.test</span>(<span class="fu">table</span>(yhat,yTest))</span>
<span id="cb186-3"><a href="classification-model-performance.html#cb186-3" aria-hidden="true" tabindex="-1"></a>kt<span class="sc">$</span>Result</span></code></pre></div>
<pre><code>## 
##  Estimate Cohen&#39;s kappa statistics and test the
##  null hypothesis that the extent of agreement is
##  same as random (kappa=0)
## 
## data:  table(yhat, yTest)
## Z = 9.7, p-value &lt;2e-16
## 95 percent confidence interval:
##  0.6972 0.8894
## sample estimates:
## [1] 0.7933</code></pre>
<p>The output of the above function contains an object named <code>Judgement</code>:</p>
<div class="sourceCode" id="cb188"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb188-1"><a href="classification-model-performance.html#cb188-1" aria-hidden="true" tabindex="-1"></a>kt<span class="sc">$</span>Judgement</span></code></pre></div>
<pre><code>## [1] &quot;Substantial agreement&quot;</code></pre>
</div>
<div id="roc" class="section level3 hasAnchor" number="8.2.3">
<h3><span class="header-section-number">8.2.3</span> ROC<a href="classification-model-performance.html#roc" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Receiver Operating Characteristic (ROC) curve uses the predicted class probabilities and determines an effective threshold such that values above the threshold are indicative of a specific event. We have shown the definitions of sensitivity and specificity above. The sensitivity is the true positive rate and specificity is true negative rate. “1 - specificity” is the false positive rate. ROC is a graph of pairs of true positive rate (sensitivity) and false positive rate (1-specificity) values that result as the test’s cutoff value is varied. The Area Under the Curve (AUC) is a common measure for two-class problem. There is usually a trade-off between sensitivity and specificity. If the threshold is set lower, then there are more samples predicted as positive and hence the sensitivity is higher. Let’s look at the predicted probability <code>yhatprob</code> in the swine disease example. The predicted probability object <code>yhatprob</code> has two columns, one is the predicted probability that a farm will have an outbreak, the other is the probability that farm will NOT have an outbreak. So the two add up to have value 1. We use the probability of outbreak (the 2nd column) for further illustration. You can use <code>roc()</code> function to get an ROC object (<code>rocCurve</code>) and then apply different functions on that object to get needed plot or ROC statistics. For example, the following code produces the ROC curve:</p>
<div class="sourceCode" id="cb190"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb190-1"><a href="classification-model-performance.html#cb190-1" aria-hidden="true" tabindex="-1"></a>rocCurve <span class="ot">&lt;-</span> pROC<span class="sc">::</span><span class="fu">roc</span>(<span class="at">response =</span> yTest,</span>
<span id="cb190-2"><a href="classification-model-performance.html#cb190-2" aria-hidden="true" tabindex="-1"></a>              <span class="at">predictor =</span> yhatprob[,<span class="dv">2</span>])</span></code></pre></div>
<pre><code>## Setting levels: control = 1, case = 0</code></pre>
<pre><code>## Setting direction: controls &gt; cases</code></pre>
<div class="sourceCode" id="cb193"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb193-1"><a href="classification-model-performance.html#cb193-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="dv">1</span><span class="sc">-</span>rocCurve<span class="sc">$</span>specificities, </span>
<span id="cb193-2"><a href="classification-model-performance.html#cb193-2" aria-hidden="true" tabindex="-1"></a>     rocCurve<span class="sc">$</span>sensitivities, </span>
<span id="cb193-3"><a href="classification-model-performance.html#cb193-3" aria-hidden="true" tabindex="-1"></a>     <span class="at">type =</span> <span class="st">&#39;l&#39;</span>,</span>
<span id="cb193-4"><a href="classification-model-performance.html#cb193-4" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&#39;1 - Specificities&#39;</span>,</span>
<span id="cb193-5"><a href="classification-model-performance.html#cb193-5" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&#39;Sensitivities&#39;</span>)</span></code></pre></div>
<p><img src="IDS_files/figure-html/unnamed-chunk-73-1.svg" width="672" /></p>
<p>The first argument of the <code>roc()</code> is, <code>response</code>, the observation vector. The second argument is <code>predictor</code> is the continuous prediction (probability or link function value). The x-axis of ROC curve is “1 - specificity” and the y-axis is “sensitivity.” ROC curve starts from (0, 0) and ends with (1, 1). A perfect model that correctly identifies all the samples will have 100% sensitivity and specificity which corresponds to the curve that also goes through (0, 1). The area under the perfect curve is 1. A model that is totally useless corresponds to a curve that is close to the diagonal line and an area under the curve about 0.5.</p>
<p>You can visually compare different models by putting their ROC curves on one plot. Or use the AUC to compare them. DeLong et al. came up a statistic test to compare AUC based on U-statistics <span class="citation">(<a href="#ref-delong1988" role="doc-biblioref">E. R. DeLong 1988</a>)</span> which can give a p-value and confidence interval. You can also use bootstrap to get a confidence interval for AUC <span class="citation">(<a href="#ref-hall2004" role="doc-biblioref">Hall P 2004</a>)</span>.</p>
<p>We can use the following code in R to get an estimate of AUC and its confidence interval:</p>
<div class="sourceCode" id="cb194"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb194-1"><a href="classification-model-performance.html#cb194-1" aria-hidden="true" tabindex="-1"></a><span class="co"># get the estimate of AUC</span></span>
<span id="cb194-2"><a href="classification-model-performance.html#cb194-2" aria-hidden="true" tabindex="-1"></a><span class="fu">auc</span>(rocCurve)</span></code></pre></div>
<pre><code>## Area under the curve: 0.989</code></pre>
<div class="sourceCode" id="cb196"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb196-1"><a href="classification-model-performance.html#cb196-1" aria-hidden="true" tabindex="-1"></a><span class="co"># get a confidence interval based on DeLong et al.</span></span>
<span id="cb196-2"><a href="classification-model-performance.html#cb196-2" aria-hidden="true" tabindex="-1"></a><span class="fu">ci.auc</span>(rocCurve)</span></code></pre></div>
<pre><code>## 95% CI: 0.979-0.999 (DeLong)</code></pre>
<p>AUC is robust to class imbalance <span class="citation">(<a href="#ref-Provost1998" role="doc-biblioref">Provost F 1998</a>; <a href="#ref-Fawcett2006" role="doc-biblioref">F. T 2006</a>)</span> hence a popular measurement. But it still boils a lot of information down to one number so there is inevitably a loss of information. It is better to double check by comparing the curve at the same time. If you care more about getting a model that will have high specificity, which is the lower part of the curve, as in the spam filtering case, you can use the area of the lower part of the curve as the performance measurement <span class="citation">(<a href="#ref-McClish1989" role="doc-biblioref">D 1989</a>)</span>. ROC is only for two-class case. Some researchers generalized it to situations with more than two categories <span class="citation">(<a href="#ref-Hand2001" role="doc-biblioref">Hand D 2001</a>; <a href="#ref-Lachiche2003" role="doc-biblioref">Lachiche N 2003</a>; <a href="#ref-Li2008" role="doc-biblioref">Li J 2008</a>)</span>.</p>
</div>
<div id="gain-and-lift-charts" class="section level3 hasAnchor" number="8.2.4">
<h3><span class="header-section-number">8.2.4</span> Gain and Lift Charts<a href="classification-model-performance.html#gain-and-lift-charts" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Gain and lift chart is a visual tool for evaluating the performance of a classification model. In the previous swine disease example, there are 160 samples in the testing data and 89 of them have a positive outcome.</p>
<div class="sourceCode" id="cb198"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb198-1"><a href="classification-model-performance.html#cb198-1" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(yTest)</span></code></pre></div>
<pre><code>## yTest
##  1  0 
## 71 89</code></pre>
<p>If we order the testing samples by the predicted probability, one would hope that the positive samples are ranked higher than the negative ones. That is what the lift charts do: rank the samples by their scores and calculate the cumulative positive rate as more samples are evaluated. In the perfect scenario, the highest-ranked 71 samples would contain all 71 positive samples. When the model is totally random, the highest-ranked x% of the data would contain about x% of the positive sample. The gain/lift charts compare the ratio between the results obtained with and without a model.</p>
<p>Let’s plot the lift charts to compare the predicted outbreak probability (<code>modelscore &lt;- yhatprob[ ,2]</code>) from random forest model we fit before with some random scores generated from a uniform distribution (<code>randomscore &lt;- runif(length(yTest))</code>).</p>
<div class="sourceCode" id="cb200"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb200-1"><a href="classification-model-performance.html#cb200-1" aria-hidden="true" tabindex="-1"></a><span class="co"># predicted outbreak probability</span></span>
<span id="cb200-2"><a href="classification-model-performance.html#cb200-2" aria-hidden="true" tabindex="-1"></a>modelscore <span class="ot">&lt;-</span> yhatprob[ ,<span class="dv">2</span>]</span>
<span id="cb200-3"><a href="classification-model-performance.html#cb200-3" aria-hidden="true" tabindex="-1"></a><span class="co"># randomly sample from a uniform distribution</span></span>
<span id="cb200-4"><a href="classification-model-performance.html#cb200-4" aria-hidden="true" tabindex="-1"></a>randomscore <span class="ot">&lt;-</span> <span class="fu">runif</span>(<span class="fu">length</span>(yTest))</span>
<span id="cb200-5"><a href="classification-model-performance.html#cb200-5" aria-hidden="true" tabindex="-1"></a>labs <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="at">modelscore =</span> <span class="st">&quot;Random Forest Prediction&quot;</span>,</span>
<span id="cb200-6"><a href="classification-model-performance.html#cb200-6" aria-hidden="true" tabindex="-1"></a>        <span class="at">randomscore =</span> <span class="st">&quot;Random Number&quot;</span>)</span>
<span id="cb200-7"><a href="classification-model-performance.html#cb200-7" aria-hidden="true" tabindex="-1"></a>liftCurve <span class="ot">&lt;-</span> caret<span class="sc">::</span><span class="fu">lift</span>(yTest <span class="sc">~</span> modelscore <span class="sc">+</span> randomscore,</span>
<span id="cb200-8"><a href="classification-model-performance.html#cb200-8" aria-hidden="true" tabindex="-1"></a>                  <span class="at">class =</span> <span class="st">&quot;1&quot;</span>, </span>
<span id="cb200-9"><a href="classification-model-performance.html#cb200-9" aria-hidden="true" tabindex="-1"></a>                  <span class="at">labels =</span> labs)</span>
<span id="cb200-10"><a href="classification-model-performance.html#cb200-10" aria-hidden="true" tabindex="-1"></a><span class="fu">xyplot</span>(liftCurve, <span class="at">auto.key =</span> <span class="fu">list</span>(<span class="at">columns =</span> <span class="dv">2</span>, <span class="at">lines =</span> T, <span class="at">points =</span> F))</span></code></pre></div>
<p><img src="IDS_files/figure-html/unnamed-chunk-76-1.svg" width="672" /></p>
<p>The x-axis is the percentage of samples tested and the y-axis is the percentage of positive samples that are detected by the model. For example, the point on the curve of random forest prediction, (8.125, 18.31) , indicates that if you order the testing samples by the predicted probability from high to low, the top 8.125% of the samples contain 18.31% of the total positive outcomes.</p>
<p>Similar to the ROC curve, we can choose the model by comparing their lift charts. Some parts of the lift curves may be more interesting than the rest. For example, if you only have a budget to clean 50% of the farms, then you should pick the model that gives the highest point when the x-axis is 50%.</p>

</div>
</div>
<!-- </div> -->
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-McClish1989" class="csl-entry">
D, McClish. 1989. <span>“Analyzing a Portion of the ROC Curve.”</span> <em>Medical Decision Making</em> 9: 190–95.
</div>
<div id="ref-delong1988" class="csl-entry">
E. R. DeLong, D. L. Clarke-Pearson, D. M. DeLong. 1988. <span>“Comparing the Areas Under Two or More Correlated Receiver Operating Characteristics Curves: A Nonparametric Approach.”</span> <em>Biometrics</em> 44: 837–45.
</div>
<div id="ref-hall2004" class="csl-entry">
Hall P, Fan Y, Hyndman R. 2004. <span>“Nonparametric Confidence Intervals for Receiver Operating Characteristic Curves.”</span> <em>Biometrika</em> 91: 743–50.
</div>
<div id="ref-Hand2001" class="csl-entry">
Hand D, Till R. 2001. <span>“A Simple Generalisation of the Area Under the ROC Curve for Multiple Class Classification Problems.”</span> <em>Machine Learning</em> 45 (2): 171–86.
</div>
<div id="ref-Cohen1960" class="csl-entry">
J, Cohen. 1960. <span>“A Coefficient of Agreement for Nominal Data.”</span> <em>Educational and Psychological Measurement</em> 20: 37–46.
</div>
<div id="ref-Lachiche2003" class="csl-entry">
Lachiche N, Flach P. 2003. <span>“Improving Accuracy and Cost of Two–Class and Multi–Class Probabilistic Classifiers Using ROC Curves.”</span> <em>In “Proceed- Ings of the Twentieth International Conference on Machine Learning</em> 20 (416–424).
</div>
<div id="ref-landis1977" class="csl-entry">
Landis JR, Koch GG. 1977. <span>“The Measurement of Observer Agreement for Categorical Data.”</span> <em>Biometrics</em> 33: 159–74.
</div>
<div id="ref-Li2008" class="csl-entry">
Li J, Fine JP. 2008. <span>“ROC Analysis with Multiple Classes and Multiple Tests: Methodology and Its Application in Microarray Studies.”</span> <em>Biostatistics</em> 9 (3): 566–76.
</div>
<div id="ref-Provost1998" class="csl-entry">
Provost F, Kohavi R, Fawcett T. 1998. <span>“The Case Against Accuracy Esti- Mation for Comparing Induction Algorithms.”</span> <em>Proceedings of the Fifteenth International Conference on Machine Learning</em>, 445–53.
</div>
<div id="ref-Fawcett2006" class="csl-entry">
T, Fawcett. 2006. <span>“An Introduction to ROC Analysis.”</span> <em>Pattern Recognition Letters</em> 27 (8): 861–74.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="regression-model-performance.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="regression-models.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": true,
"twitter": true,
"linkedin": true,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/happyrabbit/IntroDataScience/08-MeasurePerformance.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["IDS.pdf", "IDS.epub", "IDS.mobi"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section",
"scroll_highlight": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
