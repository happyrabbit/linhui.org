<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Introduction to Data Science</title>
  <meta name="description" content="This is the handouts for CE course at JSM 2017">
  <meta name="generator" content="bookdown 0.4 and GitBook 2.6.7">

  <meta property="og:title" content="Introduction to Data Science" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is the handouts for CE course at JSM 2017" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Introduction to Data Science" />
  
  <meta name="twitter:description" content="This is the handouts for CE course at JSM 2017" />
  

<meta name="author" content="Hui Lin and Ming Li">


<meta name="date" content="2017-09-28">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="measuring-performance.html">
<link rel="next" href="feature-engineering.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-0.8/htmlwidgets.js"></script>
<script src="libs/datatables-binding-0.2/datatables.js"></script>
<link href="libs/dt-core-1.10.12/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.10.12/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.10.12/js/jquery.dataTables.min.js"></script>
<link href="libs/leaflet-0.7.3/leaflet.css" rel="stylesheet" />
<script src="libs/leaflet-0.7.3/leaflet.js"></script>
<link href="libs/leafletfix-1.0.0/leafletfix.css" rel="stylesheet" />
<script src="libs/leaflet-binding-1.0.1/leaflet.js"></script>
<link href="libs/dygraphs-1.1.1/dygraph.css" rel="stylesheet" />
<script src="libs/dygraphs-1.1.1/dygraph-combined.js"></script>
<script src="libs/moment-2.8.4/moment.js"></script>
<script src="libs/moment-timezone-0.2.5/moment-timezone-with-data.js"></script>
<script src="libs/moment-fquarter-1.0.0/moment-fquarter.min.js"></script>
<script src="libs/dygraphs-binding-1.1.1.4/dygraphs.js"></script>
<script src="libs/proj4js-2.3.15/proj4.js"></script>
<link href="libs/highcharts-5.0.6/css/motion.css" rel="stylesheet" />
<script src="libs/highcharts-5.0.6/highstock.js"></script>
<script src="libs/highcharts-5.0.6/highcharts-3d.js"></script>
<script src="libs/highcharts-5.0.6/highcharts-more.js"></script>
<script src="libs/highcharts-5.0.6/modules/annotations.js"></script>
<script src="libs/highcharts-5.0.6/modules/broken-axis.js"></script>
<script src="libs/highcharts-5.0.6/modules/data.js"></script>
<script src="libs/highcharts-5.0.6/modules/drilldown.js"></script>
<script src="libs/highcharts-5.0.6/modules/exporting.js"></script>
<script src="libs/highcharts-5.0.6/modules/funnel.js"></script>
<script src="libs/highcharts-5.0.6/modules/heatmap.js"></script>
<script src="libs/highcharts-5.0.6/modules/map.js"></script>
<script src="libs/highcharts-5.0.6/modules/no-data-to-display.js"></script>
<script src="libs/highcharts-5.0.6/modules/offline-exporting.js"></script>
<script src="libs/highcharts-5.0.6/modules/solid-gauge.js"></script>
<script src="libs/highcharts-5.0.6/modules/treemap.js"></script>
<script src="libs/highcharts-5.0.6/plugins/annotations.js"></script>
<script src="libs/highcharts-5.0.6/plugins/draggable-legend.js"></script>
<script src="libs/highcharts-5.0.6/plugins/draggable-points.js"></script>
<script src="libs/highcharts-5.0.6/plugins/export-csv.js"></script>
<script src="libs/highcharts-5.0.6/plugins/grouped-categories.js"></script>
<script src="libs/highcharts-5.0.6/plugins/motion.js"></script>
<script src="libs/highcharts-5.0.6/plugins/pattern-fill-v2.js"></script>
<script src="libs/highcharts-5.0.6/plugins/tooltip-delay.js"></script>
<script src="libs/highcharts-5.0.6/custom/reset.js"></script>
<script src="libs/highcharts-5.0.6/custom/symbols-extra.js"></script>
<script src="libs/highcharts-5.0.6/custom/text-symbols.js"></script>
<link href="libs/fontawesome-4.5.0/font-awesome.min.css" rel="stylesheet" />
<link href="libs/htmlwdgtgrid-1/htmlwdgtgrid.css" rel="stylesheet" />
<script src="libs/highchart-binding-0.5.0/highchart.js"></script>
<link href="libs/bokehjs-0.11.1/bokeh.min.css" rel="stylesheet" />
<script src="libs/bokehjs-0.11.1/bokeh.min.js"></script>
<script src="libs/rbokeh-binding-0.4.2/rbokeh.js"></script>
<script src="libs/d3-3.5.12/d3.min.js"></script>
<link href="libs/metrics-graphics-2.7.0/dist/metricsgraphics.css" rel="stylesheet" />
<link href="libs/metrics-graphics-2.7.0/dist/mg_regions.css" rel="stylesheet" />
<script src="libs/metrics-graphics-2.7.0/dist/metricsgraphics.min.js"></script>
<script src="libs/metrics-graphics-2.7.0/dist/mg_regions.js"></script>
<script src="libs/metricsgraphics-binding-0.9.0/metricsgraphics.js"></script>
<script src="libs/forceNetwork-binding-0.2.11/forceNetwork.js"></script>
<script src="libs/threejs-70/three.min.js"></script>
<script src="libs/threejs-70/Detector.js"></script>
<script src="libs/threejs-70/Projector.js"></script>
<script src="libs/threejs-70/CanvasRenderer.js"></script>
<script src="libs/globe-binding-0.2.2/globe.js"></script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Copyright Statement</a></li>
<li class="chapter" data-level="" data-path="about-the-authors.html"><a href="about-the-authors.html"><i class="fa fa-check"></i>About the Authors</a></li>
<li class="chapter" data-level="" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="1" data-path="the-art-of-data-science.html"><a href="the-art-of-data-science.html"><i class="fa fa-check"></i><b>1</b> The art of data science</a><ul>
<li class="chapter" data-level="1.1" data-path="the-art-of-data-science.html"><a href="the-art-of-data-science.html#what-is-data-science"><i class="fa fa-check"></i><b>1.1</b> What is data science?</a></li>
<li class="chapter" data-level="1.2" data-path="the-art-of-data-science.html"><a href="the-art-of-data-science.html#is-it-science-totally"><i class="fa fa-check"></i><b>1.2</b> Is it science? Totally?</a></li>
<li class="chapter" data-level="1.3" data-path="the-art-of-data-science.html"><a href="the-art-of-data-science.html#what-kind-of-questions-can-data-science-solve"><i class="fa fa-check"></i><b>1.3</b> What kind of questions can data science solve?</a><ul>
<li class="chapter" data-level="1.3.1" data-path="the-art-of-data-science.html"><a href="the-art-of-data-science.html#prerequisites"><i class="fa fa-check"></i><b>1.3.1</b> Prerequisites</a></li>
<li class="chapter" data-level="1.3.2" data-path="the-art-of-data-science.html"><a href="the-art-of-data-science.html#problem-type"><i class="fa fa-check"></i><b>1.3.2</b> Problem type</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="the-art-of-data-science.html"><a href="the-art-of-data-science.html#what-are-the-required-skills-for-data-scientist"><i class="fa fa-check"></i><b>1.4</b> What are the required skills for data scientist?</a></li>
<li class="chapter" data-level="1.5" data-path="the-art-of-data-science.html"><a href="the-art-of-data-science.html#types-of-learning"><i class="fa fa-check"></i><b>1.5</b> Types of Learning</a></li>
<li class="chapter" data-level="1.6" data-path="the-art-of-data-science.html"><a href="the-art-of-data-science.html#types-of-algorithm"><i class="fa fa-check"></i><b>1.6</b> Types of Algorithm</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="soft-skills-for-data-scientists.html"><a href="soft-skills-for-data-scientists.html"><i class="fa fa-check"></i><b>2</b> Soft Skills for Data Scientists</a><ul>
<li class="chapter" data-level="2.1" data-path="soft-skills-for-data-scientists.html"><a href="soft-skills-for-data-scientists.html#comparison-between-statistician-and-data-scientist"><i class="fa fa-check"></i><b>2.1</b> Comparison between Statistician and Data Scientist</a></li>
<li class="chapter" data-level="2.2" data-path="soft-skills-for-data-scientists.html"><a href="soft-skills-for-data-scientists.html#where-data-science-team-fits"><i class="fa fa-check"></i><b>2.2</b> Where Data Science Team Fits?</a></li>
<li class="chapter" data-level="2.3" data-path="soft-skills-for-data-scientists.html"><a href="soft-skills-for-data-scientists.html#beyond-data-and-analytics"><i class="fa fa-check"></i><b>2.3</b> Beyond Data and Analytics</a></li>
<li class="chapter" data-level="2.4" data-path="soft-skills-for-data-scientists.html"><a href="soft-skills-for-data-scientists.html#data-scientist-as-a-leader"><i class="fa fa-check"></i><b>2.4</b> Data Scientist as a Leader</a></li>
<li class="chapter" data-level="2.5" data-path="soft-skills-for-data-scientists.html"><a href="soft-skills-for-data-scientists.html#three-pillars-of-knowledge"><i class="fa fa-check"></i><b>2.5</b> Three Pillars of Knowledge</a></li>
<li class="chapter" data-level="2.6" data-path="soft-skills-for-data-scientists.html"><a href="soft-skills-for-data-scientists.html#common-pitfalls-of-data-science-projects"><i class="fa fa-check"></i><b>2.6</b> Common Pitfalls of Data Science Projects</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="introduction-to-the-data.html"><a href="introduction-to-the-data.html"><i class="fa fa-check"></i><b>3</b> Introduction to the data</a><ul>
<li class="chapter" data-level="3.1" data-path="introduction-to-the-data.html"><a href="introduction-to-the-data.html#customer-data-for-clothing-company"><i class="fa fa-check"></i><b>3.1</b> Customer Data for Clothing Company</a></li>
<li class="chapter" data-level="3.2" data-path="introduction-to-the-data.html"><a href="introduction-to-the-data.html#customer-satisfaction-survey-data-from-airline-company"><i class="fa fa-check"></i><b>3.2</b> Customer Satisfaction Survey Data from Airline Company</a></li>
<li class="chapter" data-level="3.3" data-path="introduction-to-the-data.html"><a href="introduction-to-the-data.html#swine-disease-breakout-data"><i class="fa fa-check"></i><b>3.3</b> Swine Disease Breakout Data</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="data-pre-processing.html"><a href="data-pre-processing.html"><i class="fa fa-check"></i><b>4</b> Data Pre-processing</a><ul>
<li class="chapter" data-level="4.1" data-path="data-pre-processing.html"><a href="data-pre-processing.html#data-cleaning"><i class="fa fa-check"></i><b>4.1</b> Data Cleaning</a></li>
<li class="chapter" data-level="4.2" data-path="data-pre-processing.html"><a href="data-pre-processing.html#missing-values"><i class="fa fa-check"></i><b>4.2</b> Missing Values</a><ul>
<li class="chapter" data-level="4.2.1" data-path="data-pre-processing.html"><a href="data-pre-processing.html#impute-missing-values-with-medianmode"><i class="fa fa-check"></i><b>4.2.1</b> Impute missing values with median/mode</a></li>
<li class="chapter" data-level="4.2.2" data-path="data-pre-processing.html"><a href="data-pre-processing.html#k-nearest-neighbors"><i class="fa fa-check"></i><b>4.2.2</b> K-nearest neighbors</a></li>
<li class="chapter" data-level="4.2.3" data-path="data-pre-processing.html"><a href="data-pre-processing.html#bagging-tree"><i class="fa fa-check"></i><b>4.2.3</b> Bagging Tree</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="data-pre-processing.html"><a href="data-pre-processing.html#centering-and-scaling"><i class="fa fa-check"></i><b>4.3</b> Centering and Scaling</a></li>
<li class="chapter" data-level="4.4" data-path="data-pre-processing.html"><a href="data-pre-processing.html#resolve-skewness"><i class="fa fa-check"></i><b>4.4</b> Resolve Skewness</a></li>
<li class="chapter" data-level="4.5" data-path="data-pre-processing.html"><a href="data-pre-processing.html#resolve-outliers"><i class="fa fa-check"></i><b>4.5</b> Resolve Outliers</a></li>
<li class="chapter" data-level="4.6" data-path="data-pre-processing.html"><a href="data-pre-processing.html#collinearity"><i class="fa fa-check"></i><b>4.6</b> Collinearity</a></li>
<li class="chapter" data-level="4.7" data-path="data-pre-processing.html"><a href="data-pre-processing.html#sparse-variables"><i class="fa fa-check"></i><b>4.7</b> Sparse Variables</a></li>
<li class="chapter" data-level="4.8" data-path="data-pre-processing.html"><a href="data-pre-processing.html#re-encode-dummy-variables"><i class="fa fa-check"></i><b>4.8</b> Re-encode Dummy Variables</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="data-wrangling.html"><a href="data-wrangling.html"><i class="fa fa-check"></i><b>5</b> Data Wrangling</a><ul>
<li class="chapter" data-level="5.1" data-path="data-wrangling.html"><a href="data-wrangling.html#read-and-write-data"><i class="fa fa-check"></i><b>5.1</b> Read and write data</a><ul>
<li class="chapter" data-level="5.1.1" data-path="data-wrangling.html"><a href="data-wrangling.html#readr"><i class="fa fa-check"></i><b>5.1.1</b> <code>readr</code></a></li>
<li class="chapter" data-level="5.1.2" data-path="data-wrangling.html"><a href="data-wrangling.html#data.table-enhanced-data.frame"><i class="fa fa-check"></i><b>5.1.2</b> <code>data.table</code>— enhanced <code>data.frame</code></a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="data-wrangling.html"><a href="data-wrangling.html#summarize-data"><i class="fa fa-check"></i><b>5.2</b> Summarize data</a><ul>
<li class="chapter" data-level="5.2.1" data-path="data-wrangling.html"><a href="data-wrangling.html#apply-lapply-and-sapply-in-base-r"><i class="fa fa-check"></i><b>5.2.1</b> <code>apply()</code>, <code>lapply()</code> and <code>sapply()</code> in base R</a></li>
<li class="chapter" data-level="5.2.2" data-path="data-wrangling.html"><a href="data-wrangling.html#ddply-in-plyr-package"><i class="fa fa-check"></i><b>5.2.2</b> <code>ddply()</code> in <code>plyr</code> package</a></li>
<li class="chapter" data-level="5.2.3" data-path="data-wrangling.html"><a href="data-wrangling.html#dplyr-package"><i class="fa fa-check"></i><b>5.2.3</b> <code>dplyr</code> package</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="data-wrangling.html"><a href="data-wrangling.html#tidy-and-reshape-data"><i class="fa fa-check"></i><b>5.3</b> Tidy and Reshape Data</a><ul>
<li class="chapter" data-level="5.3.1" data-path="data-wrangling.html"><a href="data-wrangling.html#reshape2-package"><i class="fa fa-check"></i><b>5.3.1</b> <code>reshape2</code> package</a></li>
<li class="chapter" data-level="5.3.2" data-path="data-wrangling.html"><a href="data-wrangling.html#tidyr-package"><i class="fa fa-check"></i><b>5.3.2</b> <code>tidyr</code> package</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="measuring-performance.html"><a href="measuring-performance.html"><i class="fa fa-check"></i><b>6</b> Measuring Performance</a><ul>
<li class="chapter" data-level="6.1" data-path="measuring-performance.html"><a href="measuring-performance.html#regression-model-performance"><i class="fa fa-check"></i><b>6.1</b> Regression Model Performance</a></li>
<li class="chapter" data-level="6.2" data-path="measuring-performance.html"><a href="measuring-performance.html#classification-model-performance"><i class="fa fa-check"></i><b>6.2</b> Classification Model Performance</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="model-tuning-technique.html"><a href="model-tuning-technique.html"><i class="fa fa-check"></i><b>7</b> Model Tuning Technique</a><ul>
<li class="chapter" data-level="7.1" data-path="model-tuning-technique.html"><a href="model-tuning-technique.html#systematic-error-and-random-error"><i class="fa fa-check"></i><b>7.1</b> Systematic Error and Random Error</a><ul>
<li class="chapter" data-level="7.1.1" data-path="model-tuning-technique.html"><a href="model-tuning-technique.html#measurement-error-in-the-response"><i class="fa fa-check"></i><b>7.1.1</b> Measurement Error in the Response</a></li>
<li class="chapter" data-level="7.1.2" data-path="model-tuning-technique.html"><a href="model-tuning-technique.html#measurement-error-in-the-independent-variables"><i class="fa fa-check"></i><b>7.1.2</b> Measurement Error in the Independent Variables</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="model-tuning-technique.html"><a href="model-tuning-technique.html#data-splitting-and-resampling"><i class="fa fa-check"></i><b>7.2</b> Data Splitting and Resampling</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="feature-engineering.html"><a href="feature-engineering.html"><i class="fa fa-check"></i><b>8</b> Feature Engineering</a><ul>
<li class="chapter" data-level="8.1" data-path="feature-engineering.html"><a href="feature-engineering.html#feature-construction"><i class="fa fa-check"></i><b>8.1</b> Feature Construction</a></li>
<li class="chapter" data-level="8.2" data-path="feature-engineering.html"><a href="feature-engineering.html#feature-extraction"><i class="fa fa-check"></i><b>8.2</b> Feature Extraction</a></li>
<li class="chapter" data-level="8.3" data-path="feature-engineering.html"><a href="feature-engineering.html#feature-selection"><i class="fa fa-check"></i><b>8.3</b> Feature Selection</a><ul>
<li class="chapter" data-level="8.3.1" data-path="feature-engineering.html"><a href="feature-engineering.html#filter-method"><i class="fa fa-check"></i><b>8.3.1</b> Filter Method</a></li>
<li class="chapter" data-level="8.3.2" data-path="feature-engineering.html"><a href="feature-engineering.html#wrapper-method"><i class="fa fa-check"></i><b>8.3.2</b> Wrapper Method</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="regression-models.html"><a href="regression-models.html"><i class="fa fa-check"></i><b>9</b> Regression Models</a><ul>
<li class="chapter" data-level="9.1" data-path="regression-models.html"><a href="regression-models.html#ordinary-least-squares"><i class="fa fa-check"></i><b>9.1</b> Ordinary Least Squares</a></li>
<li class="chapter" data-level="9.2" data-path="regression-models.html"><a href="regression-models.html#multivariate-adaptive-regression-splines"><i class="fa fa-check"></i><b>9.2</b> Multivariate Adaptive Regression Splines</a></li>
<li class="chapter" data-level="9.3" data-path="regression-models.html"><a href="regression-models.html#generalized-linear-model"><i class="fa fa-check"></i><b>9.3</b> Generalized Linear Model</a></li>
<li class="chapter" data-level="9.4" data-path="regression-models.html"><a href="regression-models.html#pcr-and-pls"><i class="fa fa-check"></i><b>9.4</b> PCR and PLS</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="regularization-methods.html"><a href="regularization-methods.html"><i class="fa fa-check"></i><b>10</b> Regularization Methods</a><ul>
<li class="chapter" data-level="10.1" data-path="regularization-methods.html"><a href="regularization-methods.html#ridge-regression"><i class="fa fa-check"></i><b>10.1</b> Ridge Regression</a></li>
<li class="chapter" data-level="10.2" data-path="regularization-methods.html"><a href="regularization-methods.html#lasso"><i class="fa fa-check"></i><b>10.2</b> LASSO</a></li>
<li class="chapter" data-level="10.3" data-path="regularization-methods.html"><a href="regularization-methods.html#elastic-net"><i class="fa fa-check"></i><b>10.3</b> Elastic Net</a></li>
<li class="chapter" data-level="10.4" data-path="regularization-methods.html"><a href="regularization-methods.html#lasso-generalized-linear-model"><i class="fa fa-check"></i><b>10.4</b> LASSO Generalized Linear Model</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="tree-based-methods.html"><a href="tree-based-methods.html"><i class="fa fa-check"></i><b>11</b> Tree-Based Methods</a><ul>
<li class="chapter" data-level="11.1" data-path="tree-based-methods.html"><a href="tree-based-methods.html#splitting-criteria"><i class="fa fa-check"></i><b>11.1</b> Splitting Criteria</a></li>
<li class="chapter" data-level="11.2" data-path="tree-based-methods.html"><a href="tree-based-methods.html#tree-pruning"><i class="fa fa-check"></i><b>11.2</b> Tree Pruning</a></li>
<li class="chapter" data-level="11.3" data-path="tree-based-methods.html"><a href="tree-based-methods.html#regression-and-decision-tree-basic"><i class="fa fa-check"></i><b>11.3</b> Regression and Decision Tree Basic</a></li>
<li class="chapter" data-level="11.4" data-path="tree-based-methods.html"><a href="tree-based-methods.html#bagging-tree-1"><i class="fa fa-check"></i><b>11.4</b> Bagging Tree</a></li>
<li class="chapter" data-level="11.5" data-path="tree-based-methods.html"><a href="tree-based-methods.html#random-forest"><i class="fa fa-check"></i><b>11.5</b> Random Forest</a></li>
<li class="chapter" data-level="11.6" data-path="tree-based-methods.html"><a href="tree-based-methods.html#gradient-boosted-machine"><i class="fa fa-check"></i><b>11.6</b> Gradient Boosted Machine</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="neural-network.html"><a href="neural-network.html"><i class="fa fa-check"></i><b>12</b> Neural Network</a><ul>
<li class="chapter" data-level="12.1" data-path="neural-network.html"><a href="neural-network.html#projection-pursuit-regression"><i class="fa fa-check"></i><b>12.1</b> Projection Pursuit Regression</a></li>
<li class="chapter" data-level="12.2" data-path="neural-network.html"><a href="neural-network.html#neural-networks"><i class="fa fa-check"></i><b>12.2</b> Neural Networks</a></li>
<li class="chapter" data-level="12.3" data-path="neural-network.html"><a href="neural-network.html#fitting-neural-network"><i class="fa fa-check"></i><b>12.3</b> Fitting Neural Network</a></li>
<li class="chapter" data-level="12.4" data-path="neural-network.html"><a href="neural-network.html#model-training"><i class="fa fa-check"></i><b>12.4</b> Model Training</a></li>
<li class="chapter" data-level="12.5" data-path="neural-network.html"><a href="neural-network.html#computation-in-r"><i class="fa fa-check"></i><b>12.5</b> Computation in R</a><ul>
<li class="chapter" data-level="12.5.1" data-path="neural-network.html"><a href="neural-network.html#general-neural-network"><i class="fa fa-check"></i><b>12.5.1</b> General Neural Network</a></li>
<li class="chapter" data-level="12.5.2" data-path="neural-network.html"><a href="neural-network.html#averaged-neural-network"><i class="fa fa-check"></i><b>12.5.2</b> Averaged Neural Network</a></li>
<li class="chapter" data-level="12.5.3" data-path="neural-network.html"><a href="neural-network.html#model-comparison"><i class="fa fa-check"></i><b>12.5.3</b> Model Comparison</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="dynamicreproducible-report.html"><a href="dynamicreproducible-report.html"><i class="fa fa-check"></i><b>13</b> Dynamic/Reproducible Report</a><ul>
<li class="chapter" data-level="13.1" data-path="dynamicreproducible-report.html"><a href="dynamicreproducible-report.html#what-is-r-markdown"><i class="fa fa-check"></i><b>13.1</b> What is R Markdown?</a></li>
<li class="chapter" data-level="13.2" data-path="dynamicreproducible-report.html"><a href="dynamicreproducible-report.html#how-to-start"><i class="fa fa-check"></i><b>13.2</b> How to Start?</a><ul>
<li class="chapter" data-level="13.2.1" data-path="dynamicreproducible-report.html"><a href="dynamicreproducible-report.html#how-it-works"><i class="fa fa-check"></i><b>13.2.1</b> How It Works?</a></li>
<li class="chapter" data-level="13.2.2" data-path="dynamicreproducible-report.html"><a href="dynamicreproducible-report.html#get-started"><i class="fa fa-check"></i><b>13.2.2</b> Get Started</a></li>
<li class="chapter" data-level="13.2.3" data-path="dynamicreproducible-report.html"><a href="dynamicreproducible-report.html#markdown-basic"><i class="fa fa-check"></i><b>13.2.3</b> Markdown Basic</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="dynamicreproducible-report.html"><a href="dynamicreproducible-report.html#html"><i class="fa fa-check"></i><b>13.3</b> HTML</a><ul>
<li class="chapter" data-level="13.3.1" data-path="dynamicreproducible-report.html"><a href="dynamicreproducible-report.html#create-an-html-document"><i class="fa fa-check"></i><b>13.3.1</b> Create an HTML document</a></li>
<li class="chapter" data-level="13.3.2" data-path="dynamicreproducible-report.html"><a href="dynamicreproducible-report.html#floating-toc"><i class="fa fa-check"></i><b>13.3.2</b> Floating TOC</a></li>
<li class="chapter" data-level="13.3.3" data-path="dynamicreproducible-report.html"><a href="dynamicreproducible-report.html#code-chunks"><i class="fa fa-check"></i><b>13.3.3</b> Code Chunks</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="dynamicreproducible-report.html"><a href="dynamicreproducible-report.html#html5-slides"><i class="fa fa-check"></i><b>13.4</b> HTML5 Slides</a><ul>
<li class="chapter" data-level="13.4.1" data-path="dynamicreproducible-report.html"><a href="dynamicreproducible-report.html#ioslides-presentation"><i class="fa fa-check"></i><b>13.4.1</b> <code>ioslides</code> presentation</a></li>
<li class="chapter" data-level="13.4.2" data-path="dynamicreproducible-report.html"><a href="dynamicreproducible-report.html#slidy-presentation"><i class="fa fa-check"></i><b>13.4.2</b> <code>slidy</code> presentation</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="dynamicreproducible-report.html"><a href="dynamicreproducible-report.html#dashboards"><i class="fa fa-check"></i><b>13.5</b> Dashboards</a><ul>
<li class="chapter" data-level="13.5.1" data-path="dynamicreproducible-report.html"><a href="dynamicreproducible-report.html#layouts"><i class="fa fa-check"></i><b>13.5.1</b> Layouts</a></li>
<li class="chapter" data-level="13.5.2" data-path="dynamicreproducible-report.html"><a href="dynamicreproducible-report.html#components"><i class="fa fa-check"></i><b>13.5.2</b> Components</a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="dynamicreproducible-report.html"><a href="dynamicreproducible-report.html#shiny-dashboard"><i class="fa fa-check"></i><b>13.6</b> Shiny Dashboard</a><ul>
<li class="chapter" data-level="13.6.1" data-path="dynamicreproducible-report.html"><a href="dynamicreproducible-report.html#brief-introduction-to-shiny"><i class="fa fa-check"></i><b>13.6.1</b> Brief Introduction to Shiny</a></li>
<li class="chapter" data-level="13.6.2" data-path="dynamicreproducible-report.html"><a href="dynamicreproducible-report.html#using-shiny-with-flexdashboard"><i class="fa fa-check"></i><b>13.6.2</b> Using <code>shiny</code> with <code>flexdashboard</code></a></li>
</ul></li>
<li class="chapter" data-level="13.7" data-path="dynamicreproducible-report.html"><a href="dynamicreproducible-report.html#html-widgets-1"><i class="fa fa-check"></i><b>13.7</b> HTML Widgets</a><ul>
<li class="chapter" data-level="13.7.1" data-path="dynamicreproducible-report.html"><a href="dynamicreproducible-report.html#dt-a-wrapper-of-the-javascript-library-datatables"><i class="fa fa-check"></i><b>13.7.1</b> <code>DT</code>: A Wrapper of the JavaScript Library DataTables</a></li>
<li class="chapter" data-level="13.7.2" data-path="dynamicreproducible-report.html"><a href="dynamicreproducible-report.html#leafletinteractive-web-maps-based-on-the-leaflet-javascript-library"><i class="fa fa-check"></i><b>13.7.2</b> <code>leaflet</code>:Interactive Web-Maps Based on the Leaflet JavaScript Library</a></li>
<li class="chapter" data-level="13.7.3" data-path="dynamicreproducible-report.html"><a href="dynamicreproducible-report.html#dygraphs-interactive-plot-for-time-series-data"><i class="fa fa-check"></i><b>13.7.3</b> <code>dygraphs</code>: interactive plot for time series data</a></li>
<li class="chapter" data-level="13.7.4" data-path="dynamicreproducible-report.html"><a href="dynamicreproducible-report.html#highcharter"><i class="fa fa-check"></i><b>13.7.4</b> <code>highcharter</code></a></li>
<li class="chapter" data-level="13.7.5" data-path="dynamicreproducible-report.html"><a href="dynamicreproducible-report.html#rbokeh-is-a-visualization-library-that-provides-a-flexible-and-powerful-declarative-framework-for-creating-web-based-plots"><i class="fa fa-check"></i><b>13.7.5</b> <code>rbokeh</code> is a visualization library that provides a flexible and powerful declarative framework for creating web-based plots</a></li>
<li class="chapter" data-level="13.7.6" data-path="dynamicreproducible-report.html"><a href="dynamicreproducible-report.html#metricsgraphics-enables-easy-creation-of-d3-scatterplots-line-charts-and-histograms."><i class="fa fa-check"></i><b>13.7.6</b> <code>metricsgraphics</code> enables easy creation of D3 scatterplots, line charts, and histograms.</a></li>
<li class="chapter" data-level="13.7.7" data-path="dynamicreproducible-report.html"><a href="dynamicreproducible-report.html#networkd3-d3-javascript-network-graphs-from-r"><i class="fa fa-check"></i><b>13.7.7</b> <code>networkD3</code>: D3 JavaScript Network Graphs from R</a></li>
<li class="chapter" data-level="13.7.8" data-path="dynamicreproducible-report.html"><a href="dynamicreproducible-report.html#threejs-interactive-3d-scatter-plots-and-globes"><i class="fa fa-check"></i><b>13.7.8</b> <code>threejs</code>: Interactive 3D Scatter Plots and Globes</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i>Appendix</a><ul>
<li class="chapter" data-level="13.8" data-path="appendix.html"><a href="appendix.html#a1.-big-data-cloud-platform"><i class="fa fa-check"></i><b>13.8</b> A1. Big Data Cloud Platform</a><ul>
<li class="chapter" data-level="13.8.1" data-path="appendix.html"><a href="appendix.html#how-data-becomes-science"><i class="fa fa-check"></i><b>13.8.1</b> How Data becomes Science?</a></li>
<li class="chapter" data-level="13.8.2" data-path="appendix.html"><a href="appendix.html#power-of-cluster-of-computers"><i class="fa fa-check"></i><b>13.8.2</b> Power of Cluster of Computers</a></li>
<li class="chapter" data-level="13.8.3" data-path="appendix.html"><a href="appendix.html#introduction-of-cloud-environment"><i class="fa fa-check"></i><b>13.8.3</b> Introduction of Cloud Environment</a></li>
<li class="chapter" data-level="13.8.4" data-path="appendix.html"><a href="appendix.html#summary"><i class="fa fa-check"></i><b>13.8.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="13.9" data-path="appendix.html"><a href="appendix.html#databases-and-sql"><i class="fa fa-check"></i><b>13.9</b> Databases and SQL</a><ul>
<li class="chapter" data-level="13.9.1" data-path="appendix.html"><a href="appendix.html#database-table-and-view"><i class="fa fa-check"></i><b>13.9.1</b> Database, Table and View</a></li>
<li class="chapter" data-level="13.9.2" data-path="appendix.html"><a href="appendix.html#sample-tables"><i class="fa fa-check"></i><b>13.9.2</b> Sample Tables</a></li>
<li class="chapter" data-level="13.9.3" data-path="appendix.html"><a href="appendix.html#basic-sql-statement"><i class="fa fa-check"></i><b>13.9.3</b> Basic SQL Statement</a></li>
<li class="chapter" data-level="13.9.4" data-path="appendix.html"><a href="appendix.html#simple-select-statement"><i class="fa fa-check"></i><b>13.9.4</b> Simple SELECT Statement</a></li>
<li class="chapter" data-level="13.9.5" data-path="appendix.html"><a href="appendix.html#aggregation-functions-and-group-by"><i class="fa fa-check"></i><b>13.9.5</b> Aggregation Functions and GROUP BY</a></li>
<li class="chapter" data-level="13.9.6" data-path="appendix.html"><a href="appendix.html#join-multiple-tables"><i class="fa fa-check"></i><b>13.9.6</b> Join Multiple Tables</a></li>
<li class="chapter" data-level="13.9.7" data-path="appendix.html"><a href="appendix.html#add-more-content-into-a-table"><i class="fa fa-check"></i><b>13.9.7</b> Add More Content into a Table</a></li>
<li class="chapter" data-level="13.9.8" data-path="appendix.html"><a href="appendix.html#advanced-topics-in-database"><i class="fa fa-check"></i><b>13.9.8</b> Advanced Topics in Database</a></li>
</ul></li>
<li class="chapter" data-level="13.10" data-path="appendix.html"><a href="appendix.html#other-useful-topics"><i class="fa fa-check"></i><b>13.10</b> Other Useful Topics</a><ul>
<li class="chapter" data-level="13.10.1" data-path="appendix.html"><a href="appendix.html#linux-operation-system"><i class="fa fa-check"></i><b>13.10.1</b> Linux Operation System</a></li>
<li class="chapter" data-level="13.10.2" data-path="appendix.html"><a href="appendix.html#visualization"><i class="fa fa-check"></i><b>13.10.2</b> Visualization</a></li>
<li class="chapter" data-level="13.10.3" data-path="appendix.html"><a href="appendix.html#gpu"><i class="fa fa-check"></i><b>13.10.3</b> GPU</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>14</b> References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Data Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="model-tuning-technique" class="section level1">
<h1><span class="header-section-number">Chapter 7</span> Model Tuning Technique</h1>
<div id="systematic-error-and-random-error" class="section level2">
<h2><span class="header-section-number">7.1</span> Systematic Error and Random Error</h2>
<p>Assume <span class="math inline">\(\mathbf{X}\)</span> is <span class="math inline">\(n \times p\)</span> observation matrix and <span class="math inline">\(\mathbf{y}\)</span> is response variable, we have:</p>
<p><span class="math display">\[\mathbf{y}=f(\mathbf{X})+\mathbf{\epsilon}\]</span></p>
<p>where <span class="math inline">\(\mathbf{\epsilon}\)</span> is the random error with a mean of zero. The function <span class="math inline">\(f(\cdot)\)</span> is our modeling target, which represents the information of Y that can be explained by X. The main goals of estimating <span class="math inline">\(f(\cdot)\)</span> are inference and prediction, or sometimes both. In general, there is a trade-off between flexibility and interpretability of the model. So data scientists need to comprehend the delicate balance between the two.</p>
<p>Depending on the modeling purposes, the requirement for interpretability varies. If the prediction is the only goal, then as long as the prediction is accurate enough, the interpretability is not under consideration. In this case, people often use “black box” model, such as random forest, boosting tree, neural network, support vector machine and so on. These models are very flexible but nearly impossible to explain. Their predictive accuracy is usually higher on the training set, but not necessary when it predicts. It is not surprising since those models have a huge number of parameters and high flexibility that they can memorize the entire training data. A paper by Chiyuan Zhang et. al. in 2017 pointed out that “Deep neural networks (even just two-layer net) easily fit random labels” <span class="citation">(Chiyuan Zhang <a href="#ref-rethinkDL">2017</a>)</span>. The traditional forms of regularization, such as weight decay, dropout, and data augmentation, fail to control generalization error. It poses a conceptual challenge to statistical theory and also calls our attention when we use such black-box models.</p>
<p>Assume we have <span class="math inline">\(\hat{f}\)</span> which is an estimator of <span class="math inline">\(f\)</span>. Then we can further get <span class="math inline">\(\mathbf{\hat{y}}=\hat(f)(\mathbf(X))\)</span>. The predicted error is divided into two parts, systematic error and random error:</p>
<p><span class="math display">\[
E(\mathbf{y}-\hat{\mathbf{y}})^{2}=E[f(\mathbf{X})+\mathbf{\epsilon}-\hat{f}(\mathbf{X})]^{2}=\underset{\text{(1)}}{\underbrace{E[f(\mathbf{X})-\hat{f}(\mathbf{X})]^{2}}}+\underset{\text{(2)}}{\underbrace{Var(\mathbf{\epsilon})}}
  \label{eq:error}\]</span></p>
<p>In the above equation, (1) is the systematic error, It exists because <span class="math inline">\(\hat{f}\)</span> usually does not completely model the “systematic relation” between X and y, where the system relation refers to the stable relationship that exists on different samples. This part of the error can be improved by improving the model; (2) is the random error which represents the part of the response that can not be explained by current input data. A more complex model does not reduce the error. The biggest problem with black-box models is to fit random error as well, i.e., over-fitting. The notable feature of random error is that it can not be repeated on different samples. So a way to determine whether there is overfitting is to reserve a part of the sample as a test set and then check the performance of the trained model on the test data. Note that overfitting is a general problem, and any model may be overly fitted. Because black-box model usually has a large number of parameters, it is more suspectable to over-fitting.</p>
<p><img src="http://scientistcafe.com/book/Figure/ModelError.png" width="800px" height="400px" /></p>
<p>The systematic error can be further decomposed as:</p>
<p><span class="math display">\[
\begin{array}{ccc}
E[f(\mathbf{X})-\hat{f}(\mathbf{X})]^{2} &amp; = &amp; E\left(f(\mathbf{X})-E[\hat{f}(\mathbf{X})]+E[\hat{f}(\mathbf{X})]-\hat{f}(\mathbf{X})\right)^{2}\\
 &amp; = &amp; E\left(E[\hat{f}(\mathbf{X})]-f(\mathbf{X})\right)^{2}+E\left(\hat{f}(\mathbf{X})-E[\hat{f}(\mathbf{X})]\right)^{2}\\
 &amp; = &amp; [Bias(\hat{f}(\mathbf{X}))]^{2}+Var(\hat{f}(\mathbf{X}))
\end{array}
\]</span></p>
<p>The systematic error consists of two parts, <span class="math inline">\(Bias(\hat{f}(\mathbf{X}))\)</span> and <span class="math inline">\(Var (\hat{f}(\mathbf{X}))\)</span>. To minimize the systematic error, we need to minimize the bias and variance. The bias represents the error caused by the approximation of the reality of the model, which may be very complex. For example, linear regression assumes that there is a linear relationship between the independent variable and the response, but the perfect linear relationship in real life is not common. The relationship between x and fx in the following figure is non-linear. Therefore, despite a large sample size, linear regression can not give the accurate prediction. In other words, in this case, the prediction of the linear regression model has a high bias.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(grid)
<span class="kw">library</span>(lattice)
<span class="kw">library</span>(ggplot2)
<span class="kw">source</span>(<span class="st">&quot;https://raw.githubusercontent.com/happyrabbit/DataScientistR/master/R_Code/multiplot.r&quot;</span>)
<span class="co"># randomly simulate some non-linear samples</span>
x=<span class="kw">seq</span>(<span class="dv">1</span>,<span class="dv">10</span>,<span class="fl">0.01</span>)*pi
e=<span class="kw">rnorm</span>(<span class="kw">length</span>(x),<span class="dt">mean=</span><span class="dv">0</span>,<span class="dt">sd=</span><span class="fl">0.2</span>)
fx&lt;-<span class="kw">sin</span>(x)+e+<span class="kw">sqrt</span>(x)
dat=<span class="kw">data.frame</span>(x,fx)
<span class="co"># plot fitting result</span>
<span class="kw">ggplot</span>(dat,<span class="kw">aes</span>(x,fx))+<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>() +
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="dt">se =</span> <span class="ot">FALSE</span>)</code></pre></div>
<p><img src="CE_JSM2017_files/figure-html/unnamed-chunk-100-1.png" width="672" /></p>
<p>The estimated parameters will be different for the different training data. Intuitively, the estimated variance indicates that if we fit the same model with different data sets, how much the estimated results will change. Ideally, the change is small. For high variance models, small changes in the training data result in very different estimates. In general, a model with high flexibility also has high variance., such as the CART tree, and the initial boosting method. The Random Forest and Gradient Boosting Model aim to reduce the variance by summarizing the results obtained on different samples.</p>
<p>The blue curve in the figure below is obtained by fitting the above nonlinear observations by a smoothing method, which is highly flexible and can fit the current data tightly:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(dat,<span class="kw">aes</span>(x,fx))+<span class="kw">geom_smooth</span>(<span class="dt">span =</span> <span class="fl">0.03</span>)</code></pre></div>
<pre><code>## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39;</code></pre>
<p><img src="CE_JSM2017_files/figure-html/unnamed-chunk-101-1.png" width="672" /></p>
<p>However, this method has a high variance, and if we simulate different subsets of the sample, the result curve will change significantly:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># set random seed</span>
<span class="kw">set.seed</span>(<span class="dv">2016</span>)
<span class="co"># sample part of the data to fit model</span>
<span class="co"># sample 1</span>
idx1=<span class="kw">sample</span>(<span class="dv">1</span>:<span class="kw">length</span>(x),<span class="dv">100</span>)
dat1=<span class="kw">data.frame</span>(<span class="dt">x1=</span>x[idx1],<span class="dt">fx1=</span>fx[idx1])
p1=<span class="kw">ggplot</span>(dat1,<span class="kw">aes</span>(x1,fx1))+<span class="kw">geom_smooth</span>(<span class="dt">span =</span> <span class="fl">0.03</span>)
<span class="co"># sample 2</span>
idx2=<span class="kw">sample</span>(<span class="dv">1</span>:<span class="kw">length</span>(x),<span class="dv">100</span>)
dat2=<span class="kw">data.frame</span>(<span class="dt">x2=</span>x[idx2],<span class="dt">fx2=</span>fx[idx2])
p2=<span class="kw">ggplot</span>(dat2,<span class="kw">aes</span>(x2,fx2))+<span class="kw">geom_smooth</span>(<span class="dt">span =</span> <span class="fl">0.03</span>)
<span class="co"># sample 3</span>
idx3=<span class="kw">sample</span>(<span class="dv">1</span>:<span class="kw">length</span>(x),<span class="dv">100</span>)
dat3=<span class="kw">data.frame</span>(<span class="dt">x3=</span>x[idx3],<span class="dt">fx3=</span>fx[idx3])
p3=<span class="kw">ggplot</span>(dat3,<span class="kw">aes</span>(x3,fx3))+<span class="kw">geom_smooth</span>(<span class="dt">span =</span> <span class="fl">0.03</span>)
<span class="co"># sample 4</span>
idx4=<span class="kw">sample</span>(<span class="dv">1</span>:<span class="kw">length</span>(x),<span class="dv">100</span>)
dat4=<span class="kw">data.frame</span>(<span class="dt">x4=</span>x[idx4],<span class="dt">fx4=</span>fx[idx4])
p4=<span class="kw">ggplot</span>(dat4,<span class="kw">aes</span>(x4,fx4))+<span class="kw">geom_smooth</span>(<span class="dt">span =</span> <span class="fl">0.03</span>)
<span class="kw">multiplot</span>(p1,p2,p3,p4,<span class="dt">cols=</span><span class="dv">2</span>)</code></pre></div>
<pre><code>## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39;
## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39;
## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39;
## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39;</code></pre>
<p><img src="CE_JSM2017_files/figure-html/unnamed-chunk-102-1.png" width="672" /></p>
<p>Fitting the linear model using the same four subsets, the result barely changes:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">p1=<span class="kw">ggplot</span>(dat1,<span class="kw">aes</span>(x1,fx1))+<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>() +
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="dt">se =</span> <span class="ot">FALSE</span>)
p2=<span class="kw">ggplot</span>(dat2,<span class="kw">aes</span>(x2,fx2))+<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>() +
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="dt">se =</span> <span class="ot">FALSE</span>)
p3=<span class="kw">ggplot</span>(dat3,<span class="kw">aes</span>(x3,fx3))+<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>() +
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="dt">se =</span> <span class="ot">FALSE</span>)
p4=<span class="kw">ggplot</span>(dat4,<span class="kw">aes</span>(x4,fx4))+<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>() +
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="dt">se =</span> <span class="ot">FALSE</span>)
<span class="kw">multiplot</span>(p1,p2,p3,p4,<span class="dt">cols=</span><span class="dv">2</span>)</code></pre></div>
<p><img src="CE_JSM2017_files/figure-html/unnamed-chunk-103-1.png" width="672" /></p>
<p>In general, the variance (<span class="math inline">\(Var(\hat{f}(\mathbf{X}))\)</span>) <strong>increases</strong> and the bias (<span class="math inline">\(Bias(\hat{f}(\mathbf{X}))\)</span>) <strong>decreases</strong> as the model flexibility increases. Variance and bias together determine the systematic error (or mean square error, MSE). As we increase the flexibility of the model, at first the rate at which <span class="math inline">\(Bias(\hat{f}(\mathbf{X}))\)</span> decreases is faster than <span class="math inline">\(Var (\hat{f} (\mathbf{X}))\)</span>, so the MSE decreases. However, to some degree, higher flexibility has little effect on <span class="math inline">\(Bias(\hat{f}(\mathbf{X}))\)</span> but <span class="math inline">\(Var (\hat{f} (\mathbf{X}))\)</span> increases significantly, so the MSE increases.</p>
<div id="measurement-error-in-the-response" class="section level3">
<h3><span class="header-section-number">7.1.1</span> Measurement Error in the Response</h3>
<p>The random error (<span class="math inline">\(\mathbf{\epsilon}\)</span>) reflects the measurement error in the response. This part of the error is irreducible, and so it makes the root mean square error (RMSE) and <span class="math inline">\(R^2\)</span> have the corresponding upper and lower limits. RMSE and <span class="math inline">\(R^2\)</span> are commonly used performance measures for the regression model which we will talk in more detail later. Therefore, the random error term not only represents the part of fluctuations the model can not explain but also contains measurement error in the response variables. Section 20.2 of Applied Predictive Modeling <span class="citation">(Max Kuhn <a href="#ref-APM">2013</a>)</span> has an example that shows the effect of the measurement error in the response variable on the model performance (RMSE and <span class="math inline">\(R^2\)</span>).</p>
<p>The authors increased the error in the response proportional to a base level error which was gotten using the original data without introducing extra noise. Then fit a set of models repeatedly using the “contaminated” data sets to study the change of <span class="math inline">\(RMSE\)</span> and $R^2 $ as the level of noise. Here we use clothing consumer data for a similar illustration. Suppose many people do not want to disclose their income and so we need to use other variables to establish a model to predict income. We set up the following model:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># load data</span>
sim.dat &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;https://raw.githubusercontent.com/happyrabbit/DataScientistR/master/Data/SegData.csv &quot;</span>)
ymad&lt;-<span class="kw">mad</span>(<span class="kw">na.omit</span>(sim.dat$income))
<span class="co"># calculate z-score</span>
zs&lt;-(sim.dat$income-<span class="kw">mean</span>(<span class="kw">na.omit</span>(sim.dat$income)))/ymad
<span class="co"># which(na.omit(zs&gt;3.5)): identify outliers</span>
<span class="co"># which(is.na(zs)): identify missing values</span>
idex&lt;-<span class="kw">c</span>(<span class="kw">which</span>(<span class="kw">na.omit</span>(zs&gt;<span class="fl">3.5</span>)),<span class="kw">which</span>(<span class="kw">is.na</span>(zs)))
<span class="co"># delete rows with outliers and missing values</span>
sim.dat&lt;-sim.dat[-idex,]
fit&lt;-<span class="kw">lm</span>(income~store_exp+online_exp+store_trans+online_trans,<span class="dt">data=</span>sim.dat)</code></pre></div>
<p>The output shows that without additional noise, the root mean square error (RMSE) of the model is 29567, <span class="math inline">\(R^2\)</span> is 0.6.</p>
<p>Let’s add various degrees of noise (0 to 3 times the RMSE) to the variable <code>income</code>:</p>
<p><span class="math display">\[ RMSE \times (0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0) \]</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">noise&lt;-<span class="kw">matrix</span>(<span class="kw">rep</span>(<span class="ot">NA</span>,<span class="dv">7</span>*<span class="kw">nrow</span>(sim.dat)),<span class="dt">nrow=</span><span class="kw">nrow</span>(sim.dat),<span class="dt">ncol=</span><span class="dv">7</span>)
for (i in <span class="dv">1</span>:<span class="kw">nrow</span>(sim.dat)){
noise[i,]&lt;-<span class="kw">rnorm</span>(<span class="dv">7</span>,<span class="kw">rep</span>(<span class="dv">0</span>,<span class="dv">7</span>),<span class="kw">summary</span>(fit)$sigma*<span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">3</span>,<span class="dt">by=</span><span class="fl">0.5</span>))
}</code></pre></div>
<p>We then examine the effect of noise intensity on <span class="math inline">\(R^2\)</span> for models with different complexity. The models with complexity from low to high are ordinary linear regression, partial least square regression(PLS), multivariate adaptive regression spline (MARS), support vector machine (SVM, the kernel function is radial basis function), and random forest.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># fit ordinary linear regression</span>
rsq_linear&lt;-<span class="kw">rep</span>(<span class="dv">0</span>,<span class="kw">ncol</span>(noise))
for (i in <span class="dv">1</span>:<span class="dv">7</span>){
withnoise&lt;-sim.dat$income+noise[,i]
fit0&lt;-<span class="kw">lm</span>(withnoise~store_exp+online_exp+store_trans+online_trans,<span class="dt">data=</span>sim.dat)
rsq_linear[i]&lt;-<span class="kw">summary</span>(fit0)$adj.r.squared
}</code></pre></div>
<p>PLS is a method of linearizing nonlinear relationships through hidden layers. It is similar to the principal component regression (PCR), except that PCR does not take into account the information of the dependent variable when selecting the components, and its purpose is to find the linear combinations (i.e., unsupervised) that capture the most variance of the independent variables. When the independent variables and response variables are related, PCR can well identify the systematic relationship between them. However, when there exist independent variables not related to response variable, it will undermine PCR’s performance. And PLS maximizes the linear combination of dependencies with the response variable. In the current case, the more complex PLS does not perform better than simple linear regression.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># pls: conduct PLS and PCR</span>
<span class="kw">library</span>(pls)
rsq_pls&lt;-<span class="kw">rep</span>(<span class="dv">0</span>,<span class="kw">ncol</span>(noise))
<span class="co"># fit PLS</span>
for (i in <span class="dv">1</span>:<span class="dv">7</span>){
withnoise&lt;-sim.dat$income+noise[,i]
fit0&lt;-<span class="kw">plsr</span>(withnoise~store_exp+online_exp+store_trans+online_trans,<span class="dt">data=</span>sim.dat)
rsq_pls[i]&lt;-<span class="kw">max</span>(<span class="kw">drop</span>(<span class="kw">R2</span>(fit0, <span class="dt">estimate =</span> <span class="st">&quot;train&quot;</span>,<span class="dt">intercept =</span> <span class="ot">FALSE</span>)$val))
}</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># earth: fit mars </span>
<span class="kw">library</span>(earth)
rsq_mars&lt;-<span class="kw">rep</span>(<span class="dv">0</span>,<span class="kw">ncol</span>(noise))
for (i in <span class="dv">1</span>:<span class="dv">7</span>){
withnoise&lt;-sim.dat$income+noise[,i]
fit0&lt;-<span class="kw">earth</span>(withnoise~store_exp+online_exp+store_trans+online_trans,<span class="dt">data=</span>sim.dat)
rsq_mars[i]&lt;-fit0$rsq
}</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># caret: awesome package for tuning predictive model</span>
<span class="kw">library</span>(caret)
rsq_svm&lt;-<span class="kw">rep</span>(<span class="dv">0</span>,<span class="kw">ncol</span>(noise))
<span class="co"># Need some time to run</span>
for (i in <span class="dv">1</span>:<span class="dv">7</span>){
idex&lt;-<span class="kw">which</span>(<span class="kw">is.na</span>(sim.dat$income))
withnoise&lt;-sim.dat$income+noise[,i]
trainX&lt;-sim.dat[,<span class="kw">c</span>(<span class="st">&quot;store_exp&quot;</span>,<span class="st">&quot;online_exp&quot;</span>,<span class="st">&quot;store_trans&quot;</span>,<span class="st">&quot;online_trans&quot;</span>)]
trainY&lt;-withnoise
fit0&lt;-<span class="kw">train</span>(trainX,trainY,<span class="dt">method=</span><span class="st">&quot;svmRadial&quot;</span>,
            <span class="dt">tuneLength=</span><span class="dv">15</span>,
            <span class="dt">trControl=</span><span class="kw">trainControl</span>(<span class="dt">method=</span><span class="st">&quot;cv&quot;</span>))
rsq_svm[i]&lt;-<span class="kw">max</span>(fit0$results$Rsquared)
}</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># randomForest: random forest model</span>
<span class="kw">library</span>(randomForest)
rsq_rf&lt;-<span class="kw">rep</span>(<span class="dv">0</span>,<span class="kw">ncol</span>(noise))
<span class="co"># ntree=500 number of trees</span>
<span class="co"># na.action = na.omit  ignore missing value</span>
for (i in <span class="dv">1</span>:<span class="dv">7</span>){
withnoise&lt;-sim.dat$income+noise[,i]
fit0&lt;-<span class="kw">randomForest</span>(withnoise~store_exp+online_exp+store_trans+online_trans,<span class="dt">data=</span>sim.dat,<span class="dt">ntree=</span><span class="dv">500</span>,<span class="dt">na.action =</span> na.omit)
rsq_rf[i]&lt;-<span class="kw">tail</span>(fit0$rsq,<span class="dv">1</span>)
}
<span class="kw">library</span>(reshape2)
rsq&lt;-<span class="kw">data.frame</span>(<span class="kw">cbind</span>(<span class="dt">Noise=</span><span class="kw">c</span>(<span class="fl">0.0</span>, <span class="fl">0.5</span>, <span class="fl">1.0</span>, <span class="fl">1.5</span>, <span class="fl">2.0</span>, <span class="fl">2.5</span>, <span class="fl">3.0</span>),rsq_linear,rsq_pls,rsq_mars,rsq_svm,rsq_rf))
rsq&lt;-<span class="kw">melt</span>(rsq,<span class="dt">id.vars=</span><span class="st">&quot;Noise&quot;</span>,<span class="dt">measure.vars=</span><span class="kw">c</span>(<span class="st">&quot;rsq_linear&quot;</span>,<span class="st">&quot;rsq_pls&quot;</span>,<span class="st">&quot;rsq_mars&quot;</span>,<span class="st">&quot;rsq_svm&quot;</span>,<span class="st">&quot;rsq_rf&quot;</span>))</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(ggplot2)
<span class="kw">ggplot</span>(<span class="dt">data=</span>rsq, <span class="kw">aes</span>(<span class="dt">x=</span>Noise, <span class="dt">y=</span>value, <span class="dt">group=</span>variable, <span class="dt">colour=</span>variable)) +
<span class="st">    </span><span class="kw">geom_line</span>() +
<span class="st">    </span><span class="kw">geom_point</span>()+
<span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;R2&quot;</span>) </code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:error"></span>
<img src="CE_JSM2017_files/figure-html/error-1.png" alt="Test set $R^2$ profiles for income models when measurement system noise increases. The models are linear regression (`rsq_linear`),  Partial Least Square (rsq_pls),  Multiple Adaptive Regression Spline Regression (rsq_mars), Support Vector Machine (rsq_svm)，Random Forest (rsq_rf)" width="80%" />
<p class="caption">
Figure 7.1: Test set <span class="math inline">\(R^2\)</span> profiles for income models when measurement system noise increases. The models are linear regression (<code>rsq_linear</code>), Partial Least Square (rsq_pls), Multiple Adaptive Regression Spline Regression (rsq_mars), Support Vector Machine (rsq_svm)，Random Forest (rsq_rf)
</p>
</div>
<p>Fig. <a href="model-tuning-technique.html#fig:error">7.1</a> shows that:</p>
<p>All model performance decreases sharply with increasing noise intensity. To better anticipate model performance, it helps to understand the way variable is measured. It is something need to make clear at the beginning of an analytical project. A data scientist should be aware of the quality of the data in the database. For data from the clients, it is an important to understand the quality of the data by communication.</p>
<p>More complex model is not necessarily better. The best model in this situation is MARS, not random forests or SVM. Simple linear regression and PLS perform the worst when noise is low. MARS is more complex than linear regression and PLS, but it is simpler and easier to explain than random forest and SVM.</p>
<p>When noise increases to a certain extent, the potential structure becomes vaguer, and complex random forest model starts to fail. When the systematic measurement error is large, a simpler but not naive model may be a better choice. It is always a good practice to try different models, and select the simplest model in the case of similar performance. Model evaluation and selection represent the career “maturity” of a data scientist.</p>
</div>
<div id="measurement-error-in-the-independent-variables" class="section level3">
<h3><span class="header-section-number">7.1.2</span> Measurement Error in the Independent Variables</h3>
<p>The traditional statistical model usually assumes that the measurement of the independent variable has no error which is not possible in practice. Considering the error in the independent variables is necessary. The impact of the error depends on the following factors: (1) the magnitude of the randomness; (2) the importance of the corresponding variable in the model, and (3) the type of model used. Use variable <code>online_exp</code> as an example. The approach is similar to the previous section. Add varying degrees of noise and see its impact on the model performance. We add the following different levels of noise (0 to 3 times the standard deviation) to<code>online_exp</code>:</p>
<p><span class="math display">\[\sigma_{0} \times (0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0)\]</span></p>
<p>where <span class="math inline">\(\sigma_{0}\)</span> is the standard error of <code>online_exp</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">noise&lt;-<span class="kw">matrix</span>(<span class="kw">rep</span>(<span class="ot">NA</span>,<span class="dv">7</span>*<span class="kw">nrow</span>(sim.dat)),<span class="dt">nrow=</span><span class="kw">nrow</span>(sim.dat),<span class="dt">ncol=</span><span class="dv">7</span>)
for (i in <span class="dv">1</span>:<span class="kw">nrow</span>(sim.dat)){
noise[i,]&lt;-<span class="kw">rnorm</span>(<span class="dv">7</span>,<span class="kw">rep</span>(<span class="dv">0</span>,<span class="dv">7</span>),<span class="kw">sd</span>(sim.dat$online_exp)*<span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">3</span>,<span class="dt">by=</span><span class="fl">0.5</span>))
}</code></pre></div>
<p>Likewise, we examine the effect of noise intensity on different models (<span class="math inline">\(R^2\)</span>). The models with complexity from low to high are ordinary linear regression, partial least square regression(PLS), multivariate adaptive regression spline (MARS), support vector machine (SVM, the Kernel function is radial basis function), and random forest. The code is similar as before so not shown here.</p>
<div class="figure" style="text-align: center"><span id="fig:errorvariable"></span>
<img src="CE_JSM2017_files/figure-html/errorvariable-1.png" alt="Test set  $R^2$  profiles for income models when noise in `online_exp` increases. `rsq_linear` is linear regression, `rsq_pls` is Partial Least Square, `rsq_mars` is Multiple Adaptive Regression Spline Regression, `rsq_svm` is Support Vector Machine，`rsq_rf` is Random Forest" width="80%" />
<p class="caption">
Figure 7.2: Test set <span class="math inline">\(R^2\)</span> profiles for income models when noise in <code>online_exp</code> increases. <code>rsq_linear</code> is linear regression, <code>rsq_pls</code> is Partial Least Square, <code>rsq_mars</code> is Multiple Adaptive Regression Spline Regression, <code>rsq_svm</code> is Support Vector Machine，<code>rsq_rf</code> is Random Forest
</p>
</div>
<p>Comparing Fig. <a href="model-tuning-technique.html#fig:errorvariable">7.2</a> and Fig. <a href="model-tuning-technique.html#fig:error">7.1</a>, the influence of the two types of error is very different. The error in response can not be overcome for any model but it is not the case for the independent variables. Imagine an extreme case, if <code>online_exp</code> is completely random, that is, no information in it, the impact on the performance of random forest and support vector machine is marginal. Linear regression and PLS still perform similarly. With the increase of noise, the performance starts to decline faster. After a certain extent, it becomes steady. In general, if an independent variable contains error, other variables associated with it can compensate to some extent.</p>
</div>
</div>
<div id="data-splitting-and-resampling" class="section level2">
<h2><span class="header-section-number">7.2</span> Data Splitting and Resampling</h2>
<p>Those highly adaptable models can model complex relationships. However, they tend to overfit which leads to the poor prediction by learning too much from the data. It means that the model is very sensitive to the exact sample used to fit it. When future data is not exactly like the past data, the model prediction may have large mistakes. A simple model like ordinary linear regression tends instead to underfit which leads to the poor prediction by learning too little from the data. It systematically over-predicts or under-predicts the data regardless of how well future data resemble past data. Without evaluating models, the modeler will not know about the problem until the future samples come. Data splitting and resampling are fundamental techniques to build sound models for prediction.</p>
<p><em>Data splitting</em> is to put part of the data aside as testing set (or Hold-outs, out of bag samples) and use the rest for model training. Training samples are also called in-sample. Model performance metrics evaluated using in-sample are retrodictive not predictive.</p>
<p>The traditional business intelligence usually handles data description. Answer simple questions by querying and summarizing the data, such as:</p>
<ul>
<li>What is the monthly sales of a product in 2015?</li>
<li>What is the number of visits to our site in the past month?<br />
</li>
<li>What is the sales difference in 2015 for two different product designs?</li>
</ul>
<p>There is no need to go through the tedious process of splitting the data, tuning and testing model to answer questions of this kind. Instead, people usually use as complete data as possible and then sum or average the parts of interest.</p>
<p>Many models have parameters which cannot be directly estimated from the data, such as <span class="math inline">\(\lambda\)</span> in the lasso (penalty parameter), the number of trees in the random forest. This type of model parameter is called tuning parameter, and there is no analytical formula available to calculate the optimizing value. Tuning parameters often control the complexity of the model. Poor choice for the values can result in over-fitting or under-fitting. A standard approach to estimate tuning parameters is through cross-validation which is a data resampling approach.</p>
<p>Since to get reasonable precision of the performance values on a single test set, the size of the test set may need to be large. So more often, people will instead resample the training set. <em>Data resampling</em> operates similarly: use a subset of samples to fit a model and use the rest to evaluate model performance. This process will repeat multiple times to get a performance profile. In that sense, resampling is based on splitting. The general steps are:</p>
<ul>
<li>Define a set of candidate values for tuning parameter(s)
<ul>
<li>For each candidate value in the set
<ul>
<li>Resample data</li>
<li>Fit model</li>
<li>Predict hold-out</li>
<li>Calculate performance</li>
</ul></li>
</ul></li>
<li>Aggregate the results</li>
<li>Determine the final tuning parameter</li>
<li>Refit the model with the entire training set</li>
</ul>
<div class="figure">
<img src="http://scientistcafe.com/book/Figure/ParameterTuningProcess.png" style="width:100.0%" />

</div>
<p>We have outlined above the general procedure to tune parameters. Now let’s focus on the critical part of the process: data splitting.</p>

</div>
</div>
<h3> References</h3>
<div id="refs" class="references">
<div id="ref-rethinkDL">
<p>Chiyuan Zhang, Moritz Hardt, Samy Bengio. 2017. “Understanding Deep Learning Requires Rethinking Generalization.” <em>ArXiv:1611.03530</em>.</p>
</div>
<div id="ref-APM">
<p>Max Kuhn, Kjell Johnston. 2013. <em>Applied Predictive Modeling</em>. Springer.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="measuring-performance.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="feature-engineering.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": null,
"download": null,
"toc": {
"collapse": "subsection",
"scroll_highlight": true
},
"search": true,
"toolbar": {
"position": "fixed"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
