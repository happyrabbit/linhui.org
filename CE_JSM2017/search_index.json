[
["index.html", "The Science and Art of Data Chapter 1 Prologue", " The Science and Art of Data Hui Lin and Ming Li 2017-05-05 Chapter 1 Prologue This work is licensed under a Creative Commons Attribution-No Derivative Works 3.0 United States License. Copyright is retained by Hui Lin in all non-U.S. jurisdictions, but permission to use these materials in teaching is still granted, provided the authorship and licensing information here is displayed. The author has striven to minimize the number of errors, but no guarantee is made as to accuracy of the contents of this book. "],
["the-art-of-data-science.html", "Chapter 2 The art of data science 2.1 What is data science? 2.2 Is it science? Totally? 2.3 What kind of questions can data science solve? 2.4 What are the required skills for data scientist? 2.5 General Process of Data Science 2.6 Cloud-based computation environment", " Chapter 2 The art of data science Data science and data scientist have become buzz words. Allow me to reiterate what you may have already heard a million times in the media: data scientists are in demand and continue to grow. A study by the McKinsey Global Institute concludes, “a shortage of the analytical and managerial talent necessary to make the most of Big Data is a significant and pressing challenge (for the U.S.).” You may expect that statistician and graduate students from traditional statistics departments are great data scientist candidates. But the situation is that majority of current data scientists are not from statistics background. As David Donoho pointed out: “statistics is being marginalized here; the implicit message is that statistics is a part of what goes on in data science but not a very big part.” ( from “50 years of Data Science”). What is wrong? The activities that preoccupied statistics over centuries are now in the limelight, but those activities are claimed to belong to a new discipline and practiced by professionals from various backgrounds. Various professional statistics organizations are reacting to this confusing situation. (Page 5-7, “50 Years of Data Science”) From those discussions, Donoho summarizes the main recurring “Memes” about data sciences: The ‘Big Data’ Meme The ‘Skills’ Meme The ‘Jobs’ Meme The first two are linked together which leads to statisticians’ current position on data science. I assume everyone has heard the 3V definition of big data. The media hasn’t taken a minute break from touting “big” data. Data science trainees now need the skills to cope with such big datasets. What are those skills? You may already hear those: Hadoop, a variant of Map/Reduce for use with datasets distributed across a cluster of computers. The new skills are for dealing with organizational artifacts of large-scale cluster computing but not for better solving the real problem. A lot of data, on its own is worthless. It isn’t the size of the data that’s important. It’s what you do with it. The big data skills that so many are touting today are not skills for better solving the real problem of inference from data. Some media think they sense the trends in hiring and government funding. We are transiting to universal connectivity with a deluge of data filling telecom servers. But these facts don’t immediately create a science. The statisticians have been laying the groundwork of data science at least 50 years ago. Today’s data science is an enlargement of traditional academic statistics rather than a brand new discipline. Our goal is to help you enlarge your background to be data scientist in US enterprise environments. We will use case studies to cover how to leverage big data distributed platform (Hadoop / Hive / Spark), data wrangling, modeling, dynamic report (R markdown) and interactive dashboard (R-Shiny) to tackle real-world data science problems. One typical skill gap for statistician is data ETL (extraction, transformation and load) in production environments, and we will cover this topic as well. Data science is a combination of science and art with data as the foundation. We will also cover the “art” part to guide participant to learn soft skills to define data science problems and to effectively communicate with business stakeholders. The prerequisite knowledge is MS level education in statistics and entry level of R-Studio. 2.1 What is data science? This question is not new. When you tell people: I am a data scientist. They may think: Ah, data scientist! Yes, who don’t know data scientist is the sexist job in 21th century. If they ask further what is data science and what exactly data scientists do, it may effectively kill the conversation. Data Science doesn’t come out of the blue. Its predecessor is data analysis. Back to 1962, John Tukey wrote in “The Future of Data Analysis”: For a long time I have thought I was a statistician, interested in inferences from the particular to the general. But as I have watched mathematical statistics evolve, I have had cause to wonder and to doubt. … All in all, I have come to feel that my central interest is in data analysis, which I take to include, among other things: procedures for analyzing data, techniques for interpreting the results of such procedures, ways of planning the gathering of data to make its analysis easier, more precise or more accurate, and all the machinery and results of (mathematical) statistics which apply to analyzing data. It deeply shocked his academic readers. Aren’t you supposed to present something mathematically precise, such as definitions, theorems and proofs? If we use one sentence to summarize what John said, it is : data analysis is more than mathematics. In Sep 2015, the University of Michigan plans to invest $100 million over the next five years in a new Data Science Initiative that will enhance opportunities for student and faculty researchers across the university to tap into the enormous potential of big data. U-M Provost Martha Pollack said: “Data science has become a fourth approach to scientific discovery, in addition to experimentation, modeling and computation,…” How they define Data science here? The web site for DSI gives us an idea what Data Science is: “This coupling of scientific discovery and practice involves the collection, management, processing, analysis, visualization, and interpretation of vast amounts of heterogeneous data associated with a diverse array of scientific, translational, and interdisciplinary applications.” With the data science hype picking up stream, many professionals changed their titles to Data Scientist without any of the necessary qualifications. But at that time, data scientist title was not well defined which lead to confusion in the market, obfuscation in resumes and exaggeration of skills. Here is a list of definitions for a “data scientist”: “A data scientist is a data analyst who lives in California” “A data scientist is someone who is better at statistics than any software engineer and better - at software engineering than any statistician.” “A data scientist is a statistician who lives in San Francisco.” “Data Science is statistics on a Mac.” There is lots of confusion around Data Scientist, Statistician, Business/Financial/Risk(etc) Analyst, BI professional……It is because the obvious intersections among those. Now I see data science as a discipline to make sense of data. In order to make sense of data, statistics is an indispensable part. Meanwhile a data scientist needs many other skills. In the obscenity case of Jacobellis v. Ohio (1964), Potter Stewart wrote in his short concurrence that “hard-core pornography” was hard to define, but that “I know it when I see it.” This applies to many things including data science. It is hard to define but you know it when you see it. So instead of scratching my head to figure out one sentence definition, I am going to sketch the history of data science, what kind of questions data science can answer, and describe the skills required for being a data scientist. Hope this can give you a better depiction of data science. In the early 19th century when Legendre and Gauss came up the least square method for linear regression, only the physicists would use it to fit linear regression. Now, even non-technical people can fit linear regression using excel in 5 secs. In 1936 Fisher came up linear discriminant analysis. In 1940s, we had another widely used model – logistic regression. In 1970s, Nelder and Wedderburn formulated “generalized linear model (GLM)” which generalized linear regression by allowing the linear model to be related to the response variable via a link function and by allowing the magnitude of the variance of each measurement to be a function of its predicted value. By the end of 1970s, there were a range of analytical models while most of them were linear because the computer techniques were not advanced enough to fit non-linear model until 1980s. In 1984 Breiman et al. introduced classification and regression tree (CART) which is one of the oldest and most utilized classification and regression techniques. After that Ross Quinlan came up with more tree algorithms such as ID3, C4.5 and C5.0. In the 1990s, ensemble techniques (methods that combine many models’ predictions) began to appear. Bagging is a general approach that uses bootstrapping in conjunction with any regression or classification model to construct an ensemble. Based on the ensemble idea, Breiman came up random forest in 2001. Later, Yoav Freund and Robert Schapire came up AdaBoost.M1 algorithm. Benefiting from the increasing availability of digitized information, and the possibility to distribute that via the internet, the tool box has been expanding fast. The applications include business, health, biology, social science, politics etc. John Tukey identified 4 forces driving data analysis (there was no “data science” then): The formal theories of math/stat Acceleration developments in computers and display devices The challenge, in many fields, of more and ever larger bodies of data The emphasis on quantification in an ever wider variety of disciplines John’s 1962 list is surprisingly modern. Let’s inspect those points in today’s context. There is always a time gap between a theory and its application. We had the theories much earlier than application. Fortunately, for the past 50 years statisticians have been laying the theoretical groundwork for constructing “data science” today. The development of computer science enables us to calculate much faster and deliver results in a friendly and intuitive way. The striking transition to the internet of things generates vast amounts of commercial data. Industries have also sensed the value of exploiting that data. Data science seems certain to be a major preoccupation of commercial life in coming decades. All the four forces John identified exist today which have been driving data science. 2.2 Is it science? Totally? Let’s take one step back. What is science? Here is what John Tukey said: There are diverse views as to what makes a science, but three constituents will be judged essential by most, viz: (a1) intellectual content, (a2) organization in an understandable form, (a3) reliance upon the test of experience as the ultimate standard of validity The first one (a1) doesn’t provide useful information. And (a2) can’t distinguish science from art very well. The last one is a key character of science. The influential philosopher of science Karl Popper argued that science advances by falsifying hypotheses. If science needs to be falsifiable, then data science is not 100% science. It is true that there are some analytical results that can be validated (falsified) through cross validation or comparing prediction with future outcomes. But certainly not all of them. Even in the problem of prediction, we can’t validate it in 2nd order chaos system. We can’t scientifically validate many of the unsupervised learning or descriptive analysis, especially in the context of marketing. In that sense, data science is a combination of science and art. There is another definition of science from the famous computer scientist Donald Knuth. He said in his legendary 1974 essay Computer Programming as an Art: “Science is knowledge which we understand so well that we can teach it to a computer.” Computer is an indispensable part of data science. But can we teach computer to do all the work data scientists do today? No. So it is not totally science. Computer can’t communicate with stakeholders to transform a real life problem to be data problem. Computer doesn’t know which question can be answered through analytics. Computer doesn’t know how to explain the results to different audiences using different ways according to their backgrounds. Computer is powerful, very powerful in many ways but certainly not all. Would computer enter a ‘runaway reaction’ of self-improvement cycles so that they could surpass human in every way in the future? Well, that is not a question we are trying to answer here. If you are interested in the future of technology, there are some books you can refer to. Ray Kurzweil (The Singularity Is Near), Yuval Noah Harari (Homo Deus: A Brief History of Tomorrow) and Kevin Kelly (The Inevitable). In spite of being short-sighted, we will assume it won’t happen in my foreseeable future. To be simple I will still use data science in the rest of the book. But it is important to realize that data science includes art. 2.3 What kind of questions can data science solve? 2.3.1 What is a good question? Specific How can we increase sales? Dose the January campaign on product X increase the amount of purcahse from our 2017 retained customers? Data Representative Relevant Quality 2.3.2 Types of questions Comparison Description Clustering Classification Regression (linear/non-linear, parametric/nonparametric) 2.4 What are the required skills for data scientist? We talked about the bewildering definitions of data scientist. With the data science hype picking up stream, some professionals had begun changing their titles to Data Scientist without any of the necessary qualifications (see “Data Scientists…or Data Wannabes”). knitr::include_graphics(&quot;http://scientistcafe.com/book/Figure/SkillEN.png&quot;) 2.4.1 Types of Learning knitr::include_graphics(&quot;http://scientistcafe.com/book/Figure/LearningStyles.png&quot;) 2.4.2 Types of Algorithm knitr::include_graphics(&quot;http://scientistcafe.com/book/Figure/AlogrithmTypes.png&quot;) Uncertainty: Partial knowledge of state of the world: such as income, social media behavior, competitor’s offer Noisy observations: missing information, measurement with error (food taken), self-justification bias (nobody watch the cat video……) Phenomena not covered by our model: linear assumption, normal assumption Inherent stochasticity: even at a higher level, the modeling limitations of complicated systems are such that one might as well view the world as inherently stochastic. 2.5 General Process of Data Science knitr::include_graphics(&quot;http://scientistcafe.com/book/Figure/GeneralProcessEN.png&quot;) 2.6 Cloud-based computation environment "],
["big-data-cloud-platform.html", "Chapter 3 Big Data Cloud Platform 3.1 Introduction of Databricks cloud-based distributed system 3.2 Linux system and Hadoop environment 3.3 Database basic through Hive 3.4 Spark and H2O", " Chapter 3 Big Data Cloud Platform 3.1 Introduction of Databricks cloud-based distributed system 3.2 Linux system and Hadoop environment 3.3 Database basic through Hive 3.4 Spark and H2O "],
["introduction-to-the-data.html", "Chapter 4 Introduction to the data 4.1 Customer data for clothing company 4.2 Customer Satisfaction Survey Data from Airline Company 4.3 Swine Disease Breakout Data", " Chapter 4 Introduction to the data Before tackling analytics problem, we start by creating data to be analyzed in later chapters. Why do we simulate data instead of using real data set? Going through the simulation code helps you practice R skills. It makes the book less dependent on vagaties of finding and downloading online data set It allows you manipulate the synthetic data, run analysis and examine how the results change 4.1 Customer data for clothing company 我们先模拟一个关于某品牌服装消费者的数据，这个数据会在之后的章节中反复用到。数据中包含N=1000个观测，我们将模拟3类变量（括号内是变量对应的模拟数据框中的列标签名）： （1）人口统计学变量。 年龄（age） 性别（gender） 有房还是租房（house） （2）消费者行为变量。 2015年实体店购买该品牌服装花销（store_exp） 2015年在线购买该品牌服装花销（online_exp） 2015年实体店交易次数（store_trans） 2015年在线交易次数（online_trans） （3）客户认知问卷调查。为了进一步了解消费者，商家时常对消费者进行问卷调查，然后对调查结果进行分组，其目标是寻找在产品兴趣，市场参与度或营销反应的重要方面有显著差异的客户群。通过了解组间的不同，市场营销人员可以优化产品定位，进行更加精准的营销。这里我们假设该服装品牌对消费者进行了下面的调查，并模拟该调查问卷的回复。 你是否同意下面的申明？ 问题 1（非常不同意） 2（有点不同意） 3（中立/不知道） 4（有点同意） 5（非常同意） （Q1）：我喜欢买不同品牌的服装，比较它们 （Q2）：我喜欢买同一个品牌的服装 （Q3）：品牌的知名度对我来说非常重要 （Q4）：服装质量对我来说非常重要 （Q5）：我有特定喜欢的风格 （Q6）：我喜欢在实体店购买 （Q7）：我喜欢在网上购买 （Q8）：价格对我来说很重要 （Q9）：我喜欢不同风格的衣服 （Q10）：我喜欢自己挑选服装，不需要周围人的建议 我们进一步假设这些根据问卷调查的结果可以将消费者分成4组：价格敏感（Price），炫耀性消费（Conspicuous），质量（Quality），风格（Style）。 （本章我们不会提到如何得到这些分组；我们假设这些已知。我们会在第9章中介绍聚类分析时会更详细的说明。） 你也可以重复下面的代码，自己创建该数据。我们强烈建议读者重复数据模拟的过程，这样能加深对模型方法的理解。如果你对此不感兴趣，也可以从本书网站上直接下载数据： sim.dat&lt;-read.csv(&quot;https://raw.githubusercontent.com/happyrabbit/DataScientistR/master/Data/SegData.csv&quot;) 获得该数据集后你可以跳过本小节后半部分直接跳到下一节对分析流程的讲解。否则，继续本节。 模拟该数据的过程有些复杂，我们先模拟描述客户的变量。模拟该数据的代码分为3部分： 定义数据结构：定义变量名，变量类型，消费者分组名，各组大小。 变量分布参数，如各自的均值和方差。 在各组和各个变量上迭代，基于定义和参数设置抽取随机数。 通过这种方式组织代码，如果我们要改变部分模拟方式重新抽取数据就比较容易。例如，如果我们想要加一个组，或者改变其中某个人口统计变量的均值，只要稍微改变代码就好。我们也想通过这个结构介绍新的R代码，生成数据的第3个步骤中将用到这些代码。 # 设置随机种子，使数据模拟过程可重复 set.seed(12345) # 定义观测数目 ncust&lt;-1000 # 建立数据框存放模拟观测，初始数据框中只有一列id，即消费者编号 seg_dat&lt;-data.frame(id=as.factor(c(1:ncust))) # 指定要生成的变量，并为变量命名 vars&lt;-c(&quot;age&quot;,&quot;gender&quot;,&quot;income&quot;,&quot;house&quot;,&quot;store_exp&quot;,&quot;online_exp&quot;,&quot;store_trans&quot;,&quot;online_trans&quot;) # 每个变量对应的数据类型 # norm： 正态分布 # binom: 二项分布 # pois： 泊松分布 vartype&lt;-c(&quot;norm&quot;,&quot;binom&quot;,&quot;norm&quot;,&quot;binom&quot;,&quot;norm&quot;,&quot;norm&quot;,&quot;pois&quot;,&quot;pois&quot;) # 四个消费者分组的名称 group_name&lt;-c(&quot;Price&quot;,&quot;Conspicuous&quot;,&quot;Quality&quot;,&quot;Style&quot;) # 各消费者群组的大小 group_size&lt;-c(250,200,200,350) # group_name和group_size的第一个元素表明，对于“Price”这组消费者，我们将模拟N=250个观测。 定义好了数据的基本结构之后，我们下一步是定义分布参数，用这些参数来抽取相应数据。 这里我们要 模拟的数据有4个样本类，8个非抽样调查变量，因此我们创建一个4×8的均值矩阵，因为不同类别的 消费者对应不同的分布参数。下面代码用来创建均值矩阵： # 定义均值矩阵 mus &lt;- matrix( c( # 价格敏感（Price）类对应均值 60, 0.5, 120000,0.9, 500,200,5,2, # 炫耀性消费（Conspicuous）类对应均值 40, 0.7, 200000,0.9, 5000,5000,10,10, # 质量（Quality）类对应均值 36, 0.5, 70000, 0.4, 300, 2000,2,15, # 风格（Style）类对应均值 25, 0.2, 90000, 0.2, 200, 2000,2,20), ncol=length(vars), byrow=TRUE) 具体过程是怎样的？ 均值矩阵mus指定，例如，价格敏感（Price）类群体的第一个变量（这里是年龄age）均值为60，炫耀性消费（Conspicuous）类群体的年龄均值为40依次类推。正态分布变量需要指定均值和方差，如年龄（age）， 实体店花销（store_exp）和在线花销（online_exp）。对于二项分布（只有两个可能取值）和泊松分布变量，我们只需要规定均值。其中，性别（gender），有房还是租房（house）是二项数据，生成这样的数据需要指定得到其中某一观测值的概率，比如矩阵mus中。实体店交易次数（store_trans）和线交易次数（online_trans）是泊松变量（频数），泊松分布只有一个参数——分布均值。所以在下面的标准差矩阵sds中，非正态分布变量对应的标准差为缺失值NA。（注意这里我们只是用这些分布为例生成数据，并不意味着这些是最好的拟合变量观测的分布。例如，真实的收入数据更可能是一个有偏的分布而非正态）。 下面我们对正态分布变量创建标准差矩阵： # 每类的标准差 (NA = 标准差无定义) sds&lt;- matrix( c( # 价格敏感（Price）类对应均值 3,NA,8000,NA,100,50,NA,NA, # 炫耀性消费（Conspicuous）类对应均值 5,NA,50000,NA,1000,1500,NA,NA, # 质量（Quality）类对应均值 7,NA,10000,NA,50,200,NA,NA, # 风格（Style）类对应均值 2,NA,5000,NA,10,500,NA,NA), ncol=length(vars), byrow=TRUE) 将这两个矩阵加在一起我们就能够完全定义各个类的分布了。例如，我们来看下每个矩阵的第1行，其代表第1类群体（价格敏感）的分布参数。这些值规定该类群体的年龄（age）均值为60（见第一个矩阵第1行第1列），标准差为3（第二个矩阵第1行第1列）。另外，其中大约有50%的男性（第一个矩阵第1行第2列），年收入（income）均值为120000元，标准差为8000元。将这些设置分开存在不同表格中，将来想要修改十分容易。将数据定义和抽样过程分开是个很好的习惯。下面开始抽取数据： # 抽取非抽样调查数据 sim.dat&lt;-NULL set.seed(2016) # 对消费者类别进行循环（i） for (i in seq_along(group_name)){ # 为了核实代码，展示循环运行过程，我们在循环中添加了这样一行代码 # 函数运行时会打印出正在抽取的样本类名 cat (i, group_name[i],&quot;\\n&quot;) # 创建一个空矩阵用于存放该类消费者相关数据 seg&lt;-data.frame(matrix(NA,nrow=group_size[i], ncol=length(vars))) # 在这个类之内，对不同变量迭代，抽取相应的随机数据 for (j in seq_along(vars)){ # 在每个变量上迭代 if (vartype[j]==&quot;norm&quot;){ # 抽取正态分布变量 seg[,j]&lt;-rnorm(group_size[i], mean=mus[i,j], sd=sds[i,j]) } else if (vartype[j]==&quot;pois&quot;) { # 抽取泊松分布变量 seg[,j]&lt;-rpois(group_size[i], lambda=mus[i,j]) } else if (vartype[j]==&quot;binom&quot;){ # 抽取二项分布变量 seg[,j]&lt;-rbinom(group_size[i],size=1,prob=mus[i,j]) } else{ # 如果变量类型不是上述几种，程序停止运行并提示信息 stop (&quot;Don&#39;t have type:&quot;,vartype[j]) } } # 将该消费者类的数据依行添加到总数据集 sim.dat&lt;-rbind(sim.dat,seg) } 上面的代码是随机抽样的主要过程，其中cat()函数使得循环运行时会打印出正在抽取的样本类名，最后得到的sim.dat是初始描述客户的变量部分的数据，在对数据进行润色前，提醒大家注意两个关于R的技巧： 第一、在i循环内，我们事先定义一个有着相应行数和列数的没有元素值的数据框seg，之后每迭代一次就将样本赋值到事先定义的seg的特定行。这么做的原因是由于只要R在某个对象上添加东西——如在数据框上增加一行——它都会将原对象拷贝一份。这将使用两倍的内存，减慢运行速度。通过这种方法可以避免对内存的浪费。对这里的小数据可能感觉不出差别，但对于大数据，运行速度会有极大不同。 第二、对循环指针范围的设定用的是seq_along()而非1:length()。这是为了够避免一些常见的错误，如指针向量长度为0或者不经意将向量方向弄反了。 之后我们对描述客户的这部分数据进行完善，添加合适的列标签，将二项（0/1）变量转化为贴有标签的因子变量。 # 指定数据框的列名为我们定义的变量名 names(sim.dat)&lt;-vars # 加上一个因子列表明每个观测的对应的消费者类别 sim.dat$segment&lt;-factor(rep(group_name,times=group_size)) # 将二项变量转化为贴有标签的因子变量 # Female: 女性 # Male: 男性 sim.dat$gender&lt;-factor(sim.dat$gender, labels=c(&quot;Female&quot;,&quot;Male&quot;)) sim.dat$house&lt;-factor(sim.dat$house, labels=c(&quot;No&quot;,&quot;Yes&quot;)) # 假设在线购买和在实体店购买的次数至少为1，所以这里在原随机值上加1 sim.dat$store_trans&lt;-sim.dat$store_trans+1 sim.dat$online_trans&lt;-sim.dat$online_trans+1 # 年龄为整数 sim.dat$age&lt;-floor(sim.dat$age) 真实市场营销数据往往没有这么干净，数据缺失，以及错误输入等问题常常发生。我们最后对模拟的数据做一点“破坏”，使其更像真实的数据。我们假设一些人不愿意给出关于收入（income）的信息。我们建立一个逻辑变量idxm，然后将逻辑变量idxm值为真的对应位置消费者收入观测设为缺失值NA（R用NA表示缺失值）。我们假设年龄（age）越大的消费者对应缺失值的概率越大： # 加入缺失值 idxm &lt;- as.logical(rbinom(ncust, size=1, prob=sim.dat$age/200)) sim.dat$income[idxm]&lt;-NA 真实的数据中可能有错误的输入，或者离群点： # 错误输入，离群点 set.seed(123) idx&lt;-sample(1:ncust,5) sim.dat$age[idx[1]]&lt;-300 sim.dat$store_exp[idx[2]]&lt;- -500 sim.dat$store_exp[idx[3:5]]&lt;-c(50000,30000,30000) 到目前为止我们已经建立了一部分数据，你可以通过summary(sim.dat)检查数据。下面我们接着抽取问卷调查回复数据。我们先通过rnorm()生成正态分布随机数。但从上面的问卷调查表格中可以看到，这是一个1-5分量级的问卷，1代表非常不同意，5代表非常同意。于是接下来我们通过floor()函数将连续值转化成离散整数。 # 抽取问卷调查回复 # 问卷问题数目 nq&lt;-10 # 各类消费者对问卷回复的正态分布均值矩阵 mus2 &lt;- matrix( c( # 价格敏感（Price）类对应均值 5,2,1,3,1,4,1,4,2,4, # 炫耀性消费（Conspicuous）类对应均值 1,4,5,4,4,4,4,1,4,2, # 质量（Quality）类对应均值 5,2,3,4,3,2,4,2,3,3, # 风格（Style）类对应均值 3,1,1,2,4,1,5,3,4,2), ncol=nq, byrow=TRUE) # 方差假设都是0.2 sd2&lt;-0.2 sim.dat2&lt;-NULL set.seed(1000) # 对消费者类别进行循环（i） for (i in seq_along(group_name)){ # 为了核实代码，展示循环运行过程，我们在循环中添加了这样一行代码 # 函数运行时会打印出正在抽取的样本类名，这里不再显示输出 # cat (i, group_name[i],&quot;\\n&quot;) # 创建一个空矩阵用于存放该类消费者相关数据 seg&lt;-data.frame(matrix(NA,nrow=group_size[i], ncol=nq)) # 在这个类之内，对不同变量迭代，抽取相应的随机数据 for (j in 1:nq){ # 抽取正态分布变量 res&lt;-rnorm(group_size[i], mean=mus2[i,j], sd=sd2) # 设置上下限度 res[res&gt;5]&lt;-5 res[res&lt;1]&lt;-1 # 通过 floor()函数将连续值转化成离散整数。 seg[,j]&lt;-floor(res) } # 将该消费者类的数据添加到总数据集 sim.dat2&lt;-rbind(sim.dat2,seg) } # 为数据框添加列标签 names(sim.dat2)&lt;-paste(&quot;Q&quot;,1:10,sep=&quot;&quot;) # 合并两部分数据 sim.dat&lt;-cbind(sim.dat,sim.dat2) # 加上一个因子列表明每个观测的对应的消费者类别 sim.dat$segment&lt;-factor(rep(group_name,times=group_size)) 至此为止我们得到了需要的数据集。让我们检查一下抽取的数据集： str(sim.dat,vec.len=3) ## &#39;data.frame&#39;: 1000 obs. of 19 variables: ## $ age : int 57 63 59 60 51 59 57 57 ... ## $ gender : Factor w/ 2 levels &quot;Female&quot;,&quot;Male&quot;: 1 1 2 2 2 2 2 2 ... ## $ income : num 120963 122008 114202 113616 ... ## $ house : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 2 2 2 2 2 2 2 2 ... ## $ store_exp : num 529 478 491 348 ... ## $ online_exp : num 304 110 279 142 ... ## $ store_trans : int 2 4 7 10 4 4 5 11 ... ## $ online_trans: int 2 2 2 2 4 5 3 5 ... ## $ Q1 : int 4 4 5 5 4 4 4 5 ... ## $ Q2 : int 2 1 2 2 1 2 1 2 ... ## $ Q3 : int 1 1 1 1 1 1 1 1 ... ## $ Q4 : int 2 2 2 3 3 2 2 3 ... ## $ Q5 : int 1 1 1 1 1 1 1 1 ... ## $ Q6 : int 4 4 4 4 4 4 4 4 ... ## $ Q7 : int 1 1 1 1 1 1 1 1 ... ## $ Q8 : int 4 4 4 4 4 4 4 4 ... ## $ Q9 : int 2 1 1 2 2 1 1 2 ... ## $ Q10 : int 4 4 4 4 4 4 4 4 ... ## $ segment : Factor w/ 4 levels &quot;Conspicuous&quot;,..: 2 2 2 2 2 2 2 2 ... 可以看到，服装消费者数据有1000个观测，19个变量。前8个变量是关于样本的人口统计学和购买行为描述。Q1-Q10是关于消费者选择偏好的问卷调查回复，问卷分值量表为1-5分（最常见的市场调查设计）。最后一列是消费者类别，样本观测的模拟是根据消费者类别进行的，因此这些可以当作“真实”的消费者类别。使用随机模拟的一个重要优点就是能够通过这种方式验证模型的效果。而实际生活中的数据样本真正所属类别通常是未知的。我们在之后对聚类和判别分析进行介绍的时候会使用样本类别信息。下面我们会反复用该数据集为例。 4.2 Customer Satisfaction Survey Data from Airline Company 这一小节我们模拟一个航空公司满意度调查数据。数据中包含N=1000个受访者，每个受访者基于最近一次航班体验对3个航空公司进行评分，问卷调查一共15项，每项评分从1-9，分值越大满意度越高。这15个调查项分为4类（括号中为相应数据集中的变量名）： 购票体验 购票容易度（Easy_Reservation） 座椅选择（Preferred_Seats） 航班选择（Flight_Options） 票价（Ticket_Prices） 机舱设施 座椅舒适度（Seat_Comfort） 位置前后空间（Seat_Roominess） 随机行李存放（Overhead_Storage） 机舱清洁（Clean_Aircraft） 空航服务 礼貌（Courtesy） 友善（Friendliness） 能够提供需要的帮助（Helpfulness） 食物饮料服务（Service） 总体指数 总体满意度（Satisfaction） 再次选择次航空公司（Fly_Again） 向朋友推荐此航空公司（Recommend） # 先建立因子载荷矩阵 # 其中前12项符合双因子结构，因为每项对应一个总体因子载荷和某特定因子的载荷 # 比如购票容易度对应总体因子载荷0.33，对因特定购票因子载荷0.58 # 我们可以将结果评分看成是总体因子和特定因子共同作用的结果 loadings &lt;- matrix(c ( # 购票体验 .33, .58, .00, .00, # 购票容易度 .35, .55, .00, .00, # 座椅选择 .30, .52, .00, .00, # 航班选择 .40, .50, .00, .00, # 票价 # 机舱设施 .50, .00, .55, .00, # 座椅舒适度 .41, .00, .51, .00, # 位置前后空间 .45, .00, .57, .00, # 随机行李存放 .32, .00, .54, .00, # 机舱清洁 # 空航服务 .35, .00, .00, .50, # 礼貌 .38, .00, .00, .57, # 友善 .60, .00, .00, .50, # 能够提供需要的帮助 .52, .00, .00, .58, # 食物饮料服务 # 总体指数 .43, .10, .30, .30, # 总体满意度 .35, .50, .40, .20, # 再次选择次航空公司 .25, .50, .50, .20), # 向朋友推荐此航空公司 nrow=15,ncol=4, byrow=TRUE) # 将载荷矩阵乘以它的转秩，然后将对角线元素设置为1得到相关矩阵 cor_matrix&lt;-loadings %*% t(loadings) # Diagonal set to ones. diag(cor_matrix)&lt;-1 # 我们通过mvtnorm包模拟有特定相关矩阵的数据集 library(mvtnorm) # 设置3个航空公司对应的评分均值向量 mu1=c(5,6,5,6, 7,8,6,7, 5,5,5,5, 6,6,6) mu2=c(3,3,2,3, 5,4,5,6, 8,8,8,8, 3,3,3) mu3=c(2,2,2,2, 8,8,8,8, 8,8,8,8, 8,8,8) #设置随机种子 set.seed(123456) # 受访者ID resp.id &lt;- 1:1000 library(MASS) rating1 &lt;- mvrnorm(length(resp.id), mu=mu1, Sigma=cor_matrix) rating2 &lt;- mvrnorm(length(resp.id), mu=mu2, Sigma=cor_matrix) rating3 &lt;- mvrnorm(length(resp.id), mu=mu3, Sigma=cor_matrix) # 将分值限定在1到9之间 rating1[rating1&gt;9]&lt;-9 rating1[rating1&lt;1]&lt;-1 rating2[rating2&gt;9]&lt;-9 rating2[rating2&lt;1]&lt;-1 rating3[rating3&gt;9]&lt;-9 rating3[rating3&lt;1]&lt;-1 # 将分值转化为整数 rating1&lt;-data.frame(round(rating1,0)) rating2&lt;-data.frame(round(rating2,0)) rating3&lt;-data.frame(round(rating3,0)) rating1$ID&lt;-resp.id rating2$ID&lt;-resp.id rating3$ID&lt;-resp.id rating1$Airline&lt;-rep(&quot;AirlineCo.1&quot;,length(resp.id)) rating2$Airline&lt;-rep(&quot;AirlineCo.2&quot;,length(resp.id)) rating3$Airline&lt;-rep(&quot;AirlineCo.3&quot;,length(resp.id)) rating&lt;-rbind(rating1,rating2,rating3) # 为数据集的各列命名 names(rating)&lt;-c( &quot;Easy_Reservation&quot;, &quot;Preferred_Seats&quot;, &quot;Flight_Options&quot;, &quot;Ticket_Prices&quot;, &quot;Seat_Comfort&quot;, &quot;Seat_Roominess&quot;, &quot;Overhead_Storage&quot;, &quot;Clean_Aircraft&quot;, &quot;Courtesy&quot;, &quot;Friendliness&quot;, &quot;Helpfulness&quot;, &quot;Service&quot;, &quot;Satisfaction&quot;, &quot;Fly_Again&quot;, &quot;Recommend&quot;, &quot;ID&quot;, &quot;Airline&quot;) 让我们检查一下抽取的数据集： str(rating,vec.len=3) ## &#39;data.frame&#39;: 3000 obs. of 17 variables: ## $ Easy_Reservation: int 6 5 6 5 4 5 6 4 ... ## $ Preferred_Seats : int 5 7 6 6 5 6 6 6 ... ## $ Flight_Options : int 4 7 5 5 3 4 6 3 ... ## $ Ticket_Prices : int 5 6 6 5 6 5 5 5 ... ## $ Seat_Comfort : int 5 6 7 7 6 6 6 4 ... ## $ Seat_Roominess : int 7 8 6 8 7 8 6 5 ... ## $ Overhead_Storage: int 5 5 7 6 5 4 4 4 ... ## $ Clean_Aircraft : int 7 6 7 7 7 7 6 4 ... ## $ Courtesy : int 5 6 6 4 2 5 5 4 ... ## $ Friendliness : int 4 6 6 6 3 4 5 5 ... ## $ Helpfulness : int 6 5 6 4 4 5 5 4 ... ## $ Service : int 6 5 6 5 3 5 5 5 ... ## $ Satisfaction : int 6 7 7 5 4 6 5 5 ... ## $ Fly_Again : int 6 6 6 7 4 5 3 4 ... ## $ Recommend : int 3 6 5 5 4 5 6 5 ... ## $ ID : int 1 2 3 4 5 6 7 8 ... ## $ Airline : Factor w/ 3 levels &quot;AirlineCo.1&quot;,..: 1 1 1 1 1 1 1 1 ... 4.3 Swine Disease Breakout Data 本小节中我们将模拟一个生猪疫情数据。假设研究人员对800个养猪场进行和某生猪疫情有关的问卷调查，问卷由120个问题组成。每个问题有3个可能选项。目的是根据问卷调查回复得到每个养猪场在未来爆发疫情的概率。每个养猪场在问卷问题的3个可选项中等概率选择。第\\(i\\)个养猪场对应的疫情爆发概率服从\\(Bernoulli(1,p_{i})\\)分布。其中 \\[ln(\\frac{p_{i}}{1-p_{i}})=\\beta_{0}+\\sum_{g=1}^{G}\\mathbf{x_{i,g}^{T}}\\beta_{g}\\] \\(\\beta_{0}\\)是截距项，\\(\\mathbf{x_{i,g}}\\)是第\\(i\\)观测对应第\\(g\\)个问题的回复。这里将问题回复转化为0/1虚拟变量，因为每个问题有3个可能选项，所以\\(\\mathbf{x_{i,g}}\\)是一个取值为0/1的含有三个元素的向量。\\(\\mathbf{\\beta_{g}}\\)是对应的参数。 我们在这里考虑3类问题。第1类（问题1到问题40）问题中有两个选项对应变量有预测能力。第2类（问题41到问题80）问题中只有一个选项对结果有预测能力。第3类（问题81到问题120）对结果预测没有帮助，也就是我们希望能够去除的变量。模拟数据的参数设置如下： \\[\\mathbf{\\beta^{T}}=\\left(\\underset{question\\ 1}{\\frac{40}{3},\\underbrace{1,0,-1}},...,\\underset{question\\ 40}{\\underbrace{1,0,-1}},\\underset{question\\ 41}{\\underbrace{1,0,0}},...,\\underset{question\\ 80}{\\underbrace{1,0,0}},\\underset{question\\ 81}{\\underbrace{0,0,0}},...,\\underset{question\\ 120}{\\underbrace{0,0,0}}\\right)*\\gamma\\] 这里我们通过设置5个\\(\\gamma\\)值（\\(\\gamma \\in \\{0.1,0.25,0.5,1,2\\}\\) ）模拟了5种参数情况下的数据。 \\(\\gamma\\)越大，参数值越大，也就意味着有效问题对结果的预测性越强。对于每个参数设定模拟了20个数据集，之后我们会以这些数据为例展示不同模型变量选择的效果。模拟多个数据集是为了研究一些估值的稳定性。 # sim1_da1.csv 模拟的第一个数据集 # similar sim1_da2 and sim1_da3 # sim1.csv simulated data, the first simulation # dummy.sim1.csv dummy variables for the first simulated data with all the baseline in #code for simulation # setwd(dirname(file.choose())) # library(grplasso) nf&lt;-800 for (j in 1:20){ set.seed(19870+j) x&lt;-c(&#39;A&#39;,&#39;B&#39;,&#39;C&#39;) sim.da1&lt;-NULL for (i in 1:nf){ # sample(x, 120, replace=TRUE)-&gt;sam sim.da1&lt;-rbind(sim.da1,sample(x, 120, replace=TRUE)) } data.frame(sim.da1)-&gt;sim.da1 paste(&quot;Q&quot;, 1:120, sep = &quot;&quot;)-&gt;col paste(&quot;Farm&quot;, 1:nf, sep = &quot;&quot;)-&gt;row colnames(sim.da1)&lt;-col rownames(sim.da1)&lt;-row # 用nnet包中的class.ind()函数将问题回复编码为名义变量 library(nnet) dummy.sim1&lt;-NULL for (k in 1:ncol(sim.da1)) { tmp=class.ind(sim.da1[,k]) colnames(tmp)=paste(col[k],colnames(tmp)) dummy.sim1=cbind(dummy.sim1,tmp) } data.frame(dummy.sim1)-&gt;dummy.sim1 # 每个问题对应的3个名义变量中有重复信息 # 将C选项设置为基线回复 # 删除基线名义变量 base.idx&lt;-3*c(1:120) dummy1&lt;-dummy.sim1[,-base.idx] # 对每个r设置依次抽取相应的因变量 # 每次只对一个r值抽取，将其余代码注释掉 # 得到r=0.1 时每个农场对应的连接函数值 c(rep(c(1/10,0,-1/10),40),rep(c(1/10,0,0),40),rep(c(0,0,0),40))-&gt;s1 as.matrix(dummy.sim1)%*%s1-40/3/10-&gt;link1 # r=0.25 # c(rep(c(1/4,0,-1/4),40),rep(c(1/4,0,0),40),rep(c(0,0,0),40))-&gt;s1 # as.matrix(dummy.sim1)%*%s1-40/3/4-&gt;link1 # r=0.5 # c(rep(c(1/2,0,-1/2),40),rep(c(1/2,0,0),40),rep(c(0,0,0),40))-&gt;s1 # as.matrix(dummy.sim1)%*%s1-40/3/2-&gt;link1 # r=1 # c(rep(c(1,0,-1),40),rep(c(1,0,0),40),rep(c(0,0,0),40))-&gt;s1 # as.matrix(dummy.sim1)%*%s1-40/3-&gt;link1 # r=2 # c(rep(c(2,0,-2),40),rep(c(2,0,0),40),rep(c(0,0,0),40))-&gt;s1 # as.matrix(dummy.sim1)%*%s1-40/3/0.5-&gt;link1 # 在连接函数的基础上计算每个农场对应的爆发概率 exp(link1)/(exp(link1)+1)-&gt;hp1 # 基于爆发概率hp1，抽取相应的因变量res res&lt;-rep(9,nf) for (i in 1:nf){ sample( c(1,0),1,prob=c(hp1[i],1-hp1[i]))-&gt;res[i] } # 这里将数据存成3个不同的版本，只是为了之后不同模型使用方便 # 3个数据集都含有所有120个问题的回复，但彼此稍微有不同 # da1 含有因变量，但没有名义变量所属问题的信息 # da2 没有因变量，但最后一行包括的名义变量所属的问题 # da3 没有因变量，没有名义变量所属问题的信息 dummy1$y&lt;-res da1&lt;-dummy1 y&lt;-da1$y ind&lt;-NULL for (i in 1:120){ c(ind,rep(i,2))-&gt;ind } da2&lt;-rbind(da1[,1:240],ind) da3&lt;-da1[,1:240] # 将数据集储存起来 write.csv(da1,paste(&#39;sim&#39;,j,&#39;_da&#39;,1,&#39;.csv&#39;,sep=&#39;&#39;),row.names=F) write.csv(da2,paste(&#39;sim&#39;,j,&#39;_da&#39;,2,&#39;.csv&#39;,sep=&#39;&#39;),row.names=F) write.csv(da3,paste(&#39;sim&#39;,j,&#39;_da&#39;,3,&#39;.csv&#39;,sep=&#39;&#39;),row.names=F) write.csv(sim.da1,paste(&#39;sim&#39;,j,&#39;.csv&#39;,sep=&#39;&#39;),row.names=F) write.csv(dummy.sim1,paste(&#39;dummy.sim&#39;,j,&#39;.csv&#39;,sep=&#39;&#39;),row.names=F) } 要理解这里数据模拟的代码，读者需要了解逻辑回归和分组lasso的理论知识，这超出了本书的范围。这里的代码仅供大家参考。可以重复上面的代码生成相应的数据集。因为这里生成的数据量较大，在网上只有\\(\\gamma=2\\)对应的一次模拟的数据集。我们看下得到的数据集： library(dplyr) disease_dat&lt;-readr::read_csv(&quot;/Users/happyrabbit/Documents/GitHub/DataScientistR/Data/sim1_da1.csv&quot;) # 这里只截取最后的7列 head(subset(disease_dat,select=c( &quot;Q118.A&quot;,&quot;Q118.B&quot;,&quot;Q119.A&quot;,&quot;Q119.B&quot;,&quot;Q120.A&quot;,&quot;Q120.B&quot;,&quot;y&quot;))) 其中最后一列y代表相应农场疫情爆发情况，y=1代表从问卷调查之后5年内有疫情爆发。剩余的列表示农场问卷调查结果，如Q120.A=1对应问卷调查中第120个问题选择A的农场，类似的Q120.B=1对应第120个问题中选择B的农场，我们将选项C作为基准选项。之后我会用这个数据集展示一些相关的模型。 "],
["data-wrangling.html", "Chapter 5 Data Wrangling 5.1 Read and write data 5.2 Summarize data 5.3 Tidy and Reshape Data", " Chapter 5 Data Wrangling This chapter focuses on some of the most frequently used data manipulations and shows how to implement them in R. It is important to explore the data set with descriptive statistics (mean, standard deviation, etc.) and data visualization prior to analysis. Transform data so that the data structure is in line with the requirements of the model. You also need to summarize the results after analysis. Here we assume the readers are already familiar with some of the traditional R data operations, such as subsetting data frame, deleting variables, read and write functions (read.csv (), write.csv (), etc.) in base R. We will also skip some basic descriptive functions in R. For example, for discrete variables, we often use the frequency table to look at the frequency (table ()) of the variable at various levels as needed, or a crosstab of two variables. You can also draw a bar chart for discrete variables (bar()). For continuous variables, we need to look at the mean (mean ()), standard deviation (sd()), quantile (quantile()) of a variable from time to time. There are also functions like summary(), str() and describe() (a functions in the ‘psych’ package) that give a summary of a data frame. The focus here is to introduce some of the more efficient data wrangling methods in R. 5.1 Read and write data 5.1.1 readr You must be familar with read.csv(), read.table() and write.csv() in base R. Here we will introduce a more efficient package from RStudio in 2015 for reading and writing data: readr package. The corresponding functions are read_csv(), read_table() and write_csv(). The commands look quite similar, but readr is different in the following respects: It is 10x faster. The trick is that readr uses C++ to process the data quickly. It doesn’t change the column names. The names can start with number and “.” will not be substitued to “_”. For example: library(readr) read_csv(&quot;2015,2016,2017 1,2,3 4,5,6&quot;) readr functions do not convert strings to factors by default, are able to parse dates and times and can automatically determine the data types in each column. The killing character in my opinion is that readr provides progress bar. What makes you feel worse than waiting is not knowing how long you have to wait. Without “progress bar” might be the No.1 reason that people break up with the one they have been dating. The major functions of readr is to turn flat files into data frames: read_csv(): reads comma delimited files read_csv2(): reads semicolon separated files (common in countries where , is used as the decimal place) read_tsv(): reads tab delimited files read_delim(): reads in files with any delimiter read_fwf(): reads fixed width files. You can specify fields either by their widths with fwf_widths() or their position with fwf_positions() read_table(): reads a common variation of fixed width files where columns are separated by white space read_log(): reads Apache style log files The good thing is that those functions have similar syntax. Once you learn one, the others become easy. Here we will focus on read_csv(). The most important information for read_csv() is the path to your data: library(readr) sim.dat &lt;- read_csv(&quot;https://raw.githubusercontent.com/happyrabbit/DataScientistR/master/Data/SegData.csv &quot;) head(sim.dat) The function reads the file to R as a tibble. You can consider tibble as next iteration of data frame. They are different with data frame for the following aspects: It never changes an input’s type (i.e., no more stringsAsFactors = FALSE!) It never adjusts the names of variables It has a refined print method that shows only the first 10 rows, and all the columns that fit on screen. You can also control the default print behavior by setting options. Refer to http://r4ds.had.co.nz/tibbles.html for more information about ‘tibble’. When you run read_csv() it prints out a column specification that gives the name and type of each column. In order to better understanding how readr works, it is helpful to type in some baby data set and check the results: dat=read_csv(&quot;2015,2016,2017 100,200,300 canola,soybean,corn&quot;) print(dat) You can also add comments on the top and tell R to skip those lines: dat=read_csv(&quot;# I will never let you know that # my favorite food is carrot Date,Food,Mood Monday,carrot,happy Tuesday,carrot,happy Wednesday,carrot,happy Thursday,carrot,happy Friday,carrot,happy Saturday,carrot,extremely happy Sunday,carrot,extremely happy&quot;, skip = 2) print(dat) If you don’t have column names, set col_names = FALSE then R will assign names “X1”,“X2”… to the columns: dat=read_csv(&quot;Saturday,carrot,extremely happy Sunday,carrot,extremely happy&quot;, col_names=FALSE) print(dat) You can also pass col_names a character vector which will be used as the column names. Try to replace col_names=FALSE with col_names=c(&quot;Date&quot;,&quot;Food&quot;,&quot;Mood&quot;) and see what happen. As mentioned before, you can use read_csv2() to read semicolon separated files: dat=read_csv2(&quot;Saturday; carrot; extremely happy \\n Sunday; carrot; extremely happy&quot;, col_names=FALSE) print(dat) Here “\\n” is a convenient shortcut for adding a new line. You can use read_tsv() to read tab delimited files： dat=read_tsv(&quot;every\\tman\\tis\\ta\\tpoet\\twhen\\the\\tis\\tin\\tlove\\n&quot;, col_names = FALSE) print(dat) Or more generally, you can use read_delim() and assign separating character： dat=read_delim(&quot;THE|UNBEARABLE|RANDOMNESS|OF|LIFE\\n&quot;, delim = &quot;|&quot;, col_names = FALSE) print(dat) Another situation you will often run into is missing value. In marketing survey, people like to use “99” to represent missing. You can tell R to set all observation with value “99” as missing when you read the data: dat=read_csv(&quot;Q1,Q2,Q3 5, 4,99&quot;,na=&quot;99&quot;) print(dat) For writing data back to disk, you can use write_csv() and write_tsv(). The following two characters of the two functions increase the chances of the output file being read back in correctly: Encode strings in UTF-8 Save dates and date-times in ISO8601 format so they are easily parsed elsewhere For example: write_csv(sim.dat, &quot;sim_dat.csv&quot;) For other data types, you can use the following packages: Haven: SPSS, Stata and SAS data Readxl and xlsx: excel data(.xls and .xlsx) DBI: given data base, such as RMySQL, RSQLite and RPostgreSQL, read data directly from the database using SQL Some other useful materials: For getting data from internet, you can refere to the book “XML and Web Technologies for Data Sciences with R”. R data import/export manual rio package：https://github.com/leeper/rio 5.1.2 data.table— enhanced data.frame What is data.table? It is an R package that provides an enhanced version of data.frame. The most used object in R is data frame. Before we move on, let’s briefly review some basic characters and manipulations of data.frame: It is a set of rows and columns. Each row is of the same length and data type Every column is of the same length but can be of differing data types It has characteristics of both a matrix and a list It uses [] to subset data I will use the clothes customer data to illustrate. There are two dimensions in []. The first one indicates row and second one indicates column. It uses comma to separate them. # read data sim.dat&lt;-read.csv(&quot;https://raw.githubusercontent.com/happyrabbit/DataScientistR/master/Data/SegData.csv&quot;) # subset the first two rows sim.dat[1:2,] ## age gender income house store_exp online_exp store_trans online_trans ## 1 57 Female 120963.4 Yes 529.1344 303.5125 2 2 ## 2 63 Female 122008.1 Yes 478.0058 109.5297 4 2 ## Q1 Q2 Q3 Q4 Q5 Q6 Q7 Q8 Q9 Q10 segment ## 1 4 2 1 2 1 4 1 4 2 4 Price ## 2 4 1 1 2 1 4 1 4 1 4 Price # subset the first two rows and column 3 and 5 sim.dat[1:2,c(3,5)] ## income store_exp ## 1 120963.4 529.1344 ## 2 122008.1 478.0058 # get all rows with age&gt;70 sim.dat[sim.dat$age&gt;70,] ## age gender income house store_exp online_exp store_trans ## 288 300 Male 208017.5 Yes 5076.801 6053.485 12 ## online_trans Q1 Q2 Q3 Q4 Q5 Q6 Q7 Q8 Q9 Q10 segment ## 288 11 1 4 5 4 4 4 4 1 4 2 Conspicuous # get rows with age&gt; 60 and gender is Male # select column 3 and 4 sim.dat[sim.dat$age&gt;68 &amp; sim.dat$gender == &quot;Male&quot;, 3:4] ## income house ## 204 119552.0 No ## 288 208017.5 Yes Remember that there are usually different ways to conduct the same manipulation. For example, the following code presents three ways to calculate average number of online transactions for male and female: tapply(sim.dat$online_trans, sim.dat$gender, mean ) ## Female Male ## 15.38448 11.26233 aggregate(online_trans ~ gender, data = sim.dat, mean) ## gender online_trans ## 1 Female 15.38448 ## 2 Male 11.26233 library(dplyr) sim.dat%&gt;% group_by(gender)%&gt;% summarise(Avg_online_trans=mean(online_trans)) There is no gold standard to choose a specific function to manipulate data. The goal is to solve real problem not the tool itself. So just use whatever tool that is convenient for you. The way to use [] is straightforward. But the manipulations are limited. If you need more complicated data reshaping or aggregation, there are other packages to use such as dplyr, reshape2, tidyr etc. But the usage of those packages are not as straightforward as []. You often need to change functions. Keeping related operations together, such as subset, group, update, join etc, will allow for: concise, consistent and readable syntax irrespective of the set of operations you would like to perform to achieve your end goal performing data manipulation fluidly without the cognitive burden of having to change among different functions by knowing precisely the data required for each operation, you can automatically optimize operations effectively data.table is the package for that. If you are not familiar with other data manipulating packages and are interested in reducing programming time tremendously, then this package is for you. Other than extending the function of [], data.table has the following advantages: Offers fast import, subset, grouping, update, and joins for large data files It is easy to turn data frame to data table Can behave just like a data frame You need to install and load the package: # If you haven&#39;t install it, use the code to instal # install.packages(&quot;data.table&quot;) # load packagw library(data.table) Use data.table() to covert the existing data frame sim.dat to data table: dt &lt;- data.table(sim.dat) class(dt) ## [1] &quot;data.table&quot; &quot;data.frame&quot; Calculate mean for counts of online transactions: dt[, mean(online_trans)] ## [1] 13.546 You can’t do the same thing using data frame: sim.dat[,mean(online_trans)] Error in mean(online_trans) : object &#39;online_trans&#39; not found If you want to calculate mean by group as before, set “by =” argument: dt[ , mean(online_trans), by = gender] You can group by more than one variables. For example, group by “gender” and “house”: dt[ , mean(online_trans), by = .(gender, house)] Assign column names for aggregated variables: dt[ , .(avg = mean(online_trans)), by = .(gender, house)] data.table can accomplish all operations that aggregate() and tapply()can do for data frame. General setting of data.table Different from data frame, there are three arguments for data table: It is analogous to SQL. You don’t have to know SQL to learn data table. But experience with SQL will help you understand data table. In SQL, you select column j (use command SELECT) for row i (using command WHERE). GROUP BY in SQL will assign the variable to group the observations. Let’s review our previous code: dt[ , mean(online_trans), by = gender] The code above is equal to the following SQL： SELECT gender, avg(online_trans) FROM sim.dat GROUP BY gender R code: dt[ , .(avg = mean(online_trans)), by = .(gender, house)] is equal to SQL： SELECT gender, house, avg(online_trans) AS avg FROM sim.dat GROUP BY gender, house R code： dt[ age &lt; 40, .(avg = mean(online_trans)), by = .(gender, house)] is equal to SQL： SELECT gender, house, avg(online_trans) AS avg FROM sim.dat WHERE age &lt; 40 GROUP BY gender, house You can see the analogy between data.table and SQL. Now let’s focus on operations in data table. select row # select rows with age&lt;20 and income &gt; 80000 dt[age &lt; 20 &amp; income &gt; 80000] # select the first two rows dt[1:2] select column Selecting columns in data.table don’t need $: # select column “age” but return it as a vector # the argument for row is empty so the result will return all observations ans &lt;- dt[, age] head(ans) ## [1] 57 63 59 60 51 59 To return data.table object, put column names in list(): # Select age and online_exp columns and return as a data.table instead ans &lt;- dt[, list(age, online_exp)] head(ans) Or you can also put column names in .(): ans &lt;- dt[, .(age, online_exp)] # head(ans) To select all columns from “age” to “income”: ans &lt;- dt[, age:income, with = FALSE] head(ans,2) Delete columns using - or !: # delete columns from age to online_exp ans &lt;- dt[, -(age:online_exp), with = FALSE] ans &lt;- dt[, !(age:online_exp), with = FALSE] tabulation In data table. .N means to count。 # row count dt[, .N] ## [1] 1000 If you assign the group variable, then it will count by groups: # counts by gender dt[, .N, by= gender] # for those younger than 30, count by gender dt[age &lt; 30, .(count=.N), by= gender] Order table: # get records with the highest 5 online expense: head(dt[order(-online_exp)],5) Since data table keep some characters of data frame, they share some operations: dt[order(-online_exp)][1:5] You can also order the table by more than one variable. The following code will order the table by gender, then order within gender by online_exp: dt[order(gender, -online_exp)][1:5] Use fread() to import dat Other than read.csv in base R, we have introduced ‘read_csv’ in ‘readr’. read_csv is much faster and will provide progress bar which makes user feel much better (at least make me feel better). fread() in data.table further increase the efficiency of reading data. The following are three examples of reading the same data file topic.csv. The file includes text data scraped from an agriculture forum with 209670 rows and 6 columns: system.time(topic&lt;-read.csv(&quot;/Users/happyrabbit/Documents/GitHub/DataScientistR/Data/topic.csv&quot;)) user system elapsed 4.313 0.027 4.340 system.time(topic&lt;-readr::read_csv(&quot;/Users/happyrabbit/Documents/GitHub/DataScientistR/Data/topic.csv&quot;)) user system elapsed 0.267 0.008 0.274 system.time(topic&lt;-data.table::fread(&quot;/Users/happyrabbit/Documents/GitHub/DataScientistR/Data/topic.csv&quot;)) user system elapsed 0.217 0.005 0.221 It is clear that read_csv() is much faster than read.csv(). fread() is a little faster than read_csv(). As the size increasing, the difference will become for significant. Note that fread() will read file as data.table by default. 5.2 Summarize data 5.2.1 apply(), lapply() and sapply() in base R There are some powerful functions to summarize data in base R, such as apply(), lapply() and sapply(). They do the same basic things and are all from “apply” family: apply functions over parts of data. They differ in two important respects: the type of object they apply to the type of result they will return When do we use apply()? When we want to apply a function to margins of an array or matrix. That means our data need to be structured. The operations can be very flexible. It returns a vector or array or list of values obtained by applying a function to margins of an array or matrix. For example you can compute row and column sums for a matrix: ## simulate a matrix x &lt;- cbind(x1 =1:8, x2 = c(4:1, 2:5)) dimnames(x)[[1]] &lt;- letters[1:8] apply(x, 2, mean) ## x1 x2 ## 4.5 3.0 col.sums &lt;- apply(x, 2, sum) row.sums &lt;- apply(x, 1, sum) You can also apply other functions: ma &lt;- matrix(c(1:4, 1, 6:8), nrow = 2) ma ## [,1] [,2] [,3] [,4] ## [1,] 1 3 1 7 ## [2,] 2 4 6 8 apply(ma, 1, table) #--&gt; a list of length 2 ## [[1]] ## ## 1 3 7 ## 2 1 1 ## ## [[2]] ## ## 2 4 6 8 ## 1 1 1 1 apply(ma, 1, stats::quantile) # 5 x n matrix with rownames ## [,1] [,2] ## 0% 1 2.0 ## 25% 1 3.5 ## 50% 2 5.0 ## 75% 4 6.5 ## 100% 7 8.0 Results can have different lengths for each call. This is a trickier example. What will you get? ## Example with different lengths for each call z &lt;- array(1:24, dim = 2:4) zseq &lt;- apply(z, 1:2, function(x) seq_len(max(x))) zseq ## a 2 x 3 matrix typeof(zseq) ## list dim(zseq) ## 2 3 zseq[1,] apply(z, 3, function(x) seq_len(max(x))) lapply() applies a function over a list, data.frame or vector and returns a list of the same length. sapply() is a user-friendly version and wrapper of lapply(). By default it returns a vector, matrix or if simplify = &quot;array&quot;, an array if appropriate. apply(x, f, simplify = FALSE, USE.NAMES = FALSE) is the same as lapply(x, f). If simplify=TRUE, then it will return a data.frame instead of list. Let’s use some data with context to help you better understand the functions. Get the mean and standard deviation of all numerical variables in the data set. # Read data sim.dat&lt;-read.csv(&quot;https://raw.githubusercontent.com/happyrabbit/DataScientistR/master/Data/SegData.csv&quot;) # Get numerical variables sdat&lt;-sim.dat[,!lapply(sim.dat,class)==&quot;factor&quot;] ## Try the following code with apply() function ## apply(sim.dat,2,class) ## What is the problem? The data frame sdat only includes numeric columns. Now we can go head and use apply() to get mean and standard deviation for each column: apply(sdat, MARGIN=2,function(x) mean(na.omit(x))) ## age income store_exp online_exp store_trans ## 38.840 113543.065 1356.851 2120.181 5.350 ## online_trans Q1 Q2 Q3 Q4 ## 13.546 3.101 1.823 1.992 2.763 ## Q5 Q6 Q7 Q8 Q9 ## 2.945 2.448 3.434 2.396 3.085 ## Q10 ## 2.320 Here we defined a function using function(x) mean(na.omit(x)). It is a very simple function. It tells R to ignore the missing value when calculating the mean. MARGIN=2 tells R to apply function to each column. It is not hard to guess what MARGIN=1 mean. The result show that the average online expense is much higher than store expense. You can also compare the average scores across different questions. The command to calculate standard deviation is very similar. The only difference is to change mean() to sd(): apply(sdat, MARGIN=2,function(x) sd(na.omit(x))) ## age income store_exp online_exp store_trans ## 16.416818 49842.287197 2774.399785 1731.224308 3.695559 ## online_trans Q1 Q2 Q3 Q4 ## 7.956959 1.450139 1.168348 1.402106 1.155061 ## Q5 Q6 Q7 Q8 Q9 ## 1.284377 1.438529 1.455941 1.154347 1.118493 ## Q10 ## 1.136174 Even the average online expense is higher than store expense, the standard deviation for store expense is much higher than online expense which indicates there are very likely some big/small purchase in store. We can check it quickly: summary(sdat$store_exp) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -500.0 205.0 329.0 1357.0 597.3 50000.0 summary(sdat$online_exp) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 68.82 420.30 1942.00 2120.00 2441.00 9479.00 There are some odd values in store expense. The minimum value is -500 which is a wrong imputation which indicates that you should preprocess data before analyzing it. Checking those simple statistics will help you better understand your data. It then give you some idea how to preprocess and analyze them. How about using lapply() and sapply()? Run the following code and compare the results: lapply(sdat, function(x) sd(na.omit(x))) sapply(sdat, function(x) sd(na.omit(x))) sapply(sdat, function(x) sd(na.omit(x)), simplify = FALSE) 5.2.2 ddply() in plyr package dplyr is a set of clean and consistent tools that implement the split-apply-combine pattern in R. This is an extremely common pattern in data analysis: you solve a complex problem by breaking it down into small pieces, doing something to each piece and then combining the results back together again. [From package description] You may find the description sounds familiar. The package is sort of a wrapper of apply family. We will only introduce the main function ddply(). Because the package has next iteration which is dplyr package. We will introduce dplyr in more details. The reason we still want to spend some time on the older version is because they have similar idea and knowing the lineage will deeper your understanding of the whole family. We will use the same data frame sim.dat to illustrate. Run the following command: library(plyr) ## ------------------------------------------------------------------------- ## You have loaded plyr after dplyr - this is likely to cause problems. ## If you need functions from both plyr and dplyr, please load plyr first, then dplyr: ## library(plyr); library(dplyr) ## ------------------------------------------------------------------------- ## ## Attaching package: &#39;plyr&#39; ## The following objects are masked from &#39;package:dplyr&#39;: ## ## arrange, count, desc, failwith, id, mutate, rename, summarise, ## summarize ddply(sim.dat,&quot;segment&quot;,summarize, Age=round(mean(na.omit(age)),0), FemalePct=round(mean(gender==&quot;Female&quot;),2), HouseYes=round(mean(house==&quot;Yes&quot;),2), store_exp=round(mean(na.omit(store_exp),trim=0.1),0), online_exp=round(mean(online_exp),0), store_trans=round(mean(store_trans),1), online_trans=round(mean(online_trans),1)) ## segment Age FemalePct HouseYes store_exp online_exp store_trans ## 1 Conspicuous 42 0.32 0.86 4990 4898 10.9 ## 2 Price 60 0.45 0.94 501 205 6.1 ## 3 Quality 35 0.47 0.34 301 2013 2.9 ## 4 Style 24 0.81 0.27 200 1962 3.0 ## online_trans ## 1 11.1 ## 2 3.0 ## 3 16.0 ## 4 21.1 Now, let’s peel the onion in order. The first argument sim.dat is easy. It is the data you want to work on. The second argument &quot;segment&quot; is the column you want to group by. It is a very standard marketing segmentation problem. The final segment is the result you want to get by designing survey, collecting and analyzing data. Here we assume those segments are known and we want to understand how each group of customer look like. It is a common task in segmentation: figuring out a profile. Here we only summarize data by one categorical variable but you can group by multiply variables using ddply(sim.dat, c(&quot;segment&quot;,&quot;house&quot;), .). So the second argument tell the function we want to divide data by customer segment. The third argument summarize tells R the kind of manipulation you want to do which is to summarize data. There are other choices for this argument such as transform (transform data within each group) and subset(subset data within each group). Then the rest commands tell R the exact action. For example, Age=round(mean(na.omit(age)),0) tell R the following things: Calculate the mean of column age ignoring missing value Round the result to the specified number of decimal places Store the result to a new variable named Age The rest of the command above are similar. In the end we calculate the following for each segment: Age: average age for each segment FemalePct: percentage for each segment HouseYes: percentage of people who own a house stroe_exp: average expense in store online_exp: average expense online store_trans: average times of transactions in store online_trans: average times of online transactions There is a lot of information you can draw from those simple averages. Conspicuous: average age is about 40. Target for middle-age wealthy people. 1/3 of them are female and 2/3 are male. They may be good target for candy dad. They buy regardless the price. Almost all of them own house (0.86). It makes us wonder what is wrong with the rest 14%? They may live in Manhattan Price: They are older people, average age 60. Nearly all of them own a house(0.94). They are less likely to purchase online (store_trans=6 while online_trans=3). This is the only group that is less likely to purchase online. Quality: The average age is 35. They are not way different with Conspicuous in terms of age. But they spend much less. The percentages of male and female are similar. They prefer online shopping. More than half of them don’t own a house (0.66). Style: They are young people with average age 24. Majority of them are female (0.81). Most of them don’t own a house (0.73). They are very likely to be digital natives and definitely prefer online shopping. You may notice that Style group purchase more frequently online (online_trans=21) but the expense (online_exp=1962) is not higher. This makes us wondering what is the average expense each time so you have a better idea about the price range the group fall in. The analytical process is aggregated instead of independent steps. What you learn before will help you decide what to do next. Sometimes you also need to go backward to fix something in the previous steps. For example, you may need to check those negative expense value. We continue to use ddply() to calculate the two statistics: ddply(sim.dat,&quot;segment&quot;,summarize,avg_online=round(sum(online_exp)/sum(online_trans),2), avg_store=round(sum(store_exp)/sum(store_trans),2)) ## segment avg_online avg_store ## 1 Conspicuous 442.27 479.25 ## 2 Price 69.28 81.30 ## 3 Quality 126.05 105.12 ## 4 Style 92.83 121.07 Price group has the lowest averaged one time purchasing price. The Conspicuous group will pay the highest price. When we build profile in real life, we will need to look at the survey results too. Those simple data manipulations can provide you lots of information already. As mentioned before, other than “summarize” there are other functions such as “transform” and “subset”. For simplicity, I draw 11 random samples and 3 variables (age, store_exp and segment) from the original data according to the different segments. We will explain stratified sampling later. Here we just do it without explanation. library(caret) ## Loading required package: lattice set.seed(2016) trainIndex&lt;-createDataPartition(sim.dat$segment,p=0.01,list=F,times=1) examp&lt;-sim.dat[trainIndex,c(&quot;age&quot;,&quot;store_exp&quot;,&quot;segment&quot;)] Now data frame examp only has 11 rows and 3 columns. Let’s look at the function of transform: ddply(examp,&quot;segment&quot;,transform,store_pct=round(store_exp/sum(store_exp),2)) ## age store_exp segment store_pct ## 1 42 6319.0718 Conspicuous 0.55 ## 2 42 5106.4816 Conspicuous 0.45 ## 3 55 595.2520 Price 0.42 ## 4 64 399.3550 Price 0.28 ## 5 64 426.6653 Price 0.30 ## 6 39 362.4795 Quality 0.58 ## 7 35 260.5065 Quality 0.42 ## 8 23 205.6099 Style 0.25 ## 9 24 212.3040 Style 0.26 ## 10 24 202.1017 Style 0.25 ## 11 28 200.1906 Style 0.24 What “transform” does is to transform data within the specified group (segment) and append the result as a new column. Next let’s look at the function of “subset”: ddply(examp,&quot;segment&quot;,subset,store_exp&gt;median(store_exp)) ## age store_exp segment ## 1 42 6319.0718 Conspicuous ## 2 55 595.2520 Price ## 3 39 362.4795 Quality ## 4 23 205.6099 Style ## 5 24 212.3040 Style You get all rows with store_exp greater than its group median. 5.2.3 dplyr package dplyr provides a flexible grammar of data manipulation focusing on tools for working with data frames (hence the d in the name). It is faster and more friendly: It identifies the most important data manipulations and make they easy to use from R It performs faster for in-memory data by writing key pieces in C++ using Rcpp The interface is the same for data frame, data table or database I will illustrate the following functions in order: Display Subset Summarize Create new variable Merge Display tbl_df(): Convert the data to tibble which offers better checking and printing capabilities than traditional data frames. It will adjust output width according to fit the current window. library(dplyr) tbl_df(sim.dat) glimpse(): This is like a transposed version of tbl_df() glimpse(sim.dat) Subset Get rows with income more than 300000: library(magrittr) filter(sim.dat, income &gt;300000) %&gt;% tbl_df() Here we meet a new operator %&gt;%. It is called “Pipe operator” which pipes a value forward into an expression or function call. What you get in the left operation will be the first argument or the only argument in the right operation. x %&gt;% f(y) = f(x, y) y %&gt;% f(x, ., z) = f(x, y, z ) It is an operator from magrittr which can be really beneficial. Look at the following code. Can you tell me what it does? ave_exp &lt;- filter( summarise( group_by( filter( sim.dat, !is.na(income) ), segment ), ave_online_exp = mean(online_exp), n = n() ), n &gt; 200 ) Now look at the identical code using “%&gt;%”: avg_exp &lt;- sim.dat %&gt;% filter(!is.na(income)) %&gt;% group_by(segment) %&gt;% summarise( ave_online_exp = mean(online_exp), n = n() ) %&gt;% filter(n &gt; 200) Isn’t it much more straight forward now? Let’s read it: Delete observations from sim.dat with missing income values Group the data from step 1 by variable segment Calculate mean of online expense for each segment and save the result as a new variable named ave_online_exp Calculate the size of each segment and saved it as a new variable named n Get segments with size larger than 200 You can use distinct() to delete duplicated rows. dplyr distinct(sim.dat) sample_frac() will randomly select some rows with specified percentage. sample_n() can randomly select rows with specified number. dplyr::sample_frac(sim.dat, 0.5, replace = TRUE) dplyr::sample_n(sim.dat, 10, replace = TRUE) slice() will select rows by position: dplyr::slice(sim.dat, 10:15) It is equivalent to sim.dat[10:15,]. top_n() will select the order top n entries: dplyr::top_n(sim.dat,2,income) If you want to select columns instead of rows, you can use select(). The following are some sample codes: # select by column name dplyr::select(sim.dat,income,age,store_exp) # select columns whose name contains a character string dplyr::select(sim.dat, contains(&quot;_&quot;)) # select columns whose name ends with a character string # similar there is &quot;starts_with&quot; dplyr::select(sim.dat, ends_with(&quot;e&quot;)) # select columns Q1,Q2,Q3,Q4 and Q5 select(sim.dat, num_range(&quot;Q&quot;, 1:5)) # select columns whose names are in a group of names dplyr::select(sim.dat, one_of(c(&quot;age&quot;, &quot;income&quot;))) # select columns between age and online_exp dplyr::select(sim.dat, age:online_exp) # select all columns except for age dplyr::select(sim.dat, -age) Summarize The operations here are similar what we did before with apply() and ddply(). dplyr::summarise(sim.dat, avg_online = mean(online_trans)) ## avg_online ## 1 13.546 # apply function anyNA() to each column # you can also assign a function vector such as: c(&quot;anyNA&quot;,&quot;is.factor&quot;) dplyr::summarise_each(sim.dat, funs_(c(&quot;anyNA&quot;))) ## age gender income house store_exp online_exp store_trans online_trans ## 1 FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE ## Q1 Q2 Q3 Q4 Q5 Q6 Q7 Q8 Q9 Q10 segment ## 1 FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE You can use group_by() to indicate the variables you want to group by as before: sim.dat %&gt;% group_by(segment) %&gt;% summarise_each(funs_(c(&quot;anyNA&quot;))) Create new variable mutate() will compute and append one or more new columns: dplyr::mutate(sim.dat, total_exp = store_exp + online_exp) It will apply window function to the columns and return a column with the same length. It is a different type of function as before. # min_rank=rank(ties.method = &quot;min&quot;) # mutate_each() means apply function to each column dplyr::mutate_each(sim.dat, funs(min_rank)) The other similar function is transmute(). The differece is that transmute() will delete the original columns and only keep the new ones. dplyr::transmute(sim.dat, total_exp = store_exp + online_exp) Merge We create two baby data sets to show how the functions work. (x&lt;-data.frame(cbind(ID=c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;),x1=c(1,2,3)))) ## ID x1 ## 1 A 1 ## 2 B 2 ## 3 C 3 (y&lt;-data.frame(cbind(ID=c(&quot;B&quot;,&quot;C&quot;,&quot;D&quot;),y1=c(T,T,F)))) ## ID y1 ## 1 B TRUE ## 2 C TRUE ## 3 D FALSE # join to the left # keep all rows in x left_join(x,y,by=&quot;ID&quot;) ## ID x1 y1 ## 1 A 1 &lt;NA&gt; ## 2 B 2 TRUE ## 3 C 3 TRUE # get rows matched in both data sets inner_join(x,y,by=&quot;ID&quot;) ## ID x1 y1 ## 1 B 2 TRUE ## 2 C 3 TRUE # get rows in either data set full_join(x,y,by=&quot;ID&quot;) ## ID x1 y1 ## 1 A 1 &lt;NA&gt; ## 2 B 2 TRUE ## 3 C 3 TRUE ## 4 D &lt;NA&gt; FALSE # filter out rows in x that can be matched in y # it doesn&#39;t bring in any values from y semi_join(x,y,by=&quot;ID&quot;) ## ID x1 ## 1 B 2 ## 2 C 3 # the opposite of semi_join() # it gets rows in x that cannot be matched in y # it doesn&#39;t bring in any values from y anti_join(x,y,by=&quot;ID&quot;) ## ID x1 ## 1 A 1 There are other functions(intersect(), union() and setdiff()). Also the data frame version of rbind and cbind which are bind_rows() and bind_col(). We are not going to go through them all. You can try them yourself. If you understand the functions we introduced so far. It should be easy for you to figure out the rest. 5.3 Tidy and Reshape Data “Tidy data” represent the information from a dataset as data frames where each row is an observation and each column contains the values of a variable (i.e. an attribute of what we are observing). Depending on the situation, the requirements on what to present as rows and columns may change. In order to make data easy to work with for the problem at hand, in practice, we often need to convert data between the “wide” and the “long” format. The process feels like playing with a dough. There are two commonly used packages for this kind of manipulations: tidyr and reshape2. We will show how to tidy and reshape data using the two packages. By comparing the functions to show how they overlap and where they differ. 5.3.1 reshape2 package It is a reboot of previous package reshape. Why? Here is what I got from Stack Overflow: “reshape2 let Hadley make a rebooted reshape that was way, way faster, while avoiding busting up people’s dependencies and habits.” Take a baby subset of our exemplary clothes consumers data to illustrate: (sdat&lt;-sim.dat[1:5,1:6]) ## age gender income house store_exp online_exp ## 1 57 Female 120963.4 Yes 529.1344 303.5125 ## 2 63 Female 122008.1 Yes 478.0058 109.5297 ## 3 59 Male 114202.3 Yes 490.8107 279.2496 ## 4 60 Male 113616.3 Yes 347.8090 141.6698 ## 5 51 Male 124252.6 Yes 379.6259 112.2372 For the above data sdat, what if we want to have a variable indicating the purchasing channel (i.e. online or in-store) and another column with the corresponding expense amount? Assume we want to keep the rest of the columns the same. It is a task to change data from “wide” to “long”. There are two general ways to shape data: Use melt() to convert an object into a molten data frame, i.e. from wide to long Use dcast() to cast a molten data frame into the shape you want, i.e. from long to wide library(reshape2) ## ## Attaching package: &#39;reshape2&#39; ## The following objects are masked from &#39;package:data.table&#39;: ## ## dcast, melt (mdat &lt;- melt(sdat, measure.vars=c(&quot;store_exp&quot;,&quot;online_exp&quot;), variable.name = &quot;Channel&quot;, value.name = &quot;Expense&quot;)) ## age gender income house Channel Expense ## 1 57 Female 120963.4 Yes store_exp 529.1344 ## 2 63 Female 122008.1 Yes store_exp 478.0058 ## 3 59 Male 114202.3 Yes store_exp 490.8107 ## 4 60 Male 113616.3 Yes store_exp 347.8090 ## 5 51 Male 124252.6 Yes store_exp 379.6259 ## 6 57 Female 120963.4 Yes online_exp 303.5125 ## 7 63 Female 122008.1 Yes online_exp 109.5297 ## 8 59 Male 114202.3 Yes online_exp 279.2496 ## 9 60 Male 113616.3 Yes online_exp 141.6698 ## 10 51 Male 124252.6 Yes online_exp 112.2372 You melted the data frame sdat by two variables: store_exp and online_exp (measure.vars=c(&quot;store_exp&quot;,&quot;online_exp&quot;)). The new variable name is Channel set by command variable.name = &quot;Channel&quot;. The value name is Expense set by command value.name = &quot;Expense&quot;. You can run a regression to study the effect of purchasing channel: # Here we use all observations from sim.dat mdat&lt;-melt(sim.dat[,1:6], measure.vars=c(&quot;store_exp&quot;,&quot;online_exp&quot;), variable.name = &quot;Channel&quot;, value.name = &quot;Expense&quot;) fit&lt;-lm(Expense~gender+house+income+Channel+age,data=mdat) summary(fit) ## ## Call: ## lm(formula = Expense ~ gender + house + income + Channel + age, ## data = mdat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4208 -821 -275 533 44353 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -9.132e+02 1.560e+02 -5.855 5.76e-09 *** ## genderMale 3.572e+02 1.028e+02 3.475 0.000524 *** ## houseYes -5.687e+01 1.138e+02 -0.500 0.617275 ## income 2.834e-02 1.079e-03 26.268 &lt; 2e-16 *** ## Channelonline_exp 8.296e+02 9.772e+01 8.489 &lt; 2e-16 *** ## age -2.793e+01 3.356e+00 -8.321 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1974 on 1626 degrees of freedom ## (368 observations deleted due to missingness) ## Multiple R-squared: 0.348, Adjusted R-squared: 0.346 ## F-statistic: 173.5 on 5 and 1626 DF, p-value: &lt; 2.2e-16 You can melt() list, matrix, table too. The syntax is similar and we won’t go through every situation. Sometimes we want to convert the data from “long” to “wide”. For example, you want to compare the online and in store expense between male and female based on the house ownership. dcast(mdat, house + gender ~ Channel, sum) ## Using Expense as value column: use value.var to override. ## house gender store_exp online_exp ## 1 No Female 171102.2 583492.4 ## 2 No Male 133130.8 332499.9 ## 3 Yes Female 355320.2 500856.9 ## 4 Yes Male 697297.3 703332.0 In the above code, what is the left side of ~ are variables that you want to group by. The right side is the variable you want to spread as columns. It will use the column indicating value from melt() before. Here is “Expense” . 5.3.2 tidyr package The other package that will do similar manipulations is tidyr. Let’s get a subset to illustrate the usage. library(dplyr) # practice functions we learnt before sdat&lt;-sim.dat[1:5,]%&gt;% dplyr::select(age,gender,store_exp,store_trans) sdat %&gt;% tbl_df() gather() function in tidyr is analogous to melt() in reshape2. The following code will do the same thing as we did before using melt(): library(tidyr) msdat&lt;-tidyr::gather(sdat,&quot;variable&quot;,&quot;value&quot;,store_exp,store_trans) msdat %&gt;% tbl_df() Or if we use the pipe operation, we can write the above code as: sdat%&gt;%gather(&quot;variable&quot;,&quot;value&quot;,store_exp,store_trans) It is identical with the following code using melt(): melt(sdat, measure.vars=c(&quot;store_exp&quot;,&quot;store_trans&quot;), variable.name = &quot;variable&quot;, value.name = &quot;value&quot;) The opposite operation to gather() is spread(). The previous one stacks columns and the latter one spread the columns. msdat %&gt;% spread(variable,value) ## age gender store_exp store_trans ## 1 51 Male 379.6259 4 ## 2 57 Female 529.1344 2 ## 3 59 Male 490.8107 7 ## 4 60 Male 347.8090 10 ## 5 63 Female 478.0058 4 Another pair of functions that do opposite manipulations are separate() and unite(). sepdat&lt;- msdat %&gt;% separate(variable,c(&quot;Source&quot;,&quot;Type&quot;)) sepdat %&gt;% tbl_df() You can see that the function separates the original column “variable” to two new columns “Source” and “Type”. You can use sep= to set the string or regular express to separate the column. By default, it is “_”. The unite() function will do the opposite: combining two columns. It is like the generalization of paste() to data frame. sepdat %&gt;% unite(&quot;variable&quot;,Source,Type,sep=&quot;_&quot;) ## age gender variable value ## 1 57 Female store_exp 529.1344 ## 2 63 Female store_exp 478.0058 ## 3 59 Male store_exp 490.8107 ## 4 60 Male store_exp 347.8090 ## 5 51 Male store_exp 379.6259 ## 6 57 Female store_trans 2.0000 ## 7 63 Female store_trans 4.0000 ## 8 59 Male store_trans 7.0000 ## 9 60 Male store_trans 10.0000 ## 10 51 Male store_trans 4.0000 The reshaping manipulations may be the trickiest part. You have to practice a lot to get familiar with those functions. Unfortunately there is no short cut. "],
["data-pre-processing.html", "Chapter 6 Data Pre-processing 6.1 Start 6.2 Centering and Scaling 6.3 Resolve Skewness 6.4 Resolve Outliers 6.5 Missing Values 6.6 Collinearity 6.7 Sparse Variables 6.8 Re-encode Dummy Variables", " Chapter 6 Data Pre-processing 6.1 Start There are a number of reasons a predictive model falls (Max Kuhn 2013), such as: Inadequate data pre-processing Inadequate model validation Unjustified extrapolation Over-fitting In this blog post, I am going to summarize some common data pre-processing approaches. 6.2 Centering and Scaling It is the most straightforward data transformation. It centers and scales a variable to mean 0 and standard deviation 1. It ensures that the criterion for finding linear combinations of the predictors is based on how much variation they explain and therefore improves the numerical stability. Models involving finding linear combinations of the predictors to explain response/predictors variation need data centering and scaling, such as PCA and PLS. You can easily writing code yourself to conduct this transformation. Or the function preProcess() in package caret can apply this transformation to a set of predictors. #install packages needed library(caret) library(e1071) library(gridExtra) library(lattice) library(imputeMissings) library(RANN) library(corrplot) library(nnet) head(cars) ## speed dist ## 1 4 2 ## 2 4 10 ## 3 7 4 ## 4 7 22 ## 5 8 16 ## 6 9 10 trans&lt;-preProcess(cars,method=c(&quot;center&quot;,&quot;scale&quot;)) transformed&lt;-predict(trans,cars) par(mfrow=c(1,2)) hist(cars$dist,main=&quot;Original&quot;,xlab=&quot;dist&quot;) hist(transformed$dist,main=&quot;Centered and Scaled&quot;,xlab=&quot;dist&quot;) Sometimes you only need to scale the variable. For example, if the model adds penalty to the parameter estimates (such as \\(L_2\\) penalty is ridge regression and \\(L_1\\) penalty in LASSO), the variables need to have similar scale to ensure a fair variable selection. I am heavy user of this kind of penalty-based model in my work and I used the following quantile transformation: \\[ x_{ij}^{*}=\\frac{x_{ij}-quantile(x_{.j},0.01)}{quantile(x_{.j}-0.99)-quantile(x_{-j},0.01)} \\] The reason to use 99% and 1% quantile instead of maximum and minimum values is to resist the impact of outliers. It is easy to write a function to do it: qscale&lt;-function(dat){ for (i in 1:ncol(dat)){ up&lt;-quantile(dat[,i],0.99) low&lt;-quantile(dat[,i],0.01) diff&lt;-up-low dat[,i]&lt;-(dat[,i]-low)/diff } return(dat) } In order to illustrate, let’s simulate a data set with two variables: income and age. set.seed(2015) income&lt;-sample(seq(50000,150000,by=500),95) age&lt;-income/2000-10 noise&lt;-round(runif(95)*10,0) age&lt;-age+noise income&lt;-c(income,10000,15000,300000,250000,230000) age&lt;-c(age,30,20,25,35,95) demo&lt;-data.frame(income,age) demo$education&lt;-as.factor(sample(c(&quot;High School&quot;,&quot;Bachelor&quot;,&quot;Master&quot;,&quot;Doctor&quot;),100,replace = T,prob =c(0.7,0.15,0.12,0.03) )) summary(demo[,c(&quot;income&quot;,&quot;age&quot;)]) ## income age ## Min. : 10000 Min. :20.00 ## 1st Qu.: 76375 1st Qu.:30.25 ## Median : 98750 Median :44.25 ## Mean :103480 Mean :44.92 ## 3rd Qu.:126375 3rd Qu.:56.88 ## Max. :300000 Max. :95.00 It is clear that income and age are not on the same scale. Now apply the function qscale() on the simulated data demo. transformed&lt;-qscale(demo[,c(&quot;income&quot;,&quot;age&quot;)]) summary(transformed) ## income age ## Min. :-0.02101 Min. :-0.01904 ## 1st Qu.: 0.26077 1st Qu.: 0.17814 ## Median : 0.35576 Median : 0.44746 ## Mean : 0.37584 Mean : 0.46044 ## 3rd Qu.: 0.47304 3rd Qu.: 0.69033 ## Max. : 1.21015 Max. : 1.42375 6.3 Resolve Skewness Skewness is defined to be the third standardized central moment. The formula for the sample skewness statistics is: \\[ skewness=\\frac{\\sum(x_{i}+\\bar{x})^{3}}{(n-1)v^{3/2}}\\] \\[v=\\frac{\\sum(x_{i}=\\bar{x})^{2}}{(n-1)}\\] Skewness=0 means that the destribution is symmetric, i.e. the probability of falling on either side of the distribution’s mean is equal. You can easily tell if a distribution is skewed by simple visualization. There are different ways may help to remove skewness such as log, square root or inverse. However it is often difficult to determine from plots which transformation is most appropriate for correcting skewness. The Box-Cox procedure automatically identified a transformation from the family of power transformations that are indexed by a parameter \\(\\lambda\\). \\[ x^{*}=\\begin{cases} \\begin{array}{c} \\frac{x^{\\lambda}-1}{\\lambda}\\\\ log(x) \\end{array} &amp; \\begin{array}{c} if\\ \\lambda\\neq0\\\\ if\\ \\lambda=0 \\end{array}\\end{cases} \\] It is easy to see that this family includes log transformation (\\(\\lambda=0\\)), square transformation (\\(\\lambda=2\\)), square root (\\(\\lambda=0.5\\)), inverse (\\(\\lambda=-1\\)) and others in-between. We can still use function preProcess() in package caret to apply this transformation by chaning the method argument. (trans&lt;-preProcess(cars,method=c(&quot;BoxCox&quot;))) ## Created from 50 samples and 2 variables ## ## Pre-processing: ## - Box-Cox transformation (2) ## - ignored (0) ## ## Lambda estimates for Box-Cox transformation: ## 1, 0.5 The output shows the sample size (50), number of variables (2) and the \\(\\lambda\\) estimates for each variable. After calling the preProcess() function, the predict() method applies the results to a data frame. transformed&lt;-predict(trans,cars) par(mfrow=c(1,2)) hist(cars$dist,main=&quot;Original&quot;,xlab=&quot;dist&quot;) hist(transformed$dist,main=&quot;After BoxCox Transformation&quot;,xlab=&quot;dist&quot;) An alternative is to use function BoxCoxTrans() in package caret. (trans&lt;-BoxCoxTrans(cars$dist)) ## Box-Cox Transformation ## ## 50 data points used to estimate Lambda ## ## Input data summary: ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 2.00 26.00 36.00 42.98 56.00 120.00 ## ## Largest/Smallest: 60 ## Sample Skewness: 0.759 ## ## Estimated Lambda: 0.5 transformed&lt;-predict(trans,cars$dist) skewness(transformed) ## [1] -0.01902765 The estimated \\(\\lambda\\) is the same 0.5. Original skewness is 0.759 and after transformation, the skewness is -0.01902765 which is close to 0. You can use function skewness() in package e1071 to get the skewness statistics. 6.4 Resolve Outliers Even under certain assumptions we can statistically define outliers, it can be hard to define in some situations. You can refer to “Detection of Outliers” for more information. Some models are resistant to outliers (such as tree-based model and support vector machine). If a model is sensitive to outliers (such as linear regression and logistic regression), we can use spatial sign transformation to minimize the problem. It projects the original sample points to the surface of a sphere by: \\[x_{ij}^{*}=\\frac{x_{ij}}{\\sqrt{\\sum_{j=1}^{p}x_{ij}^{2}}}\\] As noted in the book “Applied Predictive Modeling”, Since the denominator is intended to measure the squared distance to the center of the predictor’s distribution, it is important to center and scale the predictor data prior to using this transformation. Note that, unlike centering or scaling, this manipulation of the predictors transforms them as a group. We can use spatialSign() function in caret to conduct spatial sign on demo: trans&lt;-preProcess(demo[,c(&quot;income&quot;,&quot;age&quot;)],method=c(&quot;center&quot;,&quot;scale&quot;)) transformed&lt;-predict(trans,demo[,c(&quot;income&quot;,&quot;age&quot;)]) transformed2 &lt;- spatialSign(transformed) transformed2 &lt;- as.data.frame(transformed2) p1&lt;-xyplot(income ~ age, data = transformed, main=&quot;Original&quot;) p2&lt;-xyplot(income ~ age, data = transformed2, main=&quot;After Spatial Sign&quot;) grid.arrange(p1,p2, ncol=2) 6.5 Missing Values We need a book to fully explicate this topic. Before we decide how to handle missing value, it is important to understand why the values are missing. Do the missing values have information related outcomes? Or are they missing at random? It is not the goal here to illustrate which methods to use in different missing situation. You can refer to Section 3.4 of “Applied Predictive Modeling” for more discussion on that. The objective of this post is to introduce some imputation methods and corresponding application examples using R. Survey statistics has studied the imputation extensively which focuses on making valid inferences. Missing value imputation in predictive modeling is a different problem. Saar-Tsechansky and Provost compared several different methods for applying classification to instance with missing values. “Handling Missing Values when Applying Classification Models” The following code randomly assigns some missing values to the previous data demo and names the new data set demo_missing. set.seed(100) id1&lt;-sample(1:nrow(demo),15) id2&lt;-sample(1:nrow(demo),10) id3&lt;-sample(1:nrow(demo),10) demo_missing&lt;-demo demo_missing$age[id1]&lt;-NA demo_missing$income[id2]&lt;-NA demo_missing$education[id3]&lt;-NA summary(demo_missing) ## income age education ## Min. : 15000 Min. :20.00 Bachelor :13 ## 1st Qu.: 77125 1st Qu.:30.25 Doctor : 2 ## Median : 98750 Median :44.25 High School:70 ## Mean :102811 Mean :44.43 Master : 5 ## 3rd Qu.:125250 3rd Qu.:56.25 NA&#39;s :10 ## Max. :300000 Max. :95.00 ## NA&#39;s :10 NA&#39;s :15 6.5.1 Impute missing values with median/mode You can use function impute() under package imputeMissings to impute missing values with mdedian/mode. This method is simple, fast but treats each predictor independently, and may not be accurate. demo_imp&lt;-impute(demo_missing,method=&quot;median/mode&quot;) summary(demo_imp) ## income age education ## Min. : 15000 Min. :20.00 Bachelor :13 ## 1st Qu.: 79250 1st Qu.:32.19 Doctor : 2 ## Median : 98750 Median :44.25 High School:80 ## Mean :102405 Mean :44.40 Master : 5 ## 3rd Qu.:122875 3rd Qu.:54.50 ## Max. :300000 Max. :95.00 Note that the median/mode method imputes mode to character vectors and median to numeric and integer vectors.So you can see the 10 missing values for variable “education” are imputed with “High School” since it is the mode. You can also use function ‘preProcess()’ to attain this.But it only works for numeric variable. imp&lt;-preProcess(demo_missing[,c(&quot;income&quot;,&quot;age&quot;)],method=&quot;medianImpute&quot;) demo_imp&lt;-predict(imp,demo_missing[,c(&quot;income&quot;,&quot;age&quot;)]) summary(demo_imp) ## income age ## Min. : 15000 Min. :20.00 ## 1st Qu.: 79250 1st Qu.:32.19 ## Median : 98750 Median :44.25 ## Mean :102405 Mean :44.40 ## 3rd Qu.:122875 3rd Qu.:54.50 ## Max. :300000 Max. :95.00 6.5.2 Impute missing values based on K-nearest neighbors k-nearest neighbor will find the k closest samples (Euclidian distance) in the training set and impute the mean of those “neighbors”. imp&lt;-preProcess(demo_missing[,c(&quot;income&quot;,&quot;age&quot;)],method=&quot;knnImpute&quot;,k=2) demo_imp&lt;-predict(imp,demo_missing[,c(&quot;income&quot;,&quot;age&quot;)]) ## Error in FUN(newX[, i], ...): cannot impute when all predictors are missing in the new data point Now we get a error saying “cannot impute when all predictors are missing in the new data point”. It is because there is at least one sample with both “income” and “age” missing. We can delete the corresponding row and do it again. idx&lt;-which(is.na(demo_missing$income)&amp;is.na(demo_missing$age)) imp&lt;-preProcess(demo_missing[-idx,c(&quot;income&quot;,&quot;age&quot;)],method=&quot;knnImpute&quot;,k=2) demo_imp&lt;-predict(imp,demo_missing[-idx,c(&quot;income&quot;,&quot;age&quot;)]) summary(demo_imp) ## income age ## Min. :-2.259679 Min. :-1.53784 ## 1st Qu.:-0.686725 1st Qu.:-0.88276 ## Median :-0.104506 Median :-0.01129 ## Mean :-0.006233 Mean : 0.01103 ## 3rd Qu.: 0.593512 3rd Qu.: 0.72444 ## Max. : 5.074342 Max. : 3.18343 The error doesn’t show up this time. This method considers all predictors together but it requires them to be in the same scale since the “euclidian distance” is used to find the neighbours. 6.6 Collinearity It is probably a technical term that many un-technical people also know. There is an excellent function in corrplot package with the same name corrplot() that can visualize correlation structure of a set of predictors. The function has option to reorder the variables in a way that reveals clusters of highly correlated ones. We add some columns to demo that are correlated. adddemo&lt;-demo[,-3] adddemo$added1&lt;-sqrt(demo$age)+10 adddemo$added2&lt;-log(demo$income)+demo$age adddemo$added2&lt;-log(demo$age) adddemo$added4&lt;-demo$income/1000+5*demo$age adddemo$added5&lt;-sin(demo$age) The following command will produce visualization for the correlation matrix of adddemo. corrplot(cor(adddemo),order=&quot;hclust&quot;) The size and color of the points are associated with the strength of corresponding correlation. Section 3.5 of “Applied Predictive Modeling” presents a heuristic algorithm to remove minium number of predicitors to ensure all pairwise corelations are below a certain threshold: Calculate the correlation matrix of the predictors. Determine the two predictors associated with the largest absolute pairwise correlation (call them predictors A and B). Determine the average correlation between A and the other variables. Do the same for predictor B. If A has a larger average correlation, remove it; otherwise, remove predictor B. Repeat Step 2-4 until no absolute correlations are above the threshold. The findCorrelation() function in package caret will apaply the above algorithm. (highCorr&lt;-findCorrelation(cor(adddemo),cutoff=.75)) ## [1] 5 2 3 # remove columns with high correlations filter_demo&lt;-adddemo[,-highCorr] # correlation matrix for filtered data corrplot(cor(filter_demo),order=&quot;hclust&quot;) 6.7 Sparse Variables Other than the highly related predictors, predictors with degenerate distributions need to be removed as well. Removing those variables can significant improve some models’ performance and/or stability (such as linear regression and logistic regression but tree based model is impervious to this type of predictors). One extreme example is a variable with single value which is called zero-variance variable. Similarly those variables with very low frequency of unique values are near-zero variance predictors. How to detect those variables? There are two rules: - The fraction of unique values over the sample size - The ratio of the frequency of the most prevalent value to the frequency of the second most prevalent value. The caret package funciton nearZeroVar() can filter near-zero variance predictors. #add two variables with low variance zero_demo&lt;-demo zero_demo$zero1&lt;-rep(0,nrow(demo)) zero_demo$zero2&lt;-c(1,rep(0,nrow(demo)-1)) # zero1 only has one unique value # zero2 is a vector with the first element 1 and the rest are 0s summary(zero_demo) ## income age education zero1 ## Min. : 10000 Min. :20.00 Bachelor :15 Min. :0 ## 1st Qu.: 76375 1st Qu.:30.25 Doctor : 2 1st Qu.:0 ## Median : 98750 Median :44.25 High School:77 Median :0 ## Mean :103480 Mean :44.92 Master : 6 Mean :0 ## 3rd Qu.:126375 3rd Qu.:56.88 3rd Qu.:0 ## Max. :300000 Max. :95.00 Max. :0 ## zero2 ## Min. :0.00 ## 1st Qu.:0.00 ## Median :0.00 ## Mean :0.01 ## 3rd Qu.:0.00 ## Max. :1.00 # the function will return a vector of integers indicating which columns to remove nearZeroVar(zero_demo,freqCut = 95/5, uniqueCut = 10) ## [1] 4 5 Note the two arguments in the function freqCut = and uniqueCut =. They are corresponding to the previous two rules. freqCut: the cutoff for the ratio of the most common value to the second most common value uniqueCut:the cutoff for the percentage of distinct values out of the number of total samples 6.8 Re-encode Dummy Variables Sometimes we need to recode categories to smaller bits of information named “dummy variables”. Take the variable “education” in demo for example. It has four categories: “High School”,“Bachelor”,“Master” and “Doctor”. If we recode it to be dummy variables, each category get its own dummy variable that is 0/1 indicator for that category. For a single categorical variable, we can use function class.ind() in package nnet: dumVar&lt;-class.ind(demo$education) head(dumVar) ## Bachelor Doctor High School Master ## [1,] 0 0 1 0 ## [2,] 0 0 1 0 ## [3,] 0 0 1 0 ## [4,] 0 0 1 0 ## [5,] 0 0 1 0 ## [6,] 0 0 1 0 If we want to determine encodeings for more than one variables, we can use dummyVars() in caret. dumMod&lt;-dummyVars(~income+education, data=demo, # Remove the variable name from the column name levelsOnly=T) predict(dumMod,head(demo)) ## income Bachelor Doctor High School Master ## 1 56000 0 0 1 0 ## 2 133500 0 0 1 0 ## 3 79500 0 0 1 0 ## 4 53000 0 0 1 0 ## 5 63500 0 0 1 0 ## 6 84500 0 0 1 0 To add some more complexity, we could assume joint effect of income and education. In this case, this will add 4 more columns to the resulted data frame: dumMod&lt;-dummyVars(~income+education+income:education, data=demo, levelsOnly=T) predict(dumMod,head(demo)) ## income Bachelor Doctor High School Master income:Bachelor income:Doctor ## 1 56000 0 0 1 0 0 0 ## 2 133500 0 0 1 0 0 0 ## 3 79500 0 0 1 0 0 0 ## 4 53000 0 0 1 0 0 0 ## 5 63500 0 0 1 0 0 0 ## 6 84500 0 0 1 0 0 0 ## income:High School income:Master ## 1 56000 0 ## 2 133500 0 ## 3 79500 0 ## 4 53000 0 ## 5 63500 0 ## 6 84500 0 References "],
["shiny-intro.html", "Chapter 7 R-Shiny Introduction 7.1 Softwares 7.2 Web development 7.3 Shiny 7.4 Resources", " Chapter 7 R-Shiny Introduction This tutorial is designed for Shiny beginner. You don’t need any background in the R language or Shiny to get started, although having some basic knowlege about R might be helpful. It would be helpful if you have some basic knowlege about HTML, CSS and javascript, but they are not required too. 7.1 Softwares R (required). Better to use the latest version of R (3.3.2 for the time this tutorial is written). The shiny package (required). To install the package from CRAN by using: install.packages(&quot;shiny&quot;) Rstudio (recommended). A user friendly IDE for R. Very convenient tool in building shiny apps. A web browser. Better to use Google chrome or Firefox. In this section, we’ll start from some basic concepts of web development and then introduce Shiny by explaining the advantages of Shiny and how it works. At last we’ll give a list of useful resources in learning Shiny. 7.2 Web development Here are some concepts that usually used in web development. Static web page A static web page (sometimes called a flat page/stationary page) is a web page that is delivered to the user exactly as stored. (from wiki) Figure 7.1: Static web page: is delivered to the user exactly as stored. Dynamic web page A server-side dynamic web page is a web page whose construction is controlled by an application server processing server-side scripts. (from wiki) Figure 7.2: Dynamic web page: example of server-side scripting (PHP and MySQL). Web application A Web application (Web app) is an application program that is stored on a remote server and delivered over the Internet through a browser interface. (from WhatIs.com) Web (application) framework A Web framework is a collection of packages or modules which allow developers to write Web applications or services without having to handle such low-level details as protocols, sockets or process/thread management. (from PythonWiki) Front-end web development Front-end web development, also known as client-side development is the practice of producing HTML, CSS and JavaScript for a website or web application so that a user can see and interact with them directly. (from Wiki) Back-end web development Back-end web development creates the logical back-end and core computational logic of a website. (from techopedia) 7.3 Shiny Shiny is a web application framework for R that can help turn your analyses into interactive web applications. No HTML, CSS, or JavaScript knowledge required Why Shiny? Easy to learn, easy to use. The development time is minimized. Excellent tool for data visualization. Have very strong backing: the R language Fun &amp; Cool. A Shiny app usually contains two parts: UI: controls the outlook of the web page Server: (a live R session) controls the logic How does Shiny app work? The “Server” keeps monitoring the UI. Whenever there is a change in the UI, the “Server” will follow some instructions (run some R code) accordingly and update the UI’s display. (This is the basic idea of reactive expression, which is an distinguish feature of Shiny we will talk about later.) Example library(shiny) runExample(&quot;01_hello&quot;) # a histogram In the example above, the “Server” keeps monitoring the “slider” in the page, and whenever there is a change with it, the “Server” will re-execute a block of R code to regenerate the hitogram. 7.4 Resources Shiny portal site: http://shiny.rstudio.com Tutorial (get started): http://shiny.rstudio.com/tutorial/ Articles (go deeper): http://shiny.rstudio.com/articles/ Gallery (get inspired): http://shiny.rstudio.com/gallery/ Shiny User Showcase: https://www.rstudio.com/products/shiny/shiny-user-showcase/ Shiny Apps for the Enterprise Industry Specific Shiny Apps Shiny Apps as Analytics Tools Shiny Apps that Extend Shiny Shiny Apps with Popular Appeal Shiny Apps for Teaching Shiny examples (over 100 examples): https://github.com/rstudio/shiny-examples Ask questions in the shiny google group: https://groups.google.com/forum/#!forum/shiny-discuss Articles from R blogger: http://www.r-bloggers.com/?s=shiny Gallery of user-made apps: http://www.showmeshiny.com/ 2016 Shiny Developer Conference Videos https://www.rstudio.com/resources/webinars/shiny-developer-conference/ "],
["dynamicreproducible-report.html", "Chapter 8 Dynamic/Reproducible report", " Chapter 8 Dynamic/Reproducible report "],
["soft-skills-for-data-scientists.html", "Chapter 9 Soft Skills for Data Scientists 9.1 Introduction to agile 9.2 Effective communication with business partners 9.3 Leadership skills 9.4 Decision making with uncertainty", " Chapter 9 Soft Skills for Data Scientists 9.1 Introduction to agile 9.2 Effective communication with business partners 9.3 Leadership skills 9.4 Decision making with uncertainty "],
["case-study.html", "Chapter 10 Case Study 10.1 Case 1: Customer Perception Study for Airline Company 10.2 Case 2: Swine Disease Prediction", " Chapter 10 Case Study 10.1 Case 1: Customer Perception Study for Airline Company 10.2 Case 2: Swine Disease Prediction "],
["references.html", "Chapter 11 References", " Chapter 11 References "]
]
