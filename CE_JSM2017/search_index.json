[
["index.html", "The Science and Art of Data Chapter 1 ", " The Science and Art of Data Hui Lin and Ming Li 2017-04-28 Chapter 1 This work is licensed under a Creative Commons Attribution-No Derivative Works 3.0 United States License. Copyright is retained by Hui Lin in all non-U.S. jurisdictions, but permission to use these materials in teaching is still granted, provided the authorship and licensing information here is displayed. The author has striven to minimize the number of errors, but no guarantee is made as to accuracy of the contents of this book. "],
["the-art-of-data-science.html", "Chapter 2 The art of data science 2.1 What is data science? 2.2 Is it science? Totally? 2.3 What kind of questions can data science solve? 2.4 What are the required skills for data scientist? 2.5 General Process of Data Science 2.6 Cloud-based computation environment", " Chapter 2 The art of data science Data science and Data scientist have become buzz words. Allow me to reiterate what you may have already heard a million times in the media: data scientists are in demand and continue to grow. A study by the McKinsey Global Institute concludes, “a shortage of the analytical and managerial talent necessary to make the most of Big Data is a significant and pressing challenge (for the U.S.).” You may expect that statistician and graduate students from traditional statistics departments are great data scientist candidates. But the situation is that majority of current data scientists are not from statistics background. As David Donoho pointed out: “statistics is being marginalized here; the implicit message is that statistics is a part of what goes on in data science but not a very big part.” ( from “50 years of Data Science”). What is wrong? The activities that preoccupied statistics over centuries are now in the limelight, but those activities are claimed to belong to a new discipline and practiced by professionals from various backgrounds. Various professional statistics organizations are reacting to this confusing situation. (Page 5-7, “50 Years of Data Science”) From those discussions, Donoho summarizes the main recurring “Memes” about data sciences: The ‘Big Data’ Meme The ‘Skills’ Meme The ‘Jobs’ Meme The first two are linked together which leads to statisticians’ current position on data science. I assume everyone has heard the 3V definition of big data. The media hasn’t taken a minute break from touting “big” data. Data science trainees now need the skills to cope with such big datasets. What are those skills? You may already hear those: Hadoop, a variant of Map/Reduce for use with datasets distributed across a cluster of computers. The new skills are for dealing with organizational artifacts of large-scale cluster computing but not for better solving the real problem. A lot of data, on its own is worthless. It isn’t the size of the data that’s important. It’s what you do with it. The big data skills that so many are touting today are not skills for better solving the real problem of inference from data. Some media think they sense the trends in hiring and government funding. We are transiting to universal connectivity with a deluge of data filling telecom servers. But these facts don’t immediately create a science. The statisticians have been laying the groundwork of data science at least 50 years ago. Today’s data science is an enlargement of traditional academic statistics rather than a brand new discipline. Our goal is to help you enlarge your background to be data scientist in US enterprise environments. We will use case studies to cover how to leverage big data distributed platform (Hadoop / Hive / Spark), data wrangling, modeling, dynamic report (R markdown) and interactive dashboard (R-Shiny) to tackle real-world data science problems. One typical skill gap for statistician is data ETL (extraction, transformation and load) in production environments, and we will cover this topic as well. Data science is a combination of science and art with data as the foundation. We will also cover the “art” part to guide participant to learn soft skills to define data science problems and to effectively communicate with business stakeholders. The prerequisite knowledge is MS level education in statistics and entry level of R-Studio. 2.1 What is data science? This question is not new. When you tell people: I am a data scientist. They may think: Ah, data scientist! Yes, who don’t know data scientist is the sexist job in 21th century. If they ask further what is data science and what exactly data scientists do, it may effectively kill the conversation. Data Science doesn’t come out of the blue. Its predecessor is data analysis. Back to 1962, John Tukey wrote in “The Future of Data Analysis”: For a long time I have thought I was a statistician, interested in inferences from the particular to the general. But as I have watched mathematical statistics evolve, I have had cause to wonder and to doubt. … All in all, I have come to feel that my central interest is in data analysis, which I take to include, among other things: procedures for analyzing data, techniques for interpreting the results of such procedures, ways of planning the gathering of data to make its analysis easier, more precise or more accurate, and all the machinery and results of (mathematical) statistics which apply to analyzing data. It deeply shocked his academic readers. Aren’t you supposed to present something mathematically precise, such as definitions, theorems and proofs? If we use one sentence to summarize what John said, it is : data analysis is more than mathematics. In Sep 2015, the University of Michigan plans to invest $100 million over the next five years in a new Data Science Initiative that will enhance opportunities for student and faculty researchers across the university to tap into the enormous potential of big data. U-M Provost Martha Pollack said: “Data science has become a fourth approach to scientific discovery, in addition to experimentation, modeling and computation,…” How they define Data science here? The web site for DSI gives us an idea what Data Science is: “This coupling of scientific discovery and practice involves the collection, management, processing, analysis, visualization, and interpretation of vast amounts of heterogeneous data associated with a diverse array of scientific, translational, and interdisciplinary applications.” With the data science hype picking up stream, many professionals changed their titles to Data Scientist without any of the necessary qualifications. But at that time, data scientist title was not well defined which lead to confusion in the market, obfuscation in resumes and exaggeration of skills. Here is a list of definitions for a “data scientist”: “A data scientist is a data analyst who lives in California” “A data scientist is someone who is better at statistics than any software engineer and better - at software engineering than any statistician.” “A data scientist is a statistician who lives in San Francisco.” “Data Science is statistics on a Mac.” There is lots of confusion around Data Scientist, Statistician, Business/Financial/Risk(etc) Analyst, BI professional……It is because the obvious intersections among those. Now I see data science as a discipline to make sense of data. In order to make sense of data, statistics is an indispensable part. Meanwhile a data scientist needs many other skills. In the obscenity case of Jacobellis v. Ohio (1964), Potter Stewart wrote in his short concurrence that “hard-core pornography” was hard to define, but that “I know it when I see it.” This applies to many things including data science. It is hard to define but you know it when you see it. So instead of scratching my head to figure out one sentence definition, I am going to sketch the history of data science, what kind of questions data science can answer, and describe the skills required for being a data scientist. Hope this can give you a better depiction of data science. In the early 19th century when Legendre and Gauss came up the least square method for linear regression, only the physicists would use it to fit linear regression. Now, even non-technical people can fit linear regression using excel in 5 secs. In 1936 Fisher came up linear discriminant analysis. In 1940s, we had another widely used model – logistic regression. In 1970s, Nelder and Wedderburn formulated “generalized linear model (GLM)” which generalized linear regression by allowing the linear model to be related to the response variable via a link function and by allowing the magnitude of the variance of each measurement to be a function of its predicted value. By the end of 1970s, there were a range of analytical models while most of them were linear because the computer techniques were not advanced enough to fit non-linear model until 1980s. In 1984 Breiman et al. introduced classification and regression tree (CART) which is one of the oldest and most utilized classification and regression techniques. After that Ross Quinlan came up with more tree algorithms such as ID3, C4.5 and C5.0. In the 1990s, ensemble techniques (methods that combine many models’ predictions) began to appear. Bagging is a general approach that uses bootstrapping in conjunction with any regression or classification model to construct an ensemble. Based on the ensemble idea, Breiman came up random forest in 2001. Later, Yoav Freund and Robert Schapire came up AdaBoost.M1 algorithm. Benefiting from the increasing availability of digitized information, and the possibility to distribute that via the internet, the tool box has been expanding fast. The applications include business, health, biology, social science, politics etc. John Tukey identified 4 forces driving data analysis (there was no “data science” then): The formal theories of math/stat Acceleration developments in computers and display devices The challenge, in many fields, of more and ever larger bodies of data The emphasis on quantification in an ever wider variety of disciplines John’s 1962 list is surprisingly modern. Let’s inspect those points in today’s context. There is always a time gap between a theory and its application. We had the theories much earlier than application. Fortunately, for the past 50 years statisticians have been laying the theoretical groundwork for constructing “data science” today. The development of computer science enables us to calculate much faster and deliver results in a friendly and intuitive way. The striking transition to the internet of things generates vast amounts of commercial data. Industries have also sensed the value of exploiting that data. Data science seems certain to be a major preoccupation of commercial life in coming decades. All the four forces John identified exist today which have been driving data science. 2.2 Is it science? Totally? Let’s take one step back. What is science? Here is what John Tukey said: There are diverse views as to what makes a science, but three constituents will be judged essential by most, viz: (a1) intellectual content, (a2) organization in an understandable form, (a3) reliance upon the test of experience as the ultimate standard of validity The first one (a1) doesn’t provide useful information. And (a2) can’t distinguish science from art very well. The last one is a key character of science. The influential philosopher of science Karl Popper argued that science advances by falsifying hypotheses. If science needs to be falsifiable, then data science is not 100% science. It is true that there are some analytical results that can be validated (falsified) through cross validation or comparing prediction with future outcomes. But certainly not all of them. Even in the problem of prediction, we can’t validate it in 2nd order chaos system. We can’t scientifically validate many of the unsupervised learning or descriptive analysis, especially in the context of marketing. In that sense, data science is a combination of science and art. There is another definition of science from the famous computer scientist Donald Knuth. He said in his legendary 1974 essay Computer Programming as an Art: “Science is knowledge which we understand so well that we can teach it to a computer.” Computer is an indispensable part of data science. But can we teach computer to do all the work data scientists do today? No. So it is not totally science. Computer can’t communicate with stakeholders to transform a real life problem to be data problem. Computer doesn’t know which question can be answered through analytics. Computer doesn’t know how to explain the results to different audiences using different ways according to their backgrounds. Computer is powerful, very powerful in many ways but certainly not all. Would computer enter a ‘runaway reaction’ of self-improvement cycles so that they could surpass human in every way in the future? Well, that is not a question we are trying to answer here. If you are interested in the future of technology, there are some books you can refer to. Ray Kurzweil (The Singularity Is Near), Yuval Noah Harari (Homo Deus: A Brief History of Tomorrow) and Kevin Kelly (The Inevitable). In spite of being short-sighted, we will assume it won’t happen in my foreseeable future. To be simple I will still use data science in the rest of the book. But it is important to realize that data science includes art. 2.3 What kind of questions can data science solve? 2.3.1 What is a good question? Specific How can we increase sales? Dose the January campaign on product X increase the amount of purcahse from our 2017 retained customers? Data Representative Relevant Quality 2.3.2 Types of questions Comparison Description Clustering Classification Regression (linear/non-linear, parametric/nonparametric) 2.4 What are the required skills for data scientist? We talked about the bewildering definitions of data scientist. With the data science hype picking up stream, some professionals had begun changing their titles to Data Scientist without any of the necessary qualifications (see “Data Scientists…or Data Wannabes”). knitr::include_graphics(&quot;http://scientistcafe.com/book/Figure/SkillEN.png&quot;) 2.4.1 Types of Learning knitr::include_graphics(&quot;http://scientistcafe.com/book/Figure/LearningStyles.png&quot;) 2.4.2 Types of Algorithm knitr::include_graphics(&quot;http://scientistcafe.com/book/Figure/AlogrithmTypes.png&quot;) Uncertainty: Partial knowledge of state of the world: such as income, social media behavior, competitor’s offer Noisy observations: missing information, measurement with error (food taken), self-justification bias (nobody watch the cat video……) Phenomena not covered by our model: linear assumption, normal assumption Inherent stochasticity: even at a higher level, the modeling limitations of complicated systems are such that one might as well view the world as inherently stochastic. 2.5 General Process of Data Science knitr::include_graphics(&quot;http://scientistcafe.com/book/Figure/GeneralProcessEN.png&quot;) 2.6 Cloud-based computation environment "],
["big-data-cloud-platform.html", "Chapter 3 Big Data Cloud Platform 3.1 Introduction of Databricks cloud-based distributed system 3.2 Linux system and Hadoop environment 3.3 Database basic through Hive 3.4 Spark and H2O", " Chapter 3 Big Data Cloud Platform 3.1 Introduction of Databricks cloud-based distributed system 3.2 Linux system and Hadoop environment 3.3 Database basic through Hive 3.4 Spark and H2O "],
["introduction-to-the-data.html", "Chapter 4 Introduction to the data 4.1 Customer data for clothing company 4.2 Customer Satisfaction Survey Data from Airline Company 4.3 Swine Disease Breakout Data", " Chapter 4 Introduction to the data Before tackling analytics problem, we start by creating data to be analyzed in later chapters. Why do we simulate data instead of using real data set? Going through the simulation code helps you practice R skills. It makes the book less dependent on vagaties of finding and downloading online data set It allows you manipulate the synthetic data, run analysis and examine how the results change 4.1 Customer data for clothing company 我们先模拟一个关于某品牌服装消费者的数据，这个数据会在之后的章节中反复用到。数据中包含N=1000个观测，我们将模拟3类变量（括号内是变量对应的模拟数据框中的列标签名）： （1）人口统计学变量。 年龄（age） 性别（gender） 有房还是租房（house） （2）消费者行为变量。 2015年实体店购买该品牌服装花销（store_exp） 2015年在线购买该品牌服装花销（online_exp） 2015年实体店交易次数（store_trans） 2015年在线交易次数（online_trans） （3）客户认知问卷调查。为了进一步了解消费者，商家时常对消费者进行问卷调查，然后对调查结果进行分组，其目标是寻找在产品兴趣，市场参与度或营销反应的重要方面有显著差异的客户群。通过了解组间的不同，市场营销人员可以优化产品定位，进行更加精准的营销。这里我们假设该服装品牌对消费者进行了下面的调查，并模拟该调查问卷的回复。 你是否同意下面的申明？ 问题 1（非常不同意） 2（有点不同意） 3（中立/不知道） 4（有点同意） 5（非常同意） （Q1）：我喜欢买不同品牌的服装，比较它们 （Q2）：我喜欢买同一个品牌的服装 （Q3）：品牌的知名度对我来说非常重要 （Q4）：服装质量对我来说非常重要 （Q5）：我有特定喜欢的风格 （Q6）：我喜欢在实体店购买 （Q7）：我喜欢在网上购买 （Q8）：价格对我来说很重要 （Q9）：我喜欢不同风格的衣服 （Q10）：我喜欢自己挑选服装，不需要周围人的建议 我们进一步假设这些根据问卷调查的结果可以将消费者分成4组：价格敏感（Price），炫耀性消费（Conspicuous），质量（Quality），风格（Style）。 （本章我们不会提到如何得到这些分组；我们假设这些已知。我们会在第9章中介绍聚类分析时会更详细的说明。） 你也可以重复下面的代码，自己创建该数据。我们强烈建议读者重复数据模拟的过程，这样能加深对模型方法的理解。如果你对此不感兴趣，也可以从本书网站上直接下载数据： sim.dat&lt;-read.csv(&quot;https://raw.githubusercontent.com/happyrabbit/DataScientistR/master/Data/SegData.csv&quot;) 获得该数据集后你可以跳过本小节后半部分直接跳到下一节对分析流程的讲解。否则，继续本节。 模拟该数据的过程有些复杂，我们先模拟描述客户的变量。模拟该数据的代码分为3部分： 定义数据结构：定义变量名，变量类型，消费者分组名，各组大小。 变量分布参数，如各自的均值和方差。 在各组和各个变量上迭代，基于定义和参数设置抽取随机数。 通过这种方式组织代码，如果我们要改变部分模拟方式重新抽取数据就比较容易。例如，如果我们想要加一个组，或者改变其中某个人口统计变量的均值，只要稍微改变代码就好。我们也想通过这个结构介绍新的R代码，生成数据的第3个步骤中将用到这些代码。 # 设置随机种子，使数据模拟过程可重复 set.seed(12345) # 定义观测数目 ncust&lt;-1000 # 建立数据框存放模拟观测，初始数据框中只有一列id，即消费者编号 seg_dat&lt;-data.frame(id=as.factor(c(1:ncust))) # 指定要生成的变量，并为变量命名 vars&lt;-c(&quot;age&quot;,&quot;gender&quot;,&quot;income&quot;,&quot;house&quot;,&quot;store_exp&quot;,&quot;online_exp&quot;,&quot;store_trans&quot;,&quot;online_trans&quot;) # 每个变量对应的数据类型 # norm： 正态分布 # binom: 二项分布 # pois： 泊松分布 vartype&lt;-c(&quot;norm&quot;,&quot;binom&quot;,&quot;norm&quot;,&quot;binom&quot;,&quot;norm&quot;,&quot;norm&quot;,&quot;pois&quot;,&quot;pois&quot;) # 四个消费者分组的名称 group_name&lt;-c(&quot;Price&quot;,&quot;Conspicuous&quot;,&quot;Quality&quot;,&quot;Style&quot;) # 各消费者群组的大小 group_size&lt;-c(250,200,200,350) # group_name和group_size的第一个元素表明，对于“Price”这组消费者，我们将模拟N=250个观测。 定义好了数据的基本结构之后，我们下一步是定义分布参数，用这些参数来抽取相应数据。 这里我们要 模拟的数据有4个样本类，8个非抽样调查变量，因此我们创建一个4×8的均值矩阵，因为不同类别的 消费者对应不同的分布参数。下面代码用来创建均值矩阵： # 定义均值矩阵 mus &lt;- matrix( c( # 价格敏感（Price）类对应均值 60, 0.5, 120000,0.9, 500,200,5,2, # 炫耀性消费（Conspicuous）类对应均值 40, 0.7, 200000,0.9, 5000,5000,10,10, # 质量（Quality）类对应均值 36, 0.5, 70000, 0.4, 300, 2000,2,15, # 风格（Style）类对应均值 25, 0.2, 90000, 0.2, 200, 2000,2,20), ncol=length(vars), byrow=TRUE) 具体过程是怎样的？ 均值矩阵mus指定，例如，价格敏感（Price）类群体的第一个变量（这里是年龄age）均值为60，炫耀性消费（Conspicuous）类群体的年龄均值为40依次类推。正态分布变量需要指定均值和方差，如年龄（age）， 实体店花销（store_exp）和在线花销（online_exp）。对于二项分布（只有两个可能取值）和泊松分布变量，我们只需要规定均值。其中，性别（gender），有房还是租房（house）是二项数据，生成这样的数据需要指定得到其中某一观测值的概率，比如矩阵mus中。实体店交易次数（store_trans）和线交易次数（online_trans）是泊松变量（频数），泊松分布只有一个参数——分布均值。所以在下面的标准差矩阵sds中，非正态分布变量对应的标准差为缺失值NA。（注意这里我们只是用这些分布为例生成数据，并不意味着这些是最好的拟合变量观测的分布。例如，真实的收入数据更可能是一个有偏的分布而非正态）。 下面我们对正态分布变量创建标准差矩阵： # 每类的标准差 (NA = 标准差无定义) sds&lt;- matrix( c( # 价格敏感（Price）类对应均值 3,NA,8000,NA,100,50,NA,NA, # 炫耀性消费（Conspicuous）类对应均值 5,NA,50000,NA,1000,1500,NA,NA, # 质量（Quality）类对应均值 7,NA,10000,NA,50,200,NA,NA, # 风格（Style）类对应均值 2,NA,5000,NA,10,500,NA,NA), ncol=length(vars), byrow=TRUE) 将这两个矩阵加在一起我们就能够完全定义各个类的分布了。例如，我们来看下每个矩阵的第1行，其代表第1类群体（价格敏感）的分布参数。这些值规定该类群体的年龄（age）均值为60（见第一个矩阵第1行第1列），标准差为3（第二个矩阵第1行第1列）。另外，其中大约有50%的男性（第一个矩阵第1行第2列），年收入（income）均值为120000元，标准差为8000元。将这些设置分开存在不同表格中，将来想要修改十分容易。将数据定义和抽样过程分开是个很好的习惯。下面开始抽取数据： # 抽取非抽样调查数据 sim.dat&lt;-NULL set.seed(2016) # 对消费者类别进行循环（i） for (i in seq_along(group_name)){ # 为了核实代码，展示循环运行过程，我们在循环中添加了这样一行代码 # 函数运行时会打印出正在抽取的样本类名 cat (i, group_name[i],&quot;\\n&quot;) # 创建一个空矩阵用于存放该类消费者相关数据 seg&lt;-data.frame(matrix(NA,nrow=group_size[i], ncol=length(vars))) # 在这个类之内，对不同变量迭代，抽取相应的随机数据 for (j in seq_along(vars)){ # 在每个变量上迭代 if (vartype[j]==&quot;norm&quot;){ # 抽取正态分布变量 seg[,j]&lt;-rnorm(group_size[i], mean=mus[i,j], sd=sds[i,j]) } else if (vartype[j]==&quot;pois&quot;) { # 抽取泊松分布变量 seg[,j]&lt;-rpois(group_size[i], lambda=mus[i,j]) } else if (vartype[j]==&quot;binom&quot;){ # 抽取二项分布变量 seg[,j]&lt;-rbinom(group_size[i],size=1,prob=mus[i,j]) } else{ # 如果变量类型不是上述几种，程序停止运行并提示信息 stop (&quot;Don&#39;t have type:&quot;,vartype[j]) } } # 将该消费者类的数据依行添加到总数据集 sim.dat&lt;-rbind(sim.dat,seg) } 上面的代码是随机抽样的主要过程，其中cat()函数使得循环运行时会打印出正在抽取的样本类名，最后得到的sim.dat是初始描述客户的变量部分的数据，在对数据进行润色前，提醒大家注意两个关于R的技巧： 第一、在i循环内，我们事先定义一个有着相应行数和列数的没有元素值的数据框seg，之后每迭代一次就将样本赋值到事先定义的seg的特定行。这么做的原因是由于只要R在某个对象上添加东西——如在数据框上增加一行——它都会将原对象拷贝一份。这将使用两倍的内存，减慢运行速度。通过这种方法可以避免对内存的浪费。对这里的小数据可能感觉不出差别，但对于大数据，运行速度会有极大不同。 第二、对循环指针范围的设定用的是seq_along()而非1:length()。这是为了够避免一些常见的错误，如指针向量长度为0或者不经意将向量方向弄反了。 之后我们对描述客户的这部分数据进行完善，添加合适的列标签，将二项（0/1）变量转化为贴有标签的因子变量。 # 指定数据框的列名为我们定义的变量名 names(sim.dat)&lt;-vars # 加上一个因子列表明每个观测的对应的消费者类别 sim.dat$segment&lt;-factor(rep(group_name,times=group_size)) # 将二项变量转化为贴有标签的因子变量 # Female: 女性 # Male: 男性 sim.dat$gender&lt;-factor(sim.dat$gender, labels=c(&quot;Female&quot;,&quot;Male&quot;)) sim.dat$house&lt;-factor(sim.dat$house, labels=c(&quot;No&quot;,&quot;Yes&quot;)) # 假设在线购买和在实体店购买的次数至少为1，所以这里在原随机值上加1 sim.dat$store_trans&lt;-sim.dat$store_trans+1 sim.dat$online_trans&lt;-sim.dat$online_trans+1 # 年龄为整数 sim.dat$age&lt;-floor(sim.dat$age) 真实市场营销数据往往没有这么干净，数据缺失，以及错误输入等问题常常发生。我们最后对模拟的数据做一点“破坏”，使其更像真实的数据。我们假设一些人不愿意给出关于收入（income）的信息。我们建立一个逻辑变量idxm，然后将逻辑变量idxm值为真的对应位置消费者收入观测设为缺失值NA（R用NA表示缺失值）。我们假设年龄（age）越大的消费者对应缺失值的概率越大： # 加入缺失值 idxm &lt;- as.logical(rbinom(ncust, size=1, prob=sim.dat$age/200)) sim.dat$income[idxm]&lt;-NA 真实的数据中可能有错误的输入，或者离群点： # 错误输入，离群点 set.seed(123) idx&lt;-sample(1:ncust,5) sim.dat$age[idx[1]]&lt;-300 sim.dat$store_exp[idx[2]]&lt;- -500 sim.dat$store_exp[idx[3:5]]&lt;-c(50000,30000,30000) 到目前为止我们已经建立了一部分数据，你可以通过summary(sim.dat)检查数据。下面我们接着抽取问卷调查回复数据。我们先通过rnorm()生成正态分布随机数。但从上面的问卷调查表格中可以看到，这是一个1-5分量级的问卷，1代表非常不同意，5代表非常同意。于是接下来我们通过floor()函数将连续值转化成离散整数。 # 抽取问卷调查回复 # 问卷问题数目 nq&lt;-10 # 各类消费者对问卷回复的正态分布均值矩阵 mus2 &lt;- matrix( c( # 价格敏感（Price）类对应均值 5,2,1,3,1,4,1,4,2,4, # 炫耀性消费（Conspicuous）类对应均值 1,4,5,4,4,4,4,1,4,2, # 质量（Quality）类对应均值 5,2,3,4,3,2,4,2,3,3, # 风格（Style）类对应均值 3,1,1,2,4,1,5,3,4,2), ncol=nq, byrow=TRUE) # 方差假设都是0.2 sd2&lt;-0.2 sim.dat2&lt;-NULL set.seed(1000) # 对消费者类别进行循环（i） for (i in seq_along(group_name)){ # 为了核实代码，展示循环运行过程，我们在循环中添加了这样一行代码 # 函数运行时会打印出正在抽取的样本类名，这里不再显示输出 # cat (i, group_name[i],&quot;\\n&quot;) # 创建一个空矩阵用于存放该类消费者相关数据 seg&lt;-data.frame(matrix(NA,nrow=group_size[i], ncol=nq)) # 在这个类之内，对不同变量迭代，抽取相应的随机数据 for (j in 1:nq){ # 抽取正态分布变量 res&lt;-rnorm(group_size[i], mean=mus2[i,j], sd=sd2) # 设置上下限度 res[res&gt;5]&lt;-5 res[res&lt;1]&lt;-1 # 通过 floor()函数将连续值转化成离散整数。 seg[,j]&lt;-floor(res) } # 将该消费者类的数据添加到总数据集 sim.dat2&lt;-rbind(sim.dat2,seg) } # 为数据框添加列标签 names(sim.dat2)&lt;-paste(&quot;Q&quot;,1:10,sep=&quot;&quot;) # 合并两部分数据 sim.dat&lt;-cbind(sim.dat,sim.dat2) # 加上一个因子列表明每个观测的对应的消费者类别 sim.dat$segment&lt;-factor(rep(group_name,times=group_size)) 至此为止我们得到了需要的数据集。让我们检查一下抽取的数据集： str(sim.dat,vec.len=3) ## &#39;data.frame&#39;: 1000 obs. of 19 variables: ## $ age : int 57 63 59 60 51 59 57 57 ... ## $ gender : Factor w/ 2 levels &quot;Female&quot;,&quot;Male&quot;: 1 1 2 2 2 2 2 2 ... ## $ income : num 120963 122008 114202 113616 ... ## $ house : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 2 2 2 2 2 2 2 2 ... ## $ store_exp : num 529 478 491 348 ... ## $ online_exp : num 304 110 279 142 ... ## $ store_trans : int 2 4 7 10 4 4 5 11 ... ## $ online_trans: int 2 2 2 2 4 5 3 5 ... ## $ Q1 : int 4 4 5 5 4 4 4 5 ... ## $ Q2 : int 2 1 2 2 1 2 1 2 ... ## $ Q3 : int 1 1 1 1 1 1 1 1 ... ## $ Q4 : int 2 2 2 3 3 2 2 3 ... ## $ Q5 : int 1 1 1 1 1 1 1 1 ... ## $ Q6 : int 4 4 4 4 4 4 4 4 ... ## $ Q7 : int 1 1 1 1 1 1 1 1 ... ## $ Q8 : int 4 4 4 4 4 4 4 4 ... ## $ Q9 : int 2 1 1 2 2 1 1 2 ... ## $ Q10 : int 4 4 4 4 4 4 4 4 ... ## $ segment : Factor w/ 4 levels &quot;Conspicuous&quot;,..: 2 2 2 2 2 2 2 2 ... 可以看到，服装消费者数据有1000个观测，19个变量。前8个变量是关于样本的人口统计学和购买行为描述。Q1-Q10是关于消费者选择偏好的问卷调查回复，问卷分值量表为1-5分（最常见的市场调查设计）。最后一列是消费者类别，样本观测的模拟是根据消费者类别进行的，因此这些可以当作“真实”的消费者类别。使用随机模拟的一个重要优点就是能够通过这种方式验证模型的效果。而实际生活中的数据样本真正所属类别通常是未知的。我们在之后对聚类和判别分析进行介绍的时候会使用样本类别信息。下面我们会反复用该数据集为例。 4.2 Customer Satisfaction Survey Data from Airline Company 这一小节我们模拟一个航空公司满意度调查数据。数据中包含N=1000个受访者，每个受访者基于最近一次航班体验对3个航空公司进行评分，问卷调查一共15项，每项评分从1-9，分值越大满意度越高。这15个调查项分为4类（括号中为相应数据集中的变量名）： 购票体验 购票容易度（Easy_Reservation） 座椅选择（Preferred_Seats） 航班选择（Flight_Options） 票价（Ticket_Prices） 机舱设施 座椅舒适度（Seat_Comfort） 位置前后空间（Seat_Roominess） 随机行李存放（Overhead_Storage） 机舱清洁（Clean_Aircraft） 空航服务 礼貌（Courtesy） 友善（Friendliness） 能够提供需要的帮助（Helpfulness） 食物饮料服务（Service） 总体指数 总体满意度（Satisfaction） 再次选择次航空公司（Fly_Again） 向朋友推荐此航空公司（Recommend） # 先建立因子载荷矩阵 # 其中前12项符合双因子结构，因为每项对应一个总体因子载荷和某特定因子的载荷 # 比如购票容易度对应总体因子载荷0.33，对因特定购票因子载荷0.58 # 我们可以将结果评分看成是总体因子和特定因子共同作用的结果 loadings &lt;- matrix(c ( # 购票体验 .33, .58, .00, .00, # 购票容易度 .35, .55, .00, .00, # 座椅选择 .30, .52, .00, .00, # 航班选择 .40, .50, .00, .00, # 票价 # 机舱设施 .50, .00, .55, .00, # 座椅舒适度 .41, .00, .51, .00, # 位置前后空间 .45, .00, .57, .00, # 随机行李存放 .32, .00, .54, .00, # 机舱清洁 # 空航服务 .35, .00, .00, .50, # 礼貌 .38, .00, .00, .57, # 友善 .60, .00, .00, .50, # 能够提供需要的帮助 .52, .00, .00, .58, # 食物饮料服务 # 总体指数 .43, .10, .30, .30, # 总体满意度 .35, .50, .40, .20, # 再次选择次航空公司 .25, .50, .50, .20), # 向朋友推荐此航空公司 nrow=15,ncol=4, byrow=TRUE) # 将载荷矩阵乘以它的转秩，然后将对角线元素设置为1得到相关矩阵 cor_matrix&lt;-loadings %*% t(loadings) # Diagonal set to ones. diag(cor_matrix)&lt;-1 # 我们通过mvtnorm包模拟有特定相关矩阵的数据集 library(mvtnorm) # 设置3个航空公司对应的评分均值向量 mu1=c(5,6,5,6, 7,8,6,7, 5,5,5,5, 6,6,6) mu2=c(3,3,2,3, 5,4,5,6, 8,8,8,8, 3,3,3) mu3=c(2,2,2,2, 8,8,8,8, 8,8,8,8, 8,8,8) #设置随机种子 set.seed(123456) # 受访者ID resp.id &lt;- 1:1000 library(MASS) rating1 &lt;- mvrnorm(length(resp.id), mu=mu1, Sigma=cor_matrix) rating2 &lt;- mvrnorm(length(resp.id), mu=mu2, Sigma=cor_matrix) rating3 &lt;- mvrnorm(length(resp.id), mu=mu3, Sigma=cor_matrix) # 将分值限定在1到9之间 rating1[rating1&gt;9]&lt;-9 rating1[rating1&lt;1]&lt;-1 rating2[rating2&gt;9]&lt;-9 rating2[rating2&lt;1]&lt;-1 rating3[rating3&gt;9]&lt;-9 rating3[rating3&lt;1]&lt;-1 # 将分值转化为整数 rating1&lt;-data.frame(round(rating1,0)) rating2&lt;-data.frame(round(rating2,0)) rating3&lt;-data.frame(round(rating3,0)) rating1$ID&lt;-resp.id rating2$ID&lt;-resp.id rating3$ID&lt;-resp.id rating1$Airline&lt;-rep(&quot;AirlineCo.1&quot;,length(resp.id)) rating2$Airline&lt;-rep(&quot;AirlineCo.2&quot;,length(resp.id)) rating3$Airline&lt;-rep(&quot;AirlineCo.3&quot;,length(resp.id)) rating&lt;-rbind(rating1,rating2,rating3) # 为数据集的各列命名 names(rating)&lt;-c( &quot;Easy_Reservation&quot;, &quot;Preferred_Seats&quot;, &quot;Flight_Options&quot;, &quot;Ticket_Prices&quot;, &quot;Seat_Comfort&quot;, &quot;Seat_Roominess&quot;, &quot;Overhead_Storage&quot;, &quot;Clean_Aircraft&quot;, &quot;Courtesy&quot;, &quot;Friendliness&quot;, &quot;Helpfulness&quot;, &quot;Service&quot;, &quot;Satisfaction&quot;, &quot;Fly_Again&quot;, &quot;Recommend&quot;, &quot;ID&quot;, &quot;Airline&quot;) 让我们检查一下抽取的数据集： str(rating,vec.len=3) ## &#39;data.frame&#39;: 3000 obs. of 17 variables: ## $ Easy_Reservation: int 6 5 6 5 4 5 6 4 ... ## $ Preferred_Seats : int 5 7 6 6 5 6 6 6 ... ## $ Flight_Options : int 4 7 5 5 3 4 6 3 ... ## $ Ticket_Prices : int 5 6 6 5 6 5 5 5 ... ## $ Seat_Comfort : int 5 6 7 7 6 6 6 4 ... ## $ Seat_Roominess : int 7 8 6 8 7 8 6 5 ... ## $ Overhead_Storage: int 5 5 7 6 5 4 4 4 ... ## $ Clean_Aircraft : int 7 6 7 7 7 7 6 4 ... ## $ Courtesy : int 5 6 6 4 2 5 5 4 ... ## $ Friendliness : int 4 6 6 6 3 4 5 5 ... ## $ Helpfulness : int 6 5 6 4 4 5 5 4 ... ## $ Service : int 6 5 6 5 3 5 5 5 ... ## $ Satisfaction : int 6 7 7 5 4 6 5 5 ... ## $ Fly_Again : int 6 6 6 7 4 5 3 4 ... ## $ Recommend : int 3 6 5 5 4 5 6 5 ... ## $ ID : int 1 2 3 4 5 6 7 8 ... ## $ Airline : Factor w/ 3 levels &quot;AirlineCo.1&quot;,..: 1 1 1 1 1 1 1 1 ... 4.3 Swine Disease Breakout Data 本小节中我们将模拟一个生猪疫情数据。假设研究人员对800个养猪场进行和某生猪疫情有关的问卷调查，问卷由120个问题组成。每个问题有3个可能选项。目的是根据问卷调查回复得到每个养猪场在未来爆发疫情的概率。每个养猪场在问卷问题的3个可选项中等概率选择。第\\(i\\)个养猪场对应的疫情爆发概率服从\\(Bernoulli(1,p_{i})\\)分布。其中 \\[ln(\\frac{p_{i}}{1-p_{i}})=\\beta_{0}+\\sum_{g=1}^{G}\\mathbf{x_{i,g}^{T}}\\beta_{g}\\] \\(\\beta_{0}\\)是截距项，\\(\\mathbf{x_{i,g}}\\)是第\\(i\\)观测对应第\\(g\\)个问题的回复。这里将问题回复转化为0/1虚拟变量，因为每个问题有3个可能选项，所以\\(\\mathbf{x_{i,g}}\\)是一个取值为0/1的含有三个元素的向量。\\(\\mathbf{\\beta_{g}}\\)是对应的参数。 我们在这里考虑3类问题。第1类（问题1到问题40）问题中有两个选项对应变量有预测能力。第2类（问题41到问题80）问题中只有一个选项对结果有预测能力。第3类（问题81到问题120）对结果预测没有帮助，也就是我们希望能够去除的变量。模拟数据的参数设置如下： \\[\\mathbf{\\beta^{T}}=\\left(\\underset{question\\ 1}{\\frac{40}{3},\\underbrace{1,0,-1}},...,\\underset{question\\ 40}{\\underbrace{1,0,-1}},\\underset{question\\ 41}{\\underbrace{1,0,0}},...,\\underset{question\\ 80}{\\underbrace{1,0,0}},\\underset{question\\ 81}{\\underbrace{0,0,0}},...,\\underset{question\\ 120}{\\underbrace{0,0,0}}\\right)*\\gamma\\] 这里我们通过设置5个\\(\\gamma\\)值（\\(\\gamma \\in \\{0.1,0.25,0.5,1,2\\}\\) ）模拟了5种参数情况下的数据。 \\(\\gamma\\)越大，参数值越大，也就意味着有效问题对结果的预测性越强。对于每个参数设定模拟了20个数据集，之后我们会以这些数据为例展示不同模型变量选择的效果。模拟多个数据集是为了研究一些估值的稳定性。 # sim1_da1.csv 模拟的第一个数据集 # similar sim1_da2 and sim1_da3 # sim1.csv simulated data, the first simulation # dummy.sim1.csv dummy variables for the first simulated data with all the baseline in #code for simulation # setwd(dirname(file.choose())) # library(grplasso) nf&lt;-800 for (j in 1:20){ set.seed(19870+j) x&lt;-c(&#39;A&#39;,&#39;B&#39;,&#39;C&#39;) sim.da1&lt;-NULL for (i in 1:nf){ # sample(x, 120, replace=TRUE)-&gt;sam sim.da1&lt;-rbind(sim.da1,sample(x, 120, replace=TRUE)) } data.frame(sim.da1)-&gt;sim.da1 paste(&quot;Q&quot;, 1:120, sep = &quot;&quot;)-&gt;col paste(&quot;Farm&quot;, 1:nf, sep = &quot;&quot;)-&gt;row colnames(sim.da1)&lt;-col rownames(sim.da1)&lt;-row # 用nnet包中的class.ind()函数将问题回复编码为名义变量 library(nnet) dummy.sim1&lt;-NULL for (k in 1:ncol(sim.da1)) { tmp=class.ind(sim.da1[,k]) colnames(tmp)=paste(col[k],colnames(tmp)) dummy.sim1=cbind(dummy.sim1,tmp) } data.frame(dummy.sim1)-&gt;dummy.sim1 # 每个问题对应的3个名义变量中有重复信息 # 将C选项设置为基线回复 # 删除基线名义变量 base.idx&lt;-3*c(1:120) dummy1&lt;-dummy.sim1[,-base.idx] # 对每个r设置依次抽取相应的因变量 # 每次只对一个r值抽取，将其余代码注释掉 # 得到r=0.1 时每个农场对应的连接函数值 c(rep(c(1/10,0,-1/10),40),rep(c(1/10,0,0),40),rep(c(0,0,0),40))-&gt;s1 as.matrix(dummy.sim1)%*%s1-40/3/10-&gt;link1 # r=0.25 # c(rep(c(1/4,0,-1/4),40),rep(c(1/4,0,0),40),rep(c(0,0,0),40))-&gt;s1 # as.matrix(dummy.sim1)%*%s1-40/3/4-&gt;link1 # r=0.5 # c(rep(c(1/2,0,-1/2),40),rep(c(1/2,0,0),40),rep(c(0,0,0),40))-&gt;s1 # as.matrix(dummy.sim1)%*%s1-40/3/2-&gt;link1 # r=1 # c(rep(c(1,0,-1),40),rep(c(1,0,0),40),rep(c(0,0,0),40))-&gt;s1 # as.matrix(dummy.sim1)%*%s1-40/3-&gt;link1 # r=2 # c(rep(c(2,0,-2),40),rep(c(2,0,0),40),rep(c(0,0,0),40))-&gt;s1 # as.matrix(dummy.sim1)%*%s1-40/3/0.5-&gt;link1 # 在连接函数的基础上计算每个农场对应的爆发概率 exp(link1)/(exp(link1)+1)-&gt;hp1 # 基于爆发概率hp1，抽取相应的因变量res res&lt;-rep(9,nf) for (i in 1:nf){ sample( c(1,0),1,prob=c(hp1[i],1-hp1[i]))-&gt;res[i] } # 这里将数据存成3个不同的版本，只是为了之后不同模型使用方便 # 3个数据集都含有所有120个问题的回复，但彼此稍微有不同 # da1 含有因变量，但没有名义变量所属问题的信息 # da2 没有因变量，但最后一行包括的名义变量所属的问题 # da3 没有因变量，没有名义变量所属问题的信息 dummy1$y&lt;-res da1&lt;-dummy1 y&lt;-da1$y ind&lt;-NULL for (i in 1:120){ c(ind,rep(i,2))-&gt;ind } da2&lt;-rbind(da1[,1:240],ind) da3&lt;-da1[,1:240] # 将数据集储存起来 write.csv(da1,paste(&#39;sim&#39;,j,&#39;_da&#39;,1,&#39;.csv&#39;,sep=&#39;&#39;),row.names=F) write.csv(da2,paste(&#39;sim&#39;,j,&#39;_da&#39;,2,&#39;.csv&#39;,sep=&#39;&#39;),row.names=F) write.csv(da3,paste(&#39;sim&#39;,j,&#39;_da&#39;,3,&#39;.csv&#39;,sep=&#39;&#39;),row.names=F) write.csv(sim.da1,paste(&#39;sim&#39;,j,&#39;.csv&#39;,sep=&#39;&#39;),row.names=F) write.csv(dummy.sim1,paste(&#39;dummy.sim&#39;,j,&#39;.csv&#39;,sep=&#39;&#39;),row.names=F) } 要理解这里数据模拟的代码，读者需要了解逻辑回归和分组lasso的理论知识，这超出了本书的范围。这里的代码仅供大家参考。可以重复上面的代码生成相应的数据集。因为这里生成的数据量较大，在网上只有\\(\\gamma=2\\)对应的一次模拟的数据集。我们看下得到的数据集： library(dplyr) disease_dat&lt;-readr::read_csv(&quot;/Users/happyrabbit/Documents/GitHub/DataScientistR/Data/sim1_da1.csv&quot;) # 这里只截取最后的7列 head(subset(disease_dat,select=c( &quot;Q118.A&quot;,&quot;Q118.B&quot;,&quot;Q119.A&quot;,&quot;Q119.B&quot;,&quot;Q120.A&quot;,&quot;Q120.B&quot;,&quot;y&quot;))) 其中最后一列y代表相应农场疫情爆发情况，y=1代表从问卷调查之后5年内有疫情爆发。剩余的列表示农场问卷调查结果，如Q120.A=1对应问卷调查中第120个问题选择A的农场，类似的Q120.B=1对应第120个问题中选择B的农场，我们将选项C作为基准选项。之后我会用这个数据集展示一些相关的模型。 "],
["data-wrangling.html", "Chapter 5 Data Wrangling 5.1 Read and write data 5.2 Summarize data 5.3 Tidy and Reshape Data", " Chapter 5 Data Wrangling This chapter focuses on some of the most frequently used data manipulations and shows how to implement them in R. It is important to explore the data set with descriptive statistics (mean, standard deviation, etc.) and data visualization prior to analysis. Transform data so that the data structure is in line with the requirements of the model. You also need to summarize the results after analysis. Here we assume the readers are already familiar with some of the traditional R data operations, such as subsetting data frame, deleting variables, read and write functions (read.csv (), write.csv (), etc.) in base R. We will also skip some basic descriptive functions in R. For example, for discrete variables, we often use the frequency table to look at the frequency (table ()) of the variable at various levels as needed, or a crosstab of two variables. You can also draw a bar chart for discrete variables (bar()). For continuous variables, we need to look at the mean (mean ()), standard deviation (sd()), quantile (quantile()) of a variable from time to time. There are also functions like summary(), str() and describe() (a functions in the ‘psych’ package) that give a summary of a data frame. The focus here is to introduce some of the more efficient data wrangling methods in R. 5.1 Read and write data You must be familar with read.csv(), read.table() and write.csv() in base R. Here we will introduce a more efficient package from RStudio in 2015 for reading and writing data: readr package. The corresponding functions are read_csv(), read_table() and write_csv(). The commands look quite similar, but readr is different in the following respects: It is 10x faster. The trick is that readr uses C++ to process the data quickly. It doesn’t change the column names. The names can start with number and “.” will not be substitued to “_”. For example: library(readr) print(read_csv(&quot;2015,2016,2017 1,2,3 4,5,6&quot;)) readr functions do not convert strings to factors by default, are able to parse dates and times and can automatically determine the data types in each column. The killing character in my opinion is that readr provides progress bar. What makes you feel worse than waiting is not knowing how long you have to wait. Without “progress bar” might be the No.1 reason that people break up with the one they have been dating. (add progress bar) The major functions of readr is to turn flat files into data frames: read_csv(): reads comma delimited files read_csv2(): reads semicolon separated files (common in countries where , is used as the decimal place) read_tsv(): reads tab delimited files read_delim(): reads in files with any delimiter read_fwf(): reads fixed width files. You can specify fields either by their widths with fwf_widths() or their position with fwf_positions() read_table(): reads a common variation of fixed width files where columns are separated by white space read_log(): reads Apache style log files The good thing is that those functions have similar syntax. Once you learn one, the others become easy. Here we will focus on read_csv(). The most important information for read_csv() is the path to your data: library(readr) sim.dat &lt;- read_csv(&quot;https://raw.githubusercontent.com/happyrabbit/DataScientistR/master/Data/SegData.csv &quot;) head(sim.dat) The function reads the file to R as a tibble. You can consider tibble as next iteration of data frame. They are different with data frame for the following aspects: It never changes an input’s type (i.e., no more stringsAsFactors = FALSE!) It never adjusts the names of variables It has a refined print method that shows only the first 10 rows, and all the columns that fit on screen. You can also control the default print behavior by setting options. Refer to http://r4ds.had.co.nz/tibbles.html for more information about ‘tibble’. When you run read_csv() it prints out a column specification that gives the name and type of each column. In order to better understanding how readr works, it is helpful to type in some baby data set and check the results: dat=read_csv(&quot;2015,2016,2017 100,200,300 canola,soybean,corn&quot;) print(dat) You can also add comments on the top and tell R to skip those lines: dat=read_csv(&quot;# I will never let you know that # my favorite food is carrot Date,Food,Mood Monday,carrot,happy Tuesday,carrot,happy Wednesday,carrot,happy Thursday,carrot,happy Friday,carrot,happy Saturday,carrot,extremely happy Sunday,carrot,extremely happy&quot;, skip = 2) print(dat) If you don’t have column names, set col_names = FALSE then R will assign names “X1”,“X2”… to the columns: dat=read_csv(&quot;Saturday,carrot,extremely happy Sunday,carrot,extremely happy&quot;, col_names=FALSE) print(dat) You can also pass col_names a character vector which will be used as the column names. Try to replace col_names=FALSE with col_names=c(&quot;Date&quot;,&quot;Food&quot;,&quot;Mood&quot;) and see what happen. As mentioned before, you can use read_csv2() to read semicolon separated files: dat=read_csv2(&quot;Saturday; carrot; extremely happy \\n Sunday; carrot; extremely happy&quot;, col_names=FALSE) print(dat) Here “\\n” is a convenient shortcut for adding a new line. You can use read_tsv() to read tab delimited files： dat=read_tsv(&quot;every\\tman\\tis\\ta\\tpoet\\twhen\\the\\tis\\tin\\tlove\\n&quot;, col_names = FALSE) print(dat) Or more generally, you can use read_delim() and assign separating character： dat=read_delim(&quot;THE|UNBEARABLE|RANDOMNESS|OF|LIFE\\n&quot;, delim = &quot;|&quot;, col_names = FALSE) print(dat) Another situation you will often run into is missing value. In marketing survey, people like to use “99” to represent missing. You can tell R to set all observation with value “99” as missing when you read the data: dat=read_csv(&quot;Q1,Q2,Q3 5, 4,99&quot;,na=&quot;99&quot;) print(dat) For writing data back to disk, you can use write_csv() and write_tsv(). The following two characters of the two functions increase the chances of the output file being read back in correctly: Encode strings in UTF-8 Save dates and date-times in ISO8601 format so they are easily parsed elsewhere For example: write_csv(sim.dat, &quot;sim_dat.csv&quot;) For other data types, you can use the following packages: Haven: SPSS, Stata and SAS data Readxl and xlsx: excel data(.xls and .xlsx) DBI: given data base, such as RMySQL, RSQLite and RPostgreSQL, read data directly from the database using SQL Some other useful materials: For getting data from internet, you can refere to the book “XML and Web Technologies for Data Sciences with R”. R data import/export manual rio package：https://github.com/leeper/rio 5.2 Summarize data 5.2.1 apply(), lapply() and sapply() in base R There are some powerful functions to summarize data in base R, such as apply(), lapply() and sapply(). They do the same basic things and are all from “apply” family: apply functions over parts of data. They differ in two important respects: the type of object they apply to the type of result they will return When do we use apply()? When we want to apply a function to margins of an array or matrix. That means our data need to be structured. The operations can be very flexible. It returns a vector or array or list of values obtained by applying a function to margins of an array or matrix. For example you can compute row and column sums for a matrix: ## simulate a matrix x &lt;- cbind(x1 =1:8, x2 = c(4:1, 2:5)) dimnames(x)[[1]] &lt;- letters[1:8] apply(x, 2, mean) ## x1 x2 ## 4.5 3.0 col.sums &lt;- apply(x, 2, sum) row.sums &lt;- apply(x, 1, sum) You can also apply other functions: ma &lt;- matrix(c(1:4, 1, 6:8), nrow = 2) ma ## [,1] [,2] [,3] [,4] ## [1,] 1 3 1 7 ## [2,] 2 4 6 8 apply(ma, 1, table) #--&gt; a list of length 2 ## [[1]] ## ## 1 3 7 ## 2 1 1 ## ## [[2]] ## ## 2 4 6 8 ## 1 1 1 1 apply(ma, 1, stats::quantile) # 5 x n matrix with rownames ## [,1] [,2] ## 0% 1 2.0 ## 25% 1 3.5 ## 50% 2 5.0 ## 75% 4 6.5 ## 100% 7 8.0 Results can have different lengths for each call. This is a trickier example. What will you get? ## Example with different lengths for each call z &lt;- array(1:24, dim = 2:4) zseq &lt;- apply(z, 1:2, function(x) seq_len(max(x))) zseq ## a 2 x 3 matrix typeof(zseq) ## list dim(zseq) ## 2 3 zseq[1,] apply(z, 3, function(x) seq_len(max(x))) lapply() applies a function over a list, data.frame or vector and returns a list of the same length. sapply() is a user-friendly version and wrapper of lapply(). By default it returns a vector, matrix or if simplify = &quot;array&quot;, an array if appropriate. apply(x, f, simplify = FALSE, USE.NAMES = FALSE) is the same as lapply(x, f). If simplify=TRUE, then it will return a data.frame instead of list. Let’s use some data with context to help you better understand the functions. Get the mean and standard deviation of all numerical variables in the data set. # Read data sim.dat&lt;-read.csv(&quot;https://raw.githubusercontent.com/happyrabbit/DataScientistR/master/Data/SegData.csv&quot;) # Get numerical variables sdat&lt;-sim.dat[,!lapply(sim.dat,class)==&quot;factor&quot;] ## Try the following code with apply() function ## apply(sim.dat,class) ## Why is there the error? The data frame sdat only includes numeric columns. Now we can go head and use apply() to get mean and standard deviation for each column: apply(sdat, MARGIN=2,function(x) mean(na.omit(x))) ## age income store_exp online_exp store_trans ## 38.840 113543.065 1356.851 2120.181 5.350 ## online_trans Q1 Q2 Q3 Q4 ## 13.546 3.101 1.823 1.992 2.763 ## Q5 Q6 Q7 Q8 Q9 ## 2.945 2.448 3.434 2.396 3.085 ## Q10 ## 2.320 Here we defined a function using function(x) mean(na.omit(x)). It is a very simple function. It tells R to ignore the missing value when calculating the mean. MARGIN=2 tells R to apply function to each column. It is not hard to guess what MARGIN=1 mean. The result show that the average online expense is much higher than store expense. You can also compare the average scores across different questions. The command to calculate standard deviation is very similar. The only difference is to change mean() to sd(): apply(sdat, MARGIN=2,function(x) sd(na.omit(x))) ## age income store_exp online_exp store_trans ## 16.416818 49842.287197 2774.399785 1731.224308 3.695559 ## online_trans Q1 Q2 Q3 Q4 ## 7.956959 1.450139 1.168348 1.402106 1.155061 ## Q5 Q6 Q7 Q8 Q9 ## 1.284377 1.438529 1.455941 1.154347 1.118493 ## Q10 ## 1.136174 Even the average online expense is higher than store expense, the standard deviation for store expense is much higher than online expense which indicates there are very likely some big/small purchase in store. We can check it quickly: summary(sdat$store_exp) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -500.0 205.0 329.0 1357.0 597.3 50000.0 summary(sdat$online_exp) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 68.82 420.30 1942.00 2120.00 2441.00 9479.00 There are some odd values in store expense. The minimum value is -500 which is a wrong imputation which indicates that you should preprocess data before analyzing it. Checking those simple statistics will help you better understand your data. It then give you some idea how to preprocess and analyze them. How about using lapply() and sapply()? Run the following code and compare the results: lapply(sdat, function(x) sd(na.omit(x))) sapply(sdat, function(x) sd(na.omit(x))) sapply(sdat, function(x) sd(na.omit(x)), simplify = FALSE) 5.2.2 ddply() in plyr package dplyr is a set of clean and consistent tools that implement the split-apply-combine pattern in R. This is an extremely common pattern in data analysis: you solve a complex problem by breaking it down into small pieces, doing something to each piece and then combining the results back together again. [From package description] You may find the description sounds familiar. The package is sort of a wrapper of apply family. We will only introduce the main function ddply(). Because the package has next iteration which is dplyr package. We will introduce dplyr in more details. The reason we still want to spend some time on the older version is because they have similar idea and knowing the lineage will deeper your understanding of the whole family. We will use the same data frame sim.dat to illustrate. Run the following command: library(plyr) ddply(sim.dat,&quot;segment&quot;,summarize, Age=round(mean(na.omit(age)),0), FemalePct=round(mean(gender==&quot;Female&quot;),2), HouseYes=round(mean(house==&quot;Yes&quot;),2), store_exp=round(mean(na.omit(store_exp),trim=0.1),0), online_exp=round(mean(online_exp),0), store_trans=round(mean(store_trans),1), online_trans=round(mean(online_trans),1)) ## segment Age FemalePct HouseYes store_exp online_exp store_trans ## 1 Conspicuous 42 0.32 0.86 4990 4898 10.9 ## 2 Price 60 0.45 0.94 501 205 6.1 ## 3 Quality 35 0.47 0.34 301 2013 2.9 ## 4 Style 24 0.81 0.27 200 1962 3.0 ## online_trans ## 1 11.1 ## 2 3.0 ## 3 16.0 ## 4 21.1 Now, let’s peel the onion in order. The first argument sim.dat is easy. It is the data you want to work on. The second argument &quot;segment&quot; is the column you want to group by. It is a very standard marketing segmentation problem. The final segment is the result you want to get by designing survey, collecting and analyzing data. Here we assume those segments are known and we want to understand how each group of customer look like. It is a common task in segmentation: figuring out a profile. Here we only summarize data by one categorical variable but you can group by multiply variables using ddply(sim.dat, c(&quot;segment&quot;,&quot;house&quot;), .). So the second argument tell the function we want to divide data by customer segment. The third argument summarize tells R the kind of manipulation you want to do which is to summarize data. There are other choices for this argument such as transform (transform data within each group) and subset(subset data within each group). Then the rest commands tell R the exact action. For example, Age=round(mean(na.omit(age)),0) tell R the following things: Calculate the mean of column age ignoring missing value Round the result to the specified number of decimal places Store the result to a new variable named Age The rest of the command above are similar. In the end we calculate the following for each segment: Age: average age for each segment FemalePct: percentage for each segment HouseYes: percentage of people who own a house stroe_exp: average expense in store online_exp: average expense online store_trans: average times of transactions in store online_trans: average times of online transactions There is a lot of information you can draw from those simple averages. Conspicuous: average age is about 40. Target for middle-age wealthy people. 1/3 of them are female and 2/3 are male. They may be good target for candy dad. They buy regardless the price. Almost all of them own house (0.86). It makes us wonder what is wrong with the rest 14%? They may live in Manhattan Price: They are older people, average age 60. Nearly all of them own a house(0.94). They are less likely to purchase online (store_trans=6 while online_trans=3). This is the only group that is less likely to purchase online. Quality: The average age is 35. They are not way different with Conspicuous in terms of age. But they spend much less. The percentages of male and female are similar. They prefer online shopping. More than half of them don’t own a house (0.66). Style: They are young people with average age 24. Majority of them are female (0.81). Most of them don’t own a house (0.73). They are very likely to be digital natives and definitely prefer online shopping. You may notice that Style group purchase more frequently online (online_trans=21) but the expense (online_exp=1962) is not higher. This makes us wondering what is the average expense each time so you have a better idea about the price range the group fall in. The analytical process is aggregated instead of independent steps. What you learn before will help you decide what to do next. Sometimes you also need to go backward to fix something in the previous steps. For example, you may need to check those negative expense value. We continue to use ddply() to calculate the two statistics: ddply(sim.dat,&quot;segment&quot;,summarize,avg_online=round(sum(online_exp)/sum(online_trans),2), avg_store=round(sum(store_exp)/sum(store_trans),2)) ## segment avg_online avg_store ## 1 Conspicuous 442.27 479.25 ## 2 Price 69.28 81.30 ## 3 Quality 126.05 105.12 ## 4 Style 92.83 121.07 Price group has the lowest averaged one time purchasing price. The Conspicuous group will pay the highest price. When we build profile in real life, we will need to look at the survey results too. Those simple data manipulations can provide you lots of information already. As mentioned before, other than “summarize” there are other functions such as “transform” and “subset”. For simplicity, I draw 11 random samples and 3 variables (age, store_exp and segment) from the original data according to the different segments. We will explain stratified sampling later. Here we just do it without explanation. library(caret) set.seed(2016) trainIndex&lt;-createDataPartition(sim.dat$segment,p=0.01,list=F,times=1) examp&lt;-sim.dat[trainIndex,c(&quot;age&quot;,&quot;store_exp&quot;,&quot;segment&quot;)] Now data frame examp only has 11 rows and 3 columns. Let’s look at the function of transform: ddply(examp,&quot;segment&quot;,transform,store_pct=round(store_exp/sum(store_exp),2)) ## age store_exp segment store_pct ## 1 42 6319.0718 Conspicuous 0.55 ## 2 42 5106.4816 Conspicuous 0.45 ## 3 55 595.2520 Price 0.42 ## 4 64 399.3550 Price 0.28 ## 5 64 426.6653 Price 0.30 ## 6 39 362.4795 Quality 0.58 ## 7 35 260.5065 Quality 0.42 ## 8 23 205.6099 Style 0.25 ## 9 24 212.3040 Style 0.26 ## 10 24 202.1017 Style 0.25 ## 11 28 200.1906 Style 0.24 What “transform” does is to transform data within the specified group (segment) and append the result as a new column. Next let’s look at the function of “subset”: ddply(examp,&quot;segment&quot;,subset,store_exp&gt;median(store_exp)) ## age store_exp segment ## 1 42 6319.0718 Conspicuous ## 2 55 595.2520 Price ## 3 39 362.4795 Quality ## 4 23 205.6099 Style ## 5 24 212.3040 Style You get all rows with store_exp greater than its group median. 5.2.3 dplyr package dplyr provides a flexible grammar of data manipulation focusing on tools for working with data frames (hence the d in the name). It is faster and more friendly: It identifies the most important data manipulations and make they easy to use from R It performs faster for in-memory data by writing key pieces in C++ using Rcpp The interface is the same for data frame, data table or database I will illustrate the following functions in order: Display Subset Summarize Create new variable Merge Display tbl_df(): Convert the data to tibble which offers better checking and printing capabilities than traditional data frames. It will adjust output width according to fit the current window. library(dplyr) tbl_df(sim.dat) glimpse(): This is like a transposed version of tbl_df() glimpse(sim.dat) Subset Get rows with income more than 300000: library(magrittr) filter(sim.dat, income &gt;300000) %&gt;% tbl_df() Here we meet a new operator %&gt;%. It is called “Pipe operator” which pipes a value forward into an expression or function call. What you get in the left operation will be the first argument or the only argument in the right operation. x %&gt;% f(y) = f(x, y) y %&gt;% f(x, ., z) = f(x, y, z ) It is an operator from magrittr which can be really beneficial. Look at the following code. Can you tell me what it does? ave_exp &lt;- filter( summarise( group_by( filter( sim.dat, !is.na(income) ), segment ), ave_online_exp = mean(online_exp), n = n() ), n &gt; 200 ) Now look at the identical code using “%&gt;%”: avg_exp &lt;- sim.dat %&gt;% filter(!is.na(income)) %&gt;% group_by(segment) %&gt;% summarise( ave_online_exp = mean(online_exp), n = n() ) %&gt;% filter(n &gt; 200) Isn’t it much more straight forward now? Let’s read it: Delete observations from sim.dat with missing income values Group the data from step 1 by variable segment Calculate mean of online expense for each segment and save the result as a new variable named ave_online_exp Calculate the size of each segment and saved it as a new variable named n Get segments with size larger than 200 You can use distinct() to delete duplicated rows. It is a generalization of unique() from vector to data frame. dplyr distinct(sim.dat) sample_frac() will randomly select some rows with specified percentage. sample_n() can randomly select rows with specified number. dplyr::sample_frac(sim.dat, 0.5, replace = TRUE) dplyr::sample_n(sim.dat, 10, replace = TRUE) slice() will select rows by position: dplyr::slice(sim.dat, 10:15) It is equivalent to sim.dat[10:15,]. top_n() will select the order top n entries: dplyr::top_n(sim.dat,2,income) If you want to select columns instead of rows, you can use select(). The following are some sample codes: # select by column name dplyr::select(sim.dat,income,age,store_exp) # select columns whose name contains a character string dplyr::select(sim.dat, contains(&quot;_&quot;)) # select columns whose name ends with a character string # similar there is &quot;starts_with&quot; dplyr::select(sim.dat, ends_with(&quot;e&quot;)) # select columns Q1,Q2,Q3,Q4 and Q5 select(sim.dat, num_range(&quot;Q&quot;, 1:5)) # select columns whose names are in a group of names dplyr::select(sim.dat, one_of(c(&quot;age&quot;, &quot;income&quot;))) # select columns between age and online_exp dplyr::select(sim.dat, age:online_exp) # select all columns except for age dplyr::select(sim.dat, -age) Summarize The operations here are similar what we did before with apply() and ddply(). dplyr::summarise(sim.dat, avg_online = mean(online_trans)) ## avg_online ## 1 13.546 # apply function anyNA() to each column # you can also assign a function vector such as: c(&quot;anyNA&quot;,&quot;is.factor&quot;) dplyr::summarise_each(sim.dat, funs_(c(&quot;anyNA&quot;))) ## age gender income house store_exp online_exp store_trans online_trans ## 1 FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE ## Q1 Q2 Q3 Q4 Q5 Q6 Q7 Q8 Q9 Q10 segment ## 1 FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE You can use group_by() to indicate the variables you want to group by as before: sim.dat %&gt;% group_by(segment) %&gt;% summarise_each(funs_(c(&quot;anyNA&quot;))) Create new variable mutate() will compute and append one or more new columns: dplyr::mutate(sim.dat, total_exp = store_exp + online_exp) It will apply window function to the columns and return a column with the same length. It is a different type of function as before. # min_rank=rank(ties.method = &quot;min&quot;) # mutate_each() means apply function to each column dplyr::mutate_each(sim.dat, funs(min_rank)) The other similar function is transmute(). The differece is that transmute() will delete the original columns and only keep the new ones. dplyr::transmute(sim.dat, total_exp = store_exp + online_exp) Merge We create two baby data sets to show how the functions work. (x&lt;-data.frame(cbind(ID=c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;),x1=c(1,2,3)))) ## ID x1 ## 1 A 1 ## 2 B 2 ## 3 C 3 (y&lt;-data.frame(cbind(ID=c(&quot;B&quot;,&quot;C&quot;,&quot;D&quot;),y1=c(T,T,F)))) ## ID y1 ## 1 B TRUE ## 2 C TRUE ## 3 D FALSE # join to the left # keep all rows in x left_join(x,y,by=&quot;ID&quot;) ## ID x1 y1 ## 1 A 1 &lt;NA&gt; ## 2 B 2 TRUE ## 3 C 3 TRUE # get rows matched in both data sets inner_join(x,y,by=&quot;ID&quot;) ## ID x1 y1 ## 1 B 2 TRUE ## 2 C 3 TRUE # get rows in either data set full_join(x,y,by=&quot;ID&quot;) ## ID x1 y1 ## 1 A 1 &lt;NA&gt; ## 2 B 2 TRUE ## 3 C 3 TRUE ## 4 D &lt;NA&gt; FALSE # filter out rows in x that can be matched in y # it doesn&#39;t bring in any values from y semi_join(x,y,by=&quot;ID&quot;) ## ID x1 ## 1 B 2 ## 2 C 3 # the opposite of semi_join() # it gets rows in x that cannot be matched in y # it doesn&#39;t bring in any values from y anti_join(x,y,by=&quot;ID&quot;) ## ID x1 ## 1 A 1 There are other functions(intersect(), union() and setdiff()). Also the data frame version of rbind and cbind which are bind_rows() and bind_col(). We are not going to go through them all. You can try them yourself. If you understand the functions we introduced so far. It should be easy for you to figure out the rest. 5.3 Tidy and Reshape Data “Tidy data” represent the information from a dataset as data frames where each row is an observation and each column contains the values of a variable (i.e. an attribute of what we are observing). Depending on the situation, the requirements on what to present as rows and columns may change. In order to make data easy to work with for the problem at hand, in practice, we often need to convert data between the “wide” and the “long” format. The process feels like playing with a dough. There are two commonly used packages for this kind of manipulations: tidyr and reshape2. We will show how to tidy and reshape data using the two packages. By comparing the functions to show how they overlap and where they differ. 5.3.1 reshape2 package It is a reboot of previous package reshape. Why? Here is what I got from Stack Overflow: “reshape2 let Hadley make a rebooted reshape that was way, way faster, while avoiding busting up people’s dependencies and habits.” Take a baby subset of our exemplary clothes consumers data to illustrate: (sdat&lt;-sim.dat[1:5,1:6]) ## age gender income house store_exp online_exp ## 1 57 Female 120963.4 Yes 529.1344 303.5125 ## 2 63 Female 122008.1 Yes 478.0058 109.5297 ## 3 59 Male 114202.3 Yes 490.8107 279.2496 ## 4 60 Male 113616.3 Yes 347.8090 141.6698 ## 5 51 Male 124252.6 Yes 379.6259 112.2372 For the above data sdat, what if we want to have a variable indicating the purchasing channel (i.e. online or in-store) and another column with the corresponding expense amount? Assume we want to keep the rest of the columns the same. It is a task to change data from “wide” to “long”. There are two general ways to shape data: Use melt() to convert an object into a molten data frame, i.e. from wide to long Use dcast() to cast a molten data frame into the shape you want, i.e. from long to wide library(reshape2) (mdat &lt;- melt(sdat, measure.vars=c(&quot;store_exp&quot;,&quot;online_exp&quot;), variable.name = &quot;Channel&quot;, value.name = &quot;Expense&quot;)) ## age gender income house Channel Expense ## 1 57 Female 120963.4 Yes store_exp 529.1344 ## 2 63 Female 122008.1 Yes store_exp 478.0058 ## 3 59 Male 114202.3 Yes store_exp 490.8107 ## 4 60 Male 113616.3 Yes store_exp 347.8090 ## 5 51 Male 124252.6 Yes store_exp 379.6259 ## 6 57 Female 120963.4 Yes online_exp 303.5125 ## 7 63 Female 122008.1 Yes online_exp 109.5297 ## 8 59 Male 114202.3 Yes online_exp 279.2496 ## 9 60 Male 113616.3 Yes online_exp 141.6698 ## 10 51 Male 124252.6 Yes online_exp 112.2372 You melted the data frame sdat by two variables: store_exp and online_exp (measure.vars=c(&quot;store_exp&quot;,&quot;online_exp&quot;)). The new variable name is Channel set by command variable.name = &quot;Channel&quot;. The value name is Expense set by command value.name = &quot;Expense&quot;. You can run a regression to study the effect of purchasing channel: # Here we use all observations from sim.dat mdat&lt;-melt(sim.dat[,1:6], measure.vars=c(&quot;store_exp&quot;,&quot;online_exp&quot;), variable.name = &quot;Channel&quot;, value.name = &quot;Expense&quot;) fit&lt;-lm(Expense~gender+house+income+Channel+age,data=mdat) summary(fit) ## ## Call: ## lm(formula = Expense ~ gender + house + income + Channel + age, ## data = mdat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4208 -821 -275 533 44353 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -9.132e+02 1.560e+02 -5.855 5.76e-09 *** ## genderMale 3.572e+02 1.028e+02 3.475 0.000524 *** ## houseYes -5.687e+01 1.138e+02 -0.500 0.617275 ## income 2.834e-02 1.079e-03 26.268 &lt; 2e-16 *** ## Channelonline_exp 8.296e+02 9.772e+01 8.489 &lt; 2e-16 *** ## age -2.793e+01 3.356e+00 -8.321 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1974 on 1626 degrees of freedom ## (368 observations deleted due to missingness) ## Multiple R-squared: 0.348, Adjusted R-squared: 0.346 ## F-statistic: 173.5 on 5 and 1626 DF, p-value: &lt; 2.2e-16 You can melt() list, matrix, table too. The syntax is similar and we won’t go through every situation. Sometimes we want to convert the data from “long” to “wide”. For example, you want to compare the online and in store expense between male and female based on the house ownership. dcast(mdat, house + gender ~ Channel, sum) ## Using Expense as value column: use value.var to override. ## house gender store_exp online_exp ## 1 No Female 171102.2 583492.4 ## 2 No Male 133130.8 332499.9 ## 3 Yes Female 355320.2 500856.9 ## 4 Yes Male 697297.3 703332.0 In the above code, what is the left side of ~ are variables that you want to group by. The right side is the variable you want to spread as columns. It will use the column indicating value from melt() before. Here is “Expense” . 5.3.2 tidyr package The other package that will do similar manipulations is tidyr. Let’s get a subset to illustrate the usage. library(dplyr) # practice functions we learnt before sdat&lt;-sim.dat[1:5,]%&gt;% dplyr::select(age,gender,store_exp,store_trans) sdat %&gt;% tbl_df() gather() function in tidyr is analogous to melt() in reshape2. The following code will do the same thing as we did before using melt(): library(tidyr) msdat&lt;-tidyr::gather(sdat,&quot;variable&quot;,&quot;value&quot;,store_exp,store_trans) msdat %&gt;% tbl_df() Or if we use the pipe operation, we can write the above code as: sdat%&gt;%gather(&quot;variable&quot;,&quot;value&quot;,store_exp,store_trans) It is identical with the following code using melt(): melt(sdat, measure.vars=c(&quot;store_exp&quot;,&quot;store_trans&quot;), variable.name = &quot;variable&quot;, value.name = &quot;value&quot;) The opposite operation to gather() is spread(). The previous one stacks columns and the latter one spread the columns. msdat %&gt;% spread(variable,value) ## age gender store_exp store_trans ## 1 51 Male 379.6259 4 ## 2 57 Female 529.1344 2 ## 3 59 Male 490.8107 7 ## 4 60 Male 347.8090 10 ## 5 63 Female 478.0058 4 Another pair of functions that do opposite manipulations are separate() and unite(). sepdat&lt;- msdat %&gt;% separate(variable,c(&quot;Source&quot;,&quot;Type&quot;)) sepdat %&gt;% tbl_df() You can see that the function separates the original column “variable” to two new columns “Source” and “Type”. You can use sep= to set the string or regular express to separate the column. By default, it is “_”. The unite() function will do the opposite: combining two columns. It is like the generalization of paste() to data frame. sepdat %&gt;% unite(&quot;variable&quot;,Source,Type,sep=&quot;_&quot;) ## age gender variable value ## 1 57 Female store_exp 529.1344 ## 2 63 Female store_exp 478.0058 ## 3 59 Male store_exp 490.8107 ## 4 60 Male store_exp 347.8090 ## 5 51 Male store_exp 379.6259 ## 6 57 Female store_trans 2.0000 ## 7 63 Female store_trans 4.0000 ## 8 59 Male store_trans 7.0000 ## 9 60 Male store_trans 10.0000 ## 10 51 Male store_trans 4.0000 The reshaping manipulations may be the trickiest part. You have to practice a lot to get familiar with those functions. Unfortunately there is no short cut. "],
["data-pre-processing.html", "Chapter 6 Data Pre-processing 6.1 Start 6.2 Centering and Scaling 6.3 Resolve Skewness 6.4 Resolve Outliers 6.5 Missing Values 6.6 Collinearity 6.7 Sparse Variables 6.8 Re-encode Dummy Variables", " Chapter 6 Data Pre-processing 6.1 Start There are a number of reasons a predictive model falls (Max Kuhn 2013), such as: Inadequate data pre-processing Inadequate model validation Unjustified extrapolation Over-fitting In this blog post, I am going to summarize some common data pre-processing approaches. 6.2 Centering and Scaling It is the most straightforward data transformation. It centers and scales a variable to mean 0 and standard deviation 1. It ensures that the criterion for finding linear combinations of the predictors is based on how much variation they explain and therefore improves the numerical stability. Models involving finding linear combinations of the predictors to explain response/predictors variation need data centering and scaling, such as PCA and PLS. You can easily writing code yourself to conduct this transformation. Or the function preProcess() in package caret can apply this transformation to a set of predictors. #install packages needed library(caret) library(e1071) library(gridExtra) library(lattice) library(imputeMissings) library(RANN) library(corrplot) library(nnet) head(cars) ## speed dist ## 1 4 2 ## 2 4 10 ## 3 7 4 ## 4 7 22 ## 5 8 16 ## 6 9 10 trans&lt;-preProcess(cars,method=c(&quot;center&quot;,&quot;scale&quot;)) transformed&lt;-predict(trans,cars) par(mfrow=c(1,2)) hist(cars$dist,main=&quot;Original&quot;,xlab=&quot;dist&quot;) hist(transformed$dist,main=&quot;Centered and Scaled&quot;,xlab=&quot;dist&quot;) Sometimes you only need to scale the variable. For example, if the model adds penalty to the parameter estimates (such as \\(L_2\\) penalty is ridge regression and \\(L_1\\) penalty in LASSO), the variables need to have similar scale to ensure a fair variable selection. I am heavy user of this kind of penalty-based model in my work and I used the following quantile transformation: \\[ x_{ij}^{*}=\\frac{x_{ij}-quantile(x_{.j},0.01)}{quantile(x_{.j}-0.99)-quantile(x_{-j},0.01)} \\] The reason to use 99% and 1% quantile instead of maximum and minimum values is to resist the impact of outliers. It is easy to write a function to do it: qscale&lt;-function(dat){ for (i in 1:ncol(dat)){ up&lt;-quantile(dat[,i],0.99) low&lt;-quantile(dat[,i],0.01) diff&lt;-up-low dat[,i]&lt;-(dat[,i]-low)/diff } return(dat) } In order to illustrate, let’s simulate a data set with two variables: income and age. set.seed(2015) income&lt;-sample(seq(50000,150000,by=500),95) age&lt;-income/2000-10 noise&lt;-round(runif(95)*10,0) age&lt;-age+noise income&lt;-c(income,10000,15000,300000,250000,230000) age&lt;-c(age,30,20,25,35,95) demo&lt;-data.frame(income,age) demo$education&lt;-as.factor(sample(c(&quot;High School&quot;,&quot;Bachelor&quot;,&quot;Master&quot;,&quot;Doctor&quot;),100,replace = T,prob =c(0.7,0.15,0.12,0.03) )) summary(demo[,c(&quot;income&quot;,&quot;age&quot;)]) ## income age ## Min. : 10000 Min. :20.00 ## 1st Qu.: 76375 1st Qu.:30.25 ## Median : 98750 Median :44.25 ## Mean :103480 Mean :44.92 ## 3rd Qu.:126375 3rd Qu.:56.88 ## Max. :300000 Max. :95.00 It is clear that income and age are not on the same scale. Now apply the function qscale() on the simulated data demo. transformed&lt;-qscale(demo[,c(&quot;income&quot;,&quot;age&quot;)]) summary(transformed) ## income age ## Min. :-0.02101 Min. :-0.01904 ## 1st Qu.: 0.26077 1st Qu.: 0.17814 ## Median : 0.35576 Median : 0.44746 ## Mean : 0.37584 Mean : 0.46044 ## 3rd Qu.: 0.47304 3rd Qu.: 0.69033 ## Max. : 1.21015 Max. : 1.42375 6.3 Resolve Skewness Skewness is defined to be the third standardized central moment. The formula for the sample skewness statistics is: \\[ skewness=\\frac{\\sum(x_{i}+\\bar{x})^{3}}{(n-1)v^{3/2}}\\] \\[v=\\frac{\\sum(x_{i}=\\bar{x})^{2}}{(n-1)}\\] Skewness=0 means that the destribution is symmetric, i.e. the probability of falling on either side of the distribution’s mean is equal. You can easily tell if a distribution is skewed by simple visualization. There are different ways may help to remove skewness such as log, square root or inverse. However it is often difficult to determine from plots which transformation is most appropriate for correcting skewness. The Box-Cox procedure automatically identified a transformation from the family of power transformations that are indexed by a parameter \\(\\lambda\\). \\[ x^{*}=\\begin{cases} \\begin{array}{c} \\frac{x^{\\lambda}-1}{\\lambda}\\\\ log(x) \\end{array} &amp; \\begin{array}{c} if\\ \\lambda\\neq0\\\\ if\\ \\lambda=0 \\end{array}\\end{cases} \\] It is easy to see that this family includes log transformation (\\(\\lambda=0\\)), square transformation (\\(\\lambda=2\\)), square root (\\(\\lambda=0.5\\)), inverse (\\(\\lambda=-1\\)) and others in-between. We can still use function preProcess() in package caret to apply this transformation by chaning the method argument. (trans&lt;-preProcess(cars,method=c(&quot;BoxCox&quot;))) ## Created from 50 samples and 2 variables ## ## Pre-processing: ## - Box-Cox transformation (2) ## - ignored (0) ## ## Lambda estimates for Box-Cox transformation: ## 1, 0.5 The output shows the sample size (50), number of variables (2) and the \\(\\lambda\\) estimates for each variable. After calling the preProcess() function, the predict() method applies the results to a data frame. transformed&lt;-predict(trans,cars) par(mfrow=c(1,2)) hist(cars$dist,main=&quot;Original&quot;,xlab=&quot;dist&quot;) hist(transformed$dist,main=&quot;After BoxCox Transformation&quot;,xlab=&quot;dist&quot;) An alternative is to use function BoxCoxTrans() in package caret. (trans&lt;-BoxCoxTrans(cars$dist)) ## Box-Cox Transformation ## ## 50 data points used to estimate Lambda ## ## Input data summary: ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 2.00 26.00 36.00 42.98 56.00 120.00 ## ## Largest/Smallest: 60 ## Sample Skewness: 0.759 ## ## Estimated Lambda: 0.5 transformed&lt;-predict(trans,cars$dist) skewness(transformed) ## [1] -0.01902765 The estimated \\(\\lambda\\) is the same 0.5. Original skewness is 0.759 and after transformation, the skewness is -0.01902765 which is close to 0. You can use function skewness() in package e1071 to get the skewness statistics. 6.4 Resolve Outliers Even under certain assumptions we can statistically define outliers, it can be hard to define in some situations. You can refer to “Detection of Outliers” for more information. Some models are resistant to outliers (such as tree-based model and support vector machine). If a model is sensitive to outliers (such as linear regression and logistic regression), we can use spatial sign transformation to minimize the problem. It projects the original sample points to the surface of a sphere by: \\[x_{ij}^{*}=\\frac{x_{ij}}{\\sqrt{\\sum_{j=1}^{p}x_{ij}^{2}}}\\] As noted in the book “Applied Predictive Modeling”, Since the denominator is intended to measure the squared distance to the center of the predictor’s distribution, it is important to center and scale the predictor data prior to using this transformation. Note that, unlike centering or scaling, this manipulation of the predictors transforms them as a group. We can use spatialSign() function in caret to conduct spatial sign on demo: trans&lt;-preProcess(demo[,c(&quot;income&quot;,&quot;age&quot;)],method=c(&quot;center&quot;,&quot;scale&quot;)) transformed&lt;-predict(trans,demo[,c(&quot;income&quot;,&quot;age&quot;)]) transformed2 &lt;- spatialSign(transformed) transformed2 &lt;- as.data.frame(transformed2) p1&lt;-xyplot(income ~ age, data = transformed, main=&quot;Original&quot;) p2&lt;-xyplot(income ~ age, data = transformed2, main=&quot;After Spatial Sign&quot;) grid.arrange(p1,p2, ncol=2) 6.5 Missing Values We need a book to fully explicate this topic. Before we decide how to handle missing value, it is important to understand why the values are missing. Do the missing values have information related outcomes? Or are they missing at random? It is not the goal here to illustrate which methods to use in different missing situation. You can refer to Section 3.4 of “Applied Predictive Modeling” for more discussion on that. The objective of this post is to introduce some imputation methods and corresponding application examples using R. Survey statistics has studied the imputation extensively which focuses on making valid inferences. Missing value imputation in predictive modeling is a different problem. Saar-Tsechansky and Provost compared several different methods for applying classification to instance with missing values. “Handling Missing Values when Applying Classification Models” The following code randomly assigns some missing values to the previous data demo and names the new data set demo_missing. set.seed(100) id1&lt;-sample(1:nrow(demo),15) id2&lt;-sample(1:nrow(demo),10) id3&lt;-sample(1:nrow(demo),10) demo_missing&lt;-demo demo_missing$age[id1]&lt;-NA demo_missing$income[id2]&lt;-NA demo_missing$education[id3]&lt;-NA summary(demo_missing) ## income age education ## Min. : 15000 Min. :20.00 Bachelor :13 ## 1st Qu.: 77125 1st Qu.:30.25 Doctor : 2 ## Median : 98750 Median :44.25 High School:70 ## Mean :102811 Mean :44.43 Master : 5 ## 3rd Qu.:125250 3rd Qu.:56.25 NA&#39;s :10 ## Max. :300000 Max. :95.00 ## NA&#39;s :10 NA&#39;s :15 6.5.1 Impute missing values with median/mode You can use function impute() under package imputeMissings to impute missing values with mdedian/mode. This method is simple, fast but treats each predictor independently, and may not be accurate. demo_imp&lt;-impute(demo_missing,method=&quot;median/mode&quot;) summary(demo_imp) ## income age education ## Min. : 15000 Min. :20.00 Bachelor :13 ## 1st Qu.: 79250 1st Qu.:32.19 Doctor : 2 ## Median : 98750 Median :44.25 High School:80 ## Mean :102405 Mean :44.40 Master : 5 ## 3rd Qu.:122875 3rd Qu.:54.50 ## Max. :300000 Max. :95.00 Note that the median/mode method imputes mode to character vectors and median to numeric and integer vectors.So you can see the 10 missing values for variable “education” are imputed with “High School” since it is the mode. You can also use function ‘preProcess()’ to attain this.But it only works for numeric variable. imp&lt;-preProcess(demo_missing[,c(&quot;income&quot;,&quot;age&quot;)],method=&quot;medianImpute&quot;) demo_imp&lt;-predict(imp,demo_missing[,c(&quot;income&quot;,&quot;age&quot;)]) summary(demo_imp) ## income age ## Min. : 15000 Min. :20.00 ## 1st Qu.: 79250 1st Qu.:32.19 ## Median : 98750 Median :44.25 ## Mean :102405 Mean :44.40 ## 3rd Qu.:122875 3rd Qu.:54.50 ## Max. :300000 Max. :95.00 6.5.2 Impute missing values based on K-nearest neighbors k-nearest neighbor will find the k closest samples (Euclidian distance) in the training set and impute the mean of those “neighbors”. imp&lt;-preProcess(demo_missing[,c(&quot;income&quot;,&quot;age&quot;)],method=&quot;knnImpute&quot;,k=2) demo_imp&lt;-predict(imp,demo_missing[,c(&quot;income&quot;,&quot;age&quot;)]) ## Error in FUN(newX[, i], ...): cannot impute when all predictors are missing in the new data point Now we get a error saying “cannot impute when all predictors are missing in the new data point”. It is because there is at least one sample with both “income” and “age” missing. We can delete the corresponding row and do it again. idx&lt;-which(is.na(demo_missing$income)&amp;is.na(demo_missing$age)) imp&lt;-preProcess(demo_missing[-idx,c(&quot;income&quot;,&quot;age&quot;)],method=&quot;knnImpute&quot;,k=2) demo_imp&lt;-predict(imp,demo_missing[-idx,c(&quot;income&quot;,&quot;age&quot;)]) summary(demo_imp) ## income age ## Min. :-2.259679 Min. :-1.53784 ## 1st Qu.:-0.686725 1st Qu.:-0.88276 ## Median :-0.104506 Median :-0.01129 ## Mean :-0.006233 Mean : 0.01103 ## 3rd Qu.: 0.593512 3rd Qu.: 0.72444 ## Max. : 5.074342 Max. : 3.18343 The error doesn’t show up this time. This method considers all predictors together but it requires them to be in the same scale since the “euclidian distance” is used to find the neighbours. 6.6 Collinearity It is probably a technical term that many un-technical people also know. There is an excellent function in corrplot package with the same name corrplot() that can visualize correlation structure of a set of predictors. The function has option to reorder the variables in a way that reveals clusters of highly correlated ones. We add some columns to demo that are correlated. adddemo&lt;-demo[,-3] adddemo$added1&lt;-sqrt(demo$age)+10 adddemo$added2&lt;-log(demo$income)+demo$age adddemo$added2&lt;-log(demo$age) adddemo$added4&lt;-demo$income/1000+5*demo$age adddemo$added5&lt;-sin(demo$age) The following command will produce visualization for the correlation matrix of adddemo. corrplot(cor(adddemo),order=&quot;hclust&quot;) The size and color of the points are associated with the strength of corresponding correlation. Section 3.5 of “Applied Predictive Modeling” presents a heuristic algorithm to remove minium number of predicitors to ensure all pairwise corelations are below a certain threshold: Calculate the correlation matrix of the predictors. Determine the two predictors associated with the largest absolute pairwise correlation (call them predictors A and B). Determine the average correlation between A and the other variables. Do the same for predictor B. If A has a larger average correlation, remove it; otherwise, remove predictor B. Repeat Step 2-4 until no absolute correlations are above the threshold. The findCorrelation() function in package caret will apaply the above algorithm. (highCorr&lt;-findCorrelation(cor(adddemo),cutoff=.75)) ## [1] 5 2 3 # remove columns with high correlations filter_demo&lt;-adddemo[,-highCorr] # correlation matrix for filtered data corrplot(cor(filter_demo),order=&quot;hclust&quot;) 6.7 Sparse Variables Other than the highly related predictors, predictors with degenerate distributions need to be removed as well. Removing those variables can significant improve some models’ performance and/or stability (such as linear regression and logistic regression but tree based model is impervious to this type of predictors). One extreme example is a variable with single value which is called zero-variance variable. Similarly those variables with very low frequency of unique values are near-zero variance predictors. How to detect those variables? There are two rules: - The fraction of unique values over the sample size - The ratio of the frequency of the most prevalent value to the frequency of the second most prevalent value. The caret package funciton nearZeroVar() can filter near-zero variance predictors. #add two variables with low variance zero_demo&lt;-demo zero_demo$zero1&lt;-rep(0,nrow(demo)) zero_demo$zero2&lt;-c(1,rep(0,nrow(demo)-1)) # zero1 only has one unique value # zero2 is a vector with the first element 1 and the rest are 0s summary(zero_demo) ## income age education zero1 ## Min. : 10000 Min. :20.00 Bachelor :15 Min. :0 ## 1st Qu.: 76375 1st Qu.:30.25 Doctor : 2 1st Qu.:0 ## Median : 98750 Median :44.25 High School:77 Median :0 ## Mean :103480 Mean :44.92 Master : 6 Mean :0 ## 3rd Qu.:126375 3rd Qu.:56.88 3rd Qu.:0 ## Max. :300000 Max. :95.00 Max. :0 ## zero2 ## Min. :0.00 ## 1st Qu.:0.00 ## Median :0.00 ## Mean :0.01 ## 3rd Qu.:0.00 ## Max. :1.00 # the function will return a vector of integers indicating which columns to remove nearZeroVar(zero_demo,freqCut = 95/5, uniqueCut = 10) ## [1] 4 5 Note the two arguments in the function freqCut = and uniqueCut =. They are corresponding to the previous two rules. freqCut: the cutoff for the ratio of the most common value to the second most common value uniqueCut:the cutoff for the percentage of distinct values out of the number of total samples 6.8 Re-encode Dummy Variables Sometimes we need to recode categories to smaller bits of information named “dummy variables”. Take the variable “education” in demo for example. It has four categories: “High School”,“Bachelor”,“Master” and “Doctor”. If we recode it to be dummy variables, each category get its own dummy variable that is 0/1 indicator for that category. For a single categorical variable, we can use function class.ind() in package nnet: dumVar&lt;-class.ind(demo$education) head(dumVar) ## Bachelor Doctor High School Master ## [1,] 0 0 1 0 ## [2,] 0 0 1 0 ## [3,] 0 0 1 0 ## [4,] 0 0 1 0 ## [5,] 0 0 1 0 ## [6,] 0 0 1 0 If we want to determine encodeings for more than one variables, we can use dummyVars() in caret. dumMod&lt;-dummyVars(~income+education, data=demo, # Remove the variable name from the column name levelsOnly=T) predict(dumMod,head(demo)) ## income Bachelor Doctor High School Master ## 1 56000 0 0 1 0 ## 2 133500 0 0 1 0 ## 3 79500 0 0 1 0 ## 4 53000 0 0 1 0 ## 5 63500 0 0 1 0 ## 6 84500 0 0 1 0 To add some more complexity, we could assume joint effect of income and education. In this case, this will add 4 more columns to the resulted data frame: dumMod&lt;-dummyVars(~income+education+income:education, data=demo, levelsOnly=T) predict(dumMod,head(demo)) ## income Bachelor Doctor High School Master income:Bachelor income:Doctor ## 1 56000 0 0 1 0 0 0 ## 2 133500 0 0 1 0 0 0 ## 3 79500 0 0 1 0 0 0 ## 4 53000 0 0 1 0 0 0 ## 5 63500 0 0 1 0 0 0 ## 6 84500 0 0 1 0 0 0 ## income:High School income:Master ## 1 56000 0 ## 2 133500 0 ## 3 79500 0 ## 4 53000 0 ## 5 63500 0 ## 6 84500 0 References "],
["basic-model-techniques.html", "Chapter 7 Basic Model Techniques 7.1 Supervised v.s. unsupervised model 7.2 Model Error 7.3 Data Dividing and Resampling 7.4 Summary", " Chapter 7 Basic Model Techniques 7.1 Supervised v.s. unsupervised model 建模技术可以粗略的分为有监督和无监督这两类。大部分统计学习方法都可以归于其中一种。广义上说有监督方法涉及根据一个或者多个输入变量（也称为自变量，解释变量，预测变量），估计或者预测一个结果变量（也称为因变量，响应变量）。而无监督方法只考虑自变量，没有应变量作为“监督”，我们通过这类方法探索观测数据中内在变量结构。我们在之前提到的方法中，袋状树，广义线性回归是有监督方法；主成分分析，探索性因子分析，对近0方差和高相关变量的筛选都是无监督方法。我们先介绍这里的数学公式表达。 我们用\\(n\\)表示样本量（或者观测数目）。\\(p\\)代表自变量数目。我们用\\(\\mathbf{X}\\)表示\\(n\\times p\\)观测矩阵： \\[ \\mathbf{X}=\\left[\\begin{array}{cccc} x_{11} &amp; x_{12} &amp; \\cdots &amp; x_{1p}\\\\ x_{21} &amp; x_{22} &amp; \\cdots &amp; x_{2p}\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ x_{n1} &amp; x_{n2} &amp; \\cdots &amp; x_{np} \\end{array}\\right] \\] 其中\\(x_{ij}\\)代表第i个样本第j个变量的观测，\\(i=1, \\ldots, n\\)，\\(j=1, \\ldots, p\\)。\\(\\mathbf{x_{i.}}\\)代表第i个样本的所有变量观测组成的向量，向量统一按列排： \\[ \\mathbf{x_{i.}}=\\left[\\begin{array}{c} x_{i1}\\\\ x_{i2}\\\\ \\vdots\\\\ x_{ip} \\end{array}\\right] \\] 类似的，\\(\\mathbf{x_{.j}}\\)代表第j个变量的所有样本观测组成的向量： \\[ \\mathbf{x_{.j}}=\\left[\\begin{array}{c} x_{1j}\\\\ x_{2j}\\\\ \\vdots\\\\ x_{nj} \\end{array}\\right] \\] 于是我们有： \\[ \\mathbf{X}=\\left[\\begin{array}{cccc} x_{11} &amp; x_{12} &amp; \\cdots &amp; x_{1p}\\\\ x_{21} &amp; x_{22} &amp; \\cdots &amp; x_{2p}\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ x_{n1} &amp; x_{n2} &amp; \\cdots &amp; x_{np} \\end{array}\\right]=\\left[\\begin{array}{c} \\mathbf{x_{1.}^{T}}\\\\ \\mathbf{x_{2.}^{T}}\\\\ \\vdots\\\\ \\mathbf{x_{n.}^{T}} \\end{array}\\right]=\\left[\\begin{array}{cccc} \\mathbf{x_{.1}} &amp; \\mathbf{x_{.2}} &amp; \\ldots &amp; \\mathbf{x_{.p}}\\end{array}\\right] \\] 其中\\(^{T}\\)代表矩阵转秩。我们用\\(y_{i}\\)代表第i个样本对应的响应变量。所有\\(n\\)个响应变量组成的向量为： \\[ \\mathbf{y}=\\left[\\begin{array}{c} y_{1}\\\\ y_{2}\\\\ \\vdots\\\\ y_{n} \\end{array}\\right] \\] 自变量和应变量的关系为： \\[\\mathbf{y}=f(\\mathbf{X})+\\mathbf{\\epsilon}\\] 有监督和无监督建模技术用上面的符号语言表达就是： 无监督建模：探索\\(\\mathbf{X}\\)中的自变量之间的关系 有监督建模：估计\\(\\mathbf{y}\\)和\\(\\mathbf{X}\\)之间的关系 \\(f(\\cdot)\\) 其中\\(\\mathbf{\\epsilon}\\) 是随机误差，均值为\\(\\mathbf{0}\\)。函数\\(f(\\cdot)\\)是我们的建模目标，代表X能够提供的关于Y的系统信息（和随机性相对应）。估计\\(f(\\cdot)\\)目的主要是推断或者预测，有时兼有两者。通常情况下，模型的灵活性和可解释性之间是一种此消彼长的关系——灵活性越高的模型可解释性越弱。因此数据科学家需要把握这两者间微妙的平衡。不同的建模目的对模型解释性的要求不同，因而极大影响了模型选择。如果预测是唯一目的，那么模型的解释性就不在考虑范围内，这种情况下可以使用一些复杂的灵活度高的“黑箱”模型，装袋，助推，非线性核函数支持向量机，神经网络和随机森林等。这些模型都非常灵活，但是很难解释自变量和应变量之间的关系。人们可能会觉得这些模型的预测精度通常更高，但就个人经验来说，那些灵活性不那么高的模型预测精度更高的情况时常发生。咋一看来好像不符合逻辑，但是认真想想也并不奇怪，这些模型之所以复杂，就在于它们极力拟合当前观测数据，因此它们更有可能过度拟合（把噪声也拟合进去了），这些模型在训练集上的表现可能更好，但预测未必更准确。 7.2 Model Error 7.2.1 Systematic Error and Random Error 假设我们对于\\(\\mathbf{X}\\)得到\\(f\\)的估计\\(\\hat{f}\\)，进而得到\\(\\mathbf{y}\\)的预测 \\(\\hat{\\mathbf{y}}=\\hat{f}(\\mathbf{X})\\)。预测的误差分成两部分，系统误差和随机误差： \\[ E(\\mathbf{y}-\\hat{\\mathbf{y}})^{2}=E[f(\\mathbf{X})+\\mathbf{\\epsilon}-\\hat{f}(\\mathbf{X})]^{2}=\\underset{\\text{(1)}}{\\underbrace{E[f(\\mathbf{X})-\\hat{f}(\\mathbf{X})]^{2}}}+\\underset{\\text{(2)}}{\\underbrace{Var(\\mathbf{\\epsilon})}} \\label{eq:error}\\] 其中（1）是系统误差， \\(\\hat{f}\\)通常不能彻底对\\(\\mathbf{X}\\)和\\(\\mathbf{y}\\)之间的“系统关系”建模，这里系统关系指的是在不同样本上存在的稳定关系。这一部分误差能通过改进模型得到提高；（2）是随机误差，这部分误差代表当前数据无法解释的部分，因此无法通过建立更复杂的模型来改进。那些拥有众多参数的复杂黑箱模型最大的问题就是试图通过自变量解释这部分误差，也就是过度拟合。随机误差的显著特点就是在不同的样本上是无法重复的，于是判断是否存在过度拟合的一个准则就是预留一部分样本作为测试集，然后检验训练出来的模型在测试集上的表现。这个我们随后会讲到。这里要澄清一点，过度拟合不只发生在这些黑箱模型上，其发生的根源在于参数个数太多（常超过观测个数），理论上说任何模型都可能过度拟合，只是因为黑箱模型的参数尤其多，其高灵活性和复杂度放大了过度拟合的问题。有些黑箱模型在训练的过程中会使用“袋外数据”（又称为Out of Bag [OOB]）来尽量避免过度拟合的影响。 如果建模的目的也包含推断，那么这些“黑箱”模型就不合适，这就需要在模型可以解释的范围内使用尽量灵活的模型，比如Lasso回归，多元自适应回归样条等。有人可能不同意Lasso回归是灵活的。从其本质还是传统回归的角度看，它确实没有那么灵活，受到很多模型假设的限制。但由于Lasso的罚函数能同时起到变量选择的作用，这个变量的选择的过程可以不依赖于p值之类的参数（这些参数基于数据分布假设因此具有局限性），而可以通过优化模型预测值和真实值的差距来进行变量选择，从这个角度上看，该模型是灵活的。根据笔者的应用经验，Lasso作为收缩（或变量选择）方法在实际应用中的效果非常好。对于一些市场营销或者社会心理学相关的抽样调查数据分析，分层贝叶斯可能是一种灵活有效的方法，但拟合所需的计算时间更长。 其中系统误差可以进一步分解： \\[ \\begin{array}{ccc} E[f(\\mathbf{X})-\\hat{f}(\\mathbf{X})]^{2} &amp; = &amp; E\\left(f(\\mathbf{X})-E[\\hat{f}(\\mathbf{X})]+E[\\hat{f}(\\mathbf{X})]-\\hat{f}(\\mathbf{X})\\right)^{2}\\\\ &amp; = &amp; E\\left(E[\\hat{f}(\\mathbf{X})]-f(\\mathbf{X})\\right)^{2}+E\\left(\\hat{f}(\\mathbf{X})-E[\\hat{f}(\\mathbf{X})]\\right)^{2}\\\\ &amp; = &amp; [Bias(\\hat{f}(\\mathbf{X}))]^{2}+Var(\\hat{f}(\\mathbf{X})) \\end{array} \\] 系统误差由两部分组成，估计的偏度\\(Bias(\\hat{f}(\\mathbf{X}))\\)和估计的方差\\(Var(\\hat{f}(\\mathbf{X}))\\)。上面公式告诉我们，如果要最小化系统误差，需要同时最小化估计偏度和估计方差。偏度代表用模型逼近现实情况导致的误差，这部分误差可能非常复杂。比如线性回归假设自变量和应变量之间是线性关系，但现实生活中完全的线性关系并不常见。下图中x和fx的关系就是非线性的。因此，观测样本量再大，也无法用线性回归给出准确的预测。换句话说，在这种情况下，线性回归模型的预测具有很高的偏度。 library(grid) library(lattice) library(ggplot2) # 可以从网站下载multiplot()函数代码 # 用该函数在同一张画布上放置多张ggplot图 # 需要用到grid包 source(&quot;/Users/happyrabbit/Documents/GitHub/DataScientistR/R_Code/multiplot.r&quot;) # 随机抽取一些非线性样本 x=seq(1,10,0.01)*pi e=rnorm(length(x),mean=0,sd=0.2) fx&lt;-sin(x)+e+sqrt(x) dat=data.frame(x,fx) # 绘制线性拟合图 ggplot(dat,aes(x,fx))+ geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) 由于我们通常使用训练集进行参数估计，如果训练集不同，得到的参数估计也会不同。直观的讲，估计方差表示如果我们用不同的数据集拟合相同的模型，得到估计值的变化，理想的情况是估计值的变化不会太大。对于高方差的模型，训练集的微小变化会导致很不相同的估计值。通常情况下，灵活度高的模型方差也更高，比如树模型，以及最初的助推法，后来在此基础上的随机森林模型和梯度助推法这样的集成方法的重要目标之一，就是通过汇总不同样本上得到的结果来降低估计方差。下图中的蓝色曲线是用平滑方法对上面的非线性观测进行拟合得到的，该曲线的灵活性很高，能够高度拟合当前数据： ggplot(dat,aes(x,fx))+geom_smooth(span = 0.03) ## `geom_smooth()` using method = &#39;loess&#39; 但是该方法有很高的方差，如果我们随机抽取不同的样本子集，得到的拟合曲线会有明显变化： # 设置随机种子 set.seed(2016) # 抽取其中部分样本拟合模型 # 样本1 idx1=sample(1:length(x),100) dat1=data.frame(x1=x[idx1],fx1=fx[idx1]) p1=ggplot(dat1,aes(x1,fx1))+geom_smooth(span = 0.03) # 样本2 idx2=sample(1:length(x),100) dat2=data.frame(x2=x[idx2],fx2=fx[idx2]) p2=ggplot(dat2,aes(x2,fx2))+geom_smooth(span = 0.03) # 样本3 idx3=sample(1:length(x),100) dat3=data.frame(x3=x[idx3],fx3=fx[idx3]) p3=ggplot(dat3,aes(x3,fx3))+geom_smooth(span = 0.03) # 样本4 idx4=sample(1:length(x),100) dat4=data.frame(x4=x[idx4],fx4=fx[idx4]) p4=ggplot(dat4,aes(x4,fx4))+geom_smooth(span = 0.03) multiplot(p1,p2,p3,p4,cols=2) ## `geom_smooth()` using method = &#39;loess&#39; ## `geom_smooth()` using method = &#39;loess&#39; ## `geom_smooth()` using method = &#39;loess&#39; ## `geom_smooth()` using method = &#39;loess&#39; 对相同的4个子集拟合线性模型，变化非常小： p1=ggplot(dat1,aes(x1,fx1))+ geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) p2=ggplot(dat2,aes(x2,fx2))+ geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) p3=ggplot(dat3,aes(x3,fx3))+ geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) p4=ggplot(dat4,aes(x4,fx4))+ geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) multiplot(p1,p2,p3,p4,cols=2) 总体说来，方差随着模型灵活度的增加而增加，偏差随着模型的灵活度增加而降低。在方差和偏差的变化共同决定了系统误差（或者均方误差，MSE）的变化。当我们提高模型的灵活性，偏差减小的速度开始时会超过方差增大的速度，MSE随之减小。但到了一定程度以后，提高模型的灵活性对偏差的影响不大但是方差大幅度增加，MSE随之增加。在之后的章节我们会看到，对于那些有一个控制模型灵活性的调优参数的模型，随着调优参数的减小（参数越大灵活度越低），模型误差先升高，后降低。 模型选择向来是非常困难的，这种困难不是数据分析行业特有的，很多专业领域都有类似的情况，比如医生判断病人所患的疾病，并在众多治疗方案中选择最合适的，这不是答案一目了然的选择题，决策的过程需要很多权衡和妥协。模型选择也类似，在选择过程中需要考虑具体的情况：项目目的，客户要求的精确度（这点很重要），计算量等等。这个选择的过程很难白纸黑字的像食谱一样写下来，这里我们只是尽己所能的介绍模型选择过程中需要考虑的点，以及评估不同模型的辅助性技术。具体的应用和“数据科学思维”还需要大家在从业过程中通过实践思考不断学习打磨。 7.2.2 Error in Response 若应变量包含可观的测量误差，那么这部分误差将反映在随机误差（\\(\\mathbf{\\epsilon}\\)）中。这部分误差使得均方根误差（RMSE）和\\(R^2\\)有相应的上下限。RMSE和\\(R^2\\)是回归模型常用的表现度量方法，我们在本章后面部分会进行介绍。因此，随机误差项不仅仅代表模型无法解释的波动，还含有测量误差。《应用预测建模（Applied Predictive Modeling）》(Max Kuhn 2013)的第20.2小节有一个例子展示了因变量的测量误差对模型表现（RMSE和\\(R^2\\)）的影响。作者在因变量上加入了不同强度的随机正态噪声，重复拟合不同的模型，研究模型均方根误差（RMSE）和\\(R^2\\)的变化。这里我们用服装消费者数据进行类似的展示。假设我们面对这样一个问题，实际中消费者的收入并不是那么容易收集，很多人不愿透露这样的私人信息。于是我们希望利用消费记录变量建立关于消费者收入的预测模型，模型可以对那些数据库中缺失收入信息的记录进行填补。我们建立下面模型： # 载入数据 sim.dat&lt;-read.csv(&quot;/Users/happyrabbit/Documents/GitHub/DataScientistR/Data/SegData.csv&quot;) ymad&lt;-mad(na.omit(sim.dat$income)) # 计算Z分值 zs&lt;-(sim.dat$income-mean(na.omit(sim.dat$income)))/ymad # which(na.omit(zs&gt;3.5)) 找到利群点 # which(is.na(zs)) 找到缺失值 idex&lt;-c(which(na.omit(zs&gt;3.5)),which(is.na(zs))) # 删除含有离群点和缺失值的行 sim.dat&lt;-sim.dat[-idex,] fit&lt;-lm(income~store_exp+online_exp+store_trans+online_trans,data=sim.dat) 由输出可见，在没有额外添加噪音时模型的均方根误差（RMSE）是 29567，\\(R^2\\)是 0.6。下面我们在应变量年收入（income）上添加不同程度的噪音（均方根误差的0到3倍）： \\[ RMSE \\times (0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0) \\] noise&lt;-matrix(rep(NA,7*nrow(sim.dat)),nrow=nrow(sim.dat),ncol=7) for (i in 1:nrow(sim.dat)){ noise[i,]&lt;-rnorm(7,rep(0,7),summary(fit)$sigma*seq(0,3,by=0.5)) } 我们接下来检查噪音强度对复杂度不同的模型拟合\\(R^2\\)的影响。 拟合的模型复杂度从低到高依次为：一般线性回归，偏最小二乘回归，多元自适应回归样条，支持向量机（核函数是径向基函数），随机森林。 # 拟合一般线性回归模型 rsq_linear&lt;-rep(0,ncol(noise)) for (i in 1:7){ withnoise&lt;-sim.dat$income+noise[,i] fit0&lt;-lm(withnoise~store_exp+online_exp+store_trans+online_trans,data=sim.dat) rsq_linear[i]&lt;-summary(fit0)$adj.r.squared } 下面我们接着拟合偏最小二乘回归（PLS）。偏最小二乘源自于Herman Wold的非线性迭代偏最小二乘（NIPALS）算法 (H 1966; H 1982)，是一种通过隐层级将非线性关系线性化的方法。该方法和主成分回归类似，不同在于主成分回归在选择成分的时候没有考虑因变量的信息，其目的是找到最大程度概括自变量空间变异性的线性组合（即，是无监督方法）。当自变量和因变量相关时，主成分回归能够很好的识别出它们之间的系统关系。然而，当存在和因变量不相关的自变量时，该方法的效果就会受到影响。而PLS最大程度概括与因变量相关性的线性组合。推荐大家用PLS解决那些自变量之间存在相关性，但不确定所有自变量都和因变量有关，同时希望用线性回归来解决的问题。在当前情况下，更加复杂的PLS表现效果并不比简单线性好，因为这里几个自变量都和因变量有关的不同信息（从前面的拟合结果看到所有变量都是显著的）。 # pls: 进行偏最小二乘回归和主成分回归 library(pls) rsq_pls&lt;-rep(0,ncol(noise)) # 拟合PLS模型 for (i in 1:7){ withnoise&lt;-sim.dat$income+noise[,i] fit0&lt;-plsr(withnoise~store_exp+online_exp+store_trans+online_trans,data=sim.dat) # plsr函数结果是mvr对象，需要用特定函数提取模型解释的应变量方差 rsq_pls[i]&lt;-max(drop(R2(fit0, estimate = &quot;train&quot;,intercept = FALSE)$val)) } # earth: 拟合多元自适应回归样条 library(earth) rsq_mars&lt;-rep(0,ncol(noise)) # 拟合多元自适应回归样条 for (i in 1:7){ withnoise&lt;-sim.dat$income+noise[,i] fit0&lt;-earth(withnoise~store_exp+online_exp+store_trans+online_trans,data=sim.dat) # 提取模型解释的应变量方差 rsq_mars[i]&lt;-fit0$rsq } # caret: 用于建立预测模型的包，可以拟合多种模型 library(caret) rsq_svm&lt;-rep(0,ncol(noise)) # 拟合支持向量机 # 注意：运行需要一些时间 for (i in 1:7){ idex&lt;-which(is.na(sim.dat$income)) withnoise&lt;-sim.dat$income+noise[,i] trainX&lt;-sim.dat[,c(&quot;store_exp&quot;,&quot;online_exp&quot;,&quot;store_trans&quot;,&quot;online_trans&quot;)] trainY&lt;-withnoise fit0&lt;-train(trainX,trainY,method=&quot;svmRadial&quot;, tuneLength=15, trControl=trainControl(method=&quot;cv&quot;)) # 提取模型解释的应变量方差 rsq_svm[i]&lt;-max(fit0$results$Rsquared) } # randomForest: 拟合随机森林模型 library(randomForest) rsq_rf&lt;-rep(0,ncol(noise)) # 拟合随机森林模型 # ntree=500 用500棵树 # na.action = na.omit 忽略缺失值 for (i in 1:7){ withnoise&lt;-sim.dat$income+noise[,i] fit0&lt;-randomForest(withnoise~store_exp+online_exp+store_trans+online_trans,data=sim.dat,ntree=500,na.action = na.omit) # 提取模型解释的应变量方差 rsq_rf[i]&lt;-tail(fit0$rsq,1) } # reshape2在之前介绍过，用于数据整形 library(reshape2) rsq&lt;-data.frame(cbind(Noise=c(0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0),rsq_linear,rsq_pls,rsq_mars,rsq_svm,rsq_rf)) # 将数据转化成长型 rsq&lt;-melt(rsq,id.vars=&quot;Noise&quot;,measure.vars=c(&quot;rsq_linear&quot;,&quot;rsq_pls&quot;,&quot;rsq_mars&quot;,&quot;rsq_svm&quot;,&quot;rsq_rf&quot;)) # 功能强大的绘图包 library(ggplot2) # 用ggplot2包进行可视化 ggplot(data=rsq, aes(x=Noise, y=value, group=variable, colour=variable)) + geom_line() + geom_point()+ ylab(&quot;R2&quot;) Figure 7.1: 模型\\(R^2\\)随应变量噪音强度变化，rsq_linear是简单线性回归，rsq_pls是偏最小二乘回归，rsq_mars是多元自适应回归样条回归，rsq_svm是支持向量机，rsq_rf是随机森林。 由图7.1中可以看到： 所有模型拟合效果随着噪音强度的增加急剧下降。对变量测量系统的理解能够帮助我们更好的预期模型的表现。这是在之前“数据分析一般流程”中说过的从问题到数据这个环节需要弄清的问题。你应该清楚当前数据库中已有的数据的质量。如果客户提供给你额外的数据，或者需要你从其它地方获得数据，数据质量是必须交流清楚的问题，笔者就曾在这里栽过跟头，希望大家可以避免类似的错误。 使用更加复杂的模型的效果不一定更好，如复杂的随机森林和支持向量机表现居中，简单线性回归和偏最小二乘回归在噪音低的时候拟合效果最差。效果最好的是多元自适应回归样条回归，该模型比简单线性回归复杂，但比剩下其它的模型的解释性都更强。 噪音增加到一定程度，复杂的随机森林模型能够发现的潜在结构变得更加模糊，模型表现不如其它更简单的模型。因此系统测量误差较大时，使用更简单的易于解释的模型可能是更好的选择，大家建模的时候要尽量多尝试几种模型，在表现相当的情况下选择最简单的模型，模型的评估和选择很好的反应了一个数据科学家的职业“成熟度”。 7.2.3 Error in Independent Variable 传统的统计模型通常假设自变量的测量无误差（或者随机性），这在实际中是不可能的，所以我们需要考虑自变量观测的随机性。自变量观测中的随机性产生的影响取决于如下几个因素：随机性的强度，相应因变量在模型中的重要性，使用模型的类别。我们选取自变量“在线消费”（online_exp）为例，用和上面相似的方法在该自变量上添加不同程度的噪音看其对模型拟合情况的影响。我们在自变量online_exp和上添加如下不同程度的噪音（标准差的0到3倍）： \\[ \\sigma_{0} \\times (0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0) \\] 其中\\(\\sigma_{0}\\)是在线消费观测的标准差。 noise&lt;-matrix(rep(NA,7*nrow(sim.dat)),nrow=nrow(sim.dat),ncol=7) for (i in 1:nrow(sim.dat)){ noise[i,]&lt;-rnorm(7,rep(0,7),sd(sim.dat$online_exp)*seq(0,3,by=0.5)) } 同样的，我们检查噪音强度对复杂度不同的模型拟合\\(R^2\\)的影响。拟合的模型复杂度从低到高依次为：一般线性回归，偏最小二乘回归，多元自适应回归样条，支持向量机（核函数是径向基函数），随机森林。代码和之前类似，这里就不重复展示。 Figure 7.2: 模型\\(R^2\\)随自变量(在线消费)噪音强度变化，rsq_linear是简单线性回归，rsq_pls是偏最小二乘回归，rsq_mars是多元自适应回归样条回归，rsq_svm是支持向量机，rsq_rf是随机森林。 比较图7.2和图7.1，可以看到自变量的误差和应变量误差对模型拟合结果的影响很不相同。应变量误差是无法克服的，对任何模型来说都是个硬伤。而自变量误差确不一定。试想极端的情况，在线消费这个变量完全是随机噪音，也就是所说的无信息变量，随机森林和支持向量机受的影响并不太大。线性模型和偏最小二乘回归的结果依旧基本重合，而且随着噪音的增加拟合效果开始下降较快，到一定程度后趋于平稳，如果噪音不断增加，最后拟合的情况实际上会趋近于移除“在线消费”这个变量的结果。总体说来，如果某个自变量含有误差，其它与之相关的变量在某种程度上可以进行弥补。线性模型对于自变量的观测误差的抗性普遍较差。 7.3 Data Dividing and Resampling 模型训练和选择过程都离不开数据的划分和再抽样。数据划分是将一部分数据预留出来用于模型测试，只用另外的部分数据用于模型的训练。再抽样过程牵扯到重复的从训练集中抽取样本并且在不同的样本上拟合模型，以此来得到关于拟合模型的信息。假设我们想知道某线性模型拟合度\\(R^2\\)的稳定性（也可以用其它模型拟合度量），可以重复的抽取不同的样本，然后拟合相同的线性模型，检查这些模型对应\\(R^2\\)的变化。由于牵扯到使用随机样本重复拟合模型，这个过程有一定的计算量，最近五年里，数据处理工具和技术获得了飞速的发展。除非你需要处理PB（\\(2^{50}\\)比特）级别的数据，或者每天要处理千亿级的事件，现阶段大多数技术已经能轻松满足你的需求了。 你可能会问：为什么要对数据划分和再抽样？简单的回答是避免过度拟合。在预测问题中，有时拟合的模型能很好的描述现有数据中的变量关系，但是对新样本的预测有很大的偏差，这时就发生了过度拟合。很多领域都会讨论过度拟合，如医学研究，化学计量，气象，金融和社会学研究等等。现代很多含有调优参数的分类 和回归模型有高度的灵活性，如之前提到的随机森林、支持向量机等。它们能够对复杂的关系进行建模，但是很容易过度强调不可再现的数据关系。要注意，虽然过度拟合的问题在灵活度高的模型中更加突出，所有模型（包括简单线性回归）在应用中都可能出现该问题。建模的目的是找到可重复的数据关系， 这就需要将现有数据划分成不同的数据集来调试模型参数和评估模型表现。 划分和再抽样的一般过程如下： 将样本划分成训练集和测试集 使用训练集拟合模型 将拟合的模型应用于测试集评估模型表现 关于数据划分，我们会介绍3种划分数据的方法：（1）按照结果变量划分数据；（2）按照预测变量划分数据；（3）按照时间序列划分数据。之后我们会介绍两种主要的再抽样方法： bootstrap和交互校验。 7.3.1 Training and Testing Set 关于数据划分大家可能主要会问这三个问题：（1）为什么要划分训练集和测试集？（2）多少比例的数据用于训练集？（3）具体如何划分？我们现在就对此逐一回答。 70%用于训练集，30%用于测试集数据划分示意图 刚接触数据科学的人常常会问为什么我们要预留一部分数据作为测试集而不是使用全部的数据用于训练。印象中传统商业智能声称的数据分析通常只是数据描述。通过从数据库中查询相关测量来回答简单的问题，如：2015年某产品每月销售量是多少？我们网站在过去一个月每天的访问量是多少？两种包装设计的同类产品在某大零售店上个月的销量差距多大？像这样的问题确实不用对数据进行划分，相反我们需要用尽可能完整的数据，然后对感兴趣的部分求和或者平均。假设数据观测准确，我们不需要怀疑问题的答案，因为这些问题本质上就是对数据进行某种描述总结，没有牵扯到任何分析推断。 数据科学家需要解决的不会是这样的问题，常是预测问题，或者同时还需要从预测模型中得到相应能够指导决策的推断。在这些情况下，分析的重心在于找到自变量\\(\\mathbf{X}\\)和应变量\\(\\mathbf{y}\\)之间的系统关系。这时我们就必须非常小心，因为我们在用一个样本得到一般化的结论，进而对将来可能出现的观测进行预测，这远远超越了描述统计的界限。根据彭加莱的理论，在预测未来的过程中，预测的越远的未来要求模型越精确，因为你的错误率会迅速上升。每向前预测一步，噪声会随着以一种非线性的方式迅速增加，因此我很难相信对5年以后某事件的定量预测。我们能够处理定性的事物，能够讨论系统的某些特点，但能够计算的东西是很局限的。在《黑天鹅》那本书中，作者以数学家Michael Berry的弹子球计算为例说明了这种放大效应。该实验是预测弹子球在球桌上的运动轨迹。如果弹子球的基本参数已知，你能够计算出桌面阻力，测量撞击量，那么就可以预测第1次撞击的结果。要预测第2次撞击就更为复杂一些，你需要小心确定球的初始状态，但不是不可能。如果要计算第9次撞击的结果你需要考虑某个站在桌子旁边的人的体重和产生的引力。要计算第56次撞击结果你需要考虑宇宙中的每一个基本粒子。注意这还只是单独的弹子球而没有牵扯到有着自由意志的人，以及不同人之间相互的影响。对现实世界的复杂局面，人的预测能力有着本质上的局限性。因此在实际预测分析当中，你需要很小心的界定这个可预测的边界，好比在弹子球实验中，你能预测第1次撞击的结果或者咬咬牙，再多杀一大片脑细胞做第2次撞击预测，但不要试图再进一步，承认自己的局限需要知识和勇气。回到实际分析中，如何找到预测的边界？（注：随着你经验的增长，你会遇到很多你无法预测（有时是分析）的情况。）目前我知道的方法就是在仔细确保当前情况基本符合假设的情况下，严格划分训练集和测试集，尽可能对模型的预测情况进行评估，检测预测模型的精确度和稳定性。划分背后隐含的假设是： 我们用于分析的数据展现的过程能够反应真实世界中事情的发展过程 我们想要对其建模的真实世界中事情的发展过程随着时间变化是相对稳定的。如，用上个月的数据建立的表现良好的模型，在接下来的一个月的观测上依旧能够有类似的良好表现 换句话说，我们想要知道如果我们用模型来对新样本进行预测时会发生什么。我们的预测和真实将观测到的值有多接近？预测值偏离真实值的误差大致是多少？模型的误差是不是单向的，即预测是不是总大于真实值？这些都是很自然的问题，但它们的答案并非那么容易获得。最简单的理解模型在将来数据集上表现的方法就是试图模拟这件事。虽然严格说来，在将来事件发生之前，我们不可能得到相应的数据，但是我们能够预留一部分当前的数据并将它们视为将来的观测。例如，如果我们要预测2016年哪些农民还会某品牌的种子，可以用之前到2015年的历史数据建立预测模型，然后预测2016年的购买情况。这是一个相当好的模拟，由于我们其实已经知道2016年实际购买情况，可以将预测和真实情况进行对比。 在商业促销活动和信用风险的案例中，我们得到的数据通常和某个时间点相连（或者时间区间：一周，一个月，一个促销活动期间）。通常称这样的数据有代表性（用某时间点或者时间段的数据代表普遍情况）。在这样的情况下我们通常将数据集随机分成不同部分，然后用一部分（训练集）建立模型，另外一部分（测试集）来评估模型表现，可能的话对模型做出调整。 如果这两条假设大致正确，那么当前数据就能够合理反映未来的情况。因此在这种情况下，预留一部分当前数据来估计模型在将来的表现是合理的。明确了预测模型的一些假设前提，以及划分训练集和测试集的必要性之后，下一个问题是我们该将多少比例的数据用于训练集。 一般这需要视具体情况而定。通常需要考虑的两个因素是：（1）样本量；（2）计算速度。当样本量较大时，在考虑计算速度的条件下，我一般会尝试60%，70％和80%这三个比例，看哪个效果更好。如果样本量很小，那么测试集其评估模型效果的能力将非常有限，并且在原本样本量就不大的情况下再分出一部分数据会极大影响模型拟合。这种情况下，使用再抽样技术更加有效。常用的再抽样方法有交互校验和Bootstrap。 我们可以用createResample()函数生成简单bootstrap样本，createFolds函数可以生成平衡的交互校验样本集。 具体如何划分? 划分训练集和测试集时需要小心避免两个数据集有系统差别。例如，我们不能简单的把前半部分数据当作训练集，后半部分当作测试集。因为数据有可能是以某种方式排列的，如，按收入从大到小，按访问次数多少排列等等。有一种避免数据集间随机差别的方法是用简单的随机抽样，如对每个样本我们都抛下硬币，人头面就归于训练集，菊花面就归于测试集。有时还有一些其它因素需要考虑，但本质都是随机抽样。要想真正理解划分数据背后的逻辑需要实践。下面我们介绍经常使用的几种划分方法。 按照结果变量划分数据 若结果变量\\(\\mathbf{y}\\)为分类变量，那么我们的到的测试集和训练集中结果变量各类的分布比例应该类似。可以使用caret包中的createDataPartition()函数平衡划分样本集。回到我们之前使用的服装消费者数据集，假设我们想要建立关于消费者类别（segment）的判别模型，这时结果变量为segment，我们用80%的样本训练模型，20%的样本做为测试集，且训练集和测试集中各类别的比例要尽可能相近。我们可以用如下R代码实现： # 载入数据 sim.dat&lt;-read.csv(&quot;/Users/happyrabbit/Documents/GitHub/DataScientistR/Data/SegData.csv&quot;) # 需要caret包 library(caret) # 设置随机种子这样能得到相同的抽样结果 set.seed(3456) trainIndex &lt;- createDataPartition(sim.dat$segment, p = .8, list = FALSE, times = 1) head(trainIndex) ## Resample1 ## [1,] 1 ## [2,] 2 ## [3,] 3 ## [4,] 4 ## [5,] 6 ## [6,] 7 list = FALSE选项使得返回的值是数据框。该函数还有一个选项times，用于设置划分的次数，你可以一次返回多次划分的结果，函数会返回一个（或多个）整数向量（指针向量），指明归于训练集的行（你可以设置times＝2再运行一下上面的代码看看输出有什么不同）。下面我们通过返回的指针向量（trainIndex）得到训练集和测试集： # 得到训练集 datTrain &lt;- sim.dat[ trainIndex,] # 得到测试集 datTest &lt;- sim.dat[-trainIndex,] 按照设置，训练集中该有800个样本，测试集中有200个样本。我来看看两个集合中消费者类别的比例分布是否相似： library(plyr) ddply(datTrain,&quot;segment&quot;,summarise,count=length(segment), percentage=round( length(segment)/nrow(datTrain),2)) ## segment count percentage ## 1 Conspicuous 160 0.20 ## 2 Price 200 0.25 ## 3 Quality 160 0.20 ## 4 Style 280 0.35 ddply(datTest,&quot;segment&quot;,summarise,count=length(segment), percentage=round(length(segment)/nrow(datTest),2)) ## segment count percentage ## 1 Conspicuous 40 0.20 ## 2 Price 50 0.25 ## 3 Quality 40 0.20 ## 4 Style 70 0.35 很明显两个集合中消费者类别比例分布是一样的（实际应用中两个集合分布不一定严格相似，但应该非常接近）。 按照自变量划分 还可以使用最大差异度法(Willett 2004)划分数据（maxDissim()函数）。假设样本集A中含有m个样本，样本集B含有n个样本，n&gt;m，且我们要从B中选出一些样本加到A中，该子集中的样本要尽量和A中的不同。要实现这一点，对B中的一个样本，计算A中样本和该样本的差异度（距离，这里会算出m个值，因为A中有m个样本）。然后将和A中样本最不相同的B的样本抽取出来加入A，重复这个过程，直到A的样本量达到要求。关于这么权衡这m个差异度找到和A“最不相似”的样本，有不同的方法，比如以最小的值为准，将所有距离求和等等。这里没有什么黄金法则，建议大家尝试几种方法，查看比较得到的训练/测试样本自变量分布，选取其中一种。用这种方式可以得到自变量分布相似的不同样本集。R中有不同的计算样本间差异度（基于自变量观测）的函数。caret包中调用的是proxy包中的函数。关于不同的差异度测量，见相关包的帮助文档。我们可以通过选项 obj设置和样本集A“最不相似”的样本的方式，其中minDiss表示以最小差异度为准，sumDiss表示使用差异度之和。 我们用服装数据的一个子集为例展示按照自变量抽样。这里选取年龄和收入这两个变量。 # 最大差异度抽样用到proxy包 library(proxy) # 用lattice包绘制散点图 library(lattice) # 选取年龄和收入这两个变量 testing&lt;-subset(sim.dat,select=c(&quot;age&quot;,&quot;income&quot; )) 我们先随机选取5个样本做为初始集（start），剩下的样本组成集合samplePool： set.seed(5) # 随机选取5个样本 startSet &lt;- sample(1:dim(testing)[1], 5) start &lt;- testing[startSet,] # 剩下的样本存在对象samplePool中 samplePool &lt;- testing[-startSet,] 通过maxDissim()函数从samplePool中抽取5个样本，这5个样本尽量和start中已有的样本不同。： # 通过最大化差异得到的样本存在数据框new内 # obj = minDiss 表示总体差异度以最小差异度为准 newSamp &lt;- maxDissim(start, samplePool,obj = minDiss, n = 5) new&lt;-samplePool[newSamp,] 我们再从samplePool中不用最大差异法，而是随机抽取5个样本，将这5个样本存在数据框new2中： newSet &lt;- sample(1:dim(samplePool)[1], 5) new2&lt;-testing[newSet,] 绘制散点图比较两种不同方法（new：用最大化差异法抽取的样本；new2：随机抽取的样本）抽取的样本和初始样本（start）有什么不同： start$group&lt;-rep(&quot;start&quot;,nrow(start)) new$group&lt;-rep(&quot;new&quot;,nrow(new)) new2$group&lt;-rep(&quot;new2&quot;,nrow(new2)) xyplot(age~income,data=rbind(start,new,new2),grid = TRUE, group = group, auto.key = TRUE ) Figure 7.3: 按自变量最大化差异抽样 由图7.3可见，通过最大化差异抽取的样本（new）和初始样本点（start）分布在图的不同位置。而随机抽取的新样本（new2）和原始样本更加接近。我们为什么希望每次抽取的样本和之前的不一样呢？因为我们希望最后得到的训练集和测试集覆盖的自变量观测区间相似。如果抽取的样本点都来自一个区域的话（比如全部都是年龄30以下，收入10万以下），如果讲这个样本用于训练的模型很可能不具有预测这个区域外样本的能力。反之要是用这个样本做为测试集，则无法检测模型在这个区域外样本上的表现。 按时间序列划分 对于时间序列数据，用简单随机抽样通常不是最好的方式。有一种按时间序列划分训练集和测试集的方法，关于该方法的讨论见(Hyndman and Athanasopoulos 2013)。我们临时抽取一个长度为100的来自1阶自回归模型［AR(1)］的时间序列样本，用来展示caret包中对时间序列样本划分测试集和训练集的函数createTimeSlices()。由于时间序列话题不在本书范围之内，这里不会进行过多讨论。 # 抽取符合AR(1)的时间序列向量 timedata = arima.sim(list(order=c(1,0,0), ar=-.9), n=100) # 对时间序列作图 plot(timedata, main=(expression(AR(1)~~~phi==-.9))) Figure 7.4: 时间序列样本图 图7.3展示了100个模拟的时间序列观测。对这样的数据，我们希望训练集和测试集都能覆盖到不同时段的观测。下面用createTimeSlices()函数对数据进行划分。该函数中有3个需要设置的参数： initialWindow: 初始训练集样本中的连续观测数目 horizon: 测试集中的观测数目 fixedWindow: 逻辑值，取值为FALSE时，训练集从第一个样本开始划分区间长度不固定。 timeSlices &lt;- createTimeSlices(1:length(timedata), initialWindow = 36, horizon = 12, fixedWindow = T) str(timeSlices,max.level = 1) ## List of 2 ## $ train:List of 53 ## $ test :List of 53 可以看到函数结果返回2个列表，分别含有训练集和测试集的样本索引。我们查看第一个训练集和测试集样本。 # 将训练集索引信息存在trainSlices对象内 trainSlices &lt;- timeSlices[[1]] # 将测试集索引信息存在testSlices对象内 testSlices &lt;- timeSlices[[2]] # 分别查看第一个训练集样本和测试集样本 trainSlices[[1]] ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 ## [24] 24 25 26 27 28 29 30 31 32 33 34 35 36 testSlices[[1]] ## [1] 37 38 39 40 41 42 43 44 45 46 47 48 第一个训练集样本是原数据中第1个观测到第36个观测（因为initialWindow = 36），接下来从第37到48这12个观测被划分到第一个测试集（因为horizon = 12）。你可以通过head(trainSlices)查看后续的样本索引。尝试着改变fixedWindow =的设置，然后重复上面的代码得到新的trainSlices和testSlices，然后键入： head(trainSlices) head(testSlices) 比较两种结果的不同就能够很容易理解该选项的作用了。 训练集和测试集的划分很容易理解和实现。但注意其中两个潜在的缺陷： 由于训练集和测试集的划分是随机的，所以重复这一过程在测试集上得到的误差会有波动。 由于训练集中只包含原始观测的一个子集，拟合模型使用的是部分数据。通常当数据量不是非常大的时候，使用更少的观测多少会对模型拟合造成负面影响。这就意味着该过程可能过度估计模型误差（即，使用所有观测拟合的模型的误差应该比当前估计的要小）。 7.3.2 Resampling 重抽样即对样本进行重复划分，所以是建立在数据划分的基础上。其基本原理是：用部分样本拟合模型，用剩下的样本评估模型。多次重复这一过程，然后对结果进行汇总。进行重抽样的目的可能有： 对于有调优参数的模型，如支持向量机，罚函数模型等，必须通过重抽样估计调优参数。这时的目的是针对一个模型表现的度量（如RMSE），找到能够优化该度量的调优参数值。 对于不含有调优参数的模型，如普通线性回归，最小二乘回归等，就模型拟合本身不需要重抽样，但可以通过重抽样考察模型拟合结果的稳定性，也可以用于检验模型在和训练集无关的样本上的表现。 这一小节将介绍几种主要的重抽样方法。 7.3.2.1 k-fold Cross Validation k折交叉验证的主要过程如下： 将样本随机划分为\\(k\\)个大小相当的子集 对\\(i=1…k\\) 用除了第\\(i\\)个样本集之外的样本拟合模型\\(M_{i}\\) 将\\(M_{i}\\)用在第i个样本集上，对结果进行评估 5折交叉验证 这样会得到k个模型评估结果，将这些结果进行汇总（通常是计算均值和标准差），然后基于此了解调优参数和模型表现之间的关系。联系之前介绍的不同划分方法，可以将这些划分方法应用到k折交叉验证中，使k个子集中的因变量组成尽可能平衡。k折交叉验证的一个特定是k等于样本量，这时每次只有一个预留样本，该情况也称为留一交叉验证（LOOCV），注意在这种情况下模型最终的评估结果将根据所有的预测值进行计算。通常在样本量较小的时候使用LOOCV，道理很简单，样本量小的时候我们应该用尽可能多的样本拟合模型。关于交互校验的折数，很多R函数默认设置k=10，但没有黄金标准。折数越多，每次预留在外的样本就越少，模型表现估计值和真实值之间的差距就越小。但LOOCV的计算量最大，因为其模型拟合的次数等于样本量，且每次模型拟合使用的子集样本量几乎和训练集相同。另一方面，当k值很小时（2或者3），计算效率高但是结果的方差和偏差都会增加。这意味着如果重复抽样的过程得到的结果可能很不一样。当样本量足够大时，方差和偏差的潜在影响就可以忽略不计，在这种情况下可以使用折数较低的交叉验证。这里讲到的关于计算效率和偏差之间的权衡，需要读者自己反复实践才能真正理解。 caret包中有几个关于重抽样的函数。createFolds()函数用于k折交叉验证。我们按照 消费者类别变量对服装消费者数据抽取k折交叉验证样本。 library(caret) class&lt;-sim.dat$segment #k折校验重抽样 set.seed(1) cv&lt;-createFolds(class,k=10,returnTrain=T) str(cv) ## List of 10 ## $ Fold01: int [1:900] 1 2 3 4 5 6 7 8 9 10 ... ## $ Fold02: int [1:900] 1 2 3 4 5 6 7 9 10 11 ... ## $ Fold03: int [1:900] 1 2 3 4 5 6 7 8 10 11 ... ## $ Fold04: int [1:900] 1 2 3 4 5 6 7 8 9 11 ... ## $ Fold05: int [1:900] 1 3 4 6 7 8 9 10 11 12 ... ## $ Fold06: int [1:900] 1 2 3 4 5 6 7 8 9 10 ... ## $ Fold07: int [1:900] 2 3 4 5 6 7 8 9 10 11 ... ## $ Fold08: int [1:900] 1 2 3 4 5 8 9 10 11 12 ... ## $ Fold09: int [1:900] 1 2 4 5 6 7 8 9 10 11 ... ## $ Fold10: int [1:900] 1 2 3 5 6 7 8 9 10 11 ... 结果返回10个子样本集中样本对应的行数。我们可以通过交叉验证来估计调优参数。回忆之前应变量误差的小节中拟合支持向量机模型的代码： #这里只是截取了之前的代码用于展示，并不能独立运行 fit0&lt;-train(trainX,trainY,method=&quot;svmRadial&quot;, tuneLength=15, trControl=trainControl(method=&quot;cv&quot;)) 上面代码中“method=&quot;cv&quot;”告诉R进行交叉验证，这里默认\\(k=10\\)。 7.3.2.2 Repeated Training and Testing 该方法其实就是对数据集重复多次训练集／测试集划分，用训练集建立模型，用测试集评估模型。和k折交叉验证不同，该过程生成的测试集可能有重复的样本，其通常重复更多次。对于划分比例和重复次数没有固定法则，通常将总样本的75%到80%用于训练，剩下的用于测试，用于训练的样本越接近，得到模型估计的偏差就越小。该方法中重复的次数的增加可以减少模型评估结果的不确定性，当然代价就是在模型复杂时的计算时间。当然，重复的次数也和测试集的样本占总体比例有关，如果比例小，那么得到的预测评估结果的波动性就更大，这时就需要增加重复次数来见效评估结果的不确定性。 假设我们还是按照消费者类别（segment）划分数据，这依旧可以使用之前用于划分训练集和测试集的函数createDataPartition()。记得之前该函数中的选项设置times=1么？这里只要将其设置成你想要重复的次数即可。 trainIndex &lt;- createDataPartition(sim.dat$segment, p = .8, list = FALSE, times = 5) dplyr::glimpse(trainIndex) ## int [1:800, 1:5] 1 3 4 5 6 7 8 9 10 11 ... ## - attr(*, &quot;dimnames&quot;)=List of 2 ## ..$ : NULL ## ..$ : chr [1:5] &quot;Resample1&quot; &quot;Resample2&quot; &quot;Resample3&quot; &quot;Resample4&quot; ... 类似的，对于其它划分方式，只要大家知道如何划分，重复划分应该很容易。 7.3.2.3 Bootstrap Bootstrap是一种应用及其广泛而且强大的统计工具。它可以用来定量分析参数估计或统计模型的不确定性(Efron and Tibshirani 1986)。如，可以通过Bootstrap估计线性回归模型参数拟合的标准差，这是另外一种取代p值的方法。该方法的强大在于其可以很容易应用于每个模型（说白了就是是对数据重复进行有放回随机抽样，拟合的过程），有的模型要是用传统的统计推断方法很难得到标准差和置信区间。这是一个典型的听起来很高端，其实没太多技术含量的方法。但很多天才的想法不都是这样么？之前没有人想到，之后大家都觉得这么简单粗暴有效怎么会想不到。由于是有放回抽样，一个样本可能多次被选中，且Bootstrap样本量和原数据样本量一样。这些没有被选中的样本称为“袋外（out-of-bag）样本”。选中的样本用来建立模型，带外样本用来评估模型。Efron指出，一般情况下(Efron 1983)，Bootstrap估计的模型错误率的不确定性更小。平均而言，63.2%的样本点在 Bootstrap 中出现过至少一次，因此其估计的偏差与2折交叉验证相似。如之前所述，折数越小，用于训练的样本数目越少，这意味着估计的偏差越大。增加样本量可以缓解该问题。总的来说，和交叉验证相比，Bootstrap偏差更大，不确定性更小。针对估计偏差问题，Efron对原始Bootstrap过程进行了改进，得到下面的632方法： \\[(0.632 × 原始 Bootstrap 估计错误率) + (0.368 ×显性错误率)\\] 其中显性错误率就是用所有样本进行建模，然后再作用于相同的样本集得到的模型错误率，该估计显然过度乐观。这一改进虽然在某种程度上降低了偏差，但在样本量小的时候依旧表现不佳。试想严重过度拟合的情况下显性错误率几乎是0，那么上面公式中的第二项也就不存在了，这个时候，632方法给出的错误率估计可能过度乐观。Efron 和 Tibshirani之后进一步改进了632方法，得到“632+ 方法”，进一步调整了Bootstrap 估计(Efron and Tibshirani 1997)。 7.4 Summary 本章介绍了一些基础建模技术，包括有监督和无监督的概念。模型误差分类： 系统误差：能够通过改进模型减小这部分误差 随机误差：当前数据无法解释的部分，无法通过建立更加复杂的模型来改进 此外我们讲了误差的两个来源，应变量误差和自变量误差。其中应变量误差会反映在随机误差中，这是个硬伤，无法克服。而某些自变量的误差可能通过其它相关自变量得到弥补，其影响取决于随机性强度，相应变量在模型中的重要性，以及自变量之间的相关性。 最后，也是最重要的一个话题是数据的划分和再抽样。其主要目的是为了判断模型真实的表现。我们介绍了3种数据划分的方法： 按照结果变量划分数据； 按照预测变量划分数据； 按照时间序列划分数据。 我们还介绍了两种主要的再抽样方法：bootstrap和交互校验。重抽样是建立在数据划分的基础上，其主要目的有两个： 对于有调优参数的模型估计调优参数； 考察模型拟合结果的稳定性。 其中我们讨论了不同重抽样的影响，以及在方差，偏差和计算效率之间的权衡。这里讲的所有方法都需要大家在实践中总结，才能真正成为自己的技能。 References "],
["shiny-intro.html", "Chapter 8 R-Shiny Introduction 8.1 Softwares 8.2 Web development 8.3 Shiny 8.4 Resources", " Chapter 8 R-Shiny Introduction This tutorial is designed for Shiny beginner. You don’t need any background in the R language or Shiny to get started, although having some basic knowlege about R might be helpful. It would be helpful if you have some basic knowlege about HTML, CSS and javascript, but they are not required too. 8.1 Softwares R (required). Better to use the latest version of R (3.3.2 for the time this tutorial is written). The shiny package (required). To install the package from CRAN by using: install.packages(&quot;shiny&quot;) Rstudio (recommended). A user friendly IDE for R. Very convenient tool in building shiny apps. A web browser. Better to use Google chrome or Firefox. In this section, we’ll start from some basic concepts of web development and then introduce Shiny by explaining the advantages of Shiny and how it works. At last we’ll give a list of useful resources in learning Shiny. 8.2 Web development Here are some concepts that usually used in web development. Static web page A static web page (sometimes called a flat page/stationary page) is a web page that is delivered to the user exactly as stored. (from wiki) Figure 8.1: Static web page: is delivered to the user exactly as stored. Dynamic web page A server-side dynamic web page is a web page whose construction is controlled by an application server processing server-side scripts. (from wiki) Figure 8.2: Dynamic web page: example of server-side scripting (PHP and MySQL). Web application A Web application (Web app) is an application program that is stored on a remote server and delivered over the Internet through a browser interface. (from WhatIs.com) Web (application) framework A Web framework is a collection of packages or modules which allow developers to write Web applications or services without having to handle such low-level details as protocols, sockets or process/thread management. (from PythonWiki) Front-end web development Front-end web development, also known as client-side development is the practice of producing HTML, CSS and JavaScript for a website or web application so that a user can see and interact with them directly. (from Wiki) Back-end web development Back-end web development creates the logical back-end and core computational logic of a website. (from techopedia) 8.3 Shiny Shiny is a web application framework for R that can help turn your analyses into interactive web applications. No HTML, CSS, or JavaScript knowledge required Why Shiny? Easy to learn, easy to use. The development time is minimized. Excellent tool for data visualization. Have very strong backing: the R language Fun &amp; Cool. A Shiny app usually contains two parts: UI: controls the outlook of the web page Server: (a live R session) controls the logic How does Shiny app work? The “Server” keeps monitoring the UI. Whenever there is a change in the UI, the “Server” will follow some instructions (run some R code) accordingly and update the UI’s display. (This is the basic idea of reactive expression, which is an distinguish feature of Shiny we will talk about later.) Example library(shiny) runExample(&quot;01_hello&quot;) # a histogram In the example above, the “Server” keeps monitoring the “slider” in the page, and whenever there is a change with it, the “Server” will re-execute a block of R code to regenerate the hitogram. 8.4 Resources Shiny portal site: http://shiny.rstudio.com Tutorial (get started): http://shiny.rstudio.com/tutorial/ Articles (go deeper): http://shiny.rstudio.com/articles/ Gallery (get inspired): http://shiny.rstudio.com/gallery/ Shiny User Showcase: https://www.rstudio.com/products/shiny/shiny-user-showcase/ Shiny Apps for the Enterprise Industry Specific Shiny Apps Shiny Apps as Analytics Tools Shiny Apps that Extend Shiny Shiny Apps with Popular Appeal Shiny Apps for Teaching Shiny examples (over 100 examples): https://github.com/rstudio/shiny-examples Ask questions in the shiny google group: https://groups.google.com/forum/#!forum/shiny-discuss Articles from R blogger: http://www.r-bloggers.com/?s=shiny Gallery of user-made apps: http://www.showmeshiny.com/ 2016 Shiny Developer Conference Videos https://www.rstudio.com/resources/webinars/shiny-developer-conference/ "],
["dynamicreproducible-report.html", "Chapter 9 Dynamic/Reproducible report", " Chapter 9 Dynamic/Reproducible report "],
["soft-skills-for-data-scientists.html", "Chapter 10 Soft Skills for Data Scientists 10.1 Introduction to agile 10.2 Effective communication with business partners 10.3 Leadership skills 10.4 Decision making with uncertainty", " Chapter 10 Soft Skills for Data Scientists 10.1 Introduction to agile 10.2 Effective communication with business partners 10.3 Leadership skills 10.4 Decision making with uncertainty "],
["case-study.html", "Chapter 11 Case Study 11.1 Case 1: Customer Perception Study for Airline Company 11.2 Case 2: Swine Disease Prediction", " Chapter 11 Case Study 11.1 Case 1: Customer Perception Study for Airline Company 11.2 Case 2: Swine Disease Prediction "],
["references.html", "Chapter 12 References", " Chapter 12 References "]
]
