[
["index.html", "The Science and Art of Data Chapter 1 Prologue", " The Science and Art of Data Hui Lin and Ming Li 2017-06-21 Chapter 1 Prologue This work is licensed under a Creative Commons Attribution-No Derivative Works 3.0 United States License. Copyright is retained by Hui Lin in all non-U.S. jurisdictions, but permission to use these materials in teaching is still granted, provided the authorship and licensing information here is displayed. The author has striven to minimize the number of errors, but no guarantee is made as to accuracy of the contents of this book. "],
["the-art-of-data-science.html", "Chapter 2 The art of data science 2.1 What is data science? 2.2 Is it science? Totally? 2.3 What kind of questions can data science solve? 2.4 What are the required skills for data scientist? 2.5 General Process of Data Science", " Chapter 2 The art of data science Data science and data scientist have become buzz words. Allow me to reiterate what you may have already heard a million times in the media: data scientists are in demand and demand continues to grow. A study by the McKinsey Global Institute concludes, “a shortage of the analytical and managerial talent necessary to make the most of Big Data is a significant and pressing challenge (for the U.S.).” You may expect that statisticians and graduate students from traditional statistics departments are great data scientist candidates. But the situation is that the majority of current data scientists do not have a statistical background. As David Donoho pointed out: “statistics is being marginalized here; the implicit message is that statistics is a part of what goes on in data science but not a very big part.” ( from “50 years of Data Science”). What is wrong? The activities that preoccupied statistics over centuries are now in the limelight, but those activities are claimed to belong to a new discipline and are practiced by professionals from various backgrounds. Various professional statistics organizations are reacting to this confusing situation. (Page 5-7, “50 Years of Data Science”) From those discussions, Donoho summarizes the main recurring “Memes” about data sciences: The ‘Big Data’ Meme The ‘Skills’ Meme The ‘Jobs’ Meme The first two are linked together which leads to statisticians’ current position on data science. I assume everyone has heard the 3V (volume, variety and velocity) definition of big data. The media hasn’t taken a minute break from touting “big” data. Data science trainees now need the skills to cope with such big data sets. What are those skills? You may hear about: Hadoop, system using Map/Reduce to process large data sets distributed across a cluster of computers. The new skills are for dealing with organizational artifacts of large-scale cluster computing but not for better solving the real problem. A lot of data on its own is worthless. It isn’t the size of the data that’s important. It’s what you do with it. The big data skills that so many are touting today are not skills for better solving the real problem of inference from data. Some media think they sense the trends in hiring and government funding. We are transiting to universal connectivity with a deluge of data filling telecom servers. But these facts don’t immediately create a science. The statisticians have been laying the groundwork of data science for at least 50 years. Today’s data science is an enlargement of traditional academic statistics rather than a brand new discipline. Our goal is to help you enlarge your background to become a data scientist in US enterprise environments. We will use case studies to cover how to leverage big data distributed platforms (Hadoop / Hive / Spark), data wrangling, modeling, dynamic reporting (R markdown) and interactive dashboards (R-Shiny) to tackle real-world data science problems. One typical skill gap for statisticians is data ETL (extraction, transformation and load) in production environments, and we will cover this topic as well. Data science is a combination of science and art with data as the foundation. We will also cover the “art” part to guide participants to learn soft skills to define data science problems and to effectively communicate with business stakeholders. The prerequisite knowledge is MS level education in statistics and entry level knowledge of R-Studio. 2.1 What is data science? This question is not new. When you tell people “I am a data scientist”. “Ah, data scientist!” Yes, who doesn’t know that data scientist is the sexist job in 21th century? If they ask further what is data science and what exactly do data scientists do, it may effectively kill the conversation. Data Science doesn’t come out of the blue. Its predecessor is data analysis. Back in 1962, John Tukey wrote in “The Future of Data Analysis”: For a long time I have thought I was a statistician, interested in inferences from the particular to the general. But as I have watched mathematical statistics evolve, I have had cause to wonder and to doubt. … All in all, I have come to feel that my central interest is in data analysis, which I take to include, among other things: procedures for analyzing data, techniques for interpreting the results of such procedures, ways of planning the gathering of data to make its analysis easier, more precise or more accurate, and all the machinery and results of (mathematical) statistics which apply to analyzing data. It deeply shocked his academic readers. Aren’t you supposed to present something mathematically precise, such as definitions, theorems and proofs? If we use one sentence to summarize what John said, it is : data analysis is more than mathematics. In September 2015, the University of Michigan make plans to invest $100 million over the next five years in a new Data Science Initiative that will enhance opportunities for student and faculty researchers across the university to tap into the enormous potential of big data. UM Provost Martha Pollack said: “Data science has become a fourth approach to scientific discovery, in addition to experimentation, modeling and computation,…” How does DSI define Data science? The web site for DSI gives us an idea: “This coupling of scientific discovery and practice involves the collection, management, processing, analysis, visualization, and interpretation of vast amounts of heterogeneous data associated with a diverse array of scientific, translational, and interdisciplinary applications.” With the data science hype picking up stream, many professionals changed their titles to Data Scientist without any of the necessary qualifications. But at that time, the data scientist title was not well defined which lead to confusion in the market, obfuscation in resumes, and exaggeration of skills. Here is a list of somewhat whimsical definitions for a “data scientist”: “A data scientist is a data analyst who lives in California” “A data scientist is someone who is better at statistics than any software engineer and better at software engineering than any statistician.” “A data scientist is a statistician who lives in San Francisco.” “Data Science is statistics on a Mac.” There is lots of confusion around Data Scientist, Statistician, Business/Financial/Risk(etc) Analyst, BI professional……It is because of the obvious intersections among those. We see data science as a discipline to make sense of data. In order to make sense of data, statistics is indispensable. But a data scientist also needs many other skills. In the obscenity case of Jacobellis v. Ohio (1964), Potter Stewart wrote in his short concurrence that “hard-core pornography” was hard to define, but that “I know it when I see it.” This applies to many things including data science. It is hard to define but you know it when you see it. So instead of scratching my head to figure out a one sentence definition, We are going to sketch the history of data science, what kind of questions data science can answer, and describe the skills required for being a data scientist. We hope this can give you a better depiction of data science. In the early 19th century when Legendre and Gauss came up the least squares method for linear regression, only physicists would use it to fit linear regression. Now, even non-technical people can fit linear regressions using excel. In 1936 Fisher came up with linear discriminant analysis. In the 1940s, we had another widely used model – logistic regression. In the 1970s, Nelder and Wedderburn formulated “generalized linear model (GLM)” which: “generalized linear regression by allowing the linear model to be related to the response variable via a link function and by allowing the magnitude of the variance of each measurement to be a function of its predicted value.” [from Wikipedia] By the end of the 1970s, there was a range of analytical models and most of them were linear because computers were not powerful enough to fit non-linear model until the 1980s. In 1984 Breiman et al. introduced classification and regression tree (CART) which is one of the oldest and most utilized classification and regression techniques. After that Ross Quinlan came up with more tree algorithms such as ID3, C4.5 and C5.0. In the 1990s, ensemble techniques (methods that combine many models’ predictions) began to appear. Bagging is a general approach that uses bootstrapping in conjunction with any regression or classification model to construct an ensemble. Based on the ensemble idea, Breiman came up with random forest in 2001. Later, Yoav Freund and Robert Schapire came up with the AdaBoost.M1 algorithm. Benefiting from the increasing availability of digitized information, and the possibility to distribute that via the internet, the tool box has been expanding fast. The applications include business, health, biology, social science, politics etc. John Tukey identified 4 forces driving data analysis (there was no “data science” then): The formal theories of math and statistics Acceleration of developments in computers and display devices The challenge, in many fields, of more and ever larger bodies of data The emphasis on quantification in an ever wider variety of disciplines Tukey’s 1962 list is surprisingly modern. Let’s inspect those points in today’s context. There is always a time gap between a theory and its application. We had the theories much earlier than application. Fortunately, for the past 50 years statisticians have been laying the theoretical groundwork for constructing “data science” today. The development of computers enables us to calculate much faster and deliver results in a friendly and intuitive way. The striking transition to the internet of things generates vast amounts of commercial data. Industries have also sensed the value of exploiting that data. Data science seems certain to be a major preoccupation of commercial life in coming decades. All the four forces John identified exist today and have been driving data science. 2.2 Is it science? Totally? Let’s take one step back. What is science? Here is what John Tukey said: There are diverse views as to what makes a science, but three constituents will be judged essential by most, viz: (a1) intellectual content, (a2) organization in an understandable form, (a3) reliance upon the test of experience as the ultimate standard of validity The first one (a1) doesn’t provide useful information. And (a2) can’t distinguish science from art very well. The last one is a key character of science. The influential philosopher of science Karl Popper argued that science advances by falsifying hypotheses. If science needs to be falsifiable, then data science is not 100% science. It is true that there are some analytical results that can be validated (falsified) through cross validation or comparing prediction with future outcomes. But certainly not all of them. Even in the problem of prediction, we can’t validate predictions in the 2nd order chaotic systems. We can’t scientifically validate many unsupervised learning or descriptive analysis, especially in the context of marketing. In that sense, data science is a combination of science and art. There is another definition of science from the famous computer scientist Donald Knuth. He said in his legendary 1974 essay Computer Programming as an Art: “Science is knowledge which we understand so well that we can teach it to a computer.” Computers isindispensable for data science. But can we teach computers to do all the work data scientists do today? No. So it is not totally science. Computers can’t communicate with stakeholders to transform a real life problem to be data problem. Computers don’t know which questions can be answered through analytics. Computers don’t know how to explain the results to different audiences using different ways according to their backgrounds. Computers are powerful in many ways but certainly not all. Would a computer enter a ‘runaway reaction’ of self-improvement cycles so that it could surpass human in every way in the future? Well, that is not a question we are trying to answer here. If you are interested in the future of technology, there are some books you can refer to. Ray Kurzweil (The Singularity Is Near), Yuval Noah Harari (Homo Deus: A Brief History of Tomorrow) and Kevin Kelly (The Inevitable). At the risk of being short-sighted, we will assume it won’t happen in foreseeable future. To be simple I will still use data science in the rest of the book. But it is important to realize that data science includes art. 2.3 What kind of questions can data science solve? 2.3.1 What is a good question? Specific How can we increase sales? Dose the January campaign on product X increase the amount of purcahse from our 2017 retained customers? Data Representative Relevant Quality 2.3.2 Types of questions Comparison Description Clustering Classification Regression (linear/non-linear, parametric/non-parametric) 2.4 What are the required skills for data scientist? We talked about the bewildering definitions of data scientist. With the data science hype picking up, some professionals have begun changing their titles to Data Scientist without any of the necessary qualifications (see “Data Scientists…or Data Wannabes”). 2.4.1 Types of Learning There are three broad groups of styles: supervised learning, reinforcement learning, and unsupervised learning. [Skip semi-supervised learning] In supervised learning, each observation of the predictor measurement(s) corresponds to a response measurement. There are two flavors of supervised learning: regression and classification. In regression, the response is a real number such as the total net sales in 2017, or the yield of corn next year. The goal is to approximate the response measurement as much as possible. In classification, the response is a class label, such as dichotomous response such as yes/no. The response can also have more than two categories, such as four segments of customers. A supervised learning model is a function that maps some input variables with corresponding parameters to a response y. Modeling tuning is to adjust the value of parameters to make the mapping fit the given response. In other words, it is to minimize the discrepancy between given response and the model output. When the response y is a real value, it is intuitive to define discrepancy as the squared difference between model output and given the response. When y is categorical, there are other ways to measure the difference, such as AUC, information gain. In reinforcement learning, the correct input/output pairs are not present. The model will learn from a sequence of actions and select the action maximizing the expected sum of the future rewards. There is a discount factor that makes future rewards less valuable than current rewards. Reinforcement learning is difficult for the following reasons: The rewards are not instant. If the action sequence is long, it is hard to know which action was wrong. The rewards are occasional. Each reward does not supply much information, so its impact of parameter change is limited. Typically, it is not likely to learn a large number of parameters using reinforcement learning. However, it is possible for supervised and unsupervised learning. The number of parameters in reinforcement learning usually range from dozens to maybe 1,000, but not millions. In unsupervised learning, there is no response variable. For a long time, the machine learning community overlooked unsupervised learning except for one called clustering. Moreover, many researchers thought that clustering was the only form of unsupervised learning. One reason is that it is hard to define the goal of unsupervised learning explicitly. Unsupervised learning can be used to do the following: Identify a good internal representation or pattern of the input that is useful for subsequent supervised or reinforcement learning, such as finding clusters. It is a dimension reduction tool that is to provide compact, low dimensional representations of the input, such as factor analysis. Provide a reduced number of uncorrelated learned features from original variables, such as principle component regression. 2.4.2 Types of Algorithm Uncertainty: Partial knowledge of state of the world: such as income, social media behavior, competitor’s offer Noisy observations: missing information, measurement with error (food taken), self-justification bias (nobody watches the cat video……) Phenomena not covered by our model: linear assumption, normal assumption Inherent stochasticity: even at a higher level, the modeling limitations of complicated systems are such that one might as well view the world as inherently stochastic. 2.5 General Process of Data Science knitr::include_graphics(&quot;http://scientistcafe.com/book/Figure/GeneralProcessEN.png&quot;) "],
["introduction-to-the-data.html", "Chapter 3 Introduction to the data 3.1 Customer Data for Clothing Company 3.2 Customer Satisfaction Survey Data from Airline Company 3.3 Swine Disease Breakout Data", " Chapter 3 Introduction to the data Before tackling analytics problem, we start by creating data to be analyzed in later chapters. Why do we simulate data instead of using real data set? Going through the simulation code helps you practice R skills. It makes the book less dependent on vagaties of finding and downloading online data set It allows you manipulate the synthetic data, run analysis and examine how the results change 3.1 Customer Data for Clothing Company Our first data set represents customers of a clothing company who sells products in stores and online. This data is typical of what one might get from a company’s marketing data base (the data base will have more data that the one we show here). This data includes 1000 customers for whom we have 3 types of data: Demography age: age of the respondent gender: male/female house: 0/1 variable indicating if the customer owns a house or not Sales in the past year store_exp: expense in store online_exp: expense online store_trans: times of store purchase online_trans: times of online purchase Survey on product preference It is common for companies to survey their customers and draw insights from it to guide future marketing activities. The survey is as below: How strongly do you agree or disagree with the following statements: Strong disagree Disagree Neither agree nor disagree Agree Strongly agree Q1. I like to buy clothes from different brands Q2. I buy almost all my clothes from some of my favorite brands Q3. I like to buy good brand Q4. Quality is the most important factor in my purchasing decision Q5. Style is the most important factor in my purchasing decision Q6. I prefer to buy clothes in store Q7. I prefer to buy clothes online Q8. Price is important Q9. I like to try different style Q10. I like to make a choice by myself and don’t need too much of others’ suggestions There are 4 segments of customers: Price Conspicuous Quality Style The simulation is not very straightforward and I will break it to three parts: Define data structure: variable names, variable distribution, customer segment names, segment size Variable distribution parameters: mean and variance Iterate across segments and variables. Simulate data according to specific parameters assigned By organizing code this way, it makes easy for us to change specific parts of the simulation. For example, if we want to change the distribution of one variable, we can just change the corresponding part of the code. Here is code to define data structure: # set a random number seed to make the process repeatable set.seed(12345) # define the number of observations ncust&lt;-1000 # create a data frmae for simulated data seg_dat&lt;-data.frame(id=as.factor(c(1:ncust))) # assign the variable names vars&lt;-c(&quot;age&quot;,&quot;gender&quot;,&quot;income&quot;,&quot;house&quot;,&quot;store_exp&quot;,&quot;online_exp&quot;,&quot;store_trans&quot;,&quot;online_trans&quot;) # assign distribution for each variable vartype&lt;-c(&quot;norm&quot;,&quot;binom&quot;,&quot;norm&quot;,&quot;binom&quot;,&quot;norm&quot;,&quot;norm&quot;,&quot;pois&quot;,&quot;pois&quot;) # names of 4 segments group_name&lt;-c(&quot;Price&quot;,&quot;Conspicuous&quot;,&quot;Quality&quot;,&quot;Style&quot;) # size of each segments group_size&lt;-c(250,200,200,350) The next step is to define variable distribution parameters. There are 4 segments of customers and 8 parameters. Different segments correspond to different parameters. Let’s store the parameters in a 4×8 matrix: # matrix for mean mus &lt;- matrix( c( # Price 60, 0.5, 120000,0.9, 500,200,5,2, # Conspicuous 40, 0.7, 200000,0.9, 5000,5000,10,10, # Quality 36, 0.5, 70000, 0.4, 300, 2000,2,15, # Style 25, 0.2, 90000, 0.2, 200, 2000,2,20), ncol=length(vars), byrow=TRUE) # matrix for variance sds&lt;- matrix( c( # Price 3,NA,8000,NA,100,50,NA,NA, # Conspicuous 5,NA,50000,NA,1000,1500,NA,NA, # Quality 7,NA,10000,NA,50,200,NA,NA, # Style 2,NA,5000,NA,10,500,NA,NA), ncol=length(vars), byrow=TRUE) Now we are ready to simulate data using the parameters defined above: # simulate non-survey data sim.dat&lt;-NULL set.seed(2016) # loop on customer segment (i) for (i in seq_along(group_name)){ # add this line in order to moniter the process cat (i, group_name[i],&quot;\\n&quot;) # create an empty matrix to store relevent data seg&lt;-data.frame(matrix(NA,nrow=group_size[i], ncol=length(vars))) # Simulate data within segment i for (j in seq_along(vars)){ # loop on every variable (j) if (vartype[j]==&quot;norm&quot;){ # simulate normal distribution seg[,j]&lt;-rnorm(group_size[i], mean=mus[i,j], sd=sds[i,j]) } else if (vartype[j]==&quot;pois&quot;) { # simulate poisson distribution seg[,j]&lt;-rpois(group_size[i], lambda=mus[i,j]) } else if (vartype[j]==&quot;binom&quot;){ # simulate binomial distribution seg[,j]&lt;-rbinom(group_size[i],size=1,prob=mus[i,j]) } else{ # if the distribution name is not one of the above, stop and return a message stop (&quot;Don&#39;t have type:&quot;,vartype[j]) } } sim.dat&lt;-rbind(sim.dat,seg) } Now let’s furbish the data we just simulated a little by adding tags to 0/1 binomial variables: # assign variable names names(sim.dat)&lt;-vars # assign factor levels to segment variable sim.dat$segment&lt;-factor(rep(group_name,times=group_size)) # recode gender and house variable sim.dat$gender&lt;-factor(sim.dat$gender, labels=c(&quot;Female&quot;,&quot;Male&quot;)) sim.dat$house&lt;-factor(sim.dat$house, labels=c(&quot;No&quot;,&quot;Yes&quot;)) # store_trans and online_trans are at least 1 sim.dat$store_trans&lt;-sim.dat$store_trans+1 sim.dat$online_trans&lt;-sim.dat$online_trans+1 # age is integer sim.dat$age&lt;-floor(sim.dat$age) In real world, the data always includes some noise such as missing, wrong imputation. So we will add some noise to the data: # add missing values idxm &lt;- as.logical(rbinom(ncust, size=1, prob=sim.dat$age/200)) sim.dat$income[idxm]&lt;-NA # add wrong imputations and outliers set.seed(123) idx&lt;-sample(1:ncust,5) sim.dat$age[idx[1]]&lt;-300 sim.dat$store_exp[idx[2]]&lt;- -500 sim.dat$store_exp[idx[3:5]]&lt;-c(50000,30000,30000) So far we have created part of the data. You can check it using summary(sim.dat). Next we will move on to simulate survey data. # number of survey questions nq&lt;-10 # mean matrix for different segments mus2 &lt;- matrix( c( # Price 5,2,1,3,1,4,1,4,2,4, # Conspicuous 1,4,5,4,4,4,4,1,4,2, # Quality 5,2,3,4,3,2,4,2,3,3, # Style 3,1,1,2,4,1,5,3,4,2), ncol=nq, byrow=TRUE) # assume the variance is 0.2 for all sd2&lt;-0.2 sim.dat2&lt;-NULL set.seed(1000) # loop for customer segment (i) for (i in seq_along(group_name)){ # the following line is used for checking the progress # cat (i, group_name[i],&quot;\\n&quot;) # create an empty data frame to store data seg&lt;-data.frame(matrix(NA,nrow=group_size[i], ncol=nq)) # simulate data within segment for (j in 1:nq){ # simulate normal distribution res&lt;-rnorm(group_size[i], mean=mus2[i,j], sd=sd2) # set upper and lower limit res[res&gt;5]&lt;-5 res[res&lt;1]&lt;-1 # convert continuous values to discrete integers seg[,j]&lt;-floor(res) } sim.dat2&lt;-rbind(sim.dat2,seg) } names(sim.dat2)&lt;-paste(&quot;Q&quot;,1:10,sep=&quot;&quot;) sim.dat&lt;-cbind(sim.dat,sim.dat2) sim.dat$segment&lt;-factor(rep(group_name,times=group_size)) So far we have gotten all the data. Let’s check it: str(sim.dat,vec.len=3) ## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 1000 obs. of 19 variables: ## $ age : int 57 63 59 60 51 59 57 57 ... ## $ gender : chr &quot;Female&quot; &quot;Female&quot; &quot;Male&quot; ... ## $ income : num 120963 122008 114202 113616 ... ## $ house : chr &quot;Yes&quot; &quot;Yes&quot; &quot;Yes&quot; ... ## $ store_exp : num 529 478 491 348 ... ## $ online_exp : num 304 110 279 142 ... ## $ store_trans : int 2 4 7 10 4 4 5 11 ... ## $ online_trans: int 2 2 2 2 4 5 3 5 ... ## $ Q1 : int 4 4 5 5 4 4 4 5 ... ## $ Q2 : int 2 1 2 2 1 2 1 2 ... ## $ Q3 : int 1 1 1 1 1 1 1 1 ... ## $ Q4 : int 2 2 2 3 3 2 2 3 ... ## $ Q5 : int 1 1 1 1 1 1 1 1 ... ## $ Q6 : int 4 4 4 4 4 4 4 4 ... ## $ Q7 : int 1 1 1 1 1 1 1 1 ... ## $ Q8 : int 4 4 4 4 4 4 4 4 ... ## $ Q9 : int 2 1 1 2 2 1 1 2 ... ## $ Q10 : int 4 4 4 4 4 4 4 4 ... ## $ segment : chr &quot;Price&quot; &quot;Price&quot; &quot;Price&quot; ... 3.2 Customer Satisfaction Survey Data from Airline Company We will simulate a customer satisfaction survey for three airline companies. There are N=1000 respondents and 15 questions. Market researcher asked respondents to recall the experience with different airline companies and assign a score (1-9) to each airline company for all the 15 questions. The higher the score, the more satisfied the customer to the specific item. The 15 questions are of 4 types (the variable names are in the parentheses): How satisfied are you with your______? Ticketing Ease of making reservation（Easy_Reservation） Availability of preferred seats（Preferred_Seats） Variety of flight options（Flight_Options） Ticket prices（Ticket_Prices） Aircraft Seat comfort（Seat_Comfort） Roominess of seat area（Seat_Roominess） Availability of Overhead（Overhead_Storage） Cleanliness of aircraft（Clean_Aircraft） Service Courtesy of flight attendant（Courtesy） Friendliness（Friendliness） Helpfulness（Helpfulness） Food and drinks（Service） General Overall satisfaction（Satisfaction） Purchase again（Fly_Again） Willingness to recommend（Recommend） # Create a matrix of factor loadings # This pattern is called bifactor because it has a general factor for separate components. # For example, &quot;Ease of making reservation&quot; has general factor loading 0.33, specific factor loading 0.58 # The outcome variables are formed as combinations of these general and specific factors loadings &lt;- matrix(c ( # Ticketing .33, .58, .00, .00, # Ease of making reservation .35, .55, .00, .00, # Availability of preferred seats .30, .52, .00, .00, # Variety of flight options .40, .50, .00, .00, # Ticket prices # Aircraft .50, .00, .55, .00, # Seat comfort .41, .00, .51, .00, # Roominess of seat area .45, .00, .57, .00, # Availability of Overhead .32, .00, .54, .00, # Cleanliness of aircraft # Service .35, .00, .00, .50, # Courtesy of flight attendant .38, .00, .00, .57, # Friendliness .60, .00, .00, .50, # Helpfulness .52, .00, .00, .58, # Food and drinks # General .43, .10, .30, .30, # Overall satisfaction .35, .50, .40, .20, # Purchase again .25, .50, .50, .20), # Willingness to recommend nrow=15,ncol=4, byrow=TRUE) # Matrix multiplication produces the correlation matrix except for the diagonal cor_matrix&lt;-loadings %*% t(loadings) # Diagonal set to ones diag(cor_matrix)&lt;-1 # use the mvtnorm package to randomly generate a data set with a given correlation pattern library(mvtnorm) # mean vectors of the 3 airline companies mu1=c(5,6,5,6, 7,8,6,7, 5,5,5,5, 6,6,6) mu2=c(3,3,2,3, 5,4,5,6, 8,8,8,8, 3,3,3) mu3=c(2,2,2,2, 8,8,8,8, 8,8,8,8, 8,8,8) # set random seed set.seed(123456) # respondent ID resp.id &lt;- 1:1000 library(MASS) rating1 &lt;- mvrnorm(length(resp.id), mu=mu1, Sigma=cor_matrix) rating2 &lt;- mvrnorm(length(resp.id), mu=mu2, Sigma=cor_matrix) rating3 &lt;- mvrnorm(length(resp.id), mu=mu3, Sigma=cor_matrix) # truncates scale to be between 1 and 9 rating1[rating1&gt;9]&lt;-9 rating1[rating1&lt;1]&lt;-1 rating2[rating2&gt;9]&lt;-9 rating2[rating2&lt;1]&lt;-1 rating3[rating3&gt;9]&lt;-9 rating3[rating3&lt;1]&lt;-1 # Round to single digit rating1&lt;-data.frame(round(rating1,0)) rating2&lt;-data.frame(round(rating2,0)) rating3&lt;-data.frame(round(rating3,0)) rating1$ID&lt;-resp.id rating2$ID&lt;-resp.id rating3$ID&lt;-resp.id rating1$Airline&lt;-rep(&quot;AirlineCo.1&quot;,length(resp.id)) rating2$Airline&lt;-rep(&quot;AirlineCo.2&quot;,length(resp.id)) rating3$Airline&lt;-rep(&quot;AirlineCo.3&quot;,length(resp.id)) rating&lt;-rbind(rating1,rating2,rating3) # assign names to the variables in the data frame names(rating)&lt;-c( &quot;Easy_Reservation&quot;, &quot;Preferred_Seats&quot;, &quot;Flight_Options&quot;, &quot;Ticket_Prices&quot;, &quot;Seat_Comfort&quot;, &quot;Seat_Roominess&quot;, &quot;Overhead_Storage&quot;, &quot;Clean_Aircraft&quot;, &quot;Courtesy&quot;, &quot;Friendliness&quot;, &quot;Helpfulness&quot;, &quot;Service&quot;, &quot;Satisfaction&quot;, &quot;Fly_Again&quot;, &quot;Recommend&quot;, &quot;ID&quot;, &quot;Airline&quot;) Now check the data frame we have: str(rating,vec.len=3) ## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 3000 obs. of 17 variables: ## $ Easy_Reservation: int 6 5 6 5 4 5 6 4 ... ## $ Preferred_Seats : int 5 7 6 6 5 6 6 6 ... ## $ Flight_Options : int 4 7 5 5 3 4 6 3 ... ## $ Ticket_Prices : int 5 6 6 5 6 5 5 5 ... ## $ Seat_Comfort : int 5 6 7 7 6 6 6 4 ... ## $ Seat_Roominess : int 7 8 6 8 7 8 6 5 ... ## $ Overhead_Storage: int 5 5 7 6 5 4 4 4 ... ## $ Clean_Aircraft : int 7 6 7 7 7 7 6 4 ... ## $ Courtesy : int 5 6 6 4 2 5 5 4 ... ## $ Friendliness : int 4 6 6 6 3 4 5 5 ... ## $ Helpfulness : int 6 5 6 4 4 5 5 4 ... ## $ Service : int 6 5 6 5 3 5 5 5 ... ## $ Satisfaction : int 6 7 7 5 4 6 5 5 ... ## $ Fly_Again : int 6 6 6 7 4 5 3 4 ... ## $ Recommend : int 3 6 5 5 4 5 6 5 ... ## $ ID : int 1 2 3 4 5 6 7 8 ... ## $ Airline : chr &quot;AirlineCo.1&quot; &quot;AirlineCo.1&quot; &quot;AirlineCo.1&quot; ... 3.3 Swine Disease Breakout Data In this section, we are going to simulate a data set about swine disease. We simulate 800 farms (i.e. n=800) and 120 survey questions (i.e. G=120) in each data set. There are three possible answers for each question. The outbreak status for the \\(i^{th}\\) farm is generated from a \\(Bernoulli(1, p_i)\\) distribution with \\(p_i\\) being a function of the question answers: \\[ln(\\frac{p_i}{1-p_i})=\\beta_0 + \\Sigma_{g=1}^G\\mathbf{x_{i,g}^T\\beta_{g}}\\] where \\(\\beta_0\\) is the intercept, \\(\\mathbf{x_{i,g}}\\) is a three dimensional indication vector for question answer and \\(\\mathbf(\\beta_g)\\) is the parameter vector corresponding to the \\(g^{th}\\) predictor. Three types of questions are considered regarding their effects on the outcome. The first forty survey questions are important questions such that the coefficients of the three answers to these questions are all different: \\[\\mathbf{\\beta_g}=(1,0,-1)\\times \\gamma,\\ g=1,\\dots,40\\] The second forty survey questions are also important questions but only one answer has a coefficient that is different from the other two answers: \\[\\mathbf{\\beta_g}=(1,0,0)\\times \\gamma,\\ g=41,\\dots,80\\] The last forty survey questions are also unimportant questions such that all three answers have the same coefficients: \\[\\mathbf{\\beta_g}=(0,0,0)\\times \\gamma,\\ g=81,\\dots,120\\] The baseline coefficient \\(\\beta_0\\) is set to be \\(-\\frac{40}{3}\\gamma\\) so that on average a farm have \\(50%\\) of chance to have an outbreak. The parameter \\(\\gamma\\) in the above simulation is set to control the strength of the questions’ effect on the outcome. In this simulation study, we consider the situations where \\(\\gamma = 0.1, 0.25, 0.5, 1, 2\\). So the parameter settings are: \\[\\mathbf{\\beta^{T}}=\\left(\\underset{question\\ 1}{\\frac{40}{3},\\underbrace{1,0,-1}},...,\\underset{question\\ 40}{\\underbrace{1,0,-1}},\\underset{question\\ 41}{\\underbrace{1,0,0}},...,\\underset{question\\ 80}{\\underbrace{1,0,0}},\\underset{question\\ 81}{\\underbrace{0,0,0}},...,\\underset{question\\ 120}{\\underbrace{0,0,0}}\\right)*\\gamma\\] For each value of \\(\\gamma\\), 20 data sets are simulated. The bigger \\(\\gamma\\) is, the larger the corresponding parameter. We provided the data sets with \\(\\gamma = 2\\). Let’s check the data: disease_dat&lt;-read_csv(&quot;https://raw.githubusercontent.com/happyrabbit/DataScientistR/master/Data/sim1_da1.csv&quot;) # only show the last 7 columns here head(subset(disease_dat,select=c( &quot;Q118.A&quot;,&quot;Q118.B&quot;,&quot;Q119.A&quot;,&quot;Q119.B&quot;,&quot;Q120.A&quot;,&quot;Q120.B&quot;,&quot;y&quot;))) ## # A tibble: 6 x 7 ## Q118.A Q118.B Q119.A Q119.B Q120.A Q120.B y ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1 0 0 0 0 1 1 ## 2 0 1 0 1 0 0 1 ## 3 1 0 0 0 1 0 1 ## 4 1 0 0 0 0 1 1 ## 5 1 0 0 0 1 0 0 ## 6 1 0 0 1 1 0 1 Here y indicates the outbreak situation of the farms. y=1 means there is an outbreak in 5 years after the survey. The rest columns indicate survey responses. For example Q120.A = 1 means the respondent chose A in Q120. We consider C as baseline. "],
["big-data-cloud-platform.html", "Chapter 4 Big Data Cloud Platform 4.1 How Data become Science? 4.2 Power of Cluster of Computers 4.3 Evolution of Clustering Computing 4.4 Introduction of Cloud Environment 4.5 Open Account and Create a Cluster 4.6 R Notebook 4.7 Markdown Cells 4.8 Leverage Hadoop and Spark Parallel using R Notebook 4.9 Database Basic and SQL 4.10 Other Useful Topics", " Chapter 4 Big Data Cloud Platform 4.1 How Data become Science? Data has been a friend of statistician for hundreds of years. Tabulated data are the most familiar format that we use daily. Tabulated data has been stored in pieces of paper, or tapes, or diskettes, or hard drives. Only very recently, with the development of computer hardware, software and algorithms, the volume, variety, and speed of the data suddenly beyond the capacity of traditional statistician. And data becomes a special science with the very first focus on a fundamental question: with huge amount of data, how can we store the data and quick access and process the data. In the past a few years, by utilizing commodity hardware and open source software, a big data ecosystem was created for data storage, data retrieval and parallel computation. Hadoop and Spark have become a popular platform that enable data scientist, statistician and business analyst to access the data and to build models. Programming skills in the big data platform has been the largest gap for statistician to become a successful data scientist. However, with the recent wave of cloud computing, this gap is greatly reduced. Many of the technical details have been pushed to the background and the user interface becomes much easier to learn. Cloud systems also enable quick implementation to the production environment. Now data science is emphasis more on the data itself as well as models and algorithms on top of the data instead of platform and infrastructure. 4.2 Power of Cluster of Computers We are all familiar with our laptop / desktop computers which contain mainly three components to finish computation with data: (1) Hard disk, (2) Memory, and (3) CPU as shown in Figure 41 left. The data and codes are stored in hard disk which has certain features such as relatively slow for read and write and relatively large capacity of around a few TB in today’s market. Memory is relatively fast for read and write but relatively small in capacity in the order of a few dozens of GB in today’s market. CPU is where all the computation is done. For statistical software such as R, the amount of data that it can process is limited by the computer’s memory. For a typical computer before year 2000, the memory is less than 1 GB. The memory capacity grows far slower than the availability of the data to analyze. Now it is quite often that we need to analyze data far beyond the capacity of a single computer’s memory, especially in enterprise environment. Meanwhile the computation time is growing faster than linear to solve the same problem (such as regressions) as the data size increases. Using a cluster of computers become a common way to solve big data problem. In Figure 41 (right), a cluster of computers can be viewed as one powerful machine with total memory, hard disk and CPU equivale to the sum of individual computers. It is common to have thousands of nodes for a cluster. In the past, to use a cluster of computers, users must write special codes such as (such as MPI) to take care of how data is allocated across memory and how the computation is done in a parallel fashion. Luckily with the recent new development, users are leverage a more user-friendly cloud environment for big data analysis. As data is typically beyond the size of one hard disk, the dataset itself is stored across different nodes’ hard disk (i.e. the Hadoop system mentioned below). When we perform analysis, we can assume the needed data is already distributed across many node’s memories in the cluster and algorithm are parallel in nature to leverage corresponding nodes’ CPUs to compute (i.e. the Spark system mentioned below). 4.3 Evolution of Clustering Computing Using computer clusters to solve general purpose data and analytics problems needs a lot of efforts if we have to specifically control every elements and steps such as data storage, memory allocation and parallel computation. Fortunately, high tech IT companies and open source communities have developed the entire ecosystem based on Hadoop and Spark. Users need only to know high-level scripting language such as Python and R to leverage computer clusters’ storage, memory and computation power. 4.3.1 Hadoop The very first problem internet companies face is that a lot of data has been collected and how to better store these data for future analysis. Google developed its own file system to provide efficient, reliable access to data using large clusters of commodity hardware. The open source version is known as Hadoop Distributed File System (HDFS). Both system use Map-Reduce to allocate computation across computation nodes on top of the file system. Hadoop in written in Java and writing map-reduce job using Java is a direct way to interact with Hadoop which are not familiar to data and analytics community. To help better use Hadoop system, a SQL like data warehouse system called Hive, and a scripting language for analytics interface called Pig were introduced for people with analytics background to interact with Hadoop system. Within Hive, we can create user defined function though R or Python to leverage the distributed and parallel computing infrastructure. Map-reduce on top of HDFS is the main concept of Hadoop ecosystem and each map-reduce operation require retrieve data from hard disk, computation, and then store the result into hard disk again. So, jobs on top of Hadoop requires a lot of disk operation which may slow down the computation process. 4.3.2 Spark Spark works on top of distributed file system including Hadoop with better data and analytics efficiency by leveraging in-memory operations and more tailored for data processing and analtyics. The spark system includes a SQL-like framework called Spark SQL and a parallel machine learning library called MLib. Another good news for data and analytics community is that Spark supports R and Python. We can interact with data stored in distributed file system using parallel computing across nodes easily with R and Python through Spark API and do not need to worry about how the data and computation are distributed across the cluster. We will introduce how to use R notebook to drive Spark computations. 4.4 Introduction of Cloud Environment There are many cloud computing environment such as Amazon’s AWS which provides complete list of functions for heavy duty enterprise applications. For example, Netflix runs its business entirely on AWS and Netflix does not own any data centers. For beginners, Databricks provides an easy to use cloud system for learning purpose. Databricks is a company founded by the creators of Apache Spark and it provides a user-friendly web-based notebook environment that can create Hadoop/Spark/GPU cluster on the fly and run R/Python/Scala/SQL. We will use Databricks’ community edition to run demos in this book. Please note the content of this section is adopted from the following web pages: https://docs.databricks.com/user-guide/faq/sparklyr.html http://spark.rstudio.com/index.html 4.5 Open Account and Create a Cluster Anyone can apply for a community edition for free through https://databricks.com/try-databricks and a short YouTube video illustrates the application process can be found https://youtu.be/vx-3-htFvrg. Another short YouTube video shows how to create a cluster for a cloud computation environment and create a R notebook to run R codes which can be found at https://youtu.be/0HFujX3t6TU. In fact, you can run Python/R/Scala/SQL cells, as well as markdown cells, in the same notebook by include a keyword at the beginning of each cell that we will discuss later. 4.6 R Notebook In last section video, we just created an R notebook. For an R notebook, it contains multiple cells and by default the content within each cell are R scripts. Usually each cell is a well-managed a few lines of codes that accomplish a specific task. For example, Figure 42 shows the default cell for an R notebook for cell 1. We can type in R scripts and comments same as we are using R console. By default, only the result from the last line will be shown following the cell. However, you can use print() function to output results for any lines. If we move the mouse to middle of lower edge of the cell below the results, a “+” symbol will show up and clicking on the symbol will insert a new cell below. When you click any area within a cell, it will make it editable and you will see a few icons on the top right corn of the cell where you can run the cell, as well as add a cell below or above, copy cell, cut cell, high cell etc. One quick way to run the cell is Shift+Enter when the cell is chosen. You will become familiar with the notebook environment quickly. 4.7 Markdown Cells For an R notebook, every cell by default will contain R scripts. But if we put %md, %sql or %python at the first line of a cell, that cell becomes Markdown cell, SQL script cell and Python script cell accordingly. For example, Figure 43 shows a markdown cell with scripts and the actual appearance when exits editing mode. Markdown cell provide a straightforward way to descript what each cell is doing as well as what the entire notebook is about. It is a better way than simple comment within in the code. 4.8 Leverage Hadoop and Spark Parallel using R Notebook R is a powerful tool for data analysis given the data can be fit into memory. Because of the memory bounded dataset limit, R itself cannot be used directly for big data analysis where the data is likely stored in Hadoop and Spark system. By leverage sparklyr package created by RStudio, we can use Databricks’ R notebook to analyze data stored in Spark system where the data are stored across different nodes and computation are parallel in nature to use the collection of memory units across all nodes. And the process is relative simple. In this section, we will illustrate how to use Databricks’ R notebook for big data analysis on top of Spark environment through sparklyr package. 4.8.1 Library Installation First, we need to install sparklyr package which enables the connection between master or local node to Spark cluster environments. As it will install more than 10 dependencies, it may take more than 5 minutes to finish. Be patient while it is installing! Once the installation finishes, load the sparklyr package as illustrated by the following code: # Installing sparklyr takes a few minutes, # because it installs +10 dependencies. if (!require(&quot;sparklyr&quot;)) { install.packages(&quot;sparklyr&quot;) } # Load sparklyr package. library(sparklyr) 4.8.2 Create Connection Once the library is loaded, we need create a Spark Connection to link master / local node to Spark environment. Here we use the “databricks” option for parameter method which is specific for databricks’ system. In enterprise environment, please consult your administrator for details. The created Spark Connection (i.e. sc) will be the pipe that connect master / local / terminal to the Spark Cluster. We can think of the web interface / terminal is running on a master node which has its local memory and CPU. The Spark Connection can be established with: # create a sparklyr connection sc &lt;- spark_connect(method = &quot;databricks&quot;) 4.8.3 Sample Dataset To simplify the learning process, let us use a very familiar dataset: the iris dataset. It is part of the dplyr library and let’s load that library to use the iris data frame. Here the iris dataset is still in the local node where the R notebook is running on. And we can see that the first a few lines of the iris dataset below the code after running: library(dplyr) head(iris) 4.8.4 IMPORTANT - Copy Data to Spark Environment In real applications, your data is usually very big and cannot fit into one hard disk and it is very likely your data is already in Hadoop/Spark ecosystem. You can use SparkDataFrame to analyze your data in Spark system directly. Here, we illustrate how to copy a local dataset to Spark environment and then work on that dataset in the Spark system. As we have already created the Spark Connection sc, it is fairly simple to copy data to spark system by sdf_copy_to() function as below: iris_tbl &lt;- sdf_copy_to(sc = sc, x = iris, overwrite = T) The above one line code copies iris dataset from local node to Spark cluster environment where sc is the Spark Connection we just created; x is the data frame that we want to copy; and overwrite is the option whether we want to overwrite the target object if the same name SparkDataFrame exists in the Spark environment. Finally sdf_copy_to() function will return an R object wrapping the copied SparkDataFrame. So irir_tbl can be used to refer to the iris SparkDataFrame. To check whether the iris dataset was copied to Spark environment successfully or not, we can use src_tbls( ) function to the Spark Connection (sc): src_tbls(sc) ## code to return all the dataframes associated with sc 4.8.5 Analyzing the Data Now we have successfully copied the iris dataset to the Spark environment as a SparkDataFrame. And iris_tbl is an R object wrapping the iris SparkDataFrame and we can use iris_tbl to refer the iris dataset in the Spark system (i.e. the iris SparkDataFrame). With the sparklyr packages, we can use many functions in dplyr to SparkDataFrame directly through iris_tbl, same as we are applying dplyr functions to a local R data frame in our laptop. For example, we can use %&gt;% operator to pass iris_tbl to count( ) function: iris_tbl %&gt;% count or using the head( ) function to return to return the first a few rows in iris_tbl: head(iris_tbl) or more advanced data manipulation directly to iris_tbl: iris_tbl %&gt;% mutate(Sepal_Width = ROUND(Sepal_Width * 2) / 2) %&gt;% # Bucketizing Sepal_Width group_by(Species, Sepal_Width) %&gt;% summarize(count = n(), Sepal_Length = mean(Sepal_Length), stdev = sd(Sepal_Length)) 4.8.6 Collect Results Back to Master Node Even though we can run many of the dplyr functions on SparkDataFrame, we cannot apply functions from other packages to SparkDataFrame direction (such as ggplot()). For functions that can only work on local R data frames, we must copy the SparkDataFrame back to the local node. To copy SparkDataFrame back to the local node, we use the collect() function where the argument to it is the name of the SparkDataFrame. The following code collect() the results of a few operations and assign the collected data to iris_summary variable: iris_summary &lt;- iris_tbl %&gt;% mutate(Sepal_Width = ROUND(Sepal_Width * 2) / 2) %&gt;% group_by(Species, Sepal_Width) %&gt;% summarize(count = n(), Sepal_Length = mean(Sepal_Length), stdev = sd(Sepal_Length)) %&gt;% collect Now iris_summary is a local variable to the R notebook and we can use all R packages and functions to it. In the following code, we will apply ggplot() to it, exactly the same as a stand along R console: library(ggplot2) ggplot(iris_summary, aes(Sepal_Width, Sepal_Length, color = Species)) + geom_line(size = 1.2) + geom_errorbar(aes(ymin = Sepal_Length - stdev, ymax = Sepal_Length + stdev), width = 0.05) + geom_text(aes(label = count), vjust = -0.2, hjust = 1.2, color = &quot;black&quot;) + theme(legend.position=&quot;top&quot;) 4.8.7 Fit Regression to SparkDataFrame One of the largest advantage is that, within Spark system, there are already many statistical and machine learning algorithms developed to run parallel across many CPUs with data across many memory units. So, we can easily fit a linear regression for big dataset far beyond the memory limit of one single computer. Below is an illustration of how to fit a linear regression to SparkDataFrame using R notebook: fit1 &lt;- ml_linear_regression(x = iris_tbl, response = &quot;Sepal_Length&quot;, features = c(&quot;Sepal_Width&quot;, &quot;Petal_Length&quot;, &quot;Petal_Width&quot;)) summary(fit1) In the above code, x is the R object wrapping the SparkDataFrame; response is the y-variable, features is the collection of explanatory variables. 4.8.8 Fit a K-means Cluster Through sparkly package, we can use R notebook to access many Spark Machine Learning Library (MLlib) algorithms such as linear regression, logistic regression, Survival Regression, Generalized Linear Regression, Decision Trees, Random Forests, Gradient-Boosted Trees, Principal Components Analysis, Naive-Bayes, K-Means Clustering and a few other methods. Below codes fit a k-means cluster algorithm: ## Now fit a k-means clustering using iris_tbl data ## with only two out of four features in iris_tbl fit2 &lt;- ml_kmeans(x = iris_tbl, centers = 3, features = c(&quot;Petal_Length&quot;, &quot;Petal_Width&quot;)) # print our model fit print(fit2) After the k-means model is fit, we can apply the model to predict other datasets through sdf_predict() function. Below code apply the model to iris_tbl again to predict and then the results are collected back to local variable prediction through collect() function: prediction = collect(sdf_predict(fit2, iris_tbl)) As prediction is a local variable, we can apply any R functions from any libraries to it. For example: prediction %&gt;% ggplot(aes(Petal_Length, Petal_Width)) + geom_point(aes(Petal_Width, Petal_Length, col = factor(prediction + 1)), size = 2, alpha = 0.5) + geom_point(data = fit2$centers, aes(Petal_Width, Petal_Length), col = scales::muted(c(&quot;red&quot;, &quot;green&quot;, &quot;blue&quot;)), pch = &#39;x&#39;, size = 12) + scale_color_discrete(name = &quot;Predicted Cluster&quot;, labels = paste(&quot;Cluster&quot;, 1:3)) + labs( x = &quot;Petal Length&quot;, y = &quot;Petal Width&quot;, title = &quot;K-Means Clustering&quot;, subtitle = &quot;Use Spark.ML to predict cluster membership with the iris dataset.&quot; ) 4.8.9 Summary In the above a few sub-sections, we illustrated (1) the relationship between master / local node and Spark Clusters; (2) how to copy a local data frame to a SparkDataFrame (please note if your data is already in Spark environment, there is no need to copy. This is likely to be the case for enterprise environment); (3) how to manipulate SparkDataFrame through dplyr functions with the installation of sparklyr package; (4) how to fit statistical and machine learning models to SparkDataFrame; and (5) how to collect information from SparkDataFrame back to a local data frame for future analysis. These procedures are pretty much covered the basis of big data analysis that a data scientist need to know. The above steps are published as an R notebook: https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/2961012104553482/3725396058299890/1806228006848429/latest.html 4.9 Database Basic and SQL Database has been around many years to efficient store, organize, retrieve, and update data in a systematic way. In the past, statistician usually deal with small dataset where the power of database is not significant and data are usually in the form of a text file such as csv or excel sheet. Students from traditional statistics departments usually lack the needed database knowledge which are essential and required in enterprise environment where data are stored in some form of database. Database is a system that contain a collection of tables and the relationship among these tables (i.e. schema). Table is the fundamental structure for database which contains rows and columns same as a data frame in R or Python pandas. DBMS ensures data integrate and security in real time operations. There are many database management systems (DBMS) such as Oracle, SQL Server, MySQL, Teradata, Hive, Redshift and Hana. Majority of database operations are very similar among different DBMS, and Structured Query Language (SQL) is the standard to use these systems. SQL became a standard of the American National Standards Institute (ANSI) in 1986, and of the International Organization for Standardization (ISO) in 1987. The most recent version is published at December 2016. For typical users, the fundamental knowledge is the nearly the same. In addition to the standard features, each DBMS providers include their own functions and features. So, for the same query, it may be different implementations (i.e. SQL script) for different systems. In this section, we use the Databricks’ SQL implantation (i.e. all the SQL scripts can run in Databricks SQL notebook. More recently data are stored in distributed system such as Hive in Hadoop or in memory such as Hana. Most relational database are row-based (i.e. data for each row are stored closely), and more efficient database for analytics are column-based (i.e. data for each column are stored closely). Fortunately, as a database user, we only need to learn how to write SQL scripts to retrieve and manipulate data. Even though there are different implantations of SQL across different DBMS, SQL is nearly universal across relational database including Hive and Spark, which means once we know SQL, our knowledge can be transferred among different database systems. SQL is easy to learn even if you do not have previously experience. In this session, we will go over the key concepts in database and SQL. A more detailed description of database basic can be found through a list of YouTube videos using a specific text book: https://www.youtube.com/playlist?list=PLtqstN-ayEb0H5AAo6_V5qEzWs0D-igpw 4.9.1 Database, Table and View A database is a collection of tables that are related to each other. A database has its own database name and each table has its name as well. We can think database is a “folder” where tables within a database are “files” within the folder. A table has rows and columns exactly as an R data frame. Each row (also called record) represents a unique instance of the subject and each column (also called field or attribute) represents a characteristic of the subject in the table. For each table, there is a special column called primary key which uniquely identifies each of its record. Tables within a specific database contains related information and the schema of a database illustrates all fields in every table as well as how these tables and fields relate to each other. Tables can be joined and aggregated to return specific information. View a virtual table composed of fields from one or more base tables. View does not store data, and only store table structure. Also referred as a saved query. View is typically used to protect the data stored in the table and users can only query information from a view and cannot change or update its contents. 4.9.2 Sample Tables We will use two simple tables to illustrate basic SQL operations. These two tables are from R dataset library’s state which contains US 50 states’ population and income. The first table is called divisions which has two columns: state and division and a first a few rows are shown in the following table: The second table is called metrics which contains three columns: state, population and income and first a few rows of the table is shown below: To illustrate missing information, three more rows are added at the end of the original division table with state Alberta, Ontario, and Quebec with their corresponding division NULL. Please watch the following YouTube video on how to upload a text csv file to a Databricks table: https://youtu.be/H5LxjaJgpSk 4.9.3 Basic SQL Statement After logging the Databricks and creating two tables, you can now open a notebook and now let’s choose the type of the notebook to be SQL where you can type in SQL statement and run. There a few very easy SQL statement to help us understand the database and table structure: show database: show current databases in the system create database db_name: create a new database with name db_name drop database db_name: delete database db_name (be careful when use it!!) use db_name: set up the current database to be used show tables: show all the tables within the currently used database describe tbl_name: show the structure of table with name tbl_name (i.e. list of column name and data type) drop tbl_name: delete a table with name tbl_name (be careful when use it!!) select * from metrics limit 10: show the first 10 rows of a table If you are familiar with procedural programming language such as C and FORTRN or scripting language such as R and Python, you may find SQL code a little bit strange. We should view SQL code by each specific chuck where it finishes a specific task. SQL codes descript a specific task and DBMS run and finish the task. 4.9.4 Simple SELECT Statement SELECT is the most used statements in SQL, especially for database users and business analyst. It is used to extract specific information (i.e. column or columns) FROM one or multiple tables. It can be used to combine multiple columns. WHERE can be used in SELECT statement to selected rows with specific conditions. ORDER BY can be used in SELECT statement to order the results in descend or ascend order. We can use * after SELECT to represent all columns in the table. Below is the basic structure of a SELECT statement: SELECT Col_Name1, Col_Name2 FROM Table_Name WHERE Specific_Condition ORDER BY Col_Name1, Col_Name2; where specific_condition is the typical logical conditions and only columns with TRUE will be chosen. For example, if we want to choose states and its total income where the population larger than 10000 and income less than 5000 with the result order by state name, we can use the following query select state, income*population as total_income from metrics where population &gt; 10000 and income &lt; 5000 order by state Simple SELECT statement is usually used to slicing and dicing the dataset as well as create new columns of interest using basic computation functions. 4.9.5 Aggregation Functions and GROUP BY We can also use aggregation functions in SELECT statement to summarize the data. For example, count(col_name) function will return the total number of not NULL rows for a specific column. Other aggregation function on numerical values include min(col_name), max(col_name), avg(col_name). Let’s use metrics table again to illustrate aggregation functions. For aggregation function, it takes all the rows that match WHERE condition (if any) and return one number. The following statement will calculate the maximum, minimum, and average population for all states starts with letter A to E. select sum(population) as sum_pop, max(population) as max_pop, min(population) as min_pop, avg(population) as avg_pop, count(population) as count_pop from metrics where substring(state, 1, 1) in (&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;D&#39;, &#39;E&#39;) The results from the above query only return one row as expected. Sometimes we want to find the aggregated value based on groups that can be defined by one or more columns. Instead of writing multiple SQL to calculate the aggregated value for each group, we can easily use the GROUP BY to calculate the aggregated value for each group in more SELECT statement. For example, if we want of find how many states in each division, we can use the following: select division, count(state)as number_of_states from divisions group by division Another special aggregation function is to return distinct values for one column or a combination of multiple columns. Simple use SELECT DISTINCT col_name1, col_name2 in the first line of the SELECT statement. 4.9.6 Join Multiple Tables The database system is usually designed such that each table contain a piece of specific information and oftentimes we need to JOIN multiple tables to achieve a specific task. There are few types typically JOINs: inner join (keep only rows that match the join condition from both tables), left outer join (rows from inner join + unmatched rows from the first table), right outer join (rows from inner join + unmatched rows from the second table) and full outer join (rows from inner join + unmatched rows from both tables). The typical JOIN statement is illustrated below: SELECT a.col_name1 as var1, b.col_name2 as var2 FROM tbl_one as a INNER/LEFT JOIN tabl_two ad b ON a.col_to_match = b.col_to_match For example, let’s join the division table and metrics table to find what is the average population and income for each division, and the results order by division names: select a.division, avg(b.population) as avg_pop, avg(b.income) as avg_inc from divisions as a inner join metrics as b on a.state = b.state group by division order by division 4.9.7 Add More Content into a Table We can use INSERT statement to add additional rows into a specific table, for example, we can add one more row to the metrics table by using the following query: insert into metrics values (&#39;Alberta&#39;, 4146, 7370) 4.9.8 Advanced Topics in Database Database management is a specific research areas and there are many advance topics such as how to efficiently query data using index; how to take care of data integrity when multiple users are using the same table; algorithm behind data storage (i.e. column-wise or row-wise data storage); how to design the database schema. As a typical user, these advanced topics can be learnt gradually. We hope the basic knowledge covered in this section will kick off the initial momentum to learnt SQL. As you can see, it is fairly easy to write SQL statement to retrieve, join, slice, dice and aggregate data. All the SQL scripts can be found in this notebook: https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/2961012104553482/4213233595765185/1806228006848429/latest.html 4.10 Other Useful Topics For data scientist, in additional to the above-mentioned areas, the following topics are also very important to get some exposure. 4.10.1 Linux Operation System Many of the cloud environment, servers and production systems are usually running on top of Linux operation system and some basic understanding of Linux is essential to solve various problems in a data science project. Linux system is a multiple-user system that runs robustly without interrupt for months. For a typical user, we can access to some of the functions through a commend-line type terminal. Here is a list of commonly used command: ls : show files in current directory pwd : display current directory and path mkdir dir_name : create a new directory cd dir_path: change directory through its path cd .. : go one directory level up cp file1 file2 : copy file1 to file2 mv file1 file2: rename file1 to file2 head file : show the first a few rows of file tail file : show last a few rows of file top : show current running job who : list all users that log in the system There are many Linux training material available online. Once you have a need to learn Linux, you can always learn by yourself though these online training materials. The Linux system will be configured by your system administrator, please always ask your colleague and system administrator for suggestions. Some of the commands may knock the entire system down or permanently delete useful information, please be very careful and never try any commands that you do not know exactly the consequence. There are horrible stories regarding accidence that made significant business impact due to bad operations. 4.10.2 Visualization R and Python both provide nice visualization capability and R studio even provides dynamic dashboard to illustrate real time data analytics. However, in enterprise environment, Tableau is still the most used dashboard visualization system. More recently, HTML5 and D3 become popular for data visualization. For a successful data scientist, we need to have our own recommendation of what types of visualization are useful and we may not need to implement by ourselves but we need to guide the team who implement the system. 4.10.3 GPU Many of the machine learning methods are based on linear algebra, especially deep learning related neural network algorithm. CPU is not designed to handle large-size matrix linear algebra and GPU is an efficient alternative for matrix-based linear algebra computation. In Databricks Community Edition, we can also create a GPU machine to use. If you are interested in deep learning using Spark and GPU through Databrick, please watch this video for more detail: https://databricks.com/blog/2016/10/27/gpu-acceleration-in-databricks.html "],
["data-wrangling.html", "Chapter 5 Data Wrangling 5.1 Read and write data 5.2 Summarize data 5.3 Tidy and Reshape Data", " Chapter 5 Data Wrangling This chapter focuses on some of the most frequently used data manipulations and shows how to implement them in R. It is important to explore the data set with descriptive statistics (mean, standard deviation, etc.) and data visualization prior to analysis. Transform data so that the data structure is in line with the requirements of the model. You also need to summarize the results after analysis. Here we assume the readers are already familiar with some of the traditional R data operations, such as subsetting data frame, deleting variables, read and write functions (read.csv (), write.csv (), etc.) in base R. We will also skip some basic descriptive functions in R. For example, for discrete variables, we often use the frequency table to look at the frequency (table ()) of the variable at various levels as needed, or a crosstab of two variables. You can also draw a bar chart for discrete variables (bar()). For continuous variables, we need to look at the mean (mean ()), standard deviation (sd()), quantile (quantile()) of a variable from time to time. There are also functions like summary(), str() and describe() (a functions in the ‘psych’ package) that give a summary of a data frame. The focus here is to introduce some of the more efficient data wrangling methods in R. 5.1 Read and write data 5.1.1 readr You must be familar with read.csv(), read.table() and write.csv() in base R. Here we will introduce a more efficient package from RStudio in 2015 for reading and writing data: readr package. The corresponding functions are read_csv(), read_table() and write_csv(). The commands look quite similar, but readr is different in the following respects: It is 10x faster. The trick is that readr uses C++ to process the data quickly. It doesn’t change the column names. The names can start with number and “.” will not be substitued to “_”. For example: library(readr) read_csv(&quot;2015,2016,2017 1,2,3 4,5,6&quot;) ## # A tibble: 2 x 3 ## `2015` `2016` `2017` ## &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1 2 3 ## 2 4 5 6 readr functions do not convert strings to factors by default, are able to parse dates and times and can automatically determine the data types in each column. The killing character in my opinion is that readr provides progress bar. What makes you feel worse than waiting is not knowing how long you have to wait. Without “progress bar” might be the No.1 reason that people break up with the one they have been dating. The major functions of readr is to turn flat files into data frames: read_csv(): reads comma delimited files read_csv2(): reads semicolon separated files (common in countries where , is used as the decimal place) read_tsv(): reads tab delimited files read_delim(): reads in files with any delimiter read_fwf(): reads fixed width files. You can specify fields either by their widths with fwf_widths() or their position with fwf_positions() read_table(): reads a common variation of fixed width files where columns are separated by white space read_log(): reads Apache style log files The good thing is that those functions have similar syntax. Once you learn one, the others become easy. Here we will focus on read_csv(). The most important information for read_csv() is the path to your data: library(readr) sim.dat &lt;- read_csv(&quot;https://raw.githubusercontent.com/happyrabbit/DataScientistR/master/Data/SegData.csv &quot;) head(sim.dat) ## # A tibble: 6 x 19 ## age gender income house store_exp online_exp store_trans ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 57 Female 120963.4 Yes 529.1344 303.5125 2 ## 2 63 Female 122008.1 Yes 478.0058 109.5297 4 ## 3 59 Male 114202.3 Yes 490.8107 279.2496 7 ## 4 60 Male 113616.3 Yes 347.8090 141.6698 10 ## 5 51 Male 124252.6 Yes 379.6259 112.2372 4 ## 6 59 Male 107661.5 Yes 338.3154 195.6870 4 ## # ... with 12 more variables: online_trans &lt;int&gt;, Q1 &lt;int&gt;, Q2 &lt;int&gt;, ## # Q3 &lt;int&gt;, Q4 &lt;int&gt;, Q5 &lt;int&gt;, Q6 &lt;int&gt;, Q7 &lt;int&gt;, Q8 &lt;int&gt;, Q9 &lt;int&gt;, ## # Q10 &lt;int&gt;, segment &lt;chr&gt; The function reads the file to R as a tibble. You can consider tibble as next iteration of data frame. They are different with data frame for the following aspects: It never changes an input’s type (i.e., no more stringsAsFactors = FALSE!) It never adjusts the names of variables It has a refined print method that shows only the first 10 rows, and all the columns that fit on screen. You can also control the default print behavior by setting options. Refer to http://r4ds.had.co.nz/tibbles.html for more information about ‘tibble’. When you run read_csv() it prints out a column specification that gives the name and type of each column. In order to better understanding how readr works, it is helpful to type in some baby data set and check the results: dat=read_csv(&quot;2015,2016,2017 100,200,300 canola,soybean,corn&quot;) print(dat) ## # A tibble: 2 x 3 ## `2015` `2016` `2017` ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 100 200 300 ## 2 canola soybean corn You can also add comments on the top and tell R to skip those lines: dat=read_csv(&quot;# I will never let you know that # my favorite food is carrot Date,Food,Mood Monday,carrot,happy Tuesday,carrot,happy Wednesday,carrot,happy Thursday,carrot,happy Friday,carrot,happy Saturday,carrot,extremely happy Sunday,carrot,extremely happy&quot;, skip = 2) print(dat) ## # A tibble: 7 x 3 ## Date Food Mood ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Monday carrot happy ## 2 Tuesday carrot happy ## 3 Wednesday carrot happy ## 4 Thursday carrot happy ## 5 Friday carrot happy ## 6 Saturday carrot extremely happy ## 7 Sunday carrot extremely happy If you don’t have column names, set col_names = FALSE then R will assign names “X1”,“X2”… to the columns: dat=read_csv(&quot;Saturday,carrot,extremely happy Sunday,carrot,extremely happy&quot;, col_names=FALSE) print(dat) ## # A tibble: 2 x 3 ## X1 X2 X3 ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Saturday carrot extremely happy ## 2 Sunday carrot extremely happy You can also pass col_names a character vector which will be used as the column names. Try to replace col_names=FALSE with col_names=c(&quot;Date&quot;,&quot;Food&quot;,&quot;Mood&quot;) and see what happen. As mentioned before, you can use read_csv2() to read semicolon separated files: dat=read_csv2(&quot;Saturday; carrot; extremely happy \\n Sunday; carrot; extremely happy&quot;, col_names=FALSE) print(dat) ## # A tibble: 2 x 3 ## X1 X2 X3 ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Saturday carrot extremely happy ## 2 Sunday carrot extremely happy Here “\\n” is a convenient shortcut for adding a new line. You can use read_tsv() to read tab delimited files： dat=read_tsv(&quot;every\\tman\\tis\\ta\\tpoet\\twhen\\the\\tis\\tin\\tlove\\n&quot;, col_names = FALSE) print(dat) ## # A tibble: 1 x 10 ## X1 X2 X3 X4 X5 X6 X7 X8 X9 X10 ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 every man is a poet when he is in love Or more generally, you can use read_delim() and assign separating character： dat=read_delim(&quot;THE|UNBEARABLE|RANDOMNESS|OF|LIFE\\n&quot;, delim = &quot;|&quot;, col_names = FALSE) print(dat) ## # A tibble: 1 x 5 ## X1 X2 X3 X4 X5 ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 THE UNBEARABLE RANDOMNESS OF LIFE Another situation you will often run into is missing value. In marketing survey, people like to use “99” to represent missing. You can tell R to set all observation with value “99” as missing when you read the data: dat=read_csv(&quot;Q1,Q2,Q3 5, 4,99&quot;,na=&quot;99&quot;) print(dat) ## # A tibble: 1 x 3 ## Q1 Q2 Q3 ## &lt;int&gt; &lt;int&gt; &lt;chr&gt; ## 1 5 4 &lt;NA&gt; For writing data back to disk, you can use write_csv() and write_tsv(). The following two characters of the two functions increase the chances of the output file being read back in correctly: Encode strings in UTF-8 Save dates and date-times in ISO8601 format so they are easily parsed elsewhere For example: write_csv(sim.dat, &quot;sim_dat.csv&quot;) For other data types, you can use the following packages: Haven: SPSS, Stata and SAS data Readxl and xlsx: excel data(.xls and .xlsx) DBI: given data base, such as RMySQL, RSQLite and RPostgreSQL, read data directly from the database using SQL Some other useful materials: For getting data from internet, you can refere to the book “XML and Web Technologies for Data Sciences with R”. R data import/export manual rio package：https://github.com/leeper/rio 5.1.2 data.table— enhanced data.frame What is data.table? It is an R package that provides an enhanced version of data.frame. The most used object in R is data frame. Before we move on, let’s briefly review some basic characters and manipulations of data.frame: It is a set of rows and columns. Each row is of the same length and data type Every column is of the same length but can be of differing data types It has characteristics of both a matrix and a list It uses [] to subset data I will use the clothes customer data to illustrate. There are two dimensions in []. The first one indicates row and second one indicates column. It uses comma to separate them. # read data sim.dat&lt;-read.csv(&quot;https://raw.githubusercontent.com/happyrabbit/DataScientistR/master/Data/SegData.csv&quot;) # subset the first two rows sim.dat[1:2,] ## age gender income house store_exp online_exp store_trans online_trans ## 1 57 Female 120963.4 Yes 529.1344 303.5125 2 2 ## 2 63 Female 122008.1 Yes 478.0058 109.5297 4 2 ## Q1 Q2 Q3 Q4 Q5 Q6 Q7 Q8 Q9 Q10 segment ## 1 4 2 1 2 1 4 1 4 2 4 Price ## 2 4 1 1 2 1 4 1 4 1 4 Price # subset the first two rows and column 3 and 5 sim.dat[1:2,c(3,5)] ## income store_exp ## 1 120963.4 529.1344 ## 2 122008.1 478.0058 # get all rows with age&gt;70 sim.dat[sim.dat$age&gt;70,] ## age gender income house store_exp online_exp store_trans ## 288 300 Male 208017.5 Yes 5076.801 6053.485 12 ## online_trans Q1 Q2 Q3 Q4 Q5 Q6 Q7 Q8 Q9 Q10 segment ## 288 11 1 4 5 4 4 4 4 1 4 2 Conspicuous # get rows with age&gt; 60 and gender is Male # select column 3 and 4 sim.dat[sim.dat$age&gt;68 &amp; sim.dat$gender == &quot;Male&quot;, 3:4] ## income house ## 204 119552.0 No ## 288 208017.5 Yes Remember that there are usually different ways to conduct the same manipulation. For example, the following code presents three ways to calculate average number of online transactions for male and female: tapply(sim.dat$online_trans, sim.dat$gender, mean ) ## Female Male ## 15.38448 11.26233 aggregate(online_trans ~ gender, data = sim.dat, mean) ## gender online_trans ## 1 Female 15.38448 ## 2 Male 11.26233 library(dplyr) sim.dat%&gt;% group_by(gender)%&gt;% summarise(Avg_online_trans=mean(online_trans)) ## # A tibble: 2 x 2 ## gender Avg_online_trans ## &lt;fctr&gt; &lt;dbl&gt; ## 1 Female 15.38448 ## 2 Male 11.26233 There is no gold standard to choose a specific function to manipulate data. The goal is to solve real problem not the tool itself. So just use whatever tool that is convenient for you. The way to use [] is straightforward. But the manipulations are limited. If you need more complicated data reshaping or aggregation, there are other packages to use such as dplyr, reshape2, tidyr etc. But the usage of those packages are not as straightforward as []. You often need to change functions. Keeping related operations together, such as subset, group, update, join etc, will allow for: concise, consistent and readable syntax irrespective of the set of operations you would like to perform to achieve your end goal performing data manipulation fluidly without the cognitive burden of having to change among different functions by knowing precisely the data required for each operation, you can automatically optimize operations effectively data.table is the package for that. If you are not familiar with other data manipulating packages and are interested in reducing programming time tremendously, then this package is for you. Other than extending the function of [], data.table has the following advantages: Offers fast import, subset, grouping, update, and joins for large data files It is easy to turn data frame to data table Can behave just like a data frame You need to install and load the package: # If you haven&#39;t install it, use the code to instal # install.packages(&quot;data.table&quot;) # load packagw library(data.table) Use data.table() to covert the existing data frame sim.dat to data table: dt &lt;- data.table(sim.dat) class(dt) ## [1] &quot;data.table&quot; &quot;data.frame&quot; Calculate mean for counts of online transactions: dt[, mean(online_trans)] ## [1] 13.546 You can’t do the same thing using data frame: sim.dat[,mean(online_trans)] Error in mean(online_trans) : object &#39;online_trans&#39; not found If you want to calculate mean by group as before, set “by =” argument: dt[ , mean(online_trans), by = gender] ## gender V1 ## 1: Female 15.38448 ## 2: Male 11.26233 You can group by more than one variables. For example, group by “gender” and “house”: dt[ , mean(online_trans), by = .(gender, house)] ## gender house V1 ## 1: Female Yes 11.312030 ## 2: Male Yes 8.771523 ## 3: Female No 19.145833 ## 4: Male No 16.486111 Assign column names for aggregated variables: dt[ , .(avg = mean(online_trans)), by = .(gender, house)] ## gender house avg ## 1: Female Yes 11.312030 ## 2: Male Yes 8.771523 ## 3: Female No 19.145833 ## 4: Male No 16.486111 data.table can accomplish all operations that aggregate() and tapply()can do for data frame. General setting of data.table Different from data frame, there are three arguments for data table: It is analogous to SQL. You don’t have to know SQL to learn data table. But experience with SQL will help you understand data table. In SQL, you select column j (use command SELECT) for row i (using command WHERE). GROUP BY in SQL will assign the variable to group the observations. Let’s review our previous code: dt[ , mean(online_trans), by = gender] ## gender V1 ## 1: Female 15.38448 ## 2: Male 11.26233 The code above is equal to the following SQL： SELECT gender, avg(online_trans) FROM sim.dat GROUP BY gender R code: dt[ , .(avg = mean(online_trans)), by = .(gender, house)] ## gender house avg ## 1: Female Yes 11.312030 ## 2: Male Yes 8.771523 ## 3: Female No 19.145833 ## 4: Male No 16.486111 is equal to SQL： SELECT gender, house, avg(online_trans) AS avg FROM sim.dat GROUP BY gender, house R code： dt[ age &lt; 40, .(avg = mean(online_trans)), by = .(gender, house)] ## gender house avg ## 1: Male Yes 14.45977 ## 2: Female Yes 18.14062 ## 3: Male No 18.24299 ## 4: Female No 20.10196 is equal to SQL： SELECT gender, house, avg(online_trans) AS avg FROM sim.dat WHERE age &lt; 40 GROUP BY gender, house You can see the analogy between data.table and SQL. Now let’s focus on operations in data table. select row # select rows with age&lt;20 and income &gt; 80000 dt[age &lt; 20 &amp; income &gt; 80000] ## age gender income house store_exp online_exp store_trans online_trans ## 1: 19 Female 83534.70 No 227.6686 1490.719 1 22 ## 2: 18 Female 89415.97 Yes 209.5487 1926.470 3 28 ## 3: 19 Female 92812.81 No 186.7475 1041.539 2 18 ## Q1 Q2 Q3 Q4 Q5 Q6 Q7 Q8 Q9 Q10 segment ## 1: 2 1 1 2 4 1 4 2 4 1 Style ## 2: 2 1 1 1 4 1 4 2 4 1 Style ## 3: 3 1 1 2 4 1 4 3 4 1 Style # select the first two rows dt[1:2] ## age gender income house store_exp online_exp store_trans online_trans ## 1: 57 Female 120963.4 Yes 529.1344 303.5125 2 2 ## 2: 63 Female 122008.1 Yes 478.0058 109.5297 4 2 ## Q1 Q2 Q3 Q4 Q5 Q6 Q7 Q8 Q9 Q10 segment ## 1: 4 2 1 2 1 4 1 4 2 4 Price ## 2: 4 1 1 2 1 4 1 4 1 4 Price select column Selecting columns in data.table don’t need $: # select column “age” but return it as a vector # the argument for row is empty so the result will return all observations ans &lt;- dt[, age] head(ans) ## [1] 57 63 59 60 51 59 To return data.table object, put column names in list(): # Select age and online_exp columns and return as a data.table instead ans &lt;- dt[, list(age, online_exp)] head(ans) ## age online_exp ## 1: 57 303.5125 ## 2: 63 109.5297 ## 3: 59 279.2496 ## 4: 60 141.6698 ## 5: 51 112.2372 ## 6: 59 195.6870 Or you can also put column names in .(): ans &lt;- dt[, .(age, online_exp)] # head(ans) To select all columns from “age” to “income”: ans &lt;- dt[, age:income, with = FALSE] head(ans,2) ## age gender income ## 1: 57 Female 120963.4 ## 2: 63 Female 122008.1 Delete columns using - or !: # delete columns from age to online_exp ans &lt;- dt[, -(age:online_exp), with = FALSE] ans &lt;- dt[, !(age:online_exp), with = FALSE] tabulation In data table. .N means to count。 # row count dt[, .N] ## [1] 1000 If you assign the group variable, then it will count by groups: # counts by gender dt[, .N, by= gender] ## gender N ## 1: Female 554 ## 2: Male 446 # for those younger than 30, count by gender dt[age &lt; 30, .(count=.N), by= gender] ## gender count ## 1: Female 292 ## 2: Male 86 Order table: # get records with the highest 5 online expense: head(dt[order(-online_exp)],5) ## age gender income house store_exp online_exp store_trans online_trans ## 1: 40 Female 217599.7 No 7023.684 9479.442 10 6 ## 2: 41 Female NA Yes 3786.740 8638.239 14 10 ## 3: 36 Male 228550.1 Yes 3279.621 8220.555 8 12 ## 4: 31 Female 159508.1 Yes 5177.081 8005.932 11 13 ## 5: 43 Female 190407.4 Yes 4694.922 7875.562 6 11 ## Q1 Q2 Q3 Q4 Q5 Q6 Q7 Q8 Q9 Q10 segment ## 1: 1 4 5 4 3 4 4 1 4 2 Conspicuous ## 2: 1 4 4 4 4 4 4 1 4 2 Conspicuous ## 3: 1 4 5 4 4 4 4 1 4 1 Conspicuous ## 4: 1 4 4 4 4 4 4 1 4 2 Conspicuous ## 5: 1 4 5 4 4 4 4 1 4 2 Conspicuous Since data table keep some characters of data frame, they share some operations: dt[order(-online_exp)][1:5] ## age gender income house store_exp online_exp store_trans online_trans ## 1: 40 Female 217599.7 No 7023.684 9479.442 10 6 ## 2: 41 Female NA Yes 3786.740 8638.239 14 10 ## 3: 36 Male 228550.1 Yes 3279.621 8220.555 8 12 ## 4: 31 Female 159508.1 Yes 5177.081 8005.932 11 13 ## 5: 43 Female 190407.4 Yes 4694.922 7875.562 6 11 ## Q1 Q2 Q3 Q4 Q5 Q6 Q7 Q8 Q9 Q10 segment ## 1: 1 4 5 4 3 4 4 1 4 2 Conspicuous ## 2: 1 4 4 4 4 4 4 1 4 2 Conspicuous ## 3: 1 4 5 4 4 4 4 1 4 1 Conspicuous ## 4: 1 4 4 4 4 4 4 1 4 2 Conspicuous ## 5: 1 4 5 4 4 4 4 1 4 2 Conspicuous You can also order the table by more than one variable. The following code will order the table by gender, then order within gender by online_exp: dt[order(gender, -online_exp)][1:5] ## age gender income house store_exp online_exp store_trans online_trans ## 1: 40 Female 217599.7 No 7023.684 9479.442 10 6 ## 2: 41 Female NA Yes 3786.740 8638.239 14 10 ## 3: 31 Female 159508.1 Yes 5177.081 8005.932 11 13 ## 4: 43 Female 190407.4 Yes 4694.922 7875.562 6 11 ## 5: 50 Female 263858.0 Yes 5813.802 7448.729 11 11 ## Q1 Q2 Q3 Q4 Q5 Q6 Q7 Q8 Q9 Q10 segment ## 1: 1 4 5 4 3 4 4 1 4 2 Conspicuous ## 2: 1 4 4 4 4 4 4 1 4 2 Conspicuous ## 3: 1 4 4 4 4 4 4 1 4 2 Conspicuous ## 4: 1 4 5 4 4 4 4 1 4 2 Conspicuous ## 5: 1 4 5 4 4 4 4 1 4 1 Conspicuous Use fread() to import dat Other than read.csv in base R, we have introduced ‘read_csv’ in ‘readr’. read_csv is much faster and will provide progress bar which makes user feel much better (at least make me feel better). fread() in data.table further increase the efficiency of reading data. The following are three examples of reading the same data file topic.csv. The file includes text data scraped from an agriculture forum with 209670 rows and 6 columns: system.time(topic&lt;-read.csv(&quot;/Users/happyrabbit/Documents/GitHub/DataScientistR/Data/topic.csv&quot;)) user system elapsed 4.313 0.027 4.340 system.time(topic&lt;-readr::read_csv(&quot;/Users/happyrabbit/Documents/GitHub/DataScientistR/Data/topic.csv&quot;)) user system elapsed 0.267 0.008 0.274 system.time(topic&lt;-data.table::fread(&quot;/Users/happyrabbit/Documents/GitHub/DataScientistR/Data/topic.csv&quot;)) user system elapsed 0.217 0.005 0.221 It is clear that read_csv() is much faster than read.csv(). fread() is a little faster than read_csv(). As the size increasing, the difference will become for significant. Note that fread() will read file as data.table by default. 5.2 Summarize data 5.2.1 apply(), lapply() and sapply() in base R There are some powerful functions to summarize data in base R, such as apply(), lapply() and sapply(). They do the same basic things and are all from “apply” family: apply functions over parts of data. They differ in two important respects: the type of object they apply to the type of result they will return When do we use apply()? When we want to apply a function to margins of an array or matrix. That means our data need to be structured. The operations can be very flexible. It returns a vector or array or list of values obtained by applying a function to margins of an array or matrix. For example you can compute row and column sums for a matrix: ## simulate a matrix x &lt;- cbind(x1 =1:8, x2 = c(4:1, 2:5)) dimnames(x)[[1]] &lt;- letters[1:8] apply(x, 2, mean) ## x1 x2 ## 4.5 3.0 col.sums &lt;- apply(x, 2, sum) row.sums &lt;- apply(x, 1, sum) You can also apply other functions: ma &lt;- matrix(c(1:4, 1, 6:8), nrow = 2) ma ## [,1] [,2] [,3] [,4] ## [1,] 1 3 1 7 ## [2,] 2 4 6 8 apply(ma, 1, table) #--&gt; a list of length 2 ## [[1]] ## ## 1 3 7 ## 2 1 1 ## ## [[2]] ## ## 2 4 6 8 ## 1 1 1 1 apply(ma, 1, stats::quantile) # 5 x n matrix with rownames ## [,1] [,2] ## 0% 1 2.0 ## 25% 1 3.5 ## 50% 2 5.0 ## 75% 4 6.5 ## 100% 7 8.0 Results can have different lengths for each call. This is a trickier example. What will you get? ## Example with different lengths for each call z &lt;- array(1:24, dim = 2:4) zseq &lt;- apply(z, 1:2, function(x) seq_len(max(x))) zseq ## a 2 x 3 matrix typeof(zseq) ## list dim(zseq) ## 2 3 zseq[1,] apply(z, 3, function(x) seq_len(max(x))) lapply() applies a function over a list, data.frame or vector and returns a list of the same length. sapply() is a user-friendly version and wrapper of lapply(). By default it returns a vector, matrix or if simplify = &quot;array&quot;, an array if appropriate. apply(x, f, simplify = FALSE, USE.NAMES = FALSE) is the same as lapply(x, f). If simplify=TRUE, then it will return a data.frame instead of list. Let’s use some data with context to help you better understand the functions. Get the mean and standard deviation of all numerical variables in the data set. # Read data sim.dat&lt;-read.csv(&quot;https://raw.githubusercontent.com/happyrabbit/DataScientistR/master/Data/SegData.csv&quot;) # Get numerical variables sdat&lt;-sim.dat[,!lapply(sim.dat,class)==&quot;factor&quot;] ## Try the following code with apply() function ## apply(sim.dat,2,class) ## What is the problem? The data frame sdat only includes numeric columns. Now we can go head and use apply() to get mean and standard deviation for each column: apply(sdat, MARGIN=2,function(x) mean(na.omit(x))) ## age income store_exp online_exp store_trans ## 38.840 113543.065 1356.851 2120.181 5.350 ## online_trans Q1 Q2 Q3 Q4 ## 13.546 3.101 1.823 1.992 2.763 ## Q5 Q6 Q7 Q8 Q9 ## 2.945 2.448 3.434 2.396 3.085 ## Q10 ## 2.320 Here we defined a function using function(x) mean(na.omit(x)). It is a very simple function. It tells R to ignore the missing value when calculating the mean. MARGIN=2 tells R to apply function to each column. It is not hard to guess what MARGIN=1 mean. The result show that the average online expense is much higher than store expense. You can also compare the average scores across different questions. The command to calculate standard deviation is very similar. The only difference is to change mean() to sd(): apply(sdat, MARGIN=2,function(x) sd(na.omit(x))) ## age income store_exp online_exp store_trans ## 16.416818 49842.287197 2774.399785 1731.224308 3.695559 ## online_trans Q1 Q2 Q3 Q4 ## 7.956959 1.450139 1.168348 1.402106 1.155061 ## Q5 Q6 Q7 Q8 Q9 ## 1.284377 1.438529 1.455941 1.154347 1.118493 ## Q10 ## 1.136174 Even the average online expense is higher than store expense, the standard deviation for store expense is much higher than online expense which indicates there are very likely some big/small purchase in store. We can check it quickly: summary(sdat$store_exp) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -500.0 205.0 329.0 1357.0 597.3 50000.0 summary(sdat$online_exp) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 68.82 420.30 1942.00 2120.00 2441.00 9479.00 There are some odd values in store expense. The minimum value is -500 which is a wrong imputation which indicates that you should preprocess data before analyzing it. Checking those simple statistics will help you better understand your data. It then give you some idea how to preprocess and analyze them. How about using lapply() and sapply()? Run the following code and compare the results: lapply(sdat, function(x) sd(na.omit(x))) sapply(sdat, function(x) sd(na.omit(x))) sapply(sdat, function(x) sd(na.omit(x)), simplify = FALSE) 5.2.2 ddply() in plyr package dplyr is a set of clean and consistent tools that implement the split-apply-combine pattern in R. This is an extremely common pattern in data analysis: you solve a complex problem by breaking it down into small pieces, doing something to each piece and then combining the results back together again. [From package description] You may find the description sounds familiar. The package is sort of a wrapper of apply family. We will only introduce the main function ddply(). Because the package has next iteration which is dplyr package. We will introduce dplyr in more details. The reason we still want to spend some time on the older version is because they have similar idea and knowing the lineage will deeper your understanding of the whole family. We will use the same data frame sim.dat to illustrate. Run the following command: library(plyr) ddply(sim.dat,&quot;segment&quot;,summarize, Age=round(mean(na.omit(age)),0), FemalePct=round(mean(gender==&quot;Female&quot;),2), HouseYes=round(mean(house==&quot;Yes&quot;),2), store_exp=round(mean(na.omit(store_exp),trim=0.1),0), online_exp=round(mean(online_exp),0), store_trans=round(mean(store_trans),1), online_trans=round(mean(online_trans),1)) ## segment Age FemalePct HouseYes store_exp online_exp store_trans ## 1 Conspicuous 42 0.32 0.86 4990 4898 10.9 ## 2 Price 60 0.45 0.94 501 205 6.1 ## 3 Quality 35 0.47 0.34 301 2013 2.9 ## 4 Style 24 0.81 0.27 200 1962 3.0 ## online_trans ## 1 11.1 ## 2 3.0 ## 3 16.0 ## 4 21.1 Now, let’s peel the onion in order. The first argument sim.dat is easy. It is the data you want to work on. The second argument &quot;segment&quot; is the column you want to group by. It is a very standard marketing segmentation problem. The final segment is the result you want to get by designing survey, collecting and analyzing data. Here we assume those segments are known and we want to understand how each group of customer look like. It is a common task in segmentation: figuring out a profile. Here we only summarize data by one categorical variable but you can group by multiply variables using ddply(sim.dat, c(&quot;segment&quot;,&quot;house&quot;), .). So the second argument tell the function we want to divide data by customer segment. The third argument summarize tells R the kind of manipulation you want to do which is to summarize data. There are other choices for this argument such as transform (transform data within each group) and subset(subset data within each group). Then the rest commands tell R the exact action. For example, Age=round(mean(na.omit(age)),0) tell R the following things: Calculate the mean of column age ignoring missing value Round the result to the specified number of decimal places Store the result to a new variable named Age The rest of the command above are similar. In the end we calculate the following for each segment: Age: average age for each segment FemalePct: percentage for each segment HouseYes: percentage of people who own a house stroe_exp: average expense in store online_exp: average expense online store_trans: average times of transactions in store online_trans: average times of online transactions There is a lot of information you can draw from those simple averages. Conspicuous: average age is about 40. Target for middle-age wealthy people. 1/3 of them are female and 2/3 are male. They may be good target for candy dad. They buy regardless the price. Almost all of them own house (0.86). It makes us wonder what is wrong with the rest 14%? They may live in Manhattan Price: They are older people, average age 60. Nearly all of them own a house(0.94). They are less likely to purchase online (store_trans=6 while online_trans=3). This is the only group that is less likely to purchase online. Quality: The average age is 35. They are not way different with Conspicuous in terms of age. But they spend much less. The percentages of male and female are similar. They prefer online shopping. More than half of them don’t own a house (0.66). Style: They are young people with average age 24. Majority of them are female (0.81). Most of them don’t own a house (0.73). They are very likely to be digital natives and definitely prefer online shopping. You may notice that Style group purchase more frequently online (online_trans=21) but the expense (online_exp=1962) is not higher. This makes us wondering what is the average expense each time so you have a better idea about the price range the group fall in. The analytical process is aggregated instead of independent steps. What you learn before will help you decide what to do next. Sometimes you also need to go backward to fix something in the previous steps. For example, you may need to check those negative expense value. We continue to use ddply() to calculate the two statistics: ddply(sim.dat,&quot;segment&quot;,summarize,avg_online=round(sum(online_exp)/sum(online_trans),2), avg_store=round(sum(store_exp)/sum(store_trans),2)) ## segment avg_online avg_store ## 1 Conspicuous 442.27 479.25 ## 2 Price 69.28 81.30 ## 3 Quality 126.05 105.12 ## 4 Style 92.83 121.07 Price group has the lowest averaged one time purchasing price. The Conspicuous group will pay the highest price. When we build profile in real life, we will need to look at the survey results too. Those simple data manipulations can provide you lots of information already. As mentioned before, other than “summarize” there are other functions such as “transform” and “subset”. For simplicity, I draw 11 random samples and 3 variables (age, store_exp and segment) from the original data according to the different segments. We will explain stratified sampling later. Here we just do it without explanation. library(caret) set.seed(2016) trainIndex&lt;-createDataPartition(sim.dat$segment,p=0.01,list=F,times=1) examp&lt;-sim.dat[trainIndex,c(&quot;age&quot;,&quot;store_exp&quot;,&quot;segment&quot;)] Now data frame examp only has 11 rows and 3 columns. Let’s look at the function of transform: ddply(examp,&quot;segment&quot;,transform,store_pct=round(store_exp/sum(store_exp),2)) ## age store_exp segment store_pct ## 1 42 6319.0718 Conspicuous 0.55 ## 2 42 5106.4816 Conspicuous 0.45 ## 3 55 595.2520 Price 0.42 ## 4 64 399.3550 Price 0.28 ## 5 64 426.6653 Price 0.30 ## 6 39 362.4795 Quality 0.58 ## 7 35 260.5065 Quality 0.42 ## 8 23 205.6099 Style 0.25 ## 9 24 212.3040 Style 0.26 ## 10 24 202.1017 Style 0.25 ## 11 28 200.1906 Style 0.24 What “transform” does is to transform data within the specified group (segment) and append the result as a new column. Next let’s look at the function of “subset”: ddply(examp,&quot;segment&quot;,subset,store_exp&gt;median(store_exp)) ## age store_exp segment ## 1 42 6319.0718 Conspicuous ## 2 55 595.2520 Price ## 3 39 362.4795 Quality ## 4 23 205.6099 Style ## 5 24 212.3040 Style You get all rows with store_exp greater than its group median. 5.2.3 dplyr package dplyr provides a flexible grammar of data manipulation focusing on tools for working with data frames (hence the d in the name). It is faster and more friendly: It identifies the most important data manipulations and make they easy to use from R It performs faster for in-memory data by writing key pieces in C++ using Rcpp The interface is the same for data frame, data table or database I will illustrate the following functions in order: Display Subset Summarize Create new variable Merge Display tbl_df(): Convert the data to tibble which offers better checking and printing capabilities than traditional data frames. It will adjust output width according to fit the current window. library(dplyr) tbl_df(sim.dat) glimpse(): This is like a transposed version of tbl_df() glimpse(sim.dat) Subset Get rows with income more than 300000: library(magrittr) filter(sim.dat, income &gt;300000) %&gt;% tbl_df() ## # A tibble: 4 x 19 ## age gender income house store_exp online_exp store_trans ## &lt;int&gt; &lt;fctr&gt; &lt;dbl&gt; &lt;fctr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 40 Male 301398.0 Yes 4840.461 3618.212 10 ## 2 33 Male 319704.3 Yes 5998.305 4395.923 9 ## 3 41 Male 317476.2 Yes 3029.844 4179.671 11 ## 4 37 Female 315697.2 Yes 6548.970 4284.065 13 ## # ... with 12 more variables: online_trans &lt;int&gt;, Q1 &lt;int&gt;, Q2 &lt;int&gt;, ## # Q3 &lt;int&gt;, Q4 &lt;int&gt;, Q5 &lt;int&gt;, Q6 &lt;int&gt;, Q7 &lt;int&gt;, Q8 &lt;int&gt;, Q9 &lt;int&gt;, ## # Q10 &lt;int&gt;, segment &lt;fctr&gt; Here we meet a new operator %&gt;%. It is called “Pipe operator” which pipes a value forward into an expression or function call. What you get in the left operation will be the first argument or the only argument in the right operation. x %&gt;% f(y) = f(x, y) y %&gt;% f(x, ., z) = f(x, y, z ) It is an operator from magrittr which can be really beneficial. Look at the following code. Can you tell me what it does? ave_exp &lt;- filter( summarise( group_by( filter( sim.dat, !is.na(income) ), segment ), ave_online_exp = mean(online_exp), n = n() ), n &gt; 200 ) Now look at the identical code using “%&gt;%”: avg_exp &lt;- sim.dat %&gt;% filter(!is.na(income)) %&gt;% group_by(segment) %&gt;% summarise( ave_online_exp = mean(online_exp), n = n() ) %&gt;% filter(n &gt; 200) Isn’t it much more straight forward now? Let’s read it: Delete observations from sim.dat with missing income values Group the data from step 1 by variable segment Calculate mean of online expense for each segment and save the result as a new variable named ave_online_exp Calculate the size of each segment and saved it as a new variable named n Get segments with size larger than 200 You can use distinct() to delete duplicated rows. dplyr distinct(sim.dat) sample_frac() will randomly select some rows with specified percentage. sample_n() can randomly select rows with specified number. dplyr::sample_frac(sim.dat, 0.5, replace = TRUE) dplyr::sample_n(sim.dat, 10, replace = TRUE) slice() will select rows by position: dplyr::slice(sim.dat, 10:15) It is equivalent to sim.dat[10:15,]. top_n() will select the order top n entries: dplyr::top_n(sim.dat,2,income) If you want to select columns instead of rows, you can use select(). The following are some sample codes: # select by column name dplyr::select(sim.dat,income,age,store_exp) # select columns whose name contains a character string dplyr::select(sim.dat, contains(&quot;_&quot;)) # select columns whose name ends with a character string # similar there is &quot;starts_with&quot; dplyr::select(sim.dat, ends_with(&quot;e&quot;)) # select columns Q1,Q2,Q3,Q4 and Q5 select(sim.dat, num_range(&quot;Q&quot;, 1:5)) # select columns whose names are in a group of names dplyr::select(sim.dat, one_of(c(&quot;age&quot;, &quot;income&quot;))) # select columns between age and online_exp dplyr::select(sim.dat, age:online_exp) # select all columns except for age dplyr::select(sim.dat, -age) Summarize The operations here are similar what we did before with apply() and ddply(). dplyr::summarise(sim.dat, avg_online = mean(online_trans)) ## avg_online ## 1 13.546 # apply function anyNA() to each column # you can also assign a function vector such as: c(&quot;anyNA&quot;,&quot;is.factor&quot;) dplyr::summarise_each(sim.dat, funs_(c(&quot;anyNA&quot;))) ## `summarise_each()` is deprecated. ## Use `summarise_all()`, `summarise_at()` or `summarise_if()` instead. ## To map `funs` over all variables, use `summarise_all()` ## age gender income house store_exp online_exp store_trans online_trans ## 1 FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE ## Q1 Q2 Q3 Q4 Q5 Q6 Q7 Q8 Q9 Q10 segment ## 1 FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE You can use group_by() to indicate the variables you want to group by as before: sim.dat %&gt;% group_by(segment) %&gt;% summarise_each(funs_(c(&quot;anyNA&quot;))) ## `summarise_each()` is deprecated. ## Use `summarise_all()`, `summarise_at()` or `summarise_if()` instead. ## To map `funs` over all variables, use `summarise_all()` ## # A tibble: 4 x 19 ## segment age gender income house store_exp online_exp store_trans ## &lt;fctr&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt; ## 1 Conspicuous FALSE FALSE TRUE FALSE FALSE FALSE FALSE ## 2 Price FALSE FALSE TRUE FALSE FALSE FALSE FALSE ## 3 Quality FALSE FALSE TRUE FALSE FALSE FALSE FALSE ## 4 Style FALSE FALSE TRUE FALSE FALSE FALSE FALSE ## # ... with 11 more variables: online_trans &lt;lgl&gt;, Q1 &lt;lgl&gt;, Q2 &lt;lgl&gt;, ## # Q3 &lt;lgl&gt;, Q4 &lt;lgl&gt;, Q5 &lt;lgl&gt;, Q6 &lt;lgl&gt;, Q7 &lt;lgl&gt;, Q8 &lt;lgl&gt;, Q9 &lt;lgl&gt;, ## # Q10 &lt;lgl&gt; Create new variable mutate() will compute and append one or more new columns: dplyr::mutate(sim.dat, total_exp = store_exp + online_exp) It will apply window function to the columns and return a column with the same length. It is a different type of function as before. # min_rank=rank(ties.method = &quot;min&quot;) # mutate_each() means apply function to each column dplyr::mutate_each(sim.dat, funs(min_rank)) The other similar function is transmute(). The differece is that transmute() will delete the original columns and only keep the new ones. dplyr::transmute(sim.dat, total_exp = store_exp + online_exp) Merge We create two baby data sets to show how the functions work. (x&lt;-data.frame(cbind(ID=c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;),x1=c(1,2,3)))) ## ID x1 ## 1 A 1 ## 2 B 2 ## 3 C 3 (y&lt;-data.frame(cbind(ID=c(&quot;B&quot;,&quot;C&quot;,&quot;D&quot;),y1=c(T,T,F)))) ## ID y1 ## 1 B TRUE ## 2 C TRUE ## 3 D FALSE # join to the left # keep all rows in x left_join(x,y,by=&quot;ID&quot;) ## ID x1 y1 ## 1 A 1 &lt;NA&gt; ## 2 B 2 TRUE ## 3 C 3 TRUE # get rows matched in both data sets inner_join(x,y,by=&quot;ID&quot;) ## ID x1 y1 ## 1 B 2 TRUE ## 2 C 3 TRUE # get rows in either data set full_join(x,y,by=&quot;ID&quot;) ## ID x1 y1 ## 1 A 1 &lt;NA&gt; ## 2 B 2 TRUE ## 3 C 3 TRUE ## 4 D &lt;NA&gt; FALSE # filter out rows in x that can be matched in y # it doesn&#39;t bring in any values from y semi_join(x,y,by=&quot;ID&quot;) ## ID x1 ## 1 B 2 ## 2 C 3 # the opposite of semi_join() # it gets rows in x that cannot be matched in y # it doesn&#39;t bring in any values from y anti_join(x,y,by=&quot;ID&quot;) ## ID x1 ## 1 A 1 There are other functions(intersect(), union() and setdiff()). Also the data frame version of rbind and cbind which are bind_rows() and bind_col(). We are not going to go through them all. You can try them yourself. If you understand the functions we introduced so far. It should be easy for you to figure out the rest. 5.3 Tidy and Reshape Data “Tidy data” represent the information from a dataset as data frames where each row is an observation and each column contains the values of a variable (i.e. an attribute of what we are observing). Depending on the situation, the requirements on what to present as rows and columns may change. In order to make data easy to work with for the problem at hand, in practice, we often need to convert data between the “wide” and the “long” format. The process feels like playing with a dough. There are two commonly used packages for this kind of manipulations: tidyr and reshape2. We will show how to tidy and reshape data using the two packages. By comparing the functions to show how they overlap and where they differ. 5.3.1 reshape2 package It is a reboot of previous package reshape. Why? Here is what I got from Stack Overflow: “reshape2 let Hadley make a rebooted reshape that was way, way faster, while avoiding busting up people’s dependencies and habits.” Take a baby subset of our exemplary clothes consumers data to illustrate: (sdat&lt;-sim.dat[1:5,1:6]) ## age gender income house store_exp online_exp ## 1 57 Female 120963.4 Yes 529.1344 303.5125 ## 2 63 Female 122008.1 Yes 478.0058 109.5297 ## 3 59 Male 114202.3 Yes 490.8107 279.2496 ## 4 60 Male 113616.3 Yes 347.8090 141.6698 ## 5 51 Male 124252.6 Yes 379.6259 112.2372 For the above data sdat, what if we want to have a variable indicating the purchasing channel (i.e. online or in-store) and another column with the corresponding expense amount? Assume we want to keep the rest of the columns the same. It is a task to change data from “wide” to “long”. There are two general ways to shape data: Use melt() to convert an object into a molten data frame, i.e. from wide to long Use dcast() to cast a molten data frame into the shape you want, i.e. from long to wide library(reshape2) (mdat &lt;- melt(sdat, measure.vars=c(&quot;store_exp&quot;,&quot;online_exp&quot;), variable.name = &quot;Channel&quot;, value.name = &quot;Expense&quot;)) ## age gender income house Channel Expense ## 1 57 Female 120963.4 Yes store_exp 529.1344 ## 2 63 Female 122008.1 Yes store_exp 478.0058 ## 3 59 Male 114202.3 Yes store_exp 490.8107 ## 4 60 Male 113616.3 Yes store_exp 347.8090 ## 5 51 Male 124252.6 Yes store_exp 379.6259 ## 6 57 Female 120963.4 Yes online_exp 303.5125 ## 7 63 Female 122008.1 Yes online_exp 109.5297 ## 8 59 Male 114202.3 Yes online_exp 279.2496 ## 9 60 Male 113616.3 Yes online_exp 141.6698 ## 10 51 Male 124252.6 Yes online_exp 112.2372 You melted the data frame sdat by two variables: store_exp and online_exp (measure.vars=c(&quot;store_exp&quot;,&quot;online_exp&quot;)). The new variable name is Channel set by command variable.name = &quot;Channel&quot;. The value name is Expense set by command value.name = &quot;Expense&quot;. You can run a regression to study the effect of purchasing channel: # Here we use all observations from sim.dat mdat&lt;-melt(sim.dat[,1:6], measure.vars=c(&quot;store_exp&quot;,&quot;online_exp&quot;), variable.name = &quot;Channel&quot;, value.name = &quot;Expense&quot;) fit&lt;-lm(Expense~gender+house+income+Channel+age,data=mdat) summary(fit) ## ## Call: ## lm(formula = Expense ~ gender + house + income + Channel + age, ## data = mdat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4208 -821 -275 533 44353 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -9.132e+02 1.560e+02 -5.855 5.76e-09 *** ## genderMale 3.572e+02 1.028e+02 3.475 0.000524 *** ## houseYes -5.687e+01 1.138e+02 -0.500 0.617275 ## income 2.834e-02 1.079e-03 26.268 &lt; 2e-16 *** ## Channelonline_exp 8.296e+02 9.772e+01 8.489 &lt; 2e-16 *** ## age -2.793e+01 3.356e+00 -8.321 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1974 on 1626 degrees of freedom ## (368 observations deleted due to missingness) ## Multiple R-squared: 0.348, Adjusted R-squared: 0.346 ## F-statistic: 173.5 on 5 and 1626 DF, p-value: &lt; 2.2e-16 You can melt() list, matrix, table too. The syntax is similar and we won’t go through every situation. Sometimes we want to convert the data from “long” to “wide”. For example, you want to compare the online and in store expense between male and female based on the house ownership. dcast(mdat, house + gender ~ Channel, sum) ## Using Expense as value column: use value.var to override. ## house gender store_exp online_exp ## 1 No Female 171102.2 583492.4 ## 2 No Male 133130.8 332499.9 ## 3 Yes Female 355320.2 500856.9 ## 4 Yes Male 697297.3 703332.0 In the above code, what is the left side of ~ are variables that you want to group by. The right side is the variable you want to spread as columns. It will use the column indicating value from melt() before. Here is “Expense” . 5.3.2 tidyr package The other package that will do similar manipulations is tidyr. Let’s get a subset to illustrate the usage. library(dplyr) # practice functions we learnt before sdat&lt;-sim.dat[1:5,]%&gt;% dplyr::select(age,gender,store_exp,store_trans) sdat %&gt;% tbl_df() ## # A tibble: 5 x 4 ## age gender store_exp store_trans ## * &lt;int&gt; &lt;fctr&gt; &lt;dbl&gt; &lt;int&gt; ## 1 57 Female 529.1344 2 ## 2 63 Female 478.0058 4 ## 3 59 Male 490.8107 7 ## 4 60 Male 347.8090 10 ## 5 51 Male 379.6259 4 gather() function in tidyr is analogous to melt() in reshape2. The following code will do the same thing as we did before using melt(): library(tidyr) msdat&lt;-tidyr::gather(sdat,&quot;variable&quot;,&quot;value&quot;,store_exp,store_trans) msdat %&gt;% tbl_df() ## # A tibble: 10 x 4 ## age gender variable value ## &lt;int&gt; &lt;fctr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 57 Female store_exp 529.1344 ## 2 63 Female store_exp 478.0058 ## 3 59 Male store_exp 490.8107 ## 4 60 Male store_exp 347.8090 ## 5 51 Male store_exp 379.6259 ## 6 57 Female store_trans 2.0000 ## 7 63 Female store_trans 4.0000 ## 8 59 Male store_trans 7.0000 ## 9 60 Male store_trans 10.0000 ## 10 51 Male store_trans 4.0000 Or if we use the pipe operation, we can write the above code as: sdat%&gt;%gather(&quot;variable&quot;,&quot;value&quot;,store_exp,store_trans) It is identical with the following code using melt(): melt(sdat, measure.vars=c(&quot;store_exp&quot;,&quot;store_trans&quot;), variable.name = &quot;variable&quot;, value.name = &quot;value&quot;) The opposite operation to gather() is spread(). The previous one stacks columns and the latter one spread the columns. msdat %&gt;% spread(variable,value) ## age gender store_exp store_trans ## 1 51 Male 379.6259 4 ## 2 57 Female 529.1344 2 ## 3 59 Male 490.8107 7 ## 4 60 Male 347.8090 10 ## 5 63 Female 478.0058 4 Another pair of functions that do opposite manipulations are separate() and unite(). sepdat&lt;- msdat %&gt;% separate(variable,c(&quot;Source&quot;,&quot;Type&quot;)) sepdat %&gt;% tbl_df() ## # A tibble: 10 x 5 ## age gender Source Type value ## * &lt;int&gt; &lt;fctr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 57 Female store exp 529.1344 ## 2 63 Female store exp 478.0058 ## 3 59 Male store exp 490.8107 ## 4 60 Male store exp 347.8090 ## 5 51 Male store exp 379.6259 ## 6 57 Female store trans 2.0000 ## 7 63 Female store trans 4.0000 ## 8 59 Male store trans 7.0000 ## 9 60 Male store trans 10.0000 ## 10 51 Male store trans 4.0000 You can see that the function separates the original column “variable” to two new columns “Source” and “Type”. You can use sep= to set the string or regular express to separate the column. By default, it is “_”. The unite() function will do the opposite: combining two columns. It is like the generalization of paste() to data frame. sepdat %&gt;% unite(&quot;variable&quot;,Source,Type,sep=&quot;_&quot;) ## age gender variable value ## 1 57 Female store_exp 529.1344 ## 2 63 Female store_exp 478.0058 ## 3 59 Male store_exp 490.8107 ## 4 60 Male store_exp 347.8090 ## 5 51 Male store_exp 379.6259 ## 6 57 Female store_trans 2.0000 ## 7 63 Female store_trans 4.0000 ## 8 59 Male store_trans 7.0000 ## 9 60 Male store_trans 10.0000 ## 10 51 Male store_trans 4.0000 The reshaping manipulations may be the trickiest part. You have to practice a lot to get familiar with those functions. Unfortunately there is no short cut. "],
["data-pre-processing.html", "Chapter 6 Data Pre-processing 6.1 Start 6.2 Centering and Scaling 6.3 Resolve Skewness 6.4 Resolve Outliers 6.5 Missing Values 6.6 Collinearity 6.7 Sparse Variables 6.8 Re-encode Dummy Variables", " Chapter 6 Data Pre-processing 6.1 Start There are a number of reasons a predictive model falls (Max Kuhn 2013), such as: Inadequate data pre-processing Inadequate model validation Unjustified extrapolation Over-fitting In this blog post, I am going to summarize some common data pre-processing approaches. 6.2 Centering and Scaling It is the most straightforward data transformation. It centers and scales a variable to mean 0 and standard deviation 1. It ensures that the criterion for finding linear combinations of the predictors is based on how much variation they explain and therefore improves the numerical stability. Models involving finding linear combinations of the predictors to explain response/predictors variation need data centering and scaling, such as PCA and PLS. You can easily writing code yourself to conduct this transformation. Or the function preProcess() in package caret can apply this transformation to a set of predictors. #install packages needed library(caret) library(e1071) library(gridExtra) library(lattice) library(imputeMissings) library(RANN) library(corrplot) library(nnet) head(cars) ## speed dist ## 1 4 2 ## 2 4 10 ## 3 7 4 ## 4 7 22 ## 5 8 16 ## 6 9 10 trans&lt;-preProcess(cars,method=c(&quot;center&quot;,&quot;scale&quot;)) transformed&lt;-predict(trans,cars) par(mfrow=c(1,2)) hist(cars$dist,main=&quot;Original&quot;,xlab=&quot;dist&quot;) hist(transformed$dist,main=&quot;Centered and Scaled&quot;,xlab=&quot;dist&quot;) Sometimes you only need to scale the variable. For example, if the model adds penalty to the parameter estimates (such as \\(L_2\\) penalty is ridge regression and \\(L_1\\) penalty in LASSO), the variables need to have similar scale to ensure a fair variable selection. I am heavy user of this kind of penalty-based model in my work and I used the following quantile transformation: \\[ x_{ij}^{*}=\\frac{x_{ij}-quantile(x_{.j},0.01)}{quantile(x_{.j}-0.99)-quantile(x_{-j},0.01)} \\] The reason to use 99% and 1% quantile instead of maximum and minimum values is to resist the impact of outliers. It is easy to write a function to do it: qscale&lt;-function(dat){ for (i in 1:ncol(dat)){ up&lt;-quantile(dat[,i],0.99) low&lt;-quantile(dat[,i],0.01) diff&lt;-up-low dat[,i]&lt;-(dat[,i]-low)/diff } return(dat) } In order to illustrate, let’s simulate a data set with two variables: income and age. set.seed(2015) income&lt;-sample(seq(50000,150000,by=500),95) age&lt;-income/2000-10 noise&lt;-round(runif(95)*10,0) age&lt;-age+noise income&lt;-c(income,10000,15000,300000,250000,230000) age&lt;-c(age,30,20,25,35,95) demo&lt;-data.frame(income,age) demo$education&lt;-as.factor(sample(c(&quot;High School&quot;,&quot;Bachelor&quot;,&quot;Master&quot;,&quot;Doctor&quot;),100,replace = T,prob =c(0.7,0.15,0.12,0.03) )) summary(demo[,c(&quot;income&quot;,&quot;age&quot;)]) ## income age ## Min. : 10000 Min. :20.00 ## 1st Qu.: 76375 1st Qu.:30.25 ## Median : 98750 Median :44.25 ## Mean :103480 Mean :44.92 ## 3rd Qu.:126375 3rd Qu.:56.88 ## Max. :300000 Max. :95.00 It is clear that income and age are not on the same scale. Now apply the function qscale() on the simulated data demo. transformed&lt;-qscale(demo[,c(&quot;income&quot;,&quot;age&quot;)]) summary(transformed) ## income age ## Min. :-0.02101 Min. :-0.01904 ## 1st Qu.: 0.26077 1st Qu.: 0.17814 ## Median : 0.35576 Median : 0.44746 ## Mean : 0.37584 Mean : 0.46044 ## 3rd Qu.: 0.47304 3rd Qu.: 0.69033 ## Max. : 1.21015 Max. : 1.42375 6.3 Resolve Skewness Skewness is defined to be the third standardized central moment. The formula for the sample skewness statistics is: \\[ skewness=\\frac{\\sum(x_{i}+\\bar{x})^{3}}{(n-1)v^{3/2}}\\] \\[v=\\frac{\\sum(x_{i}=\\bar{x})^{2}}{(n-1)}\\] Skewness=0 means that the destribution is symmetric, i.e. the probability of falling on either side of the distribution’s mean is equal. You can easily tell if a distribution is skewed by simple visualization. There are different ways may help to remove skewness such as log, square root or inverse. However it is often difficult to determine from plots which transformation is most appropriate for correcting skewness. The Box-Cox procedure automatically identified a transformation from the family of power transformations that are indexed by a parameter \\(\\lambda\\). \\[ x^{*}=\\begin{cases} \\begin{array}{c} \\frac{x^{\\lambda}-1}{\\lambda}\\\\ log(x) \\end{array} &amp; \\begin{array}{c} if\\ \\lambda\\neq0\\\\ if\\ \\lambda=0 \\end{array}\\end{cases} \\] It is easy to see that this family includes log transformation (\\(\\lambda=0\\)), square transformation (\\(\\lambda=2\\)), square root (\\(\\lambda=0.5\\)), inverse (\\(\\lambda=-1\\)) and others in-between. We can still use function preProcess() in package caret to apply this transformation by chaning the method argument. (trans&lt;-preProcess(cars,method=c(&quot;BoxCox&quot;))) ## Created from 50 samples and 2 variables ## ## Pre-processing: ## - Box-Cox transformation (2) ## - ignored (0) ## ## Lambda estimates for Box-Cox transformation: ## 1, 0.5 The output shows the sample size (50), number of variables (2) and the \\(\\lambda\\) estimates for each variable. After calling the preProcess() function, the predict() method applies the results to a data frame. transformed&lt;-predict(trans,cars) par(mfrow=c(1,2)) hist(cars$dist,main=&quot;Original&quot;,xlab=&quot;dist&quot;) hist(transformed$dist,main=&quot;After BoxCox Transformation&quot;,xlab=&quot;dist&quot;) An alternative is to use function BoxCoxTrans() in package caret. (trans&lt;-BoxCoxTrans(cars$dist)) ## Box-Cox Transformation ## ## 50 data points used to estimate Lambda ## ## Input data summary: ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 2.00 26.00 36.00 42.98 56.00 120.00 ## ## Largest/Smallest: 60 ## Sample Skewness: 0.759 ## ## Estimated Lambda: 0.5 transformed&lt;-predict(trans,cars$dist) skewness(transformed) ## [1] -0.01902765 The estimated \\(\\lambda\\) is the same 0.5. Original skewness is 0.759 and after transformation, the skewness is -0.01902765 which is close to 0. You can use function skewness() in package e1071 to get the skewness statistics. 6.4 Resolve Outliers Even under certain assumptions we can statistically define outliers, it can be hard to define in some situations. You can refer to “Detection of Outliers” for more information. Some models are resistant to outliers (such as tree-based model and support vector machine). If a model is sensitive to outliers (such as linear regression and logistic regression), we can use spatial sign transformation to minimize the problem. It projects the original sample points to the surface of a sphere by: \\[x_{ij}^{*}=\\frac{x_{ij}}{\\sqrt{\\sum_{j=1}^{p}x_{ij}^{2}}}\\] As noted in the book “Applied Predictive Modeling”, Since the denominator is intended to measure the squared distance to the center of the predictor’s distribution, it is important to center and scale the predictor data prior to using this transformation. Note that, unlike centering or scaling, this manipulation of the predictors transforms them as a group. We can use spatialSign() function in caret to conduct spatial sign on demo: trans&lt;-preProcess(demo[,c(&quot;income&quot;,&quot;age&quot;)],method=c(&quot;center&quot;,&quot;scale&quot;)) transformed&lt;-predict(trans,demo[,c(&quot;income&quot;,&quot;age&quot;)]) transformed2 &lt;- spatialSign(transformed) transformed2 &lt;- as.data.frame(transformed2) p1&lt;-xyplot(income ~ age, data = transformed, main=&quot;Original&quot;) p2&lt;-xyplot(income ~ age, data = transformed2, main=&quot;After Spatial Sign&quot;) grid.arrange(p1,p2, ncol=2) 6.5 Missing Values We need a book to fully explicate this topic. Before we decide how to handle missing value, it is important to understand why the values are missing. Do the missing values have information related outcomes? Or are they missing at random? It is not the goal here to illustrate which methods to use in different missing situation. You can refer to Section 3.4 of “Applied Predictive Modeling” for more discussion on that. The objective of this post is to introduce some imputation methods and corresponding application examples using R. Survey statistics has studied the imputation extensively which focuses on making valid inferences. Missing value imputation in predictive modeling is a different problem. Saar-Tsechansky and Provost compared several different methods for applying classification to instance with missing values. “Handling Missing Values when Applying Classification Models” The following code randomly assigns some missing values to the previous data demo and names the new data set demo_missing. set.seed(100) id1&lt;-sample(1:nrow(demo),15) id2&lt;-sample(1:nrow(demo),10) id3&lt;-sample(1:nrow(demo),10) demo_missing&lt;-demo demo_missing$age[id1]&lt;-NA demo_missing$income[id2]&lt;-NA demo_missing$education[id3]&lt;-NA summary(demo_missing) ## income age education ## Min. : 15000 Min. :20.00 Bachelor :13 ## 1st Qu.: 77125 1st Qu.:30.25 Doctor : 2 ## Median : 98750 Median :44.25 High School:70 ## Mean :102811 Mean :44.43 Master : 5 ## 3rd Qu.:125250 3rd Qu.:56.25 NA&#39;s :10 ## Max. :300000 Max. :95.00 ## NA&#39;s :10 NA&#39;s :15 6.5.1 Impute missing values with median/mode You can use function impute() under package imputeMissings to impute missing values with mdedian/mode. This method is simple, fast but treats each predictor independently, and may not be accurate. demo_imp&lt;-impute(demo_missing,method=&quot;median/mode&quot;) summary(demo_imp) ## income age education ## Min. : 15000 Min. :20.00 Bachelor :13 ## 1st Qu.: 79250 1st Qu.:32.19 Doctor : 2 ## Median : 98750 Median :44.25 High School:80 ## Mean :102405 Mean :44.40 Master : 5 ## 3rd Qu.:122875 3rd Qu.:54.50 ## Max. :300000 Max. :95.00 Note that the median/mode method imputes mode to character vectors and median to numeric and integer vectors.So you can see the 10 missing values for variable “education” are imputed with “High School” since it is the mode. You can also use function ‘preProcess()’ to attain this.But it only works for numeric variable. imp&lt;-preProcess(demo_missing[,c(&quot;income&quot;,&quot;age&quot;)],method=&quot;medianImpute&quot;) demo_imp&lt;-predict(imp,demo_missing[,c(&quot;income&quot;,&quot;age&quot;)]) summary(demo_imp) ## income age ## Min. : 15000 Min. :20.00 ## 1st Qu.: 79250 1st Qu.:32.19 ## Median : 98750 Median :44.25 ## Mean :102405 Mean :44.40 ## 3rd Qu.:122875 3rd Qu.:54.50 ## Max. :300000 Max. :95.00 6.5.2 Impute missing values based on K-nearest neighbors k-nearest neighbor will find the k closest samples (Euclidian distance) in the training set and impute the mean of those “neighbors”. imp&lt;-preProcess(demo_missing[,c(&quot;income&quot;,&quot;age&quot;)],method=&quot;knnImpute&quot;,k=2) demo_imp&lt;-predict(imp,demo_missing[,c(&quot;income&quot;,&quot;age&quot;)]) ## Error in FUN(newX[, i], ...): cannot impute when all predictors are missing in the new data point Now we get a error saying “cannot impute when all predictors are missing in the new data point”. It is because there is at least one sample with both “income” and “age” missing. We can delete the corresponding row and do it again. idx&lt;-which(is.na(demo_missing$income)&amp;is.na(demo_missing$age)) imp&lt;-preProcess(demo_missing[-idx,c(&quot;income&quot;,&quot;age&quot;)],method=&quot;knnImpute&quot;,k=2) demo_imp&lt;-predict(imp,demo_missing[-idx,c(&quot;income&quot;,&quot;age&quot;)]) summary(demo_imp) ## income age ## Min. :-2.259679 Min. :-1.53784 ## 1st Qu.:-0.686725 1st Qu.:-0.88276 ## Median :-0.104506 Median :-0.01129 ## Mean :-0.006233 Mean : 0.01103 ## 3rd Qu.: 0.593512 3rd Qu.: 0.72444 ## Max. : 5.074342 Max. : 3.18343 The error doesn’t show up this time. This method considers all predictors together but it requires them to be in the same scale since the “euclidian distance” is used to find the neighbours. 6.6 Collinearity It is probably a technical term that many un-technical people also know. There is an excellent function in corrplot package with the same name corrplot() that can visualize correlation structure of a set of predictors. The function has option to reorder the variables in a way that reveals clusters of highly correlated ones. We add some columns to demo that are correlated. adddemo&lt;-demo[,-3] adddemo$added1&lt;-sqrt(demo$age)+10 adddemo$added2&lt;-log(demo$income)+demo$age adddemo$added2&lt;-log(demo$age) adddemo$added4&lt;-demo$income/1000+5*demo$age adddemo$added5&lt;-sin(demo$age) The following command will produce visualization for the correlation matrix of adddemo. corrplot(cor(adddemo),order=&quot;hclust&quot;) The size and color of the points are associated with the strength of corresponding correlation. Section 3.5 of “Applied Predictive Modeling” presents a heuristic algorithm to remove minium number of predicitors to ensure all pairwise corelations are below a certain threshold: Calculate the correlation matrix of the predictors. Determine the two predictors associated with the largest absolute pairwise correlation (call them predictors A and B). Determine the average correlation between A and the other variables. Do the same for predictor B. If A has a larger average correlation, remove it; otherwise, remove predictor B. Repeat Step 2-4 until no absolute correlations are above the threshold. The findCorrelation() function in package caret will apaply the above algorithm. (highCorr&lt;-findCorrelation(cor(adddemo),cutoff=.75)) ## [1] 5 2 3 # remove columns with high correlations filter_demo&lt;-adddemo[,-highCorr] # correlation matrix for filtered data corrplot(cor(filter_demo),order=&quot;hclust&quot;) 6.7 Sparse Variables Other than the highly related predictors, predictors with degenerate distributions need to be removed as well. Removing those variables can significant improve some models’ performance and/or stability (such as linear regression and logistic regression but tree based model is impervious to this type of predictors). One extreme example is a variable with single value which is called zero-variance variable. Similarly those variables with very low frequency of unique values are near-zero variance predictors. How to detect those variables? There are two rules: - The fraction of unique values over the sample size - The ratio of the frequency of the most prevalent value to the frequency of the second most prevalent value. The caret package funciton nearZeroVar() can filter near-zero variance predictors. #add two variables with low variance zero_demo&lt;-demo zero_demo$zero1&lt;-rep(0,nrow(demo)) zero_demo$zero2&lt;-c(1,rep(0,nrow(demo)-1)) # zero1 only has one unique value # zero2 is a vector with the first element 1 and the rest are 0s summary(zero_demo) ## income age education zero1 ## Min. : 10000 Min. :20.00 Bachelor :15 Min. :0 ## 1st Qu.: 76375 1st Qu.:30.25 Doctor : 2 1st Qu.:0 ## Median : 98750 Median :44.25 High School:77 Median :0 ## Mean :103480 Mean :44.92 Master : 6 Mean :0 ## 3rd Qu.:126375 3rd Qu.:56.88 3rd Qu.:0 ## Max. :300000 Max. :95.00 Max. :0 ## zero2 ## Min. :0.00 ## 1st Qu.:0.00 ## Median :0.00 ## Mean :0.01 ## 3rd Qu.:0.00 ## Max. :1.00 # the function will return a vector of integers indicating which columns to remove nearZeroVar(zero_demo,freqCut = 95/5, uniqueCut = 10) ## [1] 4 5 Note the two arguments in the function freqCut = and uniqueCut =. They are corresponding to the previous two rules. freqCut: the cutoff for the ratio of the most common value to the second most common value uniqueCut:the cutoff for the percentage of distinct values out of the number of total samples 6.8 Re-encode Dummy Variables Sometimes we need to recode categories to smaller bits of information named “dummy variables”. Take the variable “education” in demo for example. It has four categories: “High School”,“Bachelor”,“Master” and “Doctor”. If we recode it to be dummy variables, each category get its own dummy variable that is 0/1 indicator for that category. For a single categorical variable, we can use function class.ind() in package nnet: dumVar&lt;-class.ind(demo$education) head(dumVar) ## Bachelor Doctor High School Master ## [1,] 0 0 1 0 ## [2,] 0 0 1 0 ## [3,] 0 0 1 0 ## [4,] 0 0 1 0 ## [5,] 0 0 1 0 ## [6,] 0 0 1 0 If we want to determine encodeings for more than one variables, we can use dummyVars() in caret. dumMod&lt;-dummyVars(~income+education, data=demo, # Remove the variable name from the column name levelsOnly=T) predict(dumMod,head(demo)) ## income Bachelor Doctor High School Master ## 1 56000 0 0 1 0 ## 2 133500 0 0 1 0 ## 3 79500 0 0 1 0 ## 4 53000 0 0 1 0 ## 5 63500 0 0 1 0 ## 6 84500 0 0 1 0 To add some more complexity, we could assume joint effect of income and education. In this case, this will add 4 more columns to the resulted data frame: dumMod&lt;-dummyVars(~income+education+income:education, data=demo, levelsOnly=T) predict(dumMod,head(demo)) ## income Bachelor Doctor High School Master income:Bachelor income:Doctor ## 1 56000 0 0 1 0 0 0 ## 2 133500 0 0 1 0 0 0 ## 3 79500 0 0 1 0 0 0 ## 4 53000 0 0 1 0 0 0 ## 5 63500 0 0 1 0 0 0 ## 6 84500 0 0 1 0 0 0 ## income:High School income:Master ## 1 56000 0 ## 2 133500 0 ## 3 79500 0 ## 4 53000 0 ## 5 63500 0 ## 6 84500 0 References "],
["dynamicreproducible-report.html", "Chapter 7 Dynamic/Reproducible report 7.1 What is R Markdown? 7.2 How to Start? 7.3 HTML 7.4 HTML5 Slides 7.5 Dashboards 7.6 Shiny Dashboard 7.7 HTML Widgets", " Chapter 7 Dynamic/Reproducible report 7.1 What is R Markdown? Let’s start from markdown. Markdown is a lightweight markup language designed to make authoring content easy for everyone. Here is a definition of markup language: Markup languages are designed for the processing, definition and presentation of text. The language specifies code for formatting, both the layout and style, within a text file. The code used to specify the formatting are called tags. HTML is a an example of a widely known and used markup language. Rather than writing complex markup code (e.g. LyX, XML, HTML or LaTeX), Markdown enables the use of a syntax much more like plain-text email. It is young comparing to the other markup languages. What makes markdown distinct is that it is both machine-readable and human-readable. R Markdown combines the core syntax of markdown and embedded R code chunks that are run so their output can be included in the final document. Consider how people typically create an analytical report. The author makes the graph/table, saves it as a file, and then copy and pastes it into the final report. This process relies on manual labor. The author may take a deep breath when the report is finally well-shaped. If the data changes, the author must repeat the enire process to update the graph. R Markdown comes to rescue! It provides an authoring framework for data science. You can use a single R Markdown file to do both: save and execute code generate high quality reports that can be shared with an audience R Markdown documents are fully reproducible and the most important and it is simple! 7.2 How to Start? 7.2.1 How It Work? When you run render, R Markdown feeds the .rmd file to knitr.knitr is a R package that will execute all of the code chunks and creates a new markdown (.md) document which includes the code and it’s output. The markdown file is then processed by pandoc which is responsible for creating the finished format. pandoc is a swiss-knife to convert files from one markup format into another. R Markdown encapsulates all of the above processing into a single render function. 7.2.2 Get Started Install R and RStudio. If you have RStudio installed ready, I suggest you to make sure it is in the latest version. You can install the rmarkdown package from CRAN with: install.packages(&quot;rmarkdown&quot;) R Markdown file is a plain text file that has the extension .Rmd. You can create a sample .Rmd file in R Studio: Input your document title and author name and click “OK”: The file contains three types of content: An (optional) YAML /’j?m??l/ header surrounded by --- R code chunks surrounded by ```{r} and ``` text mixed with simple text formatting R Markdown generates a new file that contains selected text, code, and results from the .Rmd file. The new file can be in the following formats: HTML PDF MS Word document slide show book dashboard package vignette Others 7.2.3 Markdown Basic Don’t worry if you are new to markdown. You can easily pick up simply by looking at a few examples of it in action. We will show some examples in a before/after style. You will see example syntax and the HTML output in R Studio. The webpage provides complete, detailed documentation for every markdown feature. 7.2.3.1 Paragraphs, Headers A paragraph is simply one or more consecutive lines of text, separated by one or more blank lines. Normal paragraphs should not be indented with spaces or tabs. You can put 1-6 hash marks (#) at the beginning of the line - the number of hashes equals the resulting HTML header level. # H1 ## H2 ### H3 #### H4 ##### H5 ###### H6 Alternatively, for H1 and H2, an underline-ish style: A First Level Header ===================== A Second Level Header --------------------- Output: 7.2.3.2 Blockquotes Blockquotes are indicated using email-style ‘&gt;’ angle brackets. In the end, it&#39;s not the years in your life that count. It&#39;s the life in your years. The sole cause of men&#39;s unhappiness is that he does not know how to stay quietly in his room. &gt; Every person takes the limits of their own field of vision for the limits of the world.[Arthur Schopenhauer] &gt; &gt; Reading makes a full man, conference a ready man, and writing an exact man.[Bacon] Output: In the end, it’s not the years in your life that count. It’s the life in your years. The sole cause of men’s unhappiness is that he does not know how to stay quietly in his room. Every person takes the limits of their own field of vision for the limits of the world.[Arthur Schopenhauer] Reading makes a full man, conference a ready man, and writing an exact man.[Bacon] 7.2.3.3 Phrase Emphasis Markdown uses asterisks and underscores to indicate spans of emphasis. Some of these words *are italic*. Some of these words _are italic also_. Use two asterisks for **bold**. Or, if you prefer, __use two underscores instead__. Strikethrough uses two tildes. ~~Scratch this.~~ Output: Some of these words are italic. Some of these words are italic also. Use two asterisks for bold. Or, if you prefer, use two underscores instead. Strikethrough uses two tildes. Scratch this. 7.2.3.4 Lists Unordered (bulleted) lists use asterisks, pluses, and hyphens (*, +, and -) as list markers. These three markers are interchangable; this: * Candy. * Gum. * Booze. Output: Candy. Gum. Booze. this: + Candy. + Gum. + Booze. Output: Candy. Gum. Booze. and this: - Candy. - Gum. - Booze. Output: Candy. Gum. Booze. Next we will show how to build HTML report and dashboard in more detail. 7.3 HTML 7.3.1 Create an HTML document To create an HTML document from R Markdown you specify the html_document output format in the front-matter of your document: --- title: &quot;Tidy and Reshape Data&quot; author: Hui Lin date: May 11, 2017 output: html_document --- You can add a table of contents using the toc option and specify the depth of headers that it applies to using the toc_depth option. For example: --- title: &quot;Tidy and Reshape Data&quot; author: Hui Lin date: May 11, 2017 output: html_document: toc: true toc_depth: 3 --- 7.3.2 Floating TOC You can specify the toc_float option to float the table of contents to the left of the main document content. The floating table of contents will always be visible even when the document is scrolled. For example: --- title: &quot;Tidy and Reshape Data&quot; author: Hui Lin date: May 11, 2017 output: html_document: toc: true toc_depth: 3 toc_float: true --- There are some options for toc_float parameter: collapsed (defaults to TRUE) controls whether the table of contents appers with only the top-level (e.g. H2) headers. When collapsed the table of contents is automatically expanded inline when necessary. smooth_scroll (defaults to TRUE) controls whether page scrolls are animated when table of contents items are navigated to via mouse clicks. For example: --- title: &quot;Tidy and Reshape Data&quot; author: Hui Lin date: May 11, 2017 output: html_document: toc: true toc_depth: 3 toc_float: collapsed: false smooth_scroll: false --- 7.3.3 Code Chunks Every code chunk will start with ```{r} and end with ```. You can type the chunk delimiters. Or there are two quick ways to incert chunks to you file: the keyboard shortcut Ctrl + Alt + I (OS X: Cmd + Option + I) the Add Chunk command in the editor toolbar When you render your .Rmd file, R Markdown will run each code chunk and embed the results beneath the code chunk in your final report. Customize Chunks Chunk output can be customized with options which are arguments in the {} of a chunk header. Here are some of the most common arguments: include = FALSE prevents code and results from appearing in the finished file. R Markdown still runs the code in the chunk, and the results can be used by other chunks. echo = FALSE prevents code, but not the results from appearing in the finished file. This is a useful way to embed figures. message = FALSE prevents messages that are generated by code from appearing in the finished file. warning = FALSE prevents warnings that are generated by code from appearing in the finished. fig.height, fig.width The width and height to use in R for plots created by the chunk (in inches). See the R Markdown Reference Guide for a complete list of knitr chunk options. Global Options To set global options that apply to every chunk in your file, call knitr::opts_chunk$set in a code chunk. Knitr will treat each option that you pass to knitr::opts_chunk$set as a global default that can be overwritten in individual chunk headers. For example, you can put the following after front-matter of your document: If you set global option as above, r markdown will prevent code for all chunks unless you overwrite in individual chunk header. Caching If the computations are long and document rendering becomes time consuming, you can use knitr caching to improve performance. You can use the chunk option cache=TRUE to enable cache, and cache.path to set the cache directory. Inline Code Code results can be inserted directly into the text of a .Rmd file by enclosing the code with r. In this way, R Markdown will display the results of inline code, but not the code. For example: Output: The current time is 2017-06-21 22:07:08 As a result, inline output is indistinguishable from the surrounding text. Inline expressions do not take knitr options. This is an R Markdown file. You can download a copy: EX1_Markdown.Rmd(output). 7.4 HTML5 Slides R Markdown supports several HTML presentation (slide show) formats. ioslides_presentation - HTML presentations with ioslides slidy_presentation - HTML presentations with slidy revealjs::revealjs_presentation - HTML presentations with reveal.js 7.4.1 ioslides presentation To create an ioslides presentation from R Markdown you specify the ioslides_presentation output format in the front-matter of your document. You can use # and ## to create a new slide. You can also use a horizontal rule (—-) to create slide without a header. For example here’s a simple slide show. You can download a copy: Ex_ioslide.Rmd(output). You can add a subtitle to a slide or section by including text after the pipe (|) character. For example: There are different display modes. The following are keyboard shortcuts for each: ‘f’: fullscreen mode ‘w’: toggle widescreen mode ‘o’: overview mode ‘h’: code highlight mode ‘p’: show presenter notes Press Esc to exit any mode. The code highlight mode enable to select subsets of code for additional emphasis by adding a special “highlight” comment around the code. For example: When you press h key, the highlighted region will be displayed with a bold font and the rest of the code will fade away. So you can help the audience focus exclusively on the highlighted region. 7.4.2 slidy presentation Creating slidy presentation is very similar to that of ioslides presentation. You specify the slidy_presentation output format in the front-matter of your document in stead of ioslides_presentation. The way you break up slides is the same with ioslides. For example here’s a simple slide show. You can download a copy: Ex_slidy.Rmd(output). Like before, there are different display modes. The following are keyboard shortcuts for each: C Show table of contents F Toggles the display of the footer A Toggles display of current vs all slides (useful for printing handouts) S Make fonts smaller B Make fonts larger For more information about other adjustments, such as appearance text style, CSS, footer elements etc. please refer to “Presentations with Slidy” 7.5 Dashboards Use R Markdown and felxdashboard package to build flexible, attractive, interactive dashboards. Some features of flexdashboard + R Markdown are: Reproducible and highly flexible to specify row and column-based layouts. Nice display: components are intelligently re-sized to fill the browser and adapted for display on mobile devices. Support for a wide variety of components including htmlwidgets; base, lattice, and grid graphics; tabular data; gauges and value boxes; and text annotations. Extensive support for text annotations to include assumptions, contextual narrative, and analysis within dashboards. Storyboard layouts for presenting sequences of visualizations and related commentary. By default dashboards are standard HTML documents that can be deployed on any web server or even attached to an email message. You can optionally add Shiny components for additional interactivity and then deploy on your own server or Shiny Server Install flexdashboard package using: install.packages(&quot;flexdashboard&quot;) Then you can create an R Markdown document with the flexdashboard::flex_dashboard output format within RStudio using the New R Markdown dialog: 7.5.1 Layouts 7.5.1.1 Layout by Column There is no better way to illustrate the syntax of latout than using example. Here is an example of two-column dashboard: The ------------------ defines columns with individual charts stacked vertically within each column. The above document defines a two-column dashboard with one chart on the left and two charts on the right. The output layout is: 7.5.1.2 Layout by Row You can similarly define row orientation by setting orientation: rows. Here is an example of two-row dashboard: The ------------------ here defines rows. The dashboard has two rows, the first of which has one chart and the second of which has two charts: 7.5.1.3 Scrolling Layout You may want to scroll rather than fit all the charts onto the page when there are lots of charts. You can set the scrolling function using the vertical_layout option. The default setting for vertical_layout is fill. You can use scrolling layout to demonstrate more charts. However, we recommend you consider using multiple pages instead which we will introduce later. The dashboard has one column with two charts: 7.5.1.4 Focal Chart This layout fills the page completely and gives prominence to a single chart at the top or on the left. For example: You can download the source code here. The resulted dashboard includes 3 charts. You can specify data-height attributes on each row to establish their relative sizes. You can also give prominence to a single chart on the left such as: The resulted dashboard includes 3 charts: 7.5.1.5 Tabset This layout displays column or row as a set of tabs. It is an alternative to scrolling layout when you have more charts. For example: You can download the source code here. The dashboard displays the right column as a set of two tabs: You can also add tabs to row: You can download the source code here. The dashboard displays the bottom row as a set of two tabs. Here the {.tabset-fade} attribute is used to enable a fade in/out effect when switching tabs: 7.5.1.6 Multiple pages This layout defines multiple pages using (==================). Each page can have its own top-level navigation tab and orientation. You can set the orientation via the data-orientation attribute: Page 1 ===================================== Column 1 {data-width=600} ------------------------------------- ### Chart 1 Column 2 {data-width=400} ------------------------------------- ### Chart 2 Page 2 {data-orientation=rows} ===================================== Row 1 {data-height=600} ------------------------------------- ### Chart 1 Row 1 {data-height=600} ------------------------------------- ### Chart 2 You can easily build the following dashboard: Click to See the Dashboard and Source Code 7.5.1.7 Storyboard If you want to present a sequence of charts and related commentary, stroyboard will be a great choice. You need to specify storyboard: trueand additional commentary will show up alongside the storyboard frames (the content after the *** separator in each section). social: menu will enable an icon to share the storyboard to your social network: source: embed allows you to embed the source code. The layout is: Here is an example of HTML Widgets Showcase storyboard. You can look at the source code by clicking “Source Code” tab at the upright corner. See the storyboard here. 7.5.2 Components 7.5.2.1 HTML Widgets The htmlwidgets framework brings JavaScript data visualization to R. The biggest advantage is the interactive character. As of writing this book, there are over 40 packages on CRAN which provide htmlwidgets. Charts based on htmlwidgets can dynamically re-size themselves so will fit within the bounds of their flexdashboard containers. Some htmlwidgets: DT: provides an R interface to the JavaScript library DataTables leaflet: a JavaScript library for creating dynamic maps that support panning and zooming along with various annotations. rbokeh: an interface to Bokeh, a powerful declarative Bokeh framework for creating web-based plots. d3heatmap: creates interactive D3 heatmaps including support for row/column highlighting and zooming. networkD3: provides tools for creating D3 JavaScript network graphs from R dygraphs: provides rich facilities for charting time-series data in R and includes support for many interactive features. plotly: provides bindings to the plotly.js library and allows you to easily translate your ggplot2 graphics into an interactive web-based version. metricsgraphics: enables easy creation of D3 scatterplots, line charts, and histograms. threejs: provides interactive 3D scatterplots and globe plot One disadvantage of htmlwidgets is that there may be performance problem for larger datasets. Because they embed their data directly in their host web page. You can use standard R graphics in the case of large dataset. 7.5.2.2 Standard R graphics A static dashboard is also a great tool for story-telling. Standard R graphics are also scaled in static dashboard with the same aspect ratios. However, it is possible for the PNG images fill the bounds of their container seamlessly. To solve that problem, you can scale figure by defining knitr fig.width and fig.height values to approximate the actual size on the page. For example: You can download the source code and see the complete output. Here is a screenshot of the output: 7.5.2.3 Tabular Data Some of the previous examples included a DataTable component. It is interactive table that you can sort, filter and paginate. You can also display simple table. Here is an example of both: You can download the source code and see the complete output. 7.5.2.4 Value Boxes If you want to call out people’s attention on one or more simple statistics in a dashboard, you can use the valueBox function. It allows you to display single values along with a title and optional icon. For example: You can download the source code and see the complete output. Here is a screenshot of part of the output: The valueBox function will emit a value with an specified icon (icon =) and color (color =). Specify Icon There are three different icon sets you can refer to. You should specify it’s full name including the prefix to icon parameter (e.g &quot;icon = &quot;fa-pencil&quot;,&quot;icon = ion-social-twitter&quot;, etc.) : Font Awesome Icons Ionicons BooBootstrap Glyphicons Specify Color You can specify color using color parameter (e.g. color = &quot;success&quot;). Available colors include “primary”, “info”, “success”, “warning”, and “danger” (the default is “primary”). You can also specify and valid CSS color (e.g. “#ffffff”, “rgb(100,100,100)”, etc.) 7.5.2.5 Gauges If your value is within a specified range such as percentage, it is more intuitive to use gauges. Output: You can download the source code and see the complete output. Those are the main components in a dashboard. More information about flesdashboard for R, refer to “flexdashboard: Easy interactive dashboards for R”. 7.6 Shiny Dashboard 7.6.1 Brief Introduction to Shiny Shiny is a web application framework for R that can help turn your analyses into interactive web applications. It is easy to learn and use. It doesn’t require HTML, CSS, or JavaScript knowledge. This section will demonstrate two examples to help you understand the basic structure of a Shiny App. With some basic understanding, the next section will show how to include shiny in a dashboard. Example 1: Customer Segment Plot The Customer Segment example is a simple application that plots the clothes customer data by segments using htmlwidget metricsgraphics. Type the following code to run the example: library(shiny) source(&quot;https://raw.githubusercontent.com/happyrabbit/linhui.org/gh-pages/CE_JSM2017/Examples/shiny1.R&quot;) shinyApp(ui = ui, server = server) A Shiny app contains two parts: ui: It defines user interface and controls the outlook of the web page. server: It includes the backend manipulation of the input. The source code for both of the components is: library(shiny) library(dplyr) library(metricsgraphics) sim.dat&lt;-readr::read_csv(&quot;https://raw.githubusercontent.com/happyrabbit/DataScientistR/master/Data/SegData.csv&quot;)%&gt;% filter(!is.na(income) &amp; age&lt;100) # Define UI for application that draws a metricsgraphics interactive plot ui &lt;- pageWithSidebar( # inpute the panel header headerPanel(&#39;Customer Segment&#39;), # sidebar with input for customer segment sidebarPanel( selectInput(&#39;seg&#39;, &#39;Segment&#39;, unique(sim.dat$segment)) ), # show a metricsgraphics plot mainPanel( metricsgraphicsOutput(&#39;plot1&#39;) ) ) # Define server logic required to draw a metricsgraphics plot server &lt;- function(input, output) { # Expression that generates a metricsgraphics The expression is # wrapped in a call to renderMetricsgraphics to indicate that: # # 1) It is &quot;reactive&quot; and therefore should be automatically # re-executed when inputs change # 2) Its output type is a renderMetricsgraphics # select the part of data needed selectedData &lt;- reactive({ dplyr::filter(sim.dat, segment == input$seg) }) # render plot output$plot1 &lt;- renderMetricsgraphics({ mjs_plot(selectedData(), x= age, y=online_exp) %&gt;% mjs_point(color_accessor=income, size_accessor=income) %&gt;% mjs_labs(x=&quot;Age&quot;, y=&quot;Online Expense&quot;) }) } # Run the application shinyApp(ui = ui, server = server) The example here has a single character input specified using a slider and a single metricsgraphics plot output. The server-side of the application generates a metricsgraphics plot. Notice that the code generating the plot is wrapped in a call to renderMetricsgraphics. There are different render calls in Shiny: renderDataTable renderImage renderPlot renderPrint renderTable renderText renderUI You can choose the appropriate one as needed. The next example is a little more complicated with more input controls. You may be confused by the reactive expression in example 1. Don’t worry. We will explain the use in the next example. Example 2: Customer Segment Plot and Summary Table Example 2 demonstrates how to include multiple inputs and render both table and graphic using htmlwidgets. Type the following code to run the application: library(shiny) source(&quot;https://raw.githubusercontent.com/happyrabbit/linhui.org/gh-pages/CE_JSM2017/Examples/shiny2.R&quot;) shinyApp(ui = ui, server = server) This example has a little more going on: three inputs: (1) customer segment; (2) x-axis variable of the plot; (3) y-axis variable of the plot two outputs: (1) a table on HTML pages with filtering, pagination, sorting features in the table; (2) a metricsgraphics plot library(shiny) library(dplyr) library(DT) library(metricsgraphics) sim.dat&lt;-readr::read_csv(&quot;https://raw.githubusercontent.com/happyrabbit/DataScientistR/master/Data/SegData.csv&quot;)%&gt;% filter(!is.na(income) &amp; age&lt;100) # Define UI for application that draws a histogram ui &lt;- pageWithSidebar( headerPanel(&#39;Customer Segment&#39;), sidebarPanel( selectInput(&#39;seg&#39;, &#39;Segment&#39;, unique(sim.dat$segment)), selectInput(&#39;xcol&#39;, &#39;X Variable&#39;, c(&quot;age&quot;)), selectInput(&#39;ycol&#39;, &#39;Y Variable&#39;, c(&quot;store_exp&quot;,&quot;online_exp&quot;,&quot;store_trans&quot;,&quot;online_trans&quot;)) ), mainPanel( metricsgraphicsOutput(&#39;plot1&#39;), dataTableOutput(&quot;summary&quot;) ) ) # Define server logic required to draw a histogram server &lt;- function(input, output) { # Combine the selected variables into a new data frame selectedData &lt;- reactive({ dplyr::filter(sim.dat, segment == input$seg) }) output$plot1 &lt;- renderMetricsgraphics({ mjs_plot(selectedData(), x= input$xcol, y=input$ycol) %&gt;% mjs_point(color_accessor=income, size_accessor=income) %&gt;% mjs_labs(x=input$xcol, y=input$ycol) }) # Generate a summary of the dataset output$summary &lt;- renderDataTable({ sim.dat%&gt;% group_by(segment)%&gt;% summarise(Age=round(mean(na.omit(age)),0), FemalePct=round(mean(gender==&quot;Female&quot;),2), HouseYes=round(mean(house==&quot;Yes&quot;),2), store_exp=round(mean(na.omit(store_exp),trim=0.1),0), online_exp=round(mean(online_exp),0), store_trans=round(mean(store_trans),1), online_trans=round(mean(online_trans),1))%&gt;% data.frame()%&gt;% datatable( rownames = FALSE, caption = &#39;Table 1: Segment Summary Table&#39;, options = list( pageLength = 4, autoWidth = TRUE) ) }) } # Run the application shinyApp(ui = ui, server = server) There are three selectInput calls in the user interface definition. Inside the mainPanel(), there are two calls metricsgraphicsOutput and dataTableOutput. The server side also has some new elements. There are: A reactive expression to return the subset of data according to user’s choice Rendering expression renderMetricsgraphics return the output$plot1 Rendering expression renderDataTable return the output$summary It is important to understand the concept of reactivity. The fundamental feature of Shiny is interactivity which means the output will change with input. The process is: user provide input backend R code will run using the input report an output back to user The changing step is through reactive programming. For more details about reactive programming, see the Reactivity Overview. RStudio provides an excellent Shiny tutorial from beginning to deep level: http://shiny.rstudio.com/tutorial/. 7.6.2 Using shiny with flexdashboard You can also create a dashboard that enables viewers to change underlying parameters and see the corresponding results. You can add shiny to flexdashboard by specifying runtime: shiny in the front-matter of your document. --- title: \"Customer Segmentation Dashboard\" output: flexdashboard::flex_dashboard: orientation: rows vertical_layout: fill source_code: embed social: menu theme: flatly runtime: shiny --- Then add one or more input controls and reactive expressions as in shiny. The difference is that when you add shiny function to flexdashboard, there is no need to use wrap the code to two components, ui and server. In that sense, using shiny in flexdashboard is easier than building Shiny App itself. An alternative way to dashboards with Shiny is to use shinydashboard package. Example: Customer Segmentation Dashboard Here is an example dashboard using the clothes customer data. 7.7 HTML Widgets R HTMLWidgets framework brings the best of JavaScript visualization to R. You can, using modern RStudio builds, use HTMLWidgets right in the RStudio environment and interact with the plotting pane as if it was a modern browser. You can embed HTML Widgets in Shiny applications and R Markdown documents to make them interactive. R developer with some JavaScript experience can develop new widgets using the seamless R/JavaScript bridge provided by the HTMLWidgets package. From geospatial mapping to time series visualization, from d3.js interactivity to beautiful interactive tabular display of data, the HTMLWidgets framework provides a foundation for that next level of interactivity and fluency in your interfaces. There are some packages that can be used to make interactive R Markdown documents. 7.7.1 DT: A Wrapper of the JavaScript Library DataTables The R package DT provides an R interface to the JavaScript library DataTables. R data objects (matrices or data frames) can be displayed as tables on HTML pages, and DataTables provides filtering, pagination, sorting, and many other features in the tables. You can refer to https://rstudio.github.io/DT/ for more details. library(DT) library(dplyr) sim.dat&lt;-read.csv(&quot;https://raw.githubusercontent.com/happyrabbit/DataScientistR/master/Data/SegData.csv&quot;) seg&lt;-sim.dat%&gt;% filter(age &lt; 100)%&gt;% group_by(segment)%&gt;% summarise(Age=round(mean(na.omit(age)),0), FemalePct=round(mean(gender==&quot;Female&quot;),2), HouseYes=round(mean(house==&quot;Yes&quot;),2), store_exp=round(mean(na.omit(store_exp),trim=0.1),0), online_exp=round(mean(online_exp),0), store_trans=round(mean(store_trans),1), online_trans=round(mean(online_trans),1))%&gt;% data.frame() datatable( seg, rownames = FALSE, caption = &#39;Table 1: Segment Summary Table&#39;, options = list( pageLength = 4, autoWidth = TRUE) ) 7.7.2 Leaflet:Interactive Web-Maps Based on the Leaflet JavaScript Library R package leaflet makes it easy to integrate and control Leaflet maps in R. The JaveScript library leaflet is for interactive maps. You can embed maps in R Markdown documents and Shiny apps. library(leaflet) leaflet() %&gt;% addTiles() %&gt;% addMarkers(lng= -76.6171, lat=39.2854, popup=&quot;JSM 2017: Baltimore Convention Center&quot;) See https://rstudio.github.io/leaflet/ for more details. 7.7.3 dygraphs: interactive plot for time series data The dygraphs package is an R interface to the dygraphs JavaScript charting library. It provides rich facilities for charting time-series data in R, including highly configurable series and axis display and interactive features like zoom/pan and series/point highlighting. See https://rstudio.github.io/dygraphs/ for more details. library(dygraphs) wikiview&lt;-read.csv(&quot;https://raw.githubusercontent.com/happyrabbit/linhui.org/gh-pages/CE_JSM2017/Slides/wikiview.csv&quot;) tr&lt;-wikiview%&gt;% filter(article == &quot;Donald_Trump&quot;)%&gt;% select(timestamp, Donald_Trump = views) iv&lt;-wikiview%&gt;% filter(article == &quot;Ivanka_Trump&quot;)%&gt;% select(timestamp, Ivanka_Trump = views) ku&lt;-wikiview%&gt;% filter(article == &quot;Jared_Kushner&quot;)%&gt;% select(timestamp, Jared_Kushner = views) cl&lt;-wikiview%&gt;% filter(article == &quot;Hillary_Clinton&quot;)%&gt;% select(timestamp, Hillary_Clinton = views) #dplot&lt;- cbind(Donald_Trump = ts(tr$Donald_Trump, frequency = 365, start=c(2016,01,01)), #Ivanka_Trump = ts(iv$Ivanka_Trump, frequency = 365, start=c(2016,01,01)), #Jared_Kushner = ts(ku$Jared_Kushner, frequency = 365, start=c(2016,01,01)), #Hillary_Clinton = ts(cl$Hillary_Clinton, frequency = 365, start=c(2016,01,01))) library(xts) library(lubridate) dplot&lt;-merge(tr,iv) dplot&lt;-merge(dplot,ku) dplot&lt;-merge(dplot,cl) dplot$timestamp&lt;-ymd(dplot$timestamp/100) dplot &lt;- xts(select(dplot, -timestamp), order.by = dplot$timestamp) dygraph(dplot, main = &quot;Wikipedia Views&quot;)%&gt;% dyRangeSelector() 7.7.4 highcharter Highcharter is a rich R interface to the popular Highcharts JavaScript graphics library library(highcharter) library(dplyr) sim.dat&lt;-read.csv(&quot;https://raw.githubusercontent.com/happyrabbit/DataScientistR/master/Data/SegData.csv&quot;) dplot&lt;-sim.dat%&gt;% filter(!is.na(income) &amp; age&lt;100) highchart() %&gt;% hc_title(text = &quot;Scatter chart: Age v.s Online Expense&quot;) %&gt;% hc_add_series_scatter(dplot$age, round(dplot$online_exp,0), round(dplot$income,0), dplot$income) ## Warning: &#39;hc_add_series_scatter&#39; is deprecated. ## Use &#39;hc_add_series&#39; instead. ## See help(&quot;Deprecated&quot;) 7.7.5 rbokeh is a visualization library that provides a flexible and powerful declarative framework for creating web-based plots library(rbokeh) dplot&lt;-sim.dat%&gt;% filter(!is.na(income) &amp; age&lt;100) p &lt;- figure() %&gt;% ly_points(age, income, data = dplot, color = segment, glyph = segment) p https://hafen.github.io/rbokeh/ rbokeh renders plots using HTML canvas and provides many mechanisms for interactivity Plots in rbokeh are build by layering plot elements, called glyphs, to create the desired visualization 7.7.6 metricsgraphics enables easy creation of D3 scatterplots, line charts, and histograms. library(metricsgraphics) dplot&lt;-sim.dat%&gt;% filter(!is.na(income) &amp; age&lt;100) mjs_plot(dplot, x= age, y=online_exp) %&gt;% mjs_point(color_accessor=income, size_accessor=income) %&gt;% mjs_labs(x=&quot;Age&quot;, y=&quot;Online Expense&quot;) https://hrbrmstr.github.io/metricsgraphics/ This makes it possible to avoid one giant function with a ton of parameters and facilitates breaking out the chart building into logical steps. While MetricsGraphics.js charts may not have the flexibility of ggplot2, you can build functional, interactive [multi-]line, scatterplot, bar charts &amp; histograms and + even link charts together. 7.7.7 networkD3: D3 JavaScript Network Graphs from R Package networkD3 provides tools for creating D3 JavaScript network graphs from R. library(networkD3) data(MisLinks, MisNodes) forceNetwork(Links = MisLinks, Nodes = MisNodes, Source = &quot;source&quot;, Target = &quot;target&quot;, Value = &quot;value&quot;, NodeID = &quot;name&quot;, Group = &quot;group&quot;, opacity = 0.4) 7.7.8 threejs: Interactive 3D Scatter Plots and Globes Package threejs provides interactive 3D scatterplots and globe plots. Here is a galary of examples from the package and also the source code http://bwlewis.github.io/rthreejs/. Here is an example of R Markdown ducument including interactive figure/table from the packages mentioned. Run the following and you can get this interactive webpage http://linhui.org/Hui s_files/SampleForInteractiveReport.html library(threejs) data(world.cities, package=&quot;maps&quot;) cities &lt;- world.cities[order(world.cities$pop,decreasing=TRUE)[1:1000],] value &lt;- 100 * cities$pop / max(cities$pop) earth &lt;- texture(system.file(&quot;images/world.jpg&quot;,package=&quot;threejs&quot;)) globejs(img=earth, lat=cities$lat, long=cities$long, value=value) As mentioned before, there are over 40 packages on CRAN which provide htmlwdgets. There are more than 80 packages in total. You can browse all available widgets in the gallery and find example uses of popular htmlwidgets in the showcase website. "],
["soft-skills-for-data-scientists.html", "Chapter 8 Soft Skills for Data Scientists 8.1 Comparison between Statistician and Data Scientist 8.2 Where Data Science Team Fits? 8.3 Beyond Data and Analytics 8.4 Data Scientist as a Leader 8.5 Three Pillars of Knowledge 8.6 Common Pitfalls of Data Science Projects", " Chapter 8 Soft Skills for Data Scientists 8.1 Comparison between Statistician and Data Scientist Statistics as a scientific area can be traced back to 1749 and statistician as a career has been around for hundreds of years with well-established theory and application. Data Scientist becomes an attractive career for only a few years along with the fact that data size and variety beyond the traditional statistician’s tool box and the fast growing of computation power. Statistician and data scientist have a lot of common background, but there are also some significant differences. Both statistician and data scientist work closely with data. For traditional statistician, the data is usually well-formatted text files with numbers and labels. The size of the data usually can be fitted in a PC’s memory. In addition to the data that statistician usually works on, data scientist must deal with more varieties of data: well-formatted data stored in a database system with size much larger than a PC’s memory or hard-disk; large amount of free text, voice, image and video files; real-time streaming data and other types of records. One particular power of statistics is that statistician can fit model and make inference based on limited data. It is quite common that once the data is given and cleaned, majority of the work is developed different models around the data. For data scientist, and for some of the problems, data is relatively abundant and modeling is just part of the overall effort and delivery actionable results is usually the focus. For statistician, data is usually brought to model. While for data scientist, sometime the data size is large and model is usually brought to where data seats. From the entire problem solving cycle, statisticians are usually not well integrated with the production system where data is obtained in real time; while data scientists are more embedded in the production system and closer to the data generation procedures. 8.2 Where Data Science Team Fits? During the past decade, vast amount of data has become available and readily accessible for analysis in many companies at different business sectors. The size, complexity and speed of increment of data suddenly beyond the traditional scope of statistical analysis or BI reporting as mentioned above. To leverage the big data collected during business operations, many companies have established new organizations of data science and machine learning. Companies have gone through different paths to create their own data science and machine learning organizations. There are three major formats of data science teams: independent of any current organizations and the team report directly to senior leadership; within each business unit and the team report to business unit leaders; within in the traditional IT organizations and the team report to IT leaders. Companies are different in many aspects, but in general the most efficient option to mine big data is a team of data scientist independent of business units and IT organizations. The independence enables the data science team to collaborate across business units and IT organizations more efficiently and the independence also provides flexibility and potential to solve corporate level strategic big data problems. For each business units, there are many business unit specific data science related problems and embedding data scientist within each business units is also an efficient way to solve business unit specific data science problems. The full cycle of data science projects from data to decision (i.e. Data  Information  Knowledge  Insight  Decision) is relatively difficulty to achieve if the data science team is part of traditional IT organizations. 8.3 Beyond Data and Analytics Data scientists usually have a good sense of data and analytics, but data scientist project is definitely more than just data and analytics. For most data science projects, there will be a business owner or leader to identify opportunities in business value; there will be program managers to ensure each data science projects fit into the overall technical program development; there will be data owners and computation resource and infrastructure owners from IT department; there will be dedicated team to make sure the data and model are under model governance and privacy guidelines; there usually a team to implement the model in production environment and then maintain and refresh the model if needed; there will be project managers to coordinate all parties to setup weekly or bi-weekly tasks such that the project meets the preset milestones and delivery results; most importantly there will be multiple rounds of discussion of resource allocation (i.e. who will pay for the data science project). Effective communication and in-depth domain knowledge about the business problem becomes essential key requirements for a successful data scientist. Data scientist will interact with people at various levels ranging from senior leaders who are setting the corporate strategies to front line employees who are doing the daily work. Data scientist needs to have the capability to view the problem from 10,000 feet above ground, as well as down to the detail to the very bottom. Most importantly, data scientist needs to be able to talk to everyone with the language they can understand and obtain the needed information from talking to people to convert a business problem into a data science problem. 8.4 Data Scientist as a Leader During the entire process of data science project defining, planning, executing and implementation, the data scientist lead needs to be involved in every step to ensure the business problem is defined correctly and the business value and success metric are evaluated reasonable. Corporates are investing heavily in data science and machine learning with very high expectation of big return. There are too many opportunities to introduce unrealistic goal and business impact for a particular data science project. The leading data scientist need to be the leader in these discussions to define the goal backed with data and analytics. Many data science projects over promise in deliverables and too optimistic on timeline and these projects eventually fail by not delivering the preset business impact within the timeline. As the data scientist in the team, we need to identify these issues early in the stage and communicate to the entire team to make sure the project has a realistic deliverable and timeline. The data scientist team also need to work closely with data owners to identify relevant internal and external data source and evaluate the quality of the data; as well as working closely with the computation infrastructure team to understand the computation resources (i.e. hardware and software) available for the data science project. 8.5 Three Pillars of Knowledge The following picture summarizes the needed three pillars of knowledge to be a successful data scientist. First and foremost, a successful data scientist needs to have a strong technical background in data mining, statistics and machine learning. The in-depth understanding of modeling with the insight about data enable a data scientist to convert a business problem to a data science problem. Second, a successful data scientist needs to have certain domain knowledge for the business sector such that he or she can understand to business problem easily. For any data science project, data scientist is going to collaborate with other team members and effective communication and leadership skills are very important, especially when you are the only data and analytics people in the room and you need to make decision with uncertainty. The last pillar is about computation environment and model implementation in big data platform and this pillar used to be the most difficult one for data scientist with statistics background (i.e. lack computer science or programming skills). Good news is that with the raise of cloud computation big data platform, this barrier is getting easier for statistician to overcome and we will discuss in more detail in next chapter. 8.6 Common Pitfalls of Data Science Projects Data science projects are usually complicated in nature and many of these data science projects eventually fail due to various reasons. We will briefly discuss a few common pitfalls in data science projects and how to avoid them. Solve the wrong problem: data science project usually starts with a very vague description and a few rounds of detailed discussion with all stake-holder involved are needed to define the busses problem. There will be lots of opportunities to introduce misalignment when mapping the business problem into specific data science methods. Especially when the quality and availability of the data are not as good as what is expected at the first place. If not well-communicated during the project, the final data science solution may not be the right one to solve the business problem. As the data scientist (sometimes the only data scientist) in the room, we must understand the business problem thoroughly and communicate regularly to business partners especially there is a change of status to make sure everyone is aligned with the progress and final deliverables. Over promise on business value: business leaders usually have high expectation on data science projects and the goal of business value and deliverables sometimes are set unrealistic and eventually beyond the scope of available data and computation resource. As the data scientist (sometimes the only data scientist) in the room, we must have our voice heard based on fact (i.e. data, analytics and resources) instead of wishful thinking. Backed with fact-based evidence, it is easier to communicate what is a realistic goal to the entire team. Too optimistic about timeline: there are lots of uncertainties in data science projects such as the data source availability and data quality, computation hardware and software, resource availability in the business team, implementation team and IT department, as well as project direction change which may delay the final delivery date. To have a better estimated timeline, get as much detail as possible for all the needed tasks and estimated each task individually and reach out to each team member to confirm their availability. Most importantly, communicate with the entire team if there are blocking factors for the project in a prompt way such that everyone aware of the situation and potential impact on the timeline. Too optimistic about data availability and quality: the most important asset in data science project is data. Even though we are at the big data age, often times there are not enough relevant data for the data science projects. The data quality is also a general problem for data science projects. A thorough data availability and quality check is needed at the beginning of the data science project to estimate the needed effort to obtain data as well as data cleaning. Model cannot be scaled: a subset of data is usually extracted to fit the model and then scaled to the entire dataset. When developing the model use a smaller dataset, we must keep in mind the amount of computation resources needed for the entire dataset. Given a relative fixed computation resource (i.e. budget) for a data science project, it is import to limit the computation time in production to a reasonable level based on business application when fits the model with a sample dataset. Take too long to fail: data science projects usually are trying to push the boundary of current applications to new territory, people do not expect all data science projects to be successful. Fail fast is generally good practice such that we can quickly find a better way to solve the problem. Data scientist needs to have an open mindset to not stuck with one idea or one approach for a long time to avoid taking too long to fail. "],
["case-study.html", "Chapter 9 Case Study 9.1 Case 1: Customer Perception Study for Airline Company 9.2 Case 2: Swine Disease Prediction", " Chapter 9 Case Study 9.1 Case 1: Customer Perception Study for Airline Company 9.2 Case 2: Swine Disease Prediction "],
["references.html", "Chapter 10 References", " Chapter 10 References "]
]
