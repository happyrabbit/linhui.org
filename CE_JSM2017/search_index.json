[
["index.html", "Preparing Statistician/Statistics Graduates to be Enterprise Data Scientist Chapter 1 Introduction 1.1 What is data science? 1.2 What kind of questions can data science solve? 1.3 What are the required skills for data scientist? 1.4 General Process of Data Science 1.5 Cloud-based computation environment", " Preparing Statistician/Statistics Graduates to be Enterprise Data Scientist Hui Lin and Ming Li 2016-12-20 Chapter 1 Introduction With the recent big data revolution, enterprises ranging from FORTUNE 500 to startup across the US are hungry for Data Scientist to bring valuable insight from data collected. Statistician and graduate students from traditional statistics departments are great data scientist candidates, but there are relative few data scientist with statistics education background. In this CE course, we will go through the needed knowledge and skills to leverage statistics background to prepare statistician to be excellent data scientist in US enterprise environments. The cloud-based computation environment will be used throughout this course with plenty of hands-on exercises. We will use case studies to cover how to leverage big data distributed platform (Hadoop / Hive / Spark), data wrangling, modeling, dynamic report (R markdown) and interactive dashboard (R-Shiny) to tackle real-world data science problems. One typical skill gap for statistician is data ETL (extraction, transformation and load) in production environments, and we will cover this topic as well. Data science is a combination of science and art with data as the foundation. We will also cover the “art” part to guide participant to learn soft skills to define data science problems and to effectively communicate with business stakeholders. The prerequisite knowledge is MS level education in statistics and entry level of R-Studio. This CE course will be one-day training. 1.1 What is data science? 1.2 What kind of questions can data science solve? 1.3 What are the required skills for data scientist? 1.4 General Process of Data Science 1.5 Cloud-based computation environment "],
["big-data-cloud-platform.html", "Chapter 2 Big Data Cloud Platform 2.1 Introduction of Databricks cloud-based distributed system 2.2 Linux system and Hadoop environment 2.3 Database basic through Hive 2.4 Spark and H2O", " Chapter 2 Big Data Cloud Platform 2.1 Introduction of Databricks cloud-based distributed system 2.2 Linux system and Hadoop environment 2.3 Database basic through Hive 2.4 Spark and H2O "],
["data-wrangling.html", "Chapter 3 Data Wrangling 3.1 Tidy data 3.2 Reshape data 3.3 Subset data 3.4 Summarize data 3.5 Combine data", " Chapter 3 Data Wrangling 3.1 Tidy data 3.2 Reshape data 3.3 Subset data 3.4 Summarize data 3.5 Combine data "],
["data-pre-processing.html", "Chapter 4 Data Pre-processing 4.1 Start 4.2 Centering and Scaling 4.3 Resolve Skewness 4.4 Resolve Outliers 4.5 Missing Values 4.6 Collinearity 4.7 Sparse Variables 4.8 Re-encode Dummy Variables", " Chapter 4 Data Pre-processing 4.1 Start There are a number of reasons a predictive model falls (Max Kuhn 2013), such as: Inadequate data pre-processing Inadequate model validation Unjustified extrapolation Over-fitting In this blog post, I am going to summarize some common data pre-processing approaches. 4.2 Centering and Scaling It is the most straightforward data transformation. It centers and scales a variable to mean 0 and standard deviation 1. It ensures that the criterion for finding linear combinations of the predictors is based on how much variation they explain and therefore improves the numerical stability. Models involving finding linear combinations of the predictors to explain response/predictors variation need data centering and scaling, such as PCA and PLS. You can easily writing code yourself to conduct this transformation. Or the function preProcess() in package caret can apply this transformation to a set of predictors. #install packages needed library(caret) library(e1071) library(gridExtra) library(lattice) library(imputeMissings) library(RANN) library(corrplot) library(nnet) head(cars) ## speed dist ## 1 4 2 ## 2 4 10 ## 3 7 4 ## 4 7 22 ## 5 8 16 ## 6 9 10 trans&lt;-preProcess(cars,method=c(&quot;center&quot;,&quot;scale&quot;)) transformed&lt;-predict(trans,cars) par(mfrow=c(1,2)) hist(cars$dist,main=&quot;Original&quot;,xlab=&quot;dist&quot;) hist(transformed$dist,main=&quot;Centered and Scaled&quot;,xlab=&quot;dist&quot;) Sometimes you only need to scale the variable. For example, if the model adds penalty to the parameter estimates (such as \\(L_2\\) penalty is ridge regression and \\(L_1\\) penalty in LASSO), the variables need to have similar scale to ensure a fair variable selection. I am heavy user of this kind of penalty-based model in my work and I used the following quantile transformation: \\[ x_{ij}^{*}=\\frac{x_{ij}-quantile(x_{.j},0.01)}{quantile(x_{.j}-0.99)-quantile(x_{-j},0.01)} \\] The reason to use 99% and 1% quantile instead of maximum and minimum values is to resist the impact of outliers. It is easy to write a function to do it: qscale&lt;-function(dat){ for (i in 1:ncol(dat)){ up&lt;-quantile(dat[,i],0.99) low&lt;-quantile(dat[,i],0.01) diff&lt;-up-low dat[,i]&lt;-(dat[,i]-low)/diff } return(dat) } In order to illustrate, let’s simulate a data set with two variables: income and age. set.seed(2015) income&lt;-sample(seq(50000,150000,by=500),95) age&lt;-income/2000-10 noise&lt;-round(runif(95)*10,0) age&lt;-age+noise income&lt;-c(income,10000,15000,300000,250000,230000) age&lt;-c(age,30,20,25,35,95) demo&lt;-data.frame(income,age) demo$education&lt;-as.factor(sample(c(&quot;High School&quot;,&quot;Bachelor&quot;,&quot;Master&quot;,&quot;Doctor&quot;),100,replace = T,prob =c(0.7,0.15,0.12,0.03) )) summary(demo[,c(&quot;income&quot;,&quot;age&quot;)]) ## income age ## Min. : 10000 Min. :20.00 ## 1st Qu.: 76375 1st Qu.:30.25 ## Median : 98750 Median :44.25 ## Mean :103480 Mean :44.92 ## 3rd Qu.:126375 3rd Qu.:56.88 ## Max. :300000 Max. :95.00 It is clear that income and age are not on the same scale. Now apply the function qscale() on the simulated data demo. transformed&lt;-qscale(demo[,c(&quot;income&quot;,&quot;age&quot;)]) summary(transformed) ## income age ## Min. :-0.02101 Min. :-0.01904 ## 1st Qu.: 0.26077 1st Qu.: 0.17814 ## Median : 0.35576 Median : 0.44746 ## Mean : 0.37584 Mean : 0.46044 ## 3rd Qu.: 0.47304 3rd Qu.: 0.69033 ## Max. : 1.21015 Max. : 1.42375 4.3 Resolve Skewness Skewness is defined to be the third standardized central moment. The formula for the sample skewness statistics is: \\[ skewness=\\frac{\\sum(x_{i}+\\bar{x})^{3}}{(n-1)v^{3/2}}\\] \\[v=\\frac{\\sum(x_{i}=\\bar{x})^{2}}{(n-1)}\\] Skewness=0 means that the destribution is symmetric, i.e. the probability of falling on either side of the distribution’s mean is equal. You can easily tell if a distribution is skewed by simple visualization. There are different ways may help to remove skewness such as log, square root or inverse. However it is often difficult to determine from plots which transformation is most appropriate for correcting skewness. The Box-Cox procedure automatically identified a transformation from the family of power transformations that are indexed by a parameter \\(\\lambda\\). \\[ x^{*}=\\begin{cases} \\begin{array}{c} \\frac{x^{\\lambda}-1}{\\lambda}\\\\ log(x) \\end{array} &amp; \\begin{array}{c} if\\ \\lambda\\neq0\\\\ if\\ \\lambda=0 \\end{array}\\end{cases} \\] It is easy to see that this family includes log transformation (\\(\\lambda=0\\)), square transformation (\\(\\lambda=2\\)), square root (\\(\\lambda=0.5\\)), inverse (\\(\\lambda=-1\\)) and others in-between. We can still use function preProcess() in package caret to apply this transformation by chaning the method argument. (trans&lt;-preProcess(cars,method=c(&quot;BoxCox&quot;))) ## Created from 50 samples and 2 variables ## ## Pre-processing: ## - Box-Cox transformation (2) ## - ignored (0) ## ## Lambda estimates for Box-Cox transformation: ## 1, 0.5 The output shows the sample size (50), number of variables (2) and the \\(\\lambda\\) estimates for each variable. After calling the preProcess() function, the predict() method applies the results to a data frame. transformed&lt;-predict(trans,cars) par(mfrow=c(1,2)) hist(cars$dist,main=&quot;Original&quot;,xlab=&quot;dist&quot;) hist(transformed$dist,main=&quot;After BoxCox Transformation&quot;,xlab=&quot;dist&quot;) An alternative is to use function BoxCoxTrans() in package caret. (trans&lt;-BoxCoxTrans(cars$dist)) ## Box-Cox Transformation ## ## 50 data points used to estimate Lambda ## ## Input data summary: ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 2.00 26.00 36.00 42.98 56.00 120.00 ## ## Largest/Smallest: 60 ## Sample Skewness: 0.759 ## ## Estimated Lambda: 0.5 transformed&lt;-predict(trans,cars$dist) skewness(transformed) ## [1] -0.01902765 The estimated \\(\\lambda\\) is the same 0.5. Original skewness is 0.759 and after transformation, the skewness is -0.01902765 which is close to 0. You can use function skewness() in package e1071 to get the skewness statistics. 4.4 Resolve Outliers Even under certain assumptions we can statistically define outliers, it can be hard to define in some situations. You can refer to “Detection of Outliers” for more information. Some models are resistant to outliers (such as tree-based model and support vector machine). If a model is sensitive to outliers (such as linear regression and logistic regression), we can use spatial sign transformation to minimize the problem. It projects the original sample points to the surface of a sphere by: \\[x_{ij}^{*}=\\frac{x_{ij}}{\\sqrt{\\sum_{j=1}^{p}x_{ij}^{2}}}\\] As noted in the book “Applied Predictive Modeling”, Since the denominator is intended to measure the squared distance to the center of the predictor’s distribution, it is important to center and scale the predictor data prior to using this transformation. Note that, unlike centering or scaling, this manipulation of the predictors transforms them as a group. We can use spatialSign() function in caret to conduct spatial sign on demo: trans&lt;-preProcess(demo[,c(&quot;income&quot;,&quot;age&quot;)],method=c(&quot;center&quot;,&quot;scale&quot;)) transformed&lt;-predict(trans,demo[,c(&quot;income&quot;,&quot;age&quot;)]) transformed2 &lt;- spatialSign(transformed) transformed2 &lt;- as.data.frame(transformed2) p1&lt;-xyplot(income ~ age, data = transformed, main=&quot;Original&quot;) p2&lt;-xyplot(income ~ age, data = transformed2, main=&quot;After Spatial Sign&quot;) grid.arrange(p1,p2, ncol=2) 4.5 Missing Values We need a book to fully explicate this topic. Before we decide how to handle missing value, it is important to understand why the values are missing. Do the missing values have information related outcomes? Or are they missing at random? It is not the goal here to illustrate which methods to use in different missing situation. You can refer to Section 3.4 of “Applied Predictive Modeling” for more discussion on that. The objective of this post is to introduce some imputation methods and corresponding application examples using R. Survey statistics has studied the imputation extensively which focuses on making valid inferences. Missing value imputation in predictive modeling is a different problem. Saar-Tsechansky and Provost compared several different methods for applying classification to instance with missing values. “Handling Missing Values when Applying Classification Models” The following code randomly assigns some missing values to the previous data demo and names the new data set demo_missing. set.seed(100) id1&lt;-sample(1:nrow(demo),15) id2&lt;-sample(1:nrow(demo),10) id3&lt;-sample(1:nrow(demo),10) demo_missing&lt;-demo demo_missing$age[id1]&lt;-NA demo_missing$income[id2]&lt;-NA demo_missing$education[id3]&lt;-NA summary(demo_missing) ## income age education ## Min. : 15000 Min. :20.00 Bachelor :13 ## 1st Qu.: 77125 1st Qu.:30.25 Doctor : 2 ## Median : 98750 Median :44.25 High School:70 ## Mean :102811 Mean :44.43 Master : 5 ## 3rd Qu.:125250 3rd Qu.:56.25 NA&#39;s :10 ## Max. :300000 Max. :95.00 ## NA&#39;s :10 NA&#39;s :15 4.5.1 Impute missing values with median/mode You can use function impute() under package imputeMissings to impute missing values with mdedian/mode. This method is simple, fast but treats each predictor independently, and may not be accurate. demo_imp&lt;-impute(demo_missing,method=&quot;median/mode&quot;) summary(demo_imp) ## income age education ## Min. : 15000 Min. :20.00 Bachelor :13 ## 1st Qu.: 79250 1st Qu.:32.19 Doctor : 2 ## Median : 98750 Median :44.25 High School:80 ## Mean :102405 Mean :44.40 Master : 5 ## 3rd Qu.:122875 3rd Qu.:54.50 ## Max. :300000 Max. :95.00 Note that the median/mode method imputes mode to character vectors and median to numeric and integer vectors.So you can see the 10 missing values for variable “education” are imputed with “High School” since it is the mode. You can also use function ‘preProcess()’ to attain this.But it only works for numeric variable. imp&lt;-preProcess(demo_missing[,c(&quot;income&quot;,&quot;age&quot;)],method=&quot;medianImpute&quot;) demo_imp&lt;-predict(imp,demo_missing[,c(&quot;income&quot;,&quot;age&quot;)]) summary(demo_imp) ## income age ## Min. : 15000 Min. :20.00 ## 1st Qu.: 79250 1st Qu.:32.19 ## Median : 98750 Median :44.25 ## Mean :102405 Mean :44.40 ## 3rd Qu.:122875 3rd Qu.:54.50 ## Max. :300000 Max. :95.00 4.5.2 Impute missing values based on K-nearest neighbors k-nearest neighbor will find the k closest samples (Euclidian distance) in the training set and impute the mean of those “neighbors”. imp&lt;-preProcess(demo_missing[,c(&quot;income&quot;,&quot;age&quot;)],method=&quot;knnImpute&quot;,k=2) demo_imp&lt;-predict(imp,demo_missing[,c(&quot;income&quot;,&quot;age&quot;)]) ## Error in FUN(newX[, i], ...): cannot impute when all predictors are missing in the new data point Now we get a error saying “cannot impute when all predictors are missing in the new data point”. It is because there is at least one sample with both “income” and “age” missing. We can delete the corresponding row and do it again. idx&lt;-which(is.na(demo_missing$income)&amp;is.na(demo_missing$age)) imp&lt;-preProcess(demo_missing[-idx,c(&quot;income&quot;,&quot;age&quot;)],method=&quot;knnImpute&quot;,k=2) demo_imp&lt;-predict(imp,demo_missing[-idx,c(&quot;income&quot;,&quot;age&quot;)]) summary(demo_imp) ## income age ## Min. :-2.259679 Min. :-1.53784 ## 1st Qu.:-0.686725 1st Qu.:-0.88276 ## Median :-0.104506 Median :-0.01129 ## Mean :-0.006233 Mean : 0.01103 ## 3rd Qu.: 0.593512 3rd Qu.: 0.72444 ## Max. : 5.074342 Max. : 3.18343 The error doesn’t show up this time. This method considers all predictors together but it requires them to be in the same scale since the “euclidian distance” is used to find the neighbours. 4.6 Collinearity It is probably a technical term that many un-technical people also know. There is an excellent function in corrplot package with the same name corrplot() that can visualize correlation structure of a set of predictors. The function has option to reorder the variables in a way that reveals clusters of highly correlated ones. We add some columns to demo that are correlated. adddemo&lt;-demo[,-3] adddemo$added1&lt;-sqrt(demo$age)+10 adddemo$added2&lt;-log(demo$income)+demo$age adddemo$added2&lt;-log(demo$age) adddemo$added4&lt;-demo$income/1000+5*demo$age adddemo$added5&lt;-sin(demo$age) The following command will produce visualization for the correlation matrix of adddemo. corrplot(cor(adddemo),order=&quot;hclust&quot;) The size and color of the points are associated with the strength of corresponding correlation. Section 3.5 of “Applied Predictive Modeling” presents a heuristic algorithm to remove minium number of predicitors to ensure all pairwise corelations are below a certain threshold: Calculate the correlation matrix of the predictors. Determine the two predictors associated with the largest absolute pairwise correlation (call them predictors A and B). Determine the average correlation between A and the other variables. Do the same for predictor B. If A has a larger average correlation, remove it; otherwise, remove predictor B. Repeat Step 2-4 until no absolute correlations are above the threshold. The findCorrelation() function in package caret will apaply the above algorithm. (highCorr&lt;-findCorrelation(cor(adddemo),cutoff=.75)) ## [1] 5 2 3 # remove columns with high correlations filter_demo&lt;-adddemo[,-highCorr] # correlation matrix for filtered data corrplot(cor(filter_demo),order=&quot;hclust&quot;) 4.7 Sparse Variables Other than the highly related predictors, predictors with degenerate distributions need to be removed as well. Removing those variables can significant improve some models’ performance and/or stability (such as linear regression and logistic regression but tree based model is impervious to this type of predictors). One extreme example is a variable with single value which is called zero-variance variable. Similarly those variables with very low frequency of unique values are near-zero variance predictors. How to detect those variables? There are two rules: - The fraction of unique values over the sample size - The ratio of the frequency of the most prevalent value to the frequency of the second most prevalent value. The caret package funciton nearZeroVar() can filter near-zero variance predictors. #add two variables with low variance zero_demo&lt;-demo zero_demo$zero1&lt;-rep(0,nrow(demo)) zero_demo$zero2&lt;-c(1,rep(0,nrow(demo)-1)) # zero1 only has one unique value # zero2 is a vector with the first element 1 and the rest are 0s summary(zero_demo) ## income age education zero1 ## Min. : 10000 Min. :20.00 Bachelor :15 Min. :0 ## 1st Qu.: 76375 1st Qu.:30.25 Doctor : 2 1st Qu.:0 ## Median : 98750 Median :44.25 High School:77 Median :0 ## Mean :103480 Mean :44.92 Master : 6 Mean :0 ## 3rd Qu.:126375 3rd Qu.:56.88 3rd Qu.:0 ## Max. :300000 Max. :95.00 Max. :0 ## zero2 ## Min. :0.00 ## 1st Qu.:0.00 ## Median :0.00 ## Mean :0.01 ## 3rd Qu.:0.00 ## Max. :1.00 # the function will return a vector of integers indicating which columns to remove nearZeroVar(zero_demo,freqCut = 95/5, uniqueCut = 10) ## [1] 4 5 Note the two arguments in the function freqCut = and uniqueCut =. They are corresponding to the previous two rules. freqCut: the cutoff for the ratio of the most common value to the second most common value uniqueCut:the cutoff for the percentage of distinct values out of the number of total samples 4.8 Re-encode Dummy Variables Sometimes we need to recode categories to smaller bits of information named “dummy variables”. Take the variable “education” in demo for example. It has four categories: “High School”,“Bachelor”,“Master” and “Doctor”. If we recode it to be dummy variables, each category get its own dummy variable that is 0/1 indicator for that category. For a single categorical variable, we can use function class.ind() in package nnet: dumVar&lt;-class.ind(demo$education) head(dumVar) ## Bachelor Doctor High School Master ## [1,] 0 0 1 0 ## [2,] 0 0 1 0 ## [3,] 0 0 1 0 ## [4,] 0 0 1 0 ## [5,] 0 0 1 0 ## [6,] 0 0 1 0 If we want to determine encodeings for more than one variables, we can use dummyVars() in caret. dumMod&lt;-dummyVars(~income+education, data=demo, # Remove the variable name from the column name levelsOnly=T) predict(dumMod,head(demo)) ## income Bachelor Doctor High School Master ## 1 56000 0 0 1 0 ## 2 133500 0 0 1 0 ## 3 79500 0 0 1 0 ## 4 53000 0 0 1 0 ## 5 63500 0 0 1 0 ## 6 84500 0 0 1 0 To add some more complexity, we could assume joint effect of income and education. In this case, this will add 4 more columns to the resulted data frame: dumMod&lt;-dummyVars(~income+education+income:education, data=demo, levelsOnly=T) predict(dumMod,head(demo)) ## income Bachelor Doctor High School Master income:Bachelor income:Doctor ## 1 56000 0 0 1 0 0 0 ## 2 133500 0 0 1 0 0 0 ## 3 79500 0 0 1 0 0 0 ## 4 53000 0 0 1 0 0 0 ## 5 63500 0 0 1 0 0 0 ## 6 84500 0 0 1 0 0 0 ## income:High School income:Master ## 1 56000 0 ## 2 133500 0 ## 3 79500 0 ## 4 53000 0 ## 5 63500 0 ## 6 84500 0 References "],
["r-shiny-introduction.html", "Chapter 5 R-Shiny Introduction", " Chapter 5 R-Shiny Introduction "],
["dynamicreproducible-report.html", "Chapter 6 Dynamic/Reproducible report", " Chapter 6 Dynamic/Reproducible report "],
["soft-skills-for-data-scientists.html", "Chapter 7 Soft Skills for Data Scientists 7.1 Introduction to agile 7.2 Effective communication with business partners 7.3 Leadership skills 7.4 Decision making with uncertainty", " Chapter 7 Soft Skills for Data Scientists 7.1 Introduction to agile 7.2 Effective communication with business partners 7.3 Leadership skills 7.4 Decision making with uncertainty "]
]
