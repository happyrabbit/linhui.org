[
["index.html", "Preparing Statistician/Statistics Graduates to be Enterprise Data Scientist Chapter 1 Introduction 1.1 What is data science? 1.2 What kind of questions can data science solve? 1.3 What are the required skills for data scientist? 1.4 General Process of Data Science 1.5 Cloud-based computation environment", " Preparing Statistician/Statistics Graduates to be Enterprise Data Scientist Hui Lin and Ming Li 2017-02-07 Chapter 1 Introduction With the recent big data revolution, enterprises ranging from FORTUNE 500 to startup across the US are hungry for Data Scientist to bring valuable insight from data collected. Statistician and graduate students from traditional statistics departments are great data scientist candidates, but there are relative few data scientist with statistics education background. In this CE course, we will go through the needed knowledge and skills to leverage statistics background to prepare statistician to be excellent data scientist in US enterprise environments. The cloud-based computation environment will be used throughout this course with plenty of hands-on exercises. We will use case studies to cover how to leverage big data distributed platform (Hadoop / Hive / Spark), data wrangling, modeling, dynamic report (R markdown) and interactive dashboard (R-Shiny) to tackle real-world data science problems. One typical skill gap for statistician is data ETL (extraction, transformation and load) in production environments, and we will cover this topic as well. Data science is a combination of science and art with data as the foundation. We will also cover the “art” part to guide participant to learn soft skills to define data science problems and to effectively communicate with business stakeholders. The prerequisite knowledge is MS level education in statistics and entry level of R-Studio. This CE course will be one-day training. 1.1 What is data science? 1.2 What kind of questions can data science solve? 1.3 What are the required skills for data scientist? 1.4 General Process of Data Science 1.5 Cloud-based computation environment "],
["big-data-cloud-platform.html", "Chapter 2 Big Data Cloud Platform 2.1 Introduction of Databricks cloud-based distributed system 2.2 Linux system and Hadoop environment 2.3 Database basic through Hive 2.4 Spark and H2O", " Chapter 2 Big Data Cloud Platform 2.1 Introduction of Databricks cloud-based distributed system 2.2 Linux system and Hadoop environment 2.3 Database basic through Hive 2.4 Spark and H2O "],
["section-3.html", "Chapter 3 数据集模拟和背景介绍 3.1 服装消费者数据 3.2 航空公司满意度调查 3.3 生猪疫情风险预测数据", " Chapter 3 数据集模拟和背景介绍 之后的章节将通过案例讨论建模的各个方面。在进入正题之前，我先用本章介绍书中使用的数据，包括模拟数据的代码，数据的获取以及数据语境背景。很多R包里有现成的数据，网上也有各种机器学习竞赛的数据，但本书用来展示模型的数据大部分是通过R得到的模拟数据集。其原因我在第??章开始已经讲过了。 3.1 服装消费者数据 我们先模拟一个关于某品牌服装消费者的数据，这个数据会在之后的章节中反复用到。数据中包含N=1000个观测，我们将模拟3类变量（括号内是变量对应的模拟数据框中的列标签名）： （1）人口统计学变量。 年龄（age） 性别（gender） 有房还是租房（house） （2）消费者行为变量。 2015年实体店购买该品牌服装花销（store_exp） 2015年在线购买该品牌服装花销（online_exp） 2015年实体店交易次数（store_trans） 2015年在线交易次数（online_trans） （3）客户认知问卷调查。为了进一步了解消费者，商家时常对消费者进行问卷调查，然后对调查结果进行分组，其目标是寻找在产品兴趣，市场参与度或营销反应的重要方面有显著差异的客户群。通过了解组间的不同，市场营销人员可以优化产品定位，进行更加精准的营销。这里我们假设该服装品牌对消费者进行了下面的调查，并模拟该调查问卷的回复。 你是否同意下面的申明？ 问题 1（非常不同意） 2（有点不同意） 3（中立/不知道） 4（有点同意） 5（非常同意） （Q1）：我喜欢买不同品牌的服装，比较它们 （Q2）：我喜欢买同一个品牌的服装 （Q3）：品牌的知名度对我来说非常重要 （Q4）：服装质量对我来说非常重要 （Q5）：我有特定喜欢的风格 （Q6）：我喜欢在实体店购买 （Q7）：我喜欢在网上购买 （Q8）：价格对我来说很重要 （Q9）：我喜欢不同风格的衣服 （Q10）：我喜欢自己挑选服装，不需要周围人的建议 我们进一步假设这些根据问卷调查的结果可以将消费者分成4组：价格敏感（Price），炫耀性消费（Conspicuous），质量（Quality），风格（Style）。 （本章我们不会提到如何得到这些分组；我们假设这些已知。我们会在第9章中介绍聚类分析时会更详细的说明。） 你也可以重复下面的代码，自己创建该数据。我们强烈建议读者重复数据模拟的过程，这样能加深对模型方法的理解。如果你对此不感兴趣，也可以从本书网站上直接下载数据： sim.dat&lt;-read.csv(&quot;https://raw.githubusercontent.com/happyrabbit/DataScientistR/master/Data/SegData.csv&quot;) 获得该数据集后你可以跳过本小节后半部分直接跳到下一节对分析流程的讲解。否则，继续本节。 模拟该数据的过程有些复杂，我们先模拟描述客户的变量。模拟该数据的代码分为3部分： 定义数据结构：定义变量名，变量类型，消费者分组名，各组大小。 变量分布参数，如各自的均值和方差。 在各组和各个变量上迭代，基于定义和参数设置抽取随机数。 通过这种方式组织代码，如果我们要改变部分模拟方式重新抽取数据就比较容易。例如，如果我们想要加一个组，或者改变其中某个人口统计变量的均值，只要稍微改变代码就好。我们也想通过这个结构介绍新的R代码，生成数据的第3个步骤中将用到这些代码。 # 设置随机种子，使数据模拟过程可重复 set.seed(12345) # 定义观测数目 ncust&lt;-1000 # 建立数据框存放模拟观测，初始数据框中只有一列id，即消费者编号 seg_dat&lt;-data.frame(id=as.factor(c(1:ncust))) # 指定要生成的变量，并为变量命名 vars&lt;-c(&quot;age&quot;,&quot;gender&quot;,&quot;income&quot;,&quot;house&quot;,&quot;store_exp&quot;,&quot;online_exp&quot;,&quot;store_trans&quot;,&quot;online_trans&quot;) # 每个变量对应的数据类型 # norm： 正态分布 # binom: 二项分布 # pois： 泊松分布 vartype&lt;-c(&quot;norm&quot;,&quot;binom&quot;,&quot;norm&quot;,&quot;binom&quot;,&quot;norm&quot;,&quot;norm&quot;,&quot;pois&quot;,&quot;pois&quot;) # 四个消费者分组的名称 group_name&lt;-c(&quot;Price&quot;,&quot;Conspicuous&quot;,&quot;Quality&quot;,&quot;Style&quot;) # 各消费者群组的大小 group_size&lt;-c(250,200,200,350) # group_name和group_size的第一个元素表明，对于“Price”这组消费者，我们将模拟N=250个观测。 定义好了数据的基本结构之后，我们下一步是定义分布参数，用这些参数来抽取相应数据。 这里我们要 模拟的数据有4个样本类，8个非抽样调查变量，因此我们创建一个4×8的均值矩阵，因为不同类别的 消费者对应不同的分布参数。下面代码用来创建均值矩阵： # 定义均值矩阵 mus &lt;- matrix( c( # 价格敏感（Price）类对应均值 60, 0.5, 120000,0.9, 500,200,5,2, # 炫耀性消费（Conspicuous）类对应均值 40, 0.7, 200000,0.9, 5000,5000,10,10, # 质量（Quality）类对应均值 36, 0.5, 70000, 0.4, 300, 2000,2,15, # 风格（Style）类对应均值 25, 0.2, 90000, 0.2, 200, 2000,2,20), ncol=length(vars), byrow=TRUE) 具体过程是怎样的？ 均值矩阵mus指定，例如，价格敏感（Price）类群体的第一个变量（这里是年龄age）均值为60，炫耀性消费（Conspicuous）类群体的年龄均值为40依次类推。正态分布变量需要指定均值和方差，如年龄（age）， 实体店花销（store_exp）和在线花销（online_exp）。对于二项分布（只有两个可能取值）和泊松分布变量，我们只需要规定均值。其中，性别（gender），有房还是租房（house）是二项数据，生成这样的数据需要指定得到其中某一观测值的概率，比如矩阵mus中。实体店交易次数（store_trans）和线交易次数（online_trans）是泊松变量（频数），泊松分布只有一个参数——分布均值。所以在下面的标准差矩阵sds中，非正态分布变量对应的标准差为缺失值NA。（注意这里我们只是用这些分布为例生成数据，并不意味着这些是最好的拟合变量观测的分布。例如，真实的收入数据更可能是一个有偏的分布而非正态）。 下面我们对正态分布变量创建标准差矩阵： # 每类的标准差 (NA = 标准差无定义) sds&lt;- matrix( c( # 价格敏感（Price）类对应均值 3,NA,8000,NA,100,50,NA,NA, # 炫耀性消费（Conspicuous）类对应均值 5,NA,50000,NA,1000,1500,NA,NA, # 质量（Quality）类对应均值 7,NA,10000,NA,50,200,NA,NA, # 风格（Style）类对应均值 2,NA,5000,NA,10,500,NA,NA), ncol=length(vars), byrow=TRUE) 将这两个矩阵加在一起我们就能够完全定义各个类的分布了。例如，我们来看下每个矩阵的第1行，其代表第1类群体（价格敏感）的分布参数。这些值规定该类群体的年龄（age）均值为60（见第一个矩阵第1行第1列），标准差为3（第二个矩阵第1行第1列）。另外，其中大约有50%的男性（第一个矩阵第1行第2列），年收入（income）均值为120000元，标准差为8000元。将这些设置分开存在不同表格中，将来想要修改十分容易。将数据定义和抽样过程分开是个很好的习惯。下面开始抽取数据： # 抽取非抽样调查数据 sim.dat&lt;-NULL set.seed(2016) # 对消费者类别进行循环（i） for (i in seq_along(group_name)){ # 为了核实代码，展示循环运行过程，我们在循环中添加了这样一行代码 # 函数运行时会打印出正在抽取的样本类名 cat (i, group_name[i],&quot;\\n&quot;) # 创建一个空矩阵用于存放该类消费者相关数据 seg&lt;-data.frame(matrix(NA,nrow=group_size[i], ncol=length(vars))) # 在这个类之内，对不同变量迭代，抽取相应的随机数据 for (j in seq_along(vars)){ # 在每个变量上迭代 if (vartype[j]==&quot;norm&quot;){ # 抽取正态分布变量 seg[,j]&lt;-rnorm(group_size[i], mean=mus[i,j], sd=sds[i,j]) } else if (vartype[j]==&quot;pois&quot;) { # 抽取泊松分布变量 seg[,j]&lt;-rpois(group_size[i], lambda=mus[i,j]) } else if (vartype[j]==&quot;binom&quot;){ # 抽取二项分布变量 seg[,j]&lt;-rbinom(group_size[i],size=1,prob=mus[i,j]) } else{ # 如果变量类型不是上述几种，程序停止运行并提示信息 stop (&quot;Don&#39;t have type:&quot;,vartype[j]) } } # 将该消费者类的数据依行添加到总数据集 sim.dat&lt;-rbind(sim.dat,seg) } 上面的代码是随机抽样的主要过程，其中cat()函数使得循环运行时会打印出正在抽取的样本类名，最后得到的sim.dat是初始描述客户的变量部分的数据，在对数据进行润色前，提醒大家注意两个关于R的技巧： 第一、在i循环内，我们事先定义一个有着相应行数和列数的没有元素值的数据框seg，之后每迭代一次就将样本赋值到事先定义的seg的特定行。这么做的原因是由于只要R在某个对象上添加东西——如在数据框上增加一行——它都会将原对象拷贝一份。这将使用两倍的内存，减慢运行速度。通过这种方法可以避免对内存的浪费。对这里的小数据可能感觉不出差别，但对于大数据，运行速度会有极大不同。 第二、对循环指针范围的设定用的是seq_along()而非1:length()。这是为了够避免一些常见的错误，如指针向量长度为0或者不经意将向量方向弄反了。 之后我们对描述客户的这部分数据进行完善，添加合适的列标签，将二项（0/1）变量转化为贴有标签的因子变量。 # 指定数据框的列名为我们定义的变量名 names(sim.dat)&lt;-vars # 加上一个因子列表明每个观测的对应的消费者类别 sim.dat$segment&lt;-factor(rep(group_name,times=group_size)) # 将二项变量转化为贴有标签的因子变量 # Female: 女性 # Male: 男性 sim.dat$gender&lt;-factor(sim.dat$gender, labels=c(&quot;Female&quot;,&quot;Male&quot;)) sim.dat$house&lt;-factor(sim.dat$house, labels=c(&quot;No&quot;,&quot;Yes&quot;)) # 假设在线购买和在实体店购买的次数至少为1，所以这里在原随机值上加1 sim.dat$store_trans&lt;-sim.dat$store_trans+1 sim.dat$online_trans&lt;-sim.dat$online_trans+1 # 年龄为整数 sim.dat$age&lt;-floor(sim.dat$age) 真实市场营销数据往往没有这么干净，数据缺失，以及错误输入等问题常常发生。我们最后对模拟的数据做一点“破坏”，使其更像真实的数据。我们假设一些人不愿意给出关于收入（income）的信息。我们建立一个逻辑变量idxm，然后将逻辑变量idxm值为真的对应位置消费者收入观测设为缺失值NA（R用NA表示缺失值）。我们假设年龄（age）越大的消费者对应缺失值的概率越大： # 加入缺失值 idxm &lt;- as.logical(rbinom(ncust, size=1, prob=sim.dat$age/200)) sim.dat$income[idxm]&lt;-NA 真实的数据中可能有错误的输入，或者离群点： # 错误输入，离群点 set.seed(123) idx&lt;-sample(1:ncust,5) sim.dat$age[idx[1]]&lt;-300 sim.dat$store_exp[idx[2]]&lt;- -500 sim.dat$store_exp[idx[3:5]]&lt;-c(50000,30000,30000) 到目前为止我们已经建立了一部分数据，你可以通过summary(sim.dat)检查数据。下面我们接着抽取问卷调查回复数据。我们先通过rnorm()生成正态分布随机数。但从上面的问卷调查表格中可以看到，这是一个1-5分量级的问卷，1代表非常不同意，5代表非常同意。于是接下来我们通过floor()函数将连续值转化成离散整数。 # 抽取问卷调查回复 # 问卷问题数目 nq&lt;-10 # 各类消费者对问卷回复的正态分布均值矩阵 mus2 &lt;- matrix( c( # 价格敏感（Price）类对应均值 5,2,1,3,1,4,1,4,2,4, # 炫耀性消费（Conspicuous）类对应均值 1,4,5,4,4,4,4,1,4,2, # 质量（Quality）类对应均值 5,2,3,4,3,2,4,2,3,3, # 风格（Style）类对应均值 3,1,1,2,4,1,5,3,4,2), ncol=nq, byrow=TRUE) # 方差假设都是0.2 sd2&lt;-0.2 sim.dat2&lt;-NULL set.seed(1000) # 对消费者类别进行循环（i） for (i in seq_along(group_name)){ # 为了核实代码，展示循环运行过程，我们在循环中添加了这样一行代码 # 函数运行时会打印出正在抽取的样本类名，这里不再显示输出 # cat (i, group_name[i],&quot;\\n&quot;) # 创建一个空矩阵用于存放该类消费者相关数据 seg&lt;-data.frame(matrix(NA,nrow=group_size[i], ncol=nq)) # 在这个类之内，对不同变量迭代，抽取相应的随机数据 for (j in 1:nq){ # 抽取正态分布变量 res&lt;-rnorm(group_size[i], mean=mus2[i,j], sd=sd2) # 设置上下限度 res[res&gt;5]&lt;-5 res[res&lt;1]&lt;-1 # 通过 floor()函数将连续值转化成离散整数。 seg[,j]&lt;-floor(res) } # 将该消费者类的数据添加到总数据集 sim.dat2&lt;-rbind(sim.dat2,seg) } # 为数据框添加列标签 names(sim.dat2)&lt;-paste(&quot;Q&quot;,1:10,sep=&quot;&quot;) # 合并两部分数据 sim.dat&lt;-cbind(sim.dat,sim.dat2) # 加上一个因子列表明每个观测的对应的消费者类别 sim.dat$segment&lt;-factor(rep(group_name,times=group_size)) 至此为止我们得到了需要的数据集。让我们检查一下抽取的数据集： str(sim.dat,vec.len=3) ## &#39;data.frame&#39;: 1000 obs. of 19 variables: ## $ age : int 57 63 59 60 51 59 57 57 ... ## $ gender : Factor w/ 2 levels &quot;Female&quot;,&quot;Male&quot;: 1 1 2 2 2 2 2 2 ... ## $ income : num 120963 122008 114202 113616 ... ## $ house : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 2 2 2 2 2 2 2 2 ... ## $ store_exp : num 529 478 491 348 ... ## $ online_exp : num 304 110 279 142 ... ## $ store_trans : int 2 4 7 10 4 4 5 11 ... ## $ online_trans: int 2 2 2 2 4 5 3 5 ... ## $ Q1 : int 4 4 5 5 4 4 4 5 ... ## $ Q2 : int 2 1 2 2 1 2 1 2 ... ## $ Q3 : int 1 1 1 1 1 1 1 1 ... ## $ Q4 : int 2 2 2 3 3 2 2 3 ... ## $ Q5 : int 1 1 1 1 1 1 1 1 ... ## $ Q6 : int 4 4 4 4 4 4 4 4 ... ## $ Q7 : int 1 1 1 1 1 1 1 1 ... ## $ Q8 : int 4 4 4 4 4 4 4 4 ... ## $ Q9 : int 2 1 1 2 2 1 1 2 ... ## $ Q10 : int 4 4 4 4 4 4 4 4 ... ## $ segment : Factor w/ 4 levels &quot;Conspicuous&quot;,..: 2 2 2 2 2 2 2 2 ... 可以看到，服装消费者数据有1000个观测，19个变量。前8个变量是关于样本的人口统计学和购买行为描述。Q1-Q10是关于消费者选择偏好的问卷调查回复，问卷分值量表为1-5分（最常见的市场调查设计）。最后一列是消费者类别，样本观测的模拟是根据消费者类别进行的，因此这些可以当作“真实”的消费者类别。使用随机模拟的一个重要优点就是能够通过这种方式验证模型的效果。而实际生活中的数据样本真正所属类别通常是未知的。我们在之后对聚类和判别分析进行介绍的时候会使用样本类别信息。下面我们会反复用该数据集为例。 3.2 航空公司满意度调查 这一小节我们模拟一个航空公司满意度调查数据。数据中包含N=1000个受访者，每个受访者基于最近一次航班体验对3个航空公司进行评分，问卷调查一共15项，每项评分从1-9，分值越大满意度越高。这15个调查项分为4类（括号中为相应数据集中的变量名）： 购票体验 购票容易度（Easy_Reservation） 座椅选择（Preferred_Seats） 航班选择（Flight_Options） 票价（Ticket_Prices） 机舱设施 座椅舒适度（Seat_Comfort） 位置前后空间（Seat_Roominess） 随机行李存放（Overhead_Storage） 机舱清洁（Clean_Aircraft） 空航服务 礼貌（Courtesy） 友善（Friendliness） 能够提供需要的帮助（Helpfulness） 食物饮料服务（Service） 总体指数 总体满意度（Satisfaction） 再次选择次航空公司（Fly_Again） 向朋友推荐此航空公司（Recommend） # 先建立因子载荷矩阵 # 其中前12项符合双因子结构，因为每项对应一个总体因子载荷和某特定因子的载荷 # 比如购票容易度对应总体因子载荷0.33，对因特定购票因子载荷0.58 # 我们可以将结果评分看成是总体因子和特定因子共同作用的结果 loadings &lt;- matrix(c ( # 购票体验 .33, .58, .00, .00, # 购票容易度 .35, .55, .00, .00, # 座椅选择 .30, .52, .00, .00, # 航班选择 .40, .50, .00, .00, # 票价 # 机舱设施 .50, .00, .55, .00, # 座椅舒适度 .41, .00, .51, .00, # 位置前后空间 .45, .00, .57, .00, # 随机行李存放 .32, .00, .54, .00, # 机舱清洁 # 空航服务 .35, .00, .00, .50, # 礼貌 .38, .00, .00, .57, # 友善 .60, .00, .00, .50, # 能够提供需要的帮助 .52, .00, .00, .58, # 食物饮料服务 # 总体指数 .43, .10, .30, .30, # 总体满意度 .35, .50, .40, .20, # 再次选择次航空公司 .25, .50, .50, .20), # 向朋友推荐此航空公司 nrow=15,ncol=4, byrow=TRUE) # 将载荷矩阵乘以它的转秩，然后将对角线元素设置为1得到相关矩阵 cor_matrix&lt;-loadings %*% t(loadings) # Diagonal set to ones. diag(cor_matrix)&lt;-1 # 我们通过mvtnorm包模拟有特定相关矩阵的数据集 library(mvtnorm) # 设置3个航空公司对应的评分均值向量 mu1=c(5,6,5,6, 7,8,6,7, 5,5,5,5, 6,6,6) mu2=c(3,3,2,3, 5,4,5,6, 8,8,8,8, 3,3,3) mu3=c(2,2,2,2, 8,8,8,8, 8,8,8,8, 8,8,8) #设置随机种子 set.seed(123456) # 受访者ID resp.id &lt;- 1:1000 library(MASS) rating1 &lt;- mvrnorm(length(resp.id), mu=mu1, Sigma=cor_matrix) rating2 &lt;- mvrnorm(length(resp.id), mu=mu2, Sigma=cor_matrix) rating3 &lt;- mvrnorm(length(resp.id), mu=mu3, Sigma=cor_matrix) # 将分值限定在1到9之间 rating1[rating1&gt;9]&lt;-9 rating1[rating1&lt;1]&lt;-1 rating2[rating2&gt;9]&lt;-9 rating2[rating2&lt;1]&lt;-1 rating3[rating3&gt;9]&lt;-9 rating3[rating3&lt;1]&lt;-1 # 将分值转化为整数 rating1&lt;-data.frame(round(rating1,0)) rating2&lt;-data.frame(round(rating2,0)) rating3&lt;-data.frame(round(rating3,0)) rating1$ID&lt;-resp.id rating2$ID&lt;-resp.id rating3$ID&lt;-resp.id rating1$Airline&lt;-rep(&quot;AirlineCo.1&quot;,length(resp.id)) rating2$Airline&lt;-rep(&quot;AirlineCo.2&quot;,length(resp.id)) rating3$Airline&lt;-rep(&quot;AirlineCo.3&quot;,length(resp.id)) rating&lt;-rbind(rating1,rating2,rating3) # 为数据集的各列命名 names(rating)&lt;-c( &quot;Easy_Reservation&quot;, &quot;Preferred_Seats&quot;, &quot;Flight_Options&quot;, &quot;Ticket_Prices&quot;, &quot;Seat_Comfort&quot;, &quot;Seat_Roominess&quot;, &quot;Overhead_Storage&quot;, &quot;Clean_Aircraft&quot;, &quot;Courtesy&quot;, &quot;Friendliness&quot;, &quot;Helpfulness&quot;, &quot;Service&quot;, &quot;Satisfaction&quot;, &quot;Fly_Again&quot;, &quot;Recommend&quot;, &quot;ID&quot;, &quot;Airline&quot;) 让我们检查一下抽取的数据集： str(rating,vec.len=3) ## &#39;data.frame&#39;: 3000 obs. of 17 variables: ## $ Easy_Reservation: int 6 5 6 5 4 5 6 4 ... ## $ Preferred_Seats : int 5 7 6 6 5 6 6 6 ... ## $ Flight_Options : int 4 7 5 5 3 4 6 3 ... ## $ Ticket_Prices : int 5 6 6 5 6 5 5 5 ... ## $ Seat_Comfort : int 5 6 7 7 6 6 6 4 ... ## $ Seat_Roominess : int 7 8 6 8 7 8 6 5 ... ## $ Overhead_Storage: int 5 5 7 6 5 4 4 4 ... ## $ Clean_Aircraft : int 7 6 7 7 7 7 6 4 ... ## $ Courtesy : int 5 6 6 4 2 5 5 4 ... ## $ Friendliness : int 4 6 6 6 3 4 5 5 ... ## $ Helpfulness : int 6 5 6 4 4 5 5 4 ... ## $ Service : int 6 5 6 5 3 5 5 5 ... ## $ Satisfaction : int 6 7 7 5 4 6 5 5 ... ## $ Fly_Again : int 6 6 6 7 4 5 3 4 ... ## $ Recommend : int 3 6 5 5 4 5 6 5 ... ## $ ID : int 1 2 3 4 5 6 7 8 ... ## $ Airline : Factor w/ 3 levels &quot;AirlineCo.1&quot;,..: 1 1 1 1 1 1 1 1 ... 3.3 生猪疫情风险预测数据 本小节中我们将模拟一个生猪疫情数据。假设研究人员对800个养猪场进行和某生猪疫情有关的问卷调查，问卷由120个问题组成。每个问题有3个可能选项。目的是根据问卷调查回复得到每个养猪场在未来爆发疫情的概率。每个养猪场在问卷问题的3个可选项中等概率选择。第\\(i\\)个养猪场对应的疫情爆发概率服从\\(Bernoulli(1,p_{i})\\)分布。其中 \\[ln(\\frac{p_{i}}{1-p_{i}})=\\beta_{0}+\\sum_{g=1}^{G}\\mathbf{x_{i,g}^{T}}\\beta_{g}\\] \\(\\beta_{0}\\)是截距项，\\(\\mathbf{x_{i,g}}\\)是第\\(i\\)观测对应第\\(g\\)个问题的回复。这里将问题回复转化为0/1虚拟变量，因为每个问题有3个可能选项，所以\\(\\mathbf{x_{i,g}}\\)是一个取值为0/1的含有三个元素的向量。\\(\\mathbf{\\beta_{g}}\\)是对应的参数。 我们在这里考虑3类问题。第1类（问题1到问题40）问题中有两个选项对应变量有预测能力。第2类（问题41到问题80）问题中只有一个选项对结果有预测能力。第3类（问题81到问题120）对结果预测没有帮助，也就是我们希望能够去除的变量。模拟数据的参数设置如下： \\[\\mathbf{\\beta^{T}}=\\left(\\underset{question\\ 1}{\\frac{40}{3},\\underbrace{1,0,-1}},...,\\underset{question\\ 40}{\\underbrace{1,0,-1}},\\underset{question\\ 41}{\\underbrace{1,0,0}},...,\\underset{question\\ 80}{\\underbrace{1,0,0}},\\underset{question\\ 81}{\\underbrace{0,0,0}},...,\\underset{question\\ 120}{\\underbrace{0,0,0}}\\right)*\\gamma\\] 这里我们通过设置5个\\(\\gamma\\)值（\\(\\gamma \\in \\{0.1,0.25,0.5,1,2\\}\\) ）模拟了5种参数情况下的数据。 \\(\\gamma\\)越大，参数值越大，也就意味着有效问题对结果的预测性越强。对于每个参数设定模拟了20个数据集，之后我们会以这些数据为例展示不同模型变量选择的效果。模拟多个数据集是为了研究一些估值的稳定性。 # sim1_da1.csv 模拟的第一个数据集 # similar sim1_da2 and sim1_da3 # sim1.csv simulated data, the first simulation # dummy.sim1.csv dummy variables for the first simulated data with all the baseline in #code for simulation # setwd(dirname(file.choose())) # library(grplasso) nf&lt;-800 for (j in 1:20){ set.seed(19870+j) x&lt;-c(&#39;A&#39;,&#39;B&#39;,&#39;C&#39;) sim.da1&lt;-NULL for (i in 1:nf){ # sample(x, 120, replace=TRUE)-&gt;sam sim.da1&lt;-rbind(sim.da1,sample(x, 120, replace=TRUE)) } data.frame(sim.da1)-&gt;sim.da1 paste(&quot;Q&quot;, 1:120, sep = &quot;&quot;)-&gt;col paste(&quot;Farm&quot;, 1:nf, sep = &quot;&quot;)-&gt;row colnames(sim.da1)&lt;-col rownames(sim.da1)&lt;-row # 用nnet包中的class.ind()函数将问题回复编码为名义变量 library(nnet) dummy.sim1&lt;-NULL for (k in 1:ncol(sim.da1)) { tmp=class.ind(sim.da1[,k]) colnames(tmp)=paste(col[k],colnames(tmp)) dummy.sim1=cbind(dummy.sim1,tmp) } data.frame(dummy.sim1)-&gt;dummy.sim1 # 每个问题对应的3个名义变量中有重复信息 # 将C选项设置为基线回复 # 删除基线名义变量 base.idx&lt;-3*c(1:120) dummy1&lt;-dummy.sim1[,-base.idx] # 对每个r设置依次抽取相应的因变量 # 每次只对一个r值抽取，将其余代码注释掉 # 得到r=0.1 时每个农场对应的连接函数值 c(rep(c(1/10,0,-1/10),40),rep(c(1/10,0,0),40),rep(c(0,0,0),40))-&gt;s1 as.matrix(dummy.sim1)%*%s1-40/3/10-&gt;link1 # r=0.25 # c(rep(c(1/4,0,-1/4),40),rep(c(1/4,0,0),40),rep(c(0,0,0),40))-&gt;s1 # as.matrix(dummy.sim1)%*%s1-40/3/4-&gt;link1 # r=0.5 # c(rep(c(1/2,0,-1/2),40),rep(c(1/2,0,0),40),rep(c(0,0,0),40))-&gt;s1 # as.matrix(dummy.sim1)%*%s1-40/3/2-&gt;link1 # r=1 # c(rep(c(1,0,-1),40),rep(c(1,0,0),40),rep(c(0,0,0),40))-&gt;s1 # as.matrix(dummy.sim1)%*%s1-40/3-&gt;link1 # r=2 # c(rep(c(2,0,-2),40),rep(c(2,0,0),40),rep(c(0,0,0),40))-&gt;s1 # as.matrix(dummy.sim1)%*%s1-40/3/0.5-&gt;link1 # 在连接函数的基础上计算每个农场对应的爆发概率 exp(link1)/(exp(link1)+1)-&gt;hp1 # 基于爆发概率hp1，抽取相应的因变量res res&lt;-rep(9,nf) for (i in 1:nf){ sample( c(1,0),1,prob=c(hp1[i],1-hp1[i]))-&gt;res[i] } # 这里将数据存成3个不同的版本，只是为了之后不同模型使用方便 # 3个数据集都含有所有120个问题的回复，但彼此稍微有不同 # da1 含有因变量，但没有名义变量所属问题的信息 # da2 没有因变量，但最后一行包括的名义变量所属的问题 # da3 没有因变量，没有名义变量所属问题的信息 dummy1$y&lt;-res da1&lt;-dummy1 y&lt;-da1$y ind&lt;-NULL for (i in 1:120){ c(ind,rep(i,2))-&gt;ind } da2&lt;-rbind(da1[,1:240],ind) da3&lt;-da1[,1:240] # 将数据集储存起来 write.csv(da1,paste(&#39;sim&#39;,j,&#39;_da&#39;,1,&#39;.csv&#39;,sep=&#39;&#39;),row.names=F) write.csv(da2,paste(&#39;sim&#39;,j,&#39;_da&#39;,2,&#39;.csv&#39;,sep=&#39;&#39;),row.names=F) write.csv(da3,paste(&#39;sim&#39;,j,&#39;_da&#39;,3,&#39;.csv&#39;,sep=&#39;&#39;),row.names=F) write.csv(sim.da1,paste(&#39;sim&#39;,j,&#39;.csv&#39;,sep=&#39;&#39;),row.names=F) write.csv(dummy.sim1,paste(&#39;dummy.sim&#39;,j,&#39;.csv&#39;,sep=&#39;&#39;),row.names=F) } 要理解这里数据模拟的代码，读者需要了解逻辑回归和分组lasso的理论知识，这超出了本书的范围。这里的代码仅供大家参考。可以重复上面的代码生成相应的数据集。因为这里生成的数据量较大，在网上只有\\(\\gamma=2\\)对应的一次模拟的数据集。我们看下得到的数据集： library(dplyr) disease_dat&lt;-readr::read_csv(&quot;/Users/happyrabbit/Documents/GitHub/DataScientistR/Data/sim1_da1.csv&quot;) # 这里只截取最后的7列 head(subset(disease_dat,select=c( &quot;Q118.A&quot;,&quot;Q118.B&quot;,&quot;Q119.A&quot;,&quot;Q119.B&quot;,&quot;Q120.A&quot;,&quot;Q120.B&quot;,&quot;y&quot;))) 其中最后一列y代表相应农场疫情爆发情况，y=1代表从问卷调查之后5年内有疫情爆发。剩余的列表示农场问卷调查结果，如Q120.A=1对应问卷调查中第120个问题选择A的农场，类似的Q120.B=1对应第120个问题中选择B的农场，我们将选项C作为基准选项。之后我会用这个数据集展示一些相关的模型。 "],
["data-wrangling.html", "Chapter 4 Data Wrangling 4.1 Tidy data 4.2 Reshape data 4.3 Subset data 4.4 Summarize data 4.5 Combine data", " Chapter 4 Data Wrangling 4.1 Tidy data 4.2 Reshape data 4.3 Subset data 4.4 Summarize data 4.5 Combine data "],
["data-pre-processing.html", "Chapter 5 Data Pre-processing 5.1 Start 5.2 Centering and Scaling 5.3 Resolve Skewness 5.4 Resolve Outliers 5.5 Missing Values 5.6 Collinearity 5.7 Sparse Variables 5.8 Re-encode Dummy Variables", " Chapter 5 Data Pre-processing 5.1 Start There are a number of reasons a predictive model falls (Max Kuhn 2013), such as: Inadequate data pre-processing Inadequate model validation Unjustified extrapolation Over-fitting In this blog post, I am going to summarize some common data pre-processing approaches. 5.2 Centering and Scaling It is the most straightforward data transformation. It centers and scales a variable to mean 0 and standard deviation 1. It ensures that the criterion for finding linear combinations of the predictors is based on how much variation they explain and therefore improves the numerical stability. Models involving finding linear combinations of the predictors to explain response/predictors variation need data centering and scaling, such as PCA and PLS. You can easily writing code yourself to conduct this transformation. Or the function preProcess() in package caret can apply this transformation to a set of predictors. #install packages needed library(caret) library(e1071) library(gridExtra) library(lattice) library(imputeMissings) library(RANN) library(corrplot) library(nnet) head(cars) ## speed dist ## 1 4 2 ## 2 4 10 ## 3 7 4 ## 4 7 22 ## 5 8 16 ## 6 9 10 trans&lt;-preProcess(cars,method=c(&quot;center&quot;,&quot;scale&quot;)) transformed&lt;-predict(trans,cars) par(mfrow=c(1,2)) hist(cars$dist,main=&quot;Original&quot;,xlab=&quot;dist&quot;) hist(transformed$dist,main=&quot;Centered and Scaled&quot;,xlab=&quot;dist&quot;) Sometimes you only need to scale the variable. For example, if the model adds penalty to the parameter estimates (such as \\(L_2\\) penalty is ridge regression and \\(L_1\\) penalty in LASSO), the variables need to have similar scale to ensure a fair variable selection. I am heavy user of this kind of penalty-based model in my work and I used the following quantile transformation: \\[ x_{ij}^{*}=\\frac{x_{ij}-quantile(x_{.j},0.01)}{quantile(x_{.j}-0.99)-quantile(x_{-j},0.01)} \\] The reason to use 99% and 1% quantile instead of maximum and minimum values is to resist the impact of outliers. It is easy to write a function to do it: qscale&lt;-function(dat){ for (i in 1:ncol(dat)){ up&lt;-quantile(dat[,i],0.99) low&lt;-quantile(dat[,i],0.01) diff&lt;-up-low dat[,i]&lt;-(dat[,i]-low)/diff } return(dat) } In order to illustrate, let’s simulate a data set with two variables: income and age. set.seed(2015) income&lt;-sample(seq(50000,150000,by=500),95) age&lt;-income/2000-10 noise&lt;-round(runif(95)*10,0) age&lt;-age+noise income&lt;-c(income,10000,15000,300000,250000,230000) age&lt;-c(age,30,20,25,35,95) demo&lt;-data.frame(income,age) demo$education&lt;-as.factor(sample(c(&quot;High School&quot;,&quot;Bachelor&quot;,&quot;Master&quot;,&quot;Doctor&quot;),100,replace = T,prob =c(0.7,0.15,0.12,0.03) )) summary(demo[,c(&quot;income&quot;,&quot;age&quot;)]) ## income age ## Min. : 10000 Min. :20.00 ## 1st Qu.: 76375 1st Qu.:30.25 ## Median : 98750 Median :44.25 ## Mean :103480 Mean :44.92 ## 3rd Qu.:126375 3rd Qu.:56.88 ## Max. :300000 Max. :95.00 It is clear that income and age are not on the same scale. Now apply the function qscale() on the simulated data demo. transformed&lt;-qscale(demo[,c(&quot;income&quot;,&quot;age&quot;)]) summary(transformed) ## income age ## Min. :-0.02101 Min. :-0.01904 ## 1st Qu.: 0.26077 1st Qu.: 0.17814 ## Median : 0.35576 Median : 0.44746 ## Mean : 0.37584 Mean : 0.46044 ## 3rd Qu.: 0.47304 3rd Qu.: 0.69033 ## Max. : 1.21015 Max. : 1.42375 5.3 Resolve Skewness Skewness is defined to be the third standardized central moment. The formula for the sample skewness statistics is: \\[ skewness=\\frac{\\sum(x_{i}+\\bar{x})^{3}}{(n-1)v^{3/2}}\\] \\[v=\\frac{\\sum(x_{i}=\\bar{x})^{2}}{(n-1)}\\] Skewness=0 means that the destribution is symmetric, i.e. the probability of falling on either side of the distribution’s mean is equal. You can easily tell if a distribution is skewed by simple visualization. There are different ways may help to remove skewness such as log, square root or inverse. However it is often difficult to determine from plots which transformation is most appropriate for correcting skewness. The Box-Cox procedure automatically identified a transformation from the family of power transformations that are indexed by a parameter \\(\\lambda\\). \\[ x^{*}=\\begin{cases} \\begin{array}{c} \\frac{x^{\\lambda}-1}{\\lambda}\\\\ log(x) \\end{array} &amp; \\begin{array}{c} if\\ \\lambda\\neq0\\\\ if\\ \\lambda=0 \\end{array}\\end{cases} \\] It is easy to see that this family includes log transformation (\\(\\lambda=0\\)), square transformation (\\(\\lambda=2\\)), square root (\\(\\lambda=0.5\\)), inverse (\\(\\lambda=-1\\)) and others in-between. We can still use function preProcess() in package caret to apply this transformation by chaning the method argument. (trans&lt;-preProcess(cars,method=c(&quot;BoxCox&quot;))) ## Created from 50 samples and 2 variables ## ## Pre-processing: ## - Box-Cox transformation (2) ## - ignored (0) ## ## Lambda estimates for Box-Cox transformation: ## 1, 0.5 The output shows the sample size (50), number of variables (2) and the \\(\\lambda\\) estimates for each variable. After calling the preProcess() function, the predict() method applies the results to a data frame. transformed&lt;-predict(trans,cars) par(mfrow=c(1,2)) hist(cars$dist,main=&quot;Original&quot;,xlab=&quot;dist&quot;) hist(transformed$dist,main=&quot;After BoxCox Transformation&quot;,xlab=&quot;dist&quot;) An alternative is to use function BoxCoxTrans() in package caret. (trans&lt;-BoxCoxTrans(cars$dist)) ## Box-Cox Transformation ## ## 50 data points used to estimate Lambda ## ## Input data summary: ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 2.00 26.00 36.00 42.98 56.00 120.00 ## ## Largest/Smallest: 60 ## Sample Skewness: 0.759 ## ## Estimated Lambda: 0.5 transformed&lt;-predict(trans,cars$dist) skewness(transformed) ## [1] -0.01902765 The estimated \\(\\lambda\\) is the same 0.5. Original skewness is 0.759 and after transformation, the skewness is -0.01902765 which is close to 0. You can use function skewness() in package e1071 to get the skewness statistics. 5.4 Resolve Outliers Even under certain assumptions we can statistically define outliers, it can be hard to define in some situations. You can refer to “Detection of Outliers” for more information. Some models are resistant to outliers (such as tree-based model and support vector machine). If a model is sensitive to outliers (such as linear regression and logistic regression), we can use spatial sign transformation to minimize the problem. It projects the original sample points to the surface of a sphere by: \\[x_{ij}^{*}=\\frac{x_{ij}}{\\sqrt{\\sum_{j=1}^{p}x_{ij}^{2}}}\\] As noted in the book “Applied Predictive Modeling”, Since the denominator is intended to measure the squared distance to the center of the predictor’s distribution, it is important to center and scale the predictor data prior to using this transformation. Note that, unlike centering or scaling, this manipulation of the predictors transforms them as a group. We can use spatialSign() function in caret to conduct spatial sign on demo: trans&lt;-preProcess(demo[,c(&quot;income&quot;,&quot;age&quot;)],method=c(&quot;center&quot;,&quot;scale&quot;)) transformed&lt;-predict(trans,demo[,c(&quot;income&quot;,&quot;age&quot;)]) transformed2 &lt;- spatialSign(transformed) transformed2 &lt;- as.data.frame(transformed2) p1&lt;-xyplot(income ~ age, data = transformed, main=&quot;Original&quot;) p2&lt;-xyplot(income ~ age, data = transformed2, main=&quot;After Spatial Sign&quot;) grid.arrange(p1,p2, ncol=2) 5.5 Missing Values We need a book to fully explicate this topic. Before we decide how to handle missing value, it is important to understand why the values are missing. Do the missing values have information related outcomes? Or are they missing at random? It is not the goal here to illustrate which methods to use in different missing situation. You can refer to Section 3.4 of “Applied Predictive Modeling” for more discussion on that. The objective of this post is to introduce some imputation methods and corresponding application examples using R. Survey statistics has studied the imputation extensively which focuses on making valid inferences. Missing value imputation in predictive modeling is a different problem. Saar-Tsechansky and Provost compared several different methods for applying classification to instance with missing values. “Handling Missing Values when Applying Classification Models” The following code randomly assigns some missing values to the previous data demo and names the new data set demo_missing. set.seed(100) id1&lt;-sample(1:nrow(demo),15) id2&lt;-sample(1:nrow(demo),10) id3&lt;-sample(1:nrow(demo),10) demo_missing&lt;-demo demo_missing$age[id1]&lt;-NA demo_missing$income[id2]&lt;-NA demo_missing$education[id3]&lt;-NA summary(demo_missing) ## income age education ## Min. : 15000 Min. :20.00 Bachelor :13 ## 1st Qu.: 77125 1st Qu.:30.25 Doctor : 2 ## Median : 98750 Median :44.25 High School:70 ## Mean :102811 Mean :44.43 Master : 5 ## 3rd Qu.:125250 3rd Qu.:56.25 NA&#39;s :10 ## Max. :300000 Max. :95.00 ## NA&#39;s :10 NA&#39;s :15 5.5.1 Impute missing values with median/mode You can use function impute() under package imputeMissings to impute missing values with mdedian/mode. This method is simple, fast but treats each predictor independently, and may not be accurate. demo_imp&lt;-impute(demo_missing,method=&quot;median/mode&quot;) summary(demo_imp) ## income age education ## Min. : 15000 Min. :20.00 Bachelor :13 ## 1st Qu.: 79250 1st Qu.:32.19 Doctor : 2 ## Median : 98750 Median :44.25 High School:80 ## Mean :102405 Mean :44.40 Master : 5 ## 3rd Qu.:122875 3rd Qu.:54.50 ## Max. :300000 Max. :95.00 Note that the median/mode method imputes mode to character vectors and median to numeric and integer vectors.So you can see the 10 missing values for variable “education” are imputed with “High School” since it is the mode. You can also use function ‘preProcess()’ to attain this.But it only works for numeric variable. imp&lt;-preProcess(demo_missing[,c(&quot;income&quot;,&quot;age&quot;)],method=&quot;medianImpute&quot;) demo_imp&lt;-predict(imp,demo_missing[,c(&quot;income&quot;,&quot;age&quot;)]) summary(demo_imp) ## income age ## Min. : 15000 Min. :20.00 ## 1st Qu.: 79250 1st Qu.:32.19 ## Median : 98750 Median :44.25 ## Mean :102405 Mean :44.40 ## 3rd Qu.:122875 3rd Qu.:54.50 ## Max. :300000 Max. :95.00 5.5.2 Impute missing values based on K-nearest neighbors k-nearest neighbor will find the k closest samples (Euclidian distance) in the training set and impute the mean of those “neighbors”. imp&lt;-preProcess(demo_missing[,c(&quot;income&quot;,&quot;age&quot;)],method=&quot;knnImpute&quot;,k=2) demo_imp&lt;-predict(imp,demo_missing[,c(&quot;income&quot;,&quot;age&quot;)]) ## Error in FUN(newX[, i], ...): cannot impute when all predictors are missing in the new data point Now we get a error saying “cannot impute when all predictors are missing in the new data point”. It is because there is at least one sample with both “income” and “age” missing. We can delete the corresponding row and do it again. idx&lt;-which(is.na(demo_missing$income)&amp;is.na(demo_missing$age)) imp&lt;-preProcess(demo_missing[-idx,c(&quot;income&quot;,&quot;age&quot;)],method=&quot;knnImpute&quot;,k=2) demo_imp&lt;-predict(imp,demo_missing[-idx,c(&quot;income&quot;,&quot;age&quot;)]) summary(demo_imp) ## income age ## Min. :-2.259679 Min. :-1.53784 ## 1st Qu.:-0.686725 1st Qu.:-0.88276 ## Median :-0.104506 Median :-0.01129 ## Mean :-0.006233 Mean : 0.01103 ## 3rd Qu.: 0.593512 3rd Qu.: 0.72444 ## Max. : 5.074342 Max. : 3.18343 The error doesn’t show up this time. This method considers all predictors together but it requires them to be in the same scale since the “euclidian distance” is used to find the neighbours. 5.6 Collinearity It is probably a technical term that many un-technical people also know. There is an excellent function in corrplot package with the same name corrplot() that can visualize correlation structure of a set of predictors. The function has option to reorder the variables in a way that reveals clusters of highly correlated ones. We add some columns to demo that are correlated. adddemo&lt;-demo[,-3] adddemo$added1&lt;-sqrt(demo$age)+10 adddemo$added2&lt;-log(demo$income)+demo$age adddemo$added2&lt;-log(demo$age) adddemo$added4&lt;-demo$income/1000+5*demo$age adddemo$added5&lt;-sin(demo$age) The following command will produce visualization for the correlation matrix of adddemo. corrplot(cor(adddemo),order=&quot;hclust&quot;) The size and color of the points are associated with the strength of corresponding correlation. Section 3.5 of “Applied Predictive Modeling” presents a heuristic algorithm to remove minium number of predicitors to ensure all pairwise corelations are below a certain threshold: Calculate the correlation matrix of the predictors. Determine the two predictors associated with the largest absolute pairwise correlation (call them predictors A and B). Determine the average correlation between A and the other variables. Do the same for predictor B. If A has a larger average correlation, remove it; otherwise, remove predictor B. Repeat Step 2-4 until no absolute correlations are above the threshold. The findCorrelation() function in package caret will apaply the above algorithm. (highCorr&lt;-findCorrelation(cor(adddemo),cutoff=.75)) ## [1] 5 2 3 # remove columns with high correlations filter_demo&lt;-adddemo[,-highCorr] # correlation matrix for filtered data corrplot(cor(filter_demo),order=&quot;hclust&quot;) 5.7 Sparse Variables Other than the highly related predictors, predictors with degenerate distributions need to be removed as well. Removing those variables can significant improve some models’ performance and/or stability (such as linear regression and logistic regression but tree based model is impervious to this type of predictors). One extreme example is a variable with single value which is called zero-variance variable. Similarly those variables with very low frequency of unique values are near-zero variance predictors. How to detect those variables? There are two rules: - The fraction of unique values over the sample size - The ratio of the frequency of the most prevalent value to the frequency of the second most prevalent value. The caret package funciton nearZeroVar() can filter near-zero variance predictors. #add two variables with low variance zero_demo&lt;-demo zero_demo$zero1&lt;-rep(0,nrow(demo)) zero_demo$zero2&lt;-c(1,rep(0,nrow(demo)-1)) # zero1 only has one unique value # zero2 is a vector with the first element 1 and the rest are 0s summary(zero_demo) ## income age education zero1 ## Min. : 10000 Min. :20.00 Bachelor :15 Min. :0 ## 1st Qu.: 76375 1st Qu.:30.25 Doctor : 2 1st Qu.:0 ## Median : 98750 Median :44.25 High School:77 Median :0 ## Mean :103480 Mean :44.92 Master : 6 Mean :0 ## 3rd Qu.:126375 3rd Qu.:56.88 3rd Qu.:0 ## Max. :300000 Max. :95.00 Max. :0 ## zero2 ## Min. :0.00 ## 1st Qu.:0.00 ## Median :0.00 ## Mean :0.01 ## 3rd Qu.:0.00 ## Max. :1.00 # the function will return a vector of integers indicating which columns to remove nearZeroVar(zero_demo,freqCut = 95/5, uniqueCut = 10) ## [1] 4 5 Note the two arguments in the function freqCut = and uniqueCut =. They are corresponding to the previous two rules. freqCut: the cutoff for the ratio of the most common value to the second most common value uniqueCut:the cutoff for the percentage of distinct values out of the number of total samples 5.8 Re-encode Dummy Variables Sometimes we need to recode categories to smaller bits of information named “dummy variables”. Take the variable “education” in demo for example. It has four categories: “High School”,“Bachelor”,“Master” and “Doctor”. If we recode it to be dummy variables, each category get its own dummy variable that is 0/1 indicator for that category. For a single categorical variable, we can use function class.ind() in package nnet: dumVar&lt;-class.ind(demo$education) head(dumVar) ## Bachelor Doctor High School Master ## [1,] 0 0 1 0 ## [2,] 0 0 1 0 ## [3,] 0 0 1 0 ## [4,] 0 0 1 0 ## [5,] 0 0 1 0 ## [6,] 0 0 1 0 If we want to determine encodeings for more than one variables, we can use dummyVars() in caret. dumMod&lt;-dummyVars(~income+education, data=demo, # Remove the variable name from the column name levelsOnly=T) predict(dumMod,head(demo)) ## income Bachelor Doctor High School Master ## 1 56000 0 0 1 0 ## 2 133500 0 0 1 0 ## 3 79500 0 0 1 0 ## 4 53000 0 0 1 0 ## 5 63500 0 0 1 0 ## 6 84500 0 0 1 0 To add some more complexity, we could assume joint effect of income and education. In this case, this will add 4 more columns to the resulted data frame: dumMod&lt;-dummyVars(~income+education+income:education, data=demo, levelsOnly=T) predict(dumMod,head(demo)) ## income Bachelor Doctor High School Master income:Bachelor income:Doctor ## 1 56000 0 0 1 0 0 0 ## 2 133500 0 0 1 0 0 0 ## 3 79500 0 0 1 0 0 0 ## 4 53000 0 0 1 0 0 0 ## 5 63500 0 0 1 0 0 0 ## 6 84500 0 0 1 0 0 0 ## income:High School income:Master ## 1 56000 0 ## 2 133500 0 ## 3 79500 0 ## 4 53000 0 ## 5 63500 0 ## 6 84500 0 References "],
["basic-model-techniques.html", "Chapter 6 Basic Model Techniques 6.1 Supervised v.s. unsupervised model 6.2 Model Error 6.3 数据划分和再抽样 6.4 本章总结", " Chapter 6 Basic Model Techniques 6.1 Supervised v.s. unsupervised model 建模技术可以粗略的分为有监督和无监督这两类。大部分统计学习方法都可以归于其中一种。广义上说有监督方法涉及根据一个或者多个输入变量（也称为自变量，解释变量，预测变量），估计或者预测一个结果变量（也称为因变量，响应变量）。而无监督方法只考虑自变量，没有应变量作为“监督”，我们通过这类方法探索观测数据中内在变量结构。我们在之前提到的方法中，袋状树，广义线性回归是有监督方法；主成分分析，探索性因子分析，对近0方差和高相关变量的筛选都是无监督方法。我们先介绍这里的数学公式表达。 我们用\\(n\\)表示样本量（或者观测数目）。\\(p\\)代表自变量数目。我们用\\(\\mathbf{X}\\)表示\\(n\\times p\\)观测矩阵： \\[ \\mathbf{X}=\\left[\\begin{array}{cccc} x_{11} &amp; x_{12} &amp; \\cdots &amp; x_{1p}\\\\ x_{21} &amp; x_{22} &amp; \\cdots &amp; x_{2p}\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ x_{n1} &amp; x_{n2} &amp; \\cdots &amp; x_{np} \\end{array}\\right] \\] 其中\\(x_{ij}\\)代表第i个样本第j个变量的观测，\\(i=1, \\ldots, n\\)，\\(j=1, \\ldots, p\\)。\\(\\mathbf{x_{i.}}\\)代表第i个样本的所有变量观测组成的向量，向量统一按列排： \\[ \\mathbf{x_{i.}}=\\left[\\begin{array}{c} x_{i1}\\\\ x_{i2}\\\\ \\vdots\\\\ x_{ip} \\end{array}\\right] \\] 类似的，\\(\\mathbf{x_{.j}}\\)代表第j个变量的所有样本观测组成的向量： \\[ \\mathbf{x_{.j}}=\\left[\\begin{array}{c} x_{1j}\\\\ x_{2j}\\\\ \\vdots\\\\ x_{nj} \\end{array}\\right] \\] 于是我们有： \\[ \\mathbf{X}=\\left[\\begin{array}{cccc} x_{11} &amp; x_{12} &amp; \\cdots &amp; x_{1p}\\\\ x_{21} &amp; x_{22} &amp; \\cdots &amp; x_{2p}\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ x_{n1} &amp; x_{n2} &amp; \\cdots &amp; x_{np} \\end{array}\\right]=\\left[\\begin{array}{c} \\mathbf{x_{1.}^{T}}\\\\ \\mathbf{x_{2.}^{T}}\\\\ \\vdots\\\\ \\mathbf{x_{n.}^{T}} \\end{array}\\right]=\\left[\\begin{array}{cccc} \\mathbf{x_{.1}} &amp; \\mathbf{x_{.2}} &amp; \\ldots &amp; \\mathbf{x_{.p}}\\end{array}\\right] \\] 其中\\(^{T}\\)代表矩阵转秩。我们用\\(y_{i}\\)代表第i个样本对应的响应变量。所有\\(n\\)个响应变量组成的向量为： \\[ \\mathbf{y}=\\left[\\begin{array}{c} y_{1}\\\\ y_{2}\\\\ \\vdots\\\\ y_{n} \\end{array}\\right] \\] 自变量和应变量的关系为： \\[\\mathbf{y}=f(\\mathbf{X})+\\mathbf{\\epsilon}\\] 有监督和无监督建模技术用上面的符号语言表达就是： 无监督建模：探索\\(\\mathbf{X}\\)中的自变量之间的关系 有监督建模：估计\\(\\mathbf{y}\\)和\\(\\mathbf{X}\\)之间的关系 \\(f(\\cdot)\\) 其中\\(\\mathbf{\\epsilon}\\) 是随机误差，均值为\\(\\mathbf{0}\\)。函数\\(f(\\cdot)\\)是我们的建模目标，代表X能够提供的关于Y的系统信息（和随机性相对应）。估计\\(f(\\cdot)\\)目的主要是推断或者预测，有时兼有两者。通常情况下，模型的灵活性和可解释性之间是一种此消彼长的关系——灵活性越高的模型可解释性越弱。因此数据科学家需要把握这两者间微妙的平衡。不同的建模目的对模型解释性的要求不同，因而极大影响了模型选择。如果预测是唯一目的，那么模型的解释性就不在考虑范围内，这种情况下可以使用一些复杂的灵活度高的“黑箱”模型，装袋，助推，非线性核函数支持向量机，神经网络和随机森林等。这些模型都非常灵活，但是很难解释自变量和应变量之间的关系。人们可能会觉得这些模型的预测精度通常更高，但就个人经验来说，那些灵活性不那么高的模型预测精度更高的情况时常发生。咋一看来好像不符合逻辑，但是认真想想也并不奇怪，这些模型之所以复杂，就在于它们极力拟合当前观测数据，因此它们更有可能过度拟合（把噪声也拟合进去了），这些模型在训练集上的表现可能更好，但预测未必更准确。 6.2 Model Error 6.2.1 Systematic Error and Random Error 假设我们对于\\(\\mathbf{X}\\)得到\\(f\\)的估计\\(\\hat{f}\\)，进而得到\\(\\mathbf{y}\\)的预测 \\(\\hat{\\mathbf{y}}=\\hat{f}(\\mathbf{X})\\)。预测的误差分成两部分，系统误差和随机误差： \\[ E(\\mathbf{y}-\\hat{\\mathbf{y}})^{2}=E[f(\\mathbf{X})+\\mathbf{\\epsilon}-\\hat{f}(\\mathbf{X})]^{2}=\\underset{\\text{(1)}}{\\underbrace{E[f(\\mathbf{X})-\\hat{f}(\\mathbf{X})]^{2}}}+\\underset{\\text{(2)}}{\\underbrace{Var(\\mathbf{\\epsilon})}} \\label{eq:error}\\] 其中（1）是系统误差， \\(\\hat{f}\\)通常不能彻底对\\(\\mathbf{X}\\)和\\(\\mathbf{y}\\)之间的“系统关系”建模，这里系统关系指的是在不同样本上存在的稳定关系。这一部分误差能通过改进模型得到提高；（2）是随机误差，这部分误差代表当前数据无法解释的部分，因此无法通过建立更复杂的模型来改进。那些拥有众多参数的复杂黑箱模型最大的问题就是试图通过自变量解释这部分误差，也就是过度拟合。随机误差的显著特点就是在不同的样本上是无法重复的，于是判断是否存在过度拟合的一个准则就是预留一部分样本作为测试集，然后检验训练出来的模型在测试集上的表现。这个我们随后会讲到。这里要澄清一点，过度拟合不只发生在这些黑箱模型上，其发生的根源在于参数个数太多（常超过观测个数），理论上说任何模型都可能过度拟合，只是因为黑箱模型的参数尤其多，其高灵活性和复杂度放大了过度拟合的问题。有些黑箱模型在训练的过程中会使用“袋外数据”（又称为Out of Bag [OOB]）来尽量避免过度拟合的影响。 如果建模的目的也包含推断，那么这些“黑箱”模型就不合适，这就需要在模型可以解释的范围内使用尽量灵活的模型，比如Lasso回归，多元自适应回归样条等。有人可能不同意Lasso回归是灵活的。从其本质还是传统回归的角度看，它确实没有那么灵活，受到很多模型假设的限制。但由于Lasso的罚函数能同时起到变量选择的作用，这个变量的选择的过程可以不依赖于p值之类的参数（这些参数基于数据分布假设因此具有局限性），而可以通过优化模型预测值和真实值的差距来进行变量选择，从这个角度上看，该模型是灵活的。根据笔者的应用经验，Lasso作为收缩（或变量选择）方法在实际应用中的效果非常好。对于一些市场营销或者社会心理学相关的抽样调查数据分析，分层贝叶斯可能是一种灵活有效的方法，但拟合所需的计算时间更长。 其中系统误差可以进一步分解： \\[ \\begin{array}{ccc} E[f(\\mathbf{X})-\\hat{f}(\\mathbf{X})]^{2} &amp; = &amp; E\\left(f(\\mathbf{X})-E[\\hat{f}(\\mathbf{X})]+E[\\hat{f}(\\mathbf{X})]-\\hat{f}(\\mathbf{X})\\right)^{2}\\\\ &amp; = &amp; E\\left(E[\\hat{f}(\\mathbf{X})]-f(\\mathbf{X})\\right)^{2}+E\\left(\\hat{f}(\\mathbf{X})-E[\\hat{f}(\\mathbf{X})]\\right)^{2}\\\\ &amp; = &amp; [Bias(\\hat{f}(\\mathbf{X}))]^{2}+Var(\\hat{f}(\\mathbf{X})) \\end{array} \\] 系统误差由两部分组成，估计的偏度\\(Bias(\\hat{f}(\\mathbf{X}))\\)和估计的方差\\(Var(\\hat{f}(\\mathbf{X}))\\)。上面公式告诉我们，如果要最小化系统误差，需要同时最小化估计偏度和估计方差。偏度代表用模型逼近现实情况导致的误差，这部分误差可能非常复杂。比如线性回归假设自变量和应变量之间是线性关系，但现实生活中完全的线性关系并不常见。下图中x和fx的关系就是非线性的。因此，观测样本量再大，也无法用线性回归给出准确的预测。换句话说，在这种情况下，线性回归模型的预测具有很高的偏度。 library(grid) library(lattice) library(ggplot2) # 可以从网站下载multiplot()函数代码 # 用该函数在同一张画布上放置多张ggplot图 # 需要用到grid包 source(&quot;/Users/happyrabbit/Documents/GitHub/DataScientistR/R_Code/multiplot.r&quot;) # 随机抽取一些非线性样本 x=seq(1,10,0.01)*pi e=rnorm(length(x),mean=0,sd=0.2) fx&lt;-sin(x)+e+sqrt(x) dat=data.frame(x,fx) # 绘制线性拟合图 ggplot(dat,aes(x,fx))+ geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) 由于我们通常使用训练集进行参数估计，如果训练集不同，得到的参数估计也会不同。直观的讲，估计方差表示如果我们用不同的数据集拟合相同的模型，得到估计值的变化，理想的情况是估计值的变化不会太大。对于高方差的模型，训练集的微小变化会导致很不相同的估计值。通常情况下，灵活度高的模型方差也更高，比如树模型，以及最初的助推法，后来在此基础上的随机森林模型和梯度助推法这样的集成方法的重要目标之一，就是通过汇总不同样本上得到的结果来降低估计方差。下图中的蓝色曲线是用平滑方法对上面的非线性观测进行拟合得到的，该曲线的灵活性很高，能够高度拟合当前数据： ggplot(dat,aes(x,fx))+geom_smooth(span = 0.03) ## `geom_smooth()` using method = &#39;loess&#39; 但是该方法有很高的方差，如果我们随机抽取不同的样本子集，得到的拟合曲线会有明显变化： # 设置随机种子 set.seed(2016) # 抽取其中部分样本拟合模型 # 样本1 idx1=sample(1:length(x),100) dat1=data.frame(x1=x[idx1],fx1=fx[idx1]) p1=ggplot(dat1,aes(x1,fx1))+geom_smooth(span = 0.03) # 样本2 idx2=sample(1:length(x),100) dat2=data.frame(x2=x[idx2],fx2=fx[idx2]) p2=ggplot(dat2,aes(x2,fx2))+geom_smooth(span = 0.03) # 样本3 idx3=sample(1:length(x),100) dat3=data.frame(x3=x[idx3],fx3=fx[idx3]) p3=ggplot(dat3,aes(x3,fx3))+geom_smooth(span = 0.03) # 样本4 idx4=sample(1:length(x),100) dat4=data.frame(x4=x[idx4],fx4=fx[idx4]) p4=ggplot(dat4,aes(x4,fx4))+geom_smooth(span = 0.03) multiplot(p1,p2,p3,p4,cols=2) ## `geom_smooth()` using method = &#39;loess&#39; ## `geom_smooth()` using method = &#39;loess&#39; ## `geom_smooth()` using method = &#39;loess&#39; ## `geom_smooth()` using method = &#39;loess&#39; 对相同的4个子集拟合线性模型，变化非常小： p1=ggplot(dat1,aes(x1,fx1))+ geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) p2=ggplot(dat2,aes(x2,fx2))+ geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) p3=ggplot(dat3,aes(x3,fx3))+ geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) p4=ggplot(dat4,aes(x4,fx4))+ geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) multiplot(p1,p2,p3,p4,cols=2) 总体说来，方差随着模型灵活度的增加而增加，偏差随着模型的灵活度增加而降低。在方差和偏差的变化共同决定了系统误差（或者均方误差，MSE）的变化。当我们提高模型的灵活性，偏差减小的速度开始时会超过方差增大的速度，MSE随之减小。但到了一定程度以后，提高模型的灵活性对偏差的影响不大但是方差大幅度增加，MSE随之增加。在之后的章节我们会看到，对于那些有一个控制模型灵活性的调优参数的模型，随着调优参数的减小（参数越大灵活度越低），模型误差先升高，后降低。 模型选择向来是非常困难的，这种困难不是数据分析行业特有的，很多专业领域都有类似的情况，比如医生判断病人所患的疾病，并在众多治疗方案中选择最合适的，这不是答案一目了然的选择题，决策的过程需要很多权衡和妥协。模型选择也类似，在选择过程中需要考虑具体的情况：项目目的，客户要求的精确度（这点很重要），计算量等等。这个选择的过程很难白纸黑字的像食谱一样写下来，这里我们只是尽己所能的介绍模型选择过程中需要考虑的点，以及评估不同模型的辅助性技术。具体的应用和“数据科学思维”还需要大家在从业过程中通过实践思考不断学习打磨。 6.2.2 应变量误差 若应变量包含可观的测量误差，那么这部分误差将反映在随机误差（\\(\\mathbf{\\epsilon}\\)）中。这部分误差使得均方根误差（RMSE）和\\(R^2\\)有相应的上下限。RMSE和\\(R^2\\)是回归模型常用的表现度量方法，我们在本章后面部分会进行介绍。因此，随机误差项不仅仅代表模型无法解释的波动，还含有测量误差。《应用预测建模（Applied Predictive Modeling）》(Max Kuhn 2013)的第20.2小节有一个例子展示了因变量的测量误差对模型表现（RMSE和\\(R^2\\)）的影响。作者在因变量上加入了不同强度的随机正态噪声，重复拟合不同的模型，研究模型均方根误差（RMSE）和\\(R^2\\)的变化。这里我们用服装消费者数据进行类似的展示。假设我们面对这样一个问题，实际中消费者的收入并不是那么容易收集，很多人不愿透露这样的私人信息。于是我们希望利用消费记录变量建立关于消费者收入的预测模型，模型可以对那些数据库中缺失收入信息的记录进行填补。我们建立下面模型： # 载入数据 sim.dat&lt;-read.csv(&quot;/Users/happyrabbit/Documents/GitHub/DataScientistR/Data/SegData.csv&quot;) ymad&lt;-mad(na.omit(sim.dat$income)) # 计算Z分值 zs&lt;-(sim.dat$income-mean(na.omit(sim.dat$income)))/ymad # which(na.omit(zs&gt;3.5)) 找到利群点 # which(is.na(zs)) 找到缺失值 idex&lt;-c(which(na.omit(zs&gt;3.5)),which(is.na(zs))) # 删除含有离群点和缺失值的行 sim.dat&lt;-sim.dat[-idex,] fit&lt;-lm(income~store_exp+online_exp+store_trans+online_trans,data=sim.dat) 由输出可见，在没有额外添加噪音时模型的均方根误差（RMSE）是 29567，\\(R^2\\)是 0.6。下面我们在应变量年收入（income）上添加不同程度的噪音（均方根误差的0到3倍）： \\[ RMSE \\times (0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0) \\] noise&lt;-matrix(rep(NA,7*nrow(sim.dat)),nrow=nrow(sim.dat),ncol=7) for (i in 1:nrow(sim.dat)){ noise[i,]&lt;-rnorm(7,rep(0,7),summary(fit)$sigma*seq(0,3,by=0.5)) } 我们接下来检查噪音强度对复杂度不同的模型拟合\\(R^2\\)的影响。 拟合的模型复杂度从低到高依次为：一般线性回归，偏最小二乘回归，多元自适应回归样条，支持向量机（核函数是径向基函数），随机森林。 # 拟合一般线性回归模型 rsq_linear&lt;-rep(0,ncol(noise)) for (i in 1:7){ withnoise&lt;-sim.dat$income+noise[,i] fit0&lt;-lm(withnoise~store_exp+online_exp+store_trans+online_trans,data=sim.dat) rsq_linear[i]&lt;-summary(fit0)$adj.r.squared } 下面我们接着拟合偏最小二乘回归（PLS）。偏最小二乘源自于Herman Wold的非线性迭代偏最小二乘（NIPALS）算法 (H 1966; H 1982)，是一种通过隐层级将非线性关系线性化的方法。该方法和主成分回归类似，不同在于主成分回归在选择成分的时候没有考虑因变量的信息，其目的是找到最大程度概括自变量空间变异性的线性组合（即，是无监督方法）。当自变量和因变量相关时，主成分回归能够很好的识别出它们之间的系统关系。然而，当存在和因变量不相关的自变量时，该方法的效果就会受到影响。而PLS最大程度概括与因变量相关性的线性组合。推荐大家用PLS解决那些自变量之间存在相关性，但不确定所有自变量都和因变量有关，同时希望用线性回归来解决的问题。在当前情况下，更加复杂的PLS表现效果并不比简单线性好，因为这里几个自变量都和因变量有关的不同信息（从前面的拟合结果看到所有变量都是显著的）。 # pls: 进行偏最小二乘回归和主成分回归 library(pls) rsq_pls&lt;-rep(0,ncol(noise)) # 拟合PLS模型 for (i in 1:7){ withnoise&lt;-sim.dat$income+noise[,i] fit0&lt;-plsr(withnoise~store_exp+online_exp+store_trans+online_trans,data=sim.dat) # plsr函数结果是mvr对象，需要用特定函数提取模型解释的应变量方差 rsq_pls[i]&lt;-max(drop(R2(fit0, estimate = &quot;train&quot;,intercept = FALSE)$val)) } # earth: 拟合多元自适应回归样条 library(earth) rsq_mars&lt;-rep(0,ncol(noise)) # 拟合多元自适应回归样条 for (i in 1:7){ withnoise&lt;-sim.dat$income+noise[,i] fit0&lt;-earth(withnoise~store_exp+online_exp+store_trans+online_trans,data=sim.dat) # 提取模型解释的应变量方差 rsq_mars[i]&lt;-fit0$rsq } # caret: 用于建立预测模型的包，可以拟合多种模型 library(caret) rsq_svm&lt;-rep(0,ncol(noise)) # 拟合支持向量机 # 注意：运行需要一些时间 for (i in 1:7){ idex&lt;-which(is.na(sim.dat$income)) withnoise&lt;-sim.dat$income+noise[,i] trainX&lt;-sim.dat[,c(&quot;store_exp&quot;,&quot;online_exp&quot;,&quot;store_trans&quot;,&quot;online_trans&quot;)] trainY&lt;-withnoise fit0&lt;-train(trainX,trainY,method=&quot;svmRadial&quot;, tuneLength=15, trControl=trainControl(method=&quot;cv&quot;)) # 提取模型解释的应变量方差 rsq_svm[i]&lt;-max(fit0$results$Rsquared) } # randomForest: 拟合随机森林模型 library(randomForest) rsq_rf&lt;-rep(0,ncol(noise)) # 拟合随机森林模型 # ntree=500 用500棵树 # na.action = na.omit 忽略缺失值 for (i in 1:7){ withnoise&lt;-sim.dat$income+noise[,i] fit0&lt;-randomForest(withnoise~store_exp+online_exp+store_trans+online_trans,data=sim.dat,ntree=500,na.action = na.omit) # 提取模型解释的应变量方差 rsq_rf[i]&lt;-tail(fit0$rsq,1) } # reshape2在之前介绍过，用于数据整形 library(reshape2) rsq&lt;-data.frame(cbind(Noise=c(0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0),rsq_linear,rsq_pls,rsq_mars,rsq_svm,rsq_rf)) # 将数据转化成长型 rsq&lt;-melt(rsq,id.vars=&quot;Noise&quot;,measure.vars=c(&quot;rsq_linear&quot;,&quot;rsq_pls&quot;,&quot;rsq_mars&quot;,&quot;rsq_svm&quot;,&quot;rsq_rf&quot;)) # 功能强大的绘图包 library(ggplot2) # 用ggplot2包进行可视化 ggplot(data=rsq, aes(x=Noise, y=value, group=variable, colour=variable)) + geom_line() + geom_point()+ ylab(&quot;R2&quot;) Figure 6.1: 模型\\(R^2\\)随应变量噪音强度变化，rsq_linear是简单线性回归，rsq_pls是偏最小二乘回归，rsq_mars是多元自适应回归样条回归，rsq_svm是支持向量机，rsq_rf是随机森林。 由图6.1中可以看到： 所有模型拟合效果随着噪音强度的增加急剧下降。对变量测量系统的理解能够帮助我们更好的预期模型的表现。这是在之前“数据分析一般流程”中说过的从问题到数据这个环节需要弄清的问题。你应该清楚当前数据库中已有的数据的质量。如果客户提供给你额外的数据，或者需要你从其它地方获得数据，数据质量是必须交流清楚的问题，笔者就曾在这里栽过跟头，希望大家可以避免类似的错误。 使用更加复杂的模型的效果不一定更好，如复杂的随机森林和支持向量机表现居中，简单线性回归和偏最小二乘回归在噪音低的时候拟合效果最差。效果最好的是多元自适应回归样条回归，该模型比简单线性回归复杂，但比剩下其它的模型的解释性都更强。 噪音增加到一定程度，复杂的随机森林模型能够发现的潜在结构变得更加模糊，模型表现不如其它更简单的模型。因此系统测量误差较大时，使用更简单的易于解释的模型可能是更好的选择，大家建模的时候要尽量多尝试几种模型，在表现相当的情况下选择最简单的模型，模型的评估和选择很好的反应了一个数据科学家的职业“成熟度”。 6.2.3 自变量误差 传统的统计模型通常假设自变量的测量无误差（或者随机性），这在实际中是不可能的，所以我们需要考虑自变量观测的随机性。自变量观测中的随机性产生的影响取决于如下几个因素：随机性的强度，相应因变量在模型中的重要性，使用模型的类别。我们选取自变量“在线消费”（online_exp）为例，用和上面相似的方法在该自变量上添加不同程度的噪音看其对模型拟合情况的影响。我们在自变量online_exp和上添加如下不同程度的噪音（标准差的0到3倍）： \\[ \\sigma_{0} \\times (0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0) \\] 其中\\(\\sigma_{0}\\)是在线消费观测的标准差。 noise&lt;-matrix(rep(NA,7*nrow(sim.dat)),nrow=nrow(sim.dat),ncol=7) for (i in 1:nrow(sim.dat)){ noise[i,]&lt;-rnorm(7,rep(0,7),sd(sim.dat$online_exp)*seq(0,3,by=0.5)) } 同样的，我们检查噪音强度对复杂度不同的模型拟合\\(R^2\\)的影响。拟合的模型复杂度从低到高依次为：一般线性回归，偏最小二乘回归，多元自适应回归样条，支持向量机（核函数是径向基函数），随机森林。代码和之前类似，这里就不重复展示。 Figure 6.2: 模型\\(R^2\\)随自变量(在线消费)噪音强度变化，rsq_linear是简单线性回归，rsq_pls是偏最小二乘回归，rsq_mars是多元自适应回归样条回归，rsq_svm是支持向量机，rsq_rf是随机森林。 比较图6.2和图6.1，可以看到自变量的误差和应变量误差对模型拟合结果的影响很不相同。应变量误差是无法克服的，对任何模型来说都是个硬伤。而自变量误差确不一定。试想极端的情况，在线消费这个变量完全是随机噪音，也就是所说的无信息变量，随机森林和支持向量机受的影响并不太大。线性模型和偏最小二乘回归的结果依旧基本重合，而且随着噪音的增加拟合效果开始下降较快，到一定程度后趋于平稳，如果噪音不断增加，最后拟合的情况实际上会趋近于移除“在线消费”这个变量的结果。总体说来，如果某个自变量含有误差，其它与之相关的变量在某种程度上可以进行弥补。线性模型对于自变量的观测误差的抗性普遍较差。 6.3 数据划分和再抽样 模型训练和选择过程都离不开数据的划分和再抽样。数据划分是将一部分数据预留出来用于模型测试，只用另外的部分数据用于模型的训练。再抽样过程牵扯到重复的从训练集中抽取样本并且在不同的样本上拟合模型，以此来得到关于拟合模型的信息。假设我们想知道某线性模型拟合度\\(R^2\\)的稳定性（也可以用其它模型拟合度量），可以重复的抽取不同的样本，然后拟合相同的线性模型，检查这些模型对应\\(R^2\\)的变化。由于牵扯到使用随机样本重复拟合模型，这个过程有一定的计算量，最近五年里，数据处理工具和技术获得了飞速的发展。除非你需要处理PB（\\(2^{50}\\)比特）级别的数据，或者每天要处理千亿级的事件，现阶段大多数技术已经能轻松满足你的需求了。 你可能会问：为什么要对数据划分和再抽样？简单的回答是避免过度拟合。在预测问题中，有时拟合的模型能很好的描述现有数据中的变量关系，但是对新样本的预测有很大的偏差，这时就发生了过度拟合。很多领域都会讨论过度拟合，如医学研究，化学计量，气象，金融和社会学研究等等。现代很多含有调优参数的分类 和回归模型有高度的灵活性，如之前提到的随机森林、支持向量机等。它们能够对复杂的关系进行建模，但是很容易过度强调不可再现的数据关系。要注意，虽然过度拟合的问题在灵活度高的模型中更加突出，所有模型（包括简单线性回归）在应用中都可能出现该问题。建模的目的是找到可重复的数据关系， 这就需要将现有数据划分成不同的数据集来调试模型参数和评估模型表现。 划分和再抽样的一般过程如下： 将样本划分成训练集和测试集 使用训练集拟合模型 将拟合的模型应用于测试集评估模型表现 关于数据划分，我们会介绍3种划分数据的方法：（1）按照结果变量划分数据；（2）按照预测变量划分数据；（3）按照时间序列划分数据。之后我们会介绍两种主要的再抽样方法： bootstrap和交互校验。 6.3.1 划分训练集和测试集 关于数据划分大家可能主要会问这三个问题：（1）为什么要划分训练集和测试集？（2）多少比例的数据用于训练集？（3）具体如何划分？我们现在就对此逐一回答。 70%用于训练集，30%用于测试集数据划分示意图 刚接触数据科学的人常常会问为什么我们要预留一部分数据作为测试集而不是使用全部的数据用于训练。印象中传统商业智能声称的数据分析通常只是数据描述。通过从数据库中查询相关测量来回答简单的问题，如：2015年某产品每月销售量是多少？我们网站在过去一个月每天的访问量是多少？两种包装设计的同类产品在某大零售店上个月的销量差距多大？像这样的问题确实不用对数据进行划分，相反我们需要用尽可能完整的数据，然后对感兴趣的部分求和或者平均。假设数据观测准确，我们不需要怀疑问题的答案，因为这些问题本质上就是对数据进行某种描述总结，没有牵扯到任何分析推断。 数据科学家需要解决的不会是这样的问题，常是预测问题，或者同时还需要从预测模型中得到相应能够指导决策的推断。在这些情况下，分析的重心在于找到自变量\\(\\mathbf{X}\\)和应变量\\(\\mathbf{y}\\)之间的系统关系。这时我们就必须非常小心，因为我们在用一个样本得到一般化的结论，进而对将来可能出现的观测进行预测，这远远超越了描述统计的界限。根据彭加莱的理论，在预测未来的过程中，预测的越远的未来要求模型越精确，因为你的错误率会迅速上升。每向前预测一步，噪声会随着以一种非线性的方式迅速增加，因此我很难相信对5年以后某事件的定量预测。我们能够处理定性的事物，能够讨论系统的某些特点，但能够计算的东西是很局限的。在《黑天鹅》那本书中，作者以数学家Michael Berry的弹子球计算为例说明了这种放大效应。该实验是预测弹子球在球桌上的运动轨迹。如果弹子球的基本参数已知，你能够计算出桌面阻力，测量撞击量，那么就可以预测第1次撞击的结果。要预测第2次撞击就更为复杂一些，你需要小心确定球的初始状态，但不是不可能。如果要计算第9次撞击的结果你需要考虑某个站在桌子旁边的人的体重和产生的引力。要计算第56次撞击结果你需要考虑宇宙中的每一个基本粒子。注意这还只是单独的弹子球而没有牵扯到有着自由意志的人，以及不同人之间相互的影响。对现实世界的复杂局面，人的预测能力有着本质上的局限性。因此在实际预测分析当中，你需要很小心的界定这个可预测的边界，好比在弹子球实验中，你能预测第1次撞击的结果或者咬咬牙，再多杀一大片脑细胞做第2次撞击预测，但不要试图再进一步，承认自己的局限需要知识和勇气。回到实际分析中，如何找到预测的边界？（注：随着你经验的增长，你会遇到很多你无法预测（有时是分析）的情况。）目前我知道的方法就是在仔细确保当前情况基本符合假设的情况下，严格划分训练集和测试集，尽可能对模型的预测情况进行评估，检测预测模型的精确度和稳定性。划分背后隐含的假设是： 我们用于分析的数据展现的过程能够反应真实世界中事情的发展过程 我们想要对其建模的真实世界中事情的发展过程随着时间变化是相对稳定的。如，用上个月的数据建立的表现良好的模型，在接下来的一个月的观测上依旧能够有类似的良好表现 换句话说，我们想要知道如果我们用模型来对新样本进行预测时会发生什么。我们的预测和真实将观测到的值有多接近？预测值偏离真实值的误差大致是多少？模型的误差是不是单向的，即预测是不是总大于真实值？这些都是很自然的问题，但它们的答案并非那么容易获得。最简单的理解模型在将来数据集上表现的方法就是试图模拟这件事。虽然严格说来，在将来事件发生之前，我们不可能得到相应的数据，但是我们能够预留一部分当前的数据并将它们视为将来的观测。例如，如果我们要预测2016年哪些农民还会某品牌的种子，可以用之前到2015年的历史数据建立预测模型，然后预测2016年的购买情况。这是一个相当好的模拟，由于我们其实已经知道2016年实际购买情况，可以将预测和真实情况进行对比。 在商业促销活动和信用风险的案例中，我们得到的数据通常和某个时间点相连（或者时间区间：一周，一个月，一个促销活动期间）。通常称这样的数据有代表性（用某时间点或者时间段的数据代表普遍情况）。在这样的情况下我们通常将数据集随机分成不同部分，然后用一部分（训练集）建立模型，另外一部分（测试集）来评估模型表现，可能的话对模型做出调整。 如果这两条假设大致正确，那么当前数据就能够合理反映未来的情况。因此在这种情况下，预留一部分当前数据来估计模型在将来的表现是合理的。明确了预测模型的一些假设前提，以及划分训练集和测试集的必要性之后，下一个问题是我们该将多少比例的数据用于训练集。 一般这需要视具体情况而定。通常需要考虑的两个因素是：（1）样本量；（2）计算速度。当样本量较大时，在考虑计算速度的条件下，我一般会尝试60%，70％和80%这三个比例，看哪个效果更好。如果样本量很小，那么测试集其评估模型效果的能力将非常有限，并且在原本样本量就不大的情况下再分出一部分数据会极大影响模型拟合。这种情况下，使用再抽样技术更加有效。常用的再抽样方法有交互校验和Bootstrap。 我们可以用createResample()函数生成简单bootstrap样本，createFolds函数可以生成平衡的交互校验样本集。 具体如何划分? 划分训练集和测试集时需要小心避免两个数据集有系统差别。例如，我们不能简单的把前半部分数据当作训练集，后半部分当作测试集。因为数据有可能是以某种方式排列的，如，按收入从大到小，按访问次数多少排列等等。有一种避免数据集间随机差别的方法是用简单的随机抽样，如对每个样本我们都抛下硬币，人头面就归于训练集，菊花面就归于测试集。有时还有一些其它因素需要考虑，但本质都是随机抽样。要想真正理解划分数据背后的逻辑需要实践。下面我们介绍经常使用的几种划分方法。 按照结果变量划分数据 若结果变量\\(\\mathbf{y}\\)为分类变量，那么我们的到的测试集和训练集中结果变量各类的分布比例应该类似。可以使用caret包中的createDataPartition()函数平衡划分样本集。回到我们之前使用的服装消费者数据集，假设我们想要建立关于消费者类别（segment）的判别模型，这时结果变量为segment，我们用80%的样本训练模型，20%的样本做为测试集，且训练集和测试集中各类别的比例要尽可能相近。我们可以用如下R代码实现： # 载入数据 sim.dat&lt;-read.csv(&quot;/Users/happyrabbit/Documents/GitHub/DataScientistR/Data/SegData.csv&quot;) # 需要caret包 library(caret) # 设置随机种子这样能得到相同的抽样结果 set.seed(3456) trainIndex &lt;- createDataPartition(sim.dat$segment, p = .8, list = FALSE, times = 1) head(trainIndex) ## Resample1 ## [1,] 1 ## [2,] 2 ## [3,] 3 ## [4,] 4 ## [5,] 6 ## [6,] 7 list = FALSE选项使得返回的值是数据框。该函数还有一个选项times，用于设置划分的次数，你可以一次返回多次划分的结果，函数会返回一个（或多个）整数向量（指针向量），指明归于训练集的行（你可以设置times＝2再运行一下上面的代码看看输出有什么不同）。下面我们通过返回的指针向量（trainIndex）得到训练集和测试集： # 得到训练集 datTrain &lt;- sim.dat[ trainIndex,] # 得到测试集 datTest &lt;- sim.dat[-trainIndex,] 按照设置，训练集中该有800个样本，测试集中有200个样本。我来看看两个集合中消费者类别的比例分布是否相似： library(plyr) ## ------------------------------------------------------------------------- ## You have loaded plyr after dplyr - this is likely to cause problems. ## If you need functions from both plyr and dplyr, please load plyr first, then dplyr: ## library(plyr); library(dplyr) ## ------------------------------------------------------------------------- ## ## Attaching package: &#39;plyr&#39; ## The following objects are masked from &#39;package:dplyr&#39;: ## ## arrange, count, desc, failwith, id, mutate, rename, summarise, ## summarize ddply(datTrain,&quot;segment&quot;,summarise,count=length(segment), percentage=round( length(segment)/nrow(datTrain),2)) ## segment count percentage ## 1 Conspicuous 160 0.20 ## 2 Price 200 0.25 ## 3 Quality 160 0.20 ## 4 Style 280 0.35 ddply(datTest,&quot;segment&quot;,summarise,count=length(segment), percentage=round(length(segment)/nrow(datTest),2)) ## segment count percentage ## 1 Conspicuous 40 0.20 ## 2 Price 50 0.25 ## 3 Quality 40 0.20 ## 4 Style 70 0.35 很明显两个集合中消费者类别比例分布是一样的（实际应用中两个集合分布不一定严格相似，但应该非常接近）。 按照自变量划分 还可以使用最大差异度法(Willett 2004)划分数据（maxDissim()函数）。假设样本集A中含有m个样本，样本集B含有n个样本，n&gt;m，且我们要从B中选出一些样本加到A中，该子集中的样本要尽量和A中的不同。要实现这一点，对B中的一个样本，计算A中样本和该样本的差异度（距离，这里会算出m个值，因为A中有m个样本）。然后将和A中样本最不相同的B的样本抽取出来加入A，重复这个过程，直到A的样本量达到要求。关于这么权衡这m个差异度找到和A“最不相似”的样本，有不同的方法，比如以最小的值为准，将所有距离求和等等。这里没有什么黄金法则，建议大家尝试几种方法，查看比较得到的训练/测试样本自变量分布，选取其中一种。用这种方式可以得到自变量分布相似的不同样本集。R中有不同的计算样本间差异度（基于自变量观测）的函数。caret包中调用的是proxy包中的函数。关于不同的差异度测量，见相关包的帮助文档。我们可以通过选项 obj设置和样本集A“最不相似”的样本的方式，其中minDiss表示以最小差异度为准，sumDiss表示使用差异度之和。 我们用服装数据的一个子集为例展示按照自变量抽样。这里选取年龄和收入这两个变量。 # 最大差异度抽样用到proxy包 library(proxy) # 用lattice包绘制散点图 library(lattice) # 选取年龄和收入这两个变量 testing&lt;-subset(sim.dat,select=c(&quot;age&quot;,&quot;income&quot; )) 我们先随机选取5个样本做为初始集（start），剩下的样本组成集合samplePool： set.seed(5) # 随机选取5个样本 startSet &lt;- sample(1:dim(testing)[1], 5) start &lt;- testing[startSet,] # 剩下的样本存在对象samplePool中 samplePool &lt;- testing[-startSet,] 通过maxDissim()函数从samplePool中抽取5个样本，这5个样本尽量和start中已有的样本不同。： # 通过最大化差异得到的样本存在数据框new内 # obj = minDiss 表示总体差异度以最小差异度为准 newSamp &lt;- maxDissim(start, samplePool,obj = minDiss, n = 5) new&lt;-samplePool[newSamp,] 我们再从samplePool中不用最大差异法，而是随机抽取5个样本，将这5个样本存在数据框new2中： newSet &lt;- sample(1:dim(samplePool)[1], 5) new2&lt;-testing[newSet,] 绘制散点图比较两种不同方法（new：用最大化差异法抽取的样本；new2：随机抽取的样本）抽取的样本和初始样本（start）有什么不同： start$group&lt;-rep(&quot;start&quot;,nrow(start)) new$group&lt;-rep(&quot;new&quot;,nrow(new)) new2$group&lt;-rep(&quot;new2&quot;,nrow(new2)) xyplot(age~income,data=rbind(start,new,new2),grid = TRUE, group = group, auto.key = TRUE ) Figure 6.3: 按自变量最大化差异抽样 由图6.3可见，通过最大化差异抽取的样本（new）和初始样本点（start）分布在图的不同位置。而随机抽取的新样本（new2）和原始样本更加接近。我们为什么希望每次抽取的样本和之前的不一样呢？因为我们希望最后得到的训练集和测试集覆盖的自变量观测区间相似。如果抽取的样本点都来自一个区域的话（比如全部都是年龄30以下，收入10万以下），如果讲这个样本用于训练的模型很可能不具有预测这个区域外样本的能力。反之要是用这个样本做为测试集，则无法检测模型在这个区域外样本上的表现。 按时间序列划分 对于时间序列数据，用简单随机抽样通常不是最好的方式。有一种按时间序列划分训练集和测试集的方法，关于该方法的讨论见(Hyndman and Athanasopoulos 2013)。我们临时抽取一个长度为100的来自1阶自回归模型［AR(1)］的时间序列样本，用来展示caret包中对时间序列样本划分测试集和训练集的函数createTimeSlices()。由于时间序列话题不在本书范围之内，这里不会进行过多讨论。 # 抽取符合AR(1)的时间序列向量 timedata = arima.sim(list(order=c(1,0,0), ar=-.9), n=100) # 对时间序列作图 plot(timedata, main=(expression(AR(1)~~~phi==-.9))) Figure 6.4: 时间序列样本图 图6.3展示了100个模拟的时间序列观测。对这样的数据，我们希望训练集和测试集都能覆盖到不同时段的观测。下面用createTimeSlices()函数对数据进行划分。该函数中有3个需要设置的参数： initialWindow: 初始训练集样本中的连续观测数目 horizon: 测试集中的观测数目 fixedWindow: 逻辑值，取值为FALSE时，训练集从第一个样本开始划分区间长度不固定。 timeSlices &lt;- createTimeSlices(1:length(timedata), initialWindow = 36, horizon = 12, fixedWindow = T) str(timeSlices,max.level = 1) ## List of 2 ## $ train:List of 53 ## $ test :List of 53 可以看到函数结果返回2个列表，分别含有训练集和测试集的样本索引。我们查看第一个训练集和测试集样本。 # 将训练集索引信息存在trainSlices对象内 trainSlices &lt;- timeSlices[[1]] # 将测试集索引信息存在testSlices对象内 testSlices &lt;- timeSlices[[2]] # 分别查看第一个训练集样本和测试集样本 trainSlices[[1]] ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 ## [24] 24 25 26 27 28 29 30 31 32 33 34 35 36 testSlices[[1]] ## [1] 37 38 39 40 41 42 43 44 45 46 47 48 第一个训练集样本是原数据中第1个观测到第36个观测（因为initialWindow = 36），接下来从第37到48这12个观测被划分到第一个测试集（因为horizon = 12）。你可以通过head(trainSlices)查看后续的样本索引。尝试着改变fixedWindow =的设置，然后重复上面的代码得到新的trainSlices和testSlices，然后键入： head(trainSlices) head(testSlices) 比较两种结果的不同就能够很容易理解该选项的作用了。 训练集和测试集的划分很容易理解和实现。但注意其中两个潜在的缺陷： 由于训练集和测试集的划分是随机的，所以重复这一过程在测试集上得到的误差会有波动。 由于训练集中只包含原始观测的一个子集，拟合模型使用的是部分数据。通常当数据量不是非常大的时候，使用更少的观测多少会对模型拟合造成负面影响。这就意味着该过程可能过度估计模型误差（即，使用所有观测拟合的模型的误差应该比当前估计的要小）。 6.3.2 重抽样 重抽样即对样本进行重复划分，所以是建立在数据划分的基础上。其基本原理是：用部分样本拟合模型，用剩下的样本评估模型。多次重复这一过程，然后对结果进行汇总。进行重抽样的目的可能有： 对于有调优参数的模型，如支持向量机，罚函数模型等，必须通过重抽样估计调优参数。这时的目的是针对一个模型表现的度量（如RMSE），找到能够优化该度量的调优参数值。 对于不含有调优参数的模型，如普通线性回归，最小二乘回归等，就模型拟合本身不需要重抽样，但可以通过重抽样考察模型拟合结果的稳定性，也可以用于检验模型在和训练集无关的样本上的表现。 这一小节将介绍几种主要的重抽样方法。 6.3.2.1 k折交叉验证 k折交叉验证的主要过程如下： 将样本随机划分为\\(k\\)个大小相当的子集 对\\(i=1…k\\) 用除了第\\(i\\)个样本集之外的样本拟合模型\\(M_{i}\\) 将\\(M_{i}\\)用在第i个样本集上，对结果进行评估 5折交叉验证 这样会得到k个模型评估结果，将这些结果进行汇总（通常是计算均值和标准差），然后基于此了解调优参数和模型表现之间的关系。联系之前介绍的不同划分方法，可以将这些划分方法应用到k折交叉验证中，使k个子集中的因变量组成尽可能平衡。k折交叉验证的一个特定是k等于样本量，这时每次只有一个预留样本，该情况也称为留一交叉验证（LOOCV），注意在这种情况下模型最终的评估结果将根据所有的预测值进行计算。通常在样本量较小的时候使用LOOCV，道理很简单，样本量小的时候我们应该用尽可能多的样本拟合模型。关于交互校验的折数，很多R函数默认设置k=10，但没有黄金标准。折数越多，每次预留在外的样本就越少，模型表现估计值和真实值之间的差距就越小。但LOOCV的计算量最大，因为其模型拟合的次数等于样本量，且每次模型拟合使用的子集样本量几乎和训练集相同。另一方面，当k值很小时（2或者3），计算效率高但是结果的方差和偏差都会增加。这意味着如果重复抽样的过程得到的结果可能很不一样。当样本量足够大时，方差和偏差的潜在影响就可以忽略不计，在这种情况下可以使用折数较低的交叉验证。这里讲到的关于计算效率和偏差之间的权衡，需要读者自己反复实践才能真正理解。 caret包中有几个关于重抽样的函数。createFolds()函数用于k折交叉验证。我们按照 消费者类别变量对服装消费者数据抽取k折交叉验证样本。 library(caret) class&lt;-sim.dat$segment #k折校验重抽样 set.seed(1) cv&lt;-createFolds(class,k=10,returnTrain=T) str(cv) ## List of 10 ## $ Fold01: int [1:900] 1 2 3 4 5 6 7 8 9 10 ... ## $ Fold02: int [1:900] 1 2 3 4 5 6 7 9 10 11 ... ## $ Fold03: int [1:900] 1 2 3 4 5 6 7 8 10 11 ... ## $ Fold04: int [1:900] 1 2 3 4 5 6 7 8 9 11 ... ## $ Fold05: int [1:900] 1 3 4 6 7 8 9 10 11 12 ... ## $ Fold06: int [1:900] 1 2 3 4 5 6 7 8 9 10 ... ## $ Fold07: int [1:900] 2 3 4 5 6 7 8 9 10 11 ... ## $ Fold08: int [1:900] 1 2 3 4 5 8 9 10 11 12 ... ## $ Fold09: int [1:900] 1 2 4 5 6 7 8 9 10 11 ... ## $ Fold10: int [1:900] 1 2 3 5 6 7 8 9 10 11 ... 结果返回10个子样本集中样本对应的行数。我们可以通过交叉验证来估计调优参数。回忆之前应变量误差的小节中拟合支持向量机模型的代码： #这里只是截取了之前的代码用于展示，并不能独立运行 fit0&lt;-train(trainX,trainY,method=&quot;svmRadial&quot;, tuneLength=15, trControl=trainControl(method=&quot;cv&quot;)) 上面代码中“method=&quot;cv&quot;”告诉R进行交叉验证，这里默认\\(k=10\\)。 6.3.2.2 重复训练/测试集划分 该方法其实就是对数据集重复多次训练集／测试集划分，用训练集建立模型，用测试集评估模型。和k折交叉验证不同，该过程生成的测试集可能有重复的样本，其通常重复更多次。对于划分比例和重复次数没有固定法则，通常将总样本的75%到80%用于训练，剩下的用于测试，用于训练的样本越接近，得到模型估计的偏差就越小。该方法中重复的次数的增加可以减少模型评估结果的不确定性，当然代价就是在模型复杂时的计算时间。当然，重复的次数也和测试集的样本占总体比例有关，如果比例小，那么得到的预测评估结果的波动性就更大，这时就需要增加重复次数来见效评估结果的不确定性。 假设我们还是按照消费者类别（segment）划分数据，这依旧可以使用之前用于划分训练集和测试集的函数createDataPartition()。记得之前该函数中的选项设置times=1么？这里只要将其设置成你想要重复的次数即可。 trainIndex &lt;- createDataPartition(sim.dat$segment, p = .8, list = FALSE, times = 5) dplyr::glimpse(trainIndex) ## int [1:800, 1:5] 1 3 4 5 6 7 8 9 10 11 ... ## - attr(*, &quot;dimnames&quot;)=List of 2 ## ..$ : NULL ## ..$ : chr [1:5] &quot;Resample1&quot; &quot;Resample2&quot; &quot;Resample3&quot; &quot;Resample4&quot; ... 类似的，对于其它划分方式，只要大家知道如何划分，重复划分应该很容易。 6.3.2.3 Bootstrap 方法 Bootstrap是一种应用及其广泛而且强大的统计工具。它可以用来定量分析参数估计或统计模型的不确定性(Efron and Tibshirani 1986)。如，可以通过Bootstrap估计线性回归模型参数拟合的标准差，这是另外一种取代p值的方法。该方法的强大在于其可以很容易应用于每个模型（说白了就是是对数据重复进行有放回随机抽样，拟合的过程），有的模型要是用传统的统计推断方法很难得到标准差和置信区间。这是一个典型的听起来很高端，其实没太多技术含量的方法。但很多天才的想法不都是这样么？之前没有人想到，之后大家都觉得这么简单粗暴有效怎么会想不到。由于是有放回抽样，一个样本可能多次被选中，且Bootstrap样本量和原数据样本量一样。这些没有被选中的样本称为“袋外（out-of-bag）样本”。选中的样本用来建立模型，带外样本用来评估模型。Efron指出，一般情况下(Efron 1983)，Bootstrap估计的模型错误率的不确定性更小。平均而言，63.2%的样本点在 Bootstrap 中出现过至少一次，因此其估计的偏差与2折交叉验证相似。如之前所述，折数越小，用于训练的样本数目越少，这意味着估计的偏差越大。增加样本量可以缓解该问题。总的来说，和交叉验证相比，Bootstrap偏差更大，不确定性更小。针对估计偏差问题，Efron对原始Bootstrap过程进行了改进，得到下面的632方法： \\[(0.632 × 原始 Bootstrap 估计错误率) + (0.368 ×显性错误率)\\] 其中显性错误率就是用所有样本进行建模，然后再作用于相同的样本集得到的模型错误率，该估计显然过度乐观。这一改进虽然在某种程度上降低了偏差，但在样本量小的时候依旧表现不佳。试想严重过度拟合的情况下显性错误率几乎是0，那么上面公式中的第二项也就不存在了，这个时候，632方法给出的错误率估计可能过度乐观。Efron 和 Tibshirani之后进一步改进了632方法，得到“632+ 方法”，进一步调整了Bootstrap 估计(Efron and Tibshirani 1997)。 6.4 本章总结 本章介绍了一些基础建模技术，包括有监督和无监督的概念。模型误差分类： 系统误差：能够通过改进模型减小这部分误差 随机误差：当前数据无法解释的部分，无法通过建立更加复杂的模型来改进 此外我们讲了误差的两个来源，应变量误差和自变量误差。其中应变量误差会反映在随机误差中，这是个硬伤，无法克服。而某些自变量的误差可能通过其它相关自变量得到弥补，其影响取决于随机性强度，相应变量在模型中的重要性，以及自变量之间的相关性。 最后，也是最重要的一个话题是数据的划分和再抽样。其主要目的是为了判断模型真实的表现。我们介绍了3种数据划分的方法： 按照结果变量划分数据； 按照预测变量划分数据； 按照时间序列划分数据。 我们还介绍了两种主要的再抽样方法：bootstrap和交互校验。重抽样是建立在数据划分的基础上，其主要目的有两个： 对于有调优参数的模型估计调优参数； 考察模型拟合结果的稳定性。 其中我们讨论了不同重抽样的影响，以及在方差，偏差和计算效率之间的权衡。这里讲的所有方法都需要大家在实践中总结，才能真正成为自己的技能。 References "],
["shiny-intro.html", "Chapter 7 R-Shiny Introduction 7.1 Softwares 7.2 Web development 7.3 Shiny 7.4 Resources", " Chapter 7 R-Shiny Introduction This tutorial is designed for Shiny beginner. You don’t need any background in the R language or Shiny to get started, although having some basic knowlege about R might be helpful. It would be helpful if you have some basic knowlege about HTML, CSS and javascript, but they are not required too. 7.1 Softwares R (required). Better to use the latest version of R (3.3.2 for the time this tutorial is written). The shiny package (required). To install the package from CRAN by using: install.packages(&quot;shiny&quot;) Rstudio (recommended). A user friendly IDE for R. Very convenient tool in building shiny apps. A web browser. Better to use Google chrome or Firefox. In this section, we’ll start from some basic concepts of web development and then introduce Shiny by explaining the advantages of Shiny and how it works. At last we’ll give a list of useful resources in learning Shiny. 7.2 Web development Here are some concepts that usually used in web development. Static web page A static web page (sometimes called a flat page/stationary page) is a web page that is delivered to the user exactly as stored. (from wiki) Figure 7.1: Static web page: is delivered to the user exactly as stored. Dynamic web page A server-side dynamic web page is a web page whose construction is controlled by an application server processing server-side scripts. (from wiki) Figure 7.2: Dynamic web page: example of server-side scripting (PHP and MySQL). Web application A Web application (Web app) is an application program that is stored on a remote server and delivered over the Internet through a browser interface. (from WhatIs.com) Web (application) framework A Web framework is a collection of packages or modules which allow developers to write Web applications or services without having to handle such low-level details as protocols, sockets or process/thread management. (from PythonWiki) Front-end web development Front-end web development, also known as client-side development is the practice of producing HTML, CSS and JavaScript for a website or web application so that a user can see and interact with them directly. (from Wiki) Back-end web development Back-end web development creates the logical back-end and core computational logic of a website. (from techopedia) 7.3 Shiny Shiny is a web application framework for R that can help turn your analyses into interactive web applications. No HTML, CSS, or JavaScript knowledge required Why Shiny? Easy to learn, easy to use. The development time is minimized. Excellent tool for data visualization. Have very strong backing: the R language Fun &amp; Cool. A Shiny app usually contains two parts: UI: controls the outlook of the web page Server: (a live R session) controls the logic How does Shiny app work? The “Server” keeps monitoring the UI. Whenever there is a change in the UI, the “Server” will follow some instructions (run some R code) accordingly and update the UI’s display. (This is the basic idea of reactive expression, which is an distinguish feature of Shiny we will talk about later.) Example library(shiny) runExample(&quot;01_hello&quot;) # a histogram In the example above, the “Server” keeps monitoring the “slider” in the page, and whenever there is a change with it, the “Server” will re-execute a block of R code to regenerate the hitogram. 7.4 Resources Shiny portal site: http://shiny.rstudio.com Tutorial (get started): http://shiny.rstudio.com/tutorial/ Articles (go deeper): http://shiny.rstudio.com/articles/ Gallery (get inspired): http://shiny.rstudio.com/gallery/ Shiny User Showcase: https://www.rstudio.com/products/shiny/shiny-user-showcase/ Shiny Apps for the Enterprise Industry Specific Shiny Apps Shiny Apps as Analytics Tools Shiny Apps that Extend Shiny Shiny Apps with Popular Appeal Shiny Apps for Teaching Shiny examples (over 100 examples): https://github.com/rstudio/shiny-examples Ask questions in the shiny google group: https://groups.google.com/forum/#!forum/shiny-discuss Articles from R blogger: http://www.r-bloggers.com/?s=shiny Gallery of user-made apps: http://www.showmeshiny.com/ 2016 Shiny Developer Conference Videos https://www.rstudio.com/resources/webinars/shiny-developer-conference/ "],
["dynamicreproducible-report.html", "Chapter 8 Dynamic/Reproducible report", " Chapter 8 Dynamic/Reproducible report "],
["soft-skills-for-data-scientists.html", "Chapter 9 Soft Skills for Data Scientists 9.1 Introduction to agile 9.2 Effective communication with business partners 9.3 Leadership skills 9.4 Decision making with uncertainty", " Chapter 9 Soft Skills for Data Scientists 9.1 Introduction to agile 9.2 Effective communication with business partners 9.3 Leadership skills 9.4 Decision making with uncertainty "],
["references.html", "Chapter 10 References", " Chapter 10 References "]
]
