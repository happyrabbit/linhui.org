<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>The Science and Art of Data</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="This is the handouts for CE course at JSM 2017">
  <meta name="generator" content="bookdown 0.1 and GitBook 2.6.7">

  <meta property="og:title" content="The Science and Art of Data" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is the handouts for CE course at JSM 2017" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="The Science and Art of Data" />
  
  <meta name="twitter:description" content="This is the handouts for CE course at JSM 2017" />
  

<meta name="author" content="Hui Lin and Ming Li">

<meta name="date" content="2017-06-12">

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="introduction-to-the-data.html">
<link rel="next" href="soft-skills-for-data-scientists.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-0.8/htmlwidgets.js"></script>
<script src="libs/datatables-binding-0.2/datatables.js"></script>
<link href="libs/dt-core-1.10.12/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.10.12/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.10.12/js/jquery.dataTables.min.js"></script>
<link href="libs/leaflet-0.7.3/leaflet.css" rel="stylesheet" />
<script src="libs/leaflet-0.7.3/leaflet.js"></script>
<link href="libs/leafletfix-1.0.0/leafletfix.css" rel="stylesheet" />
<script src="libs/leaflet-binding-1.0.1/leaflet.js"></script>
<link href="libs/dygraphs-1.1.1/dygraph.css" rel="stylesheet" />
<script src="libs/dygraphs-1.1.1/dygraph-combined.js"></script>
<script src="libs/moment-2.8.4/moment.js"></script>
<script src="libs/moment-timezone-0.2.5/moment-timezone-with-data.js"></script>
<script src="libs/moment-fquarter-1.0.0/moment-fquarter.min.js"></script>
<script src="libs/dygraphs-binding-1.1.1.4/dygraphs.js"></script>
<script src="libs/proj4js-2.3.15/proj4.js"></script>
<link href="libs/highcharts-5.0.6/css/motion.css" rel="stylesheet" />
<script src="libs/highcharts-5.0.6/highstock.js"></script>
<script src="libs/highcharts-5.0.6/highcharts-3d.js"></script>
<script src="libs/highcharts-5.0.6/highcharts-more.js"></script>
<script src="libs/highcharts-5.0.6/modules/annotations.js"></script>
<script src="libs/highcharts-5.0.6/modules/broken-axis.js"></script>
<script src="libs/highcharts-5.0.6/modules/data.js"></script>
<script src="libs/highcharts-5.0.6/modules/drilldown.js"></script>
<script src="libs/highcharts-5.0.6/modules/exporting.js"></script>
<script src="libs/highcharts-5.0.6/modules/funnel.js"></script>
<script src="libs/highcharts-5.0.6/modules/heatmap.js"></script>
<script src="libs/highcharts-5.0.6/modules/map.js"></script>
<script src="libs/highcharts-5.0.6/modules/no-data-to-display.js"></script>
<script src="libs/highcharts-5.0.6/modules/offline-exporting.js"></script>
<script src="libs/highcharts-5.0.6/modules/solid-gauge.js"></script>
<script src="libs/highcharts-5.0.6/modules/treemap.js"></script>
<script src="libs/highcharts-5.0.6/plugins/annotations.js"></script>
<script src="libs/highcharts-5.0.6/plugins/draggable-legend.js"></script>
<script src="libs/highcharts-5.0.6/plugins/draggable-points.js"></script>
<script src="libs/highcharts-5.0.6/plugins/export-csv.js"></script>
<script src="libs/highcharts-5.0.6/plugins/grouped-categories.js"></script>
<script src="libs/highcharts-5.0.6/plugins/motion.js"></script>
<script src="libs/highcharts-5.0.6/plugins/pattern-fill-v2.js"></script>
<script src="libs/highcharts-5.0.6/plugins/tooltip-delay.js"></script>
<script src="libs/highcharts-5.0.6/custom/reset.js"></script>
<script src="libs/highcharts-5.0.6/custom/symbols-extra.js"></script>
<script src="libs/highcharts-5.0.6/custom/text-symbols.js"></script>
<link href="libs/fontawesome-4.5.0/font-awesome.min.css" rel="stylesheet" />
<link href="libs/htmlwdgtgrid-1/htmlwdgtgrid.css" rel="stylesheet" />
<script src="libs/highchart-binding-0.5.0/highchart.js"></script>
<link href="libs/bokehjs-0.11.1/bokeh.min.css" rel="stylesheet" />
<script src="libs/bokehjs-0.11.1/bokeh.min.js"></script>
<script src="libs/rbokeh-binding-0.4.2/rbokeh.js"></script>
<script src="libs/d3-3.5.12/d3.min.js"></script>
<link href="libs/metrics-graphics-2.7.0/dist/metricsgraphics.css" rel="stylesheet" />
<link href="libs/metrics-graphics-2.7.0/dist/mg_regions.css" rel="stylesheet" />
<script src="libs/metrics-graphics-2.7.0/dist/metricsgraphics.min.js"></script>
<script src="libs/metrics-graphics-2.7.0/dist/mg_regions.js"></script>
<script src="libs/metricsgraphics-binding-0.9.0/metricsgraphics.js"></script>
<script src="libs/forceNetwork-binding-0.2.11/forceNetwork.js"></script>
<script src="libs/threejs-70/three.min.js"></script>
<script src="libs/threejs-70/Detector.js"></script>
<script src="libs/threejs-70/Projector.js"></script>
<script src="libs/threejs-70/CanvasRenderer.js"></script>
<script src="libs/globe-binding-0.2.2/globe.js"></script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Prologue</a></li>
<li class="chapter" data-level="2" data-path="the-art-of-data-science.html"><a href="the-art-of-data-science.html"><i class="fa fa-check"></i><b>2</b> The art of data science</a><ul>
<li class="chapter" data-level="2.1" data-path="the-art-of-data-science.html"><a href="the-art-of-data-science.html#what-is-data-science"><i class="fa fa-check"></i><b>2.1</b> What is data science?</a></li>
<li class="chapter" data-level="2.2" data-path="the-art-of-data-science.html"><a href="the-art-of-data-science.html#is-it-science-totally"><i class="fa fa-check"></i><b>2.2</b> Is it science? Totally?</a></li>
<li class="chapter" data-level="2.3" data-path="the-art-of-data-science.html"><a href="the-art-of-data-science.html#what-kind-of-questions-can-data-science-solve"><i class="fa fa-check"></i><b>2.3</b> What kind of questions can data science solve?</a><ul>
<li class="chapter" data-level="2.3.1" data-path="the-art-of-data-science.html"><a href="the-art-of-data-science.html#what-is-a-good-question"><i class="fa fa-check"></i><b>2.3.1</b> What is a good question?</a></li>
<li class="chapter" data-level="2.3.2" data-path="the-art-of-data-science.html"><a href="the-art-of-data-science.html#types-of-questions"><i class="fa fa-check"></i><b>2.3.2</b> Types of questions</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="the-art-of-data-science.html"><a href="the-art-of-data-science.html#what-are-the-required-skills-for-data-scientist"><i class="fa fa-check"></i><b>2.4</b> What are the required skills for data scientist?</a><ul>
<li class="chapter" data-level="2.4.1" data-path="the-art-of-data-science.html"><a href="the-art-of-data-science.html#types-of-learning"><i class="fa fa-check"></i><b>2.4.1</b> Types of Learning</a></li>
<li class="chapter" data-level="2.4.2" data-path="the-art-of-data-science.html"><a href="the-art-of-data-science.html#types-of-algorithm"><i class="fa fa-check"></i><b>2.4.2</b> Types of Algorithm</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="the-art-of-data-science.html"><a href="the-art-of-data-science.html#general-process-of-data-science"><i class="fa fa-check"></i><b>2.5</b> General Process of Data Science</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="introduction-to-the-data.html"><a href="introduction-to-the-data.html"><i class="fa fa-check"></i><b>3</b> Introduction to the data</a><ul>
<li class="chapter" data-level="3.1" data-path="introduction-to-the-data.html"><a href="introduction-to-the-data.html#customer-data-for-clothing-company"><i class="fa fa-check"></i><b>3.1</b> Customer Data for Clothing Company</a></li>
<li class="chapter" data-level="3.2" data-path="introduction-to-the-data.html"><a href="introduction-to-the-data.html#customer-satisfaction-survey-data-from-airline-company"><i class="fa fa-check"></i><b>3.2</b> Customer Satisfaction Survey Data from Airline Company</a></li>
<li class="chapter" data-level="3.3" data-path="introduction-to-the-data.html"><a href="introduction-to-the-data.html#swine-disease-breakout-data"><i class="fa fa-check"></i><b>3.3</b> Swine Disease Breakout Data</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="big-data-cloud-platform.html"><a href="big-data-cloud-platform.html"><i class="fa fa-check"></i><b>4</b> Big Data Cloud Platform</a><ul>
<li class="chapter" data-level="4.1" data-path="big-data-cloud-platform.html"><a href="big-data-cloud-platform.html#how-data-become-science"><i class="fa fa-check"></i><b>4.1</b> How Data become Science?</a></li>
<li class="chapter" data-level="4.2" data-path="big-data-cloud-platform.html"><a href="big-data-cloud-platform.html#power-of-cluster-of-computers"><i class="fa fa-check"></i><b>4.2</b> Power of Cluster of Computers</a></li>
<li class="chapter" data-level="4.3" data-path="big-data-cloud-platform.html"><a href="big-data-cloud-platform.html#introduction-of-cloud-environment"><i class="fa fa-check"></i><b>4.3</b> Introduction of Cloud Environment</a></li>
<li class="chapter" data-level="4.4" data-path="big-data-cloud-platform.html"><a href="big-data-cloud-platform.html#open-account-and-create-a-cluster"><i class="fa fa-check"></i><b>4.4</b> Open Account and Create a Cluster</a></li>
<li class="chapter" data-level="4.5" data-path="big-data-cloud-platform.html"><a href="big-data-cloud-platform.html#r-notebook"><i class="fa fa-check"></i><b>4.5</b> R Notebook</a></li>
<li class="chapter" data-level="4.6" data-path="big-data-cloud-platform.html"><a href="big-data-cloud-platform.html#markdown-cells"><i class="fa fa-check"></i><b>4.6</b> Markdown Cells</a></li>
<li class="chapter" data-level="4.7" data-path="big-data-cloud-platform.html"><a href="big-data-cloud-platform.html#leverage-hadoop-and-spark-parallel-using-r-notebook"><i class="fa fa-check"></i><b>4.7</b> Leverage Hadoop and Spark Parallel using R Notebook</a><ul>
<li class="chapter" data-level="4.7.1" data-path="big-data-cloud-platform.html"><a href="big-data-cloud-platform.html#library-installation"><i class="fa fa-check"></i><b>4.7.1</b> Library Installation</a></li>
<li class="chapter" data-level="4.7.2" data-path="big-data-cloud-platform.html"><a href="big-data-cloud-platform.html#create-connection"><i class="fa fa-check"></i><b>4.7.2</b> Create Connection</a></li>
<li class="chapter" data-level="4.7.3" data-path="big-data-cloud-platform.html"><a href="big-data-cloud-platform.html#sample-dataset"><i class="fa fa-check"></i><b>4.7.3</b> Sample Dataset</a></li>
<li class="chapter" data-level="4.7.4" data-path="big-data-cloud-platform.html"><a href="big-data-cloud-platform.html#important---copy-data-to-spark-environment"><i class="fa fa-check"></i><b>4.7.4</b> IMPORTANT - Copy Data to Spark Environment</a></li>
<li class="chapter" data-level="4.7.5" data-path="big-data-cloud-platform.html"><a href="big-data-cloud-platform.html#analyzing-the-data"><i class="fa fa-check"></i><b>4.7.5</b> Analyzing the Data</a></li>
<li class="chapter" data-level="4.7.6" data-path="big-data-cloud-platform.html"><a href="big-data-cloud-platform.html#collect-results-back-to-master-node"><i class="fa fa-check"></i><b>4.7.6</b> Collect Results Back to Master Node</a></li>
<li class="chapter" data-level="4.7.7" data-path="big-data-cloud-platform.html"><a href="big-data-cloud-platform.html#fit-regression-to-sparkdataframe"><i class="fa fa-check"></i><b>4.7.7</b> Fit Regression to SparkDataFrame</a></li>
<li class="chapter" data-level="4.7.8" data-path="big-data-cloud-platform.html"><a href="big-data-cloud-platform.html#fit-a-k-means-cluster"><i class="fa fa-check"></i><b>4.7.8</b> Fit a K-means Cluster</a></li>
<li class="chapter" data-level="4.7.9" data-path="big-data-cloud-platform.html"><a href="big-data-cloud-platform.html#summary"><i class="fa fa-check"></i><b>4.7.9</b> Summary</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="soft-skills-for-data-scientists.html"><a href="soft-skills-for-data-scientists.html"><i class="fa fa-check"></i><b>5</b> Soft Skills for Data Scientists</a><ul>
<li class="chapter" data-level="5.1" data-path="soft-skills-for-data-scientists.html"><a href="soft-skills-for-data-scientists.html#comparison-between-statistician-and-data-scientist"><i class="fa fa-check"></i><b>5.1</b> Comparison between Statistician and Data Scientist</a></li>
<li class="chapter" data-level="5.2" data-path="soft-skills-for-data-scientists.html"><a href="soft-skills-for-data-scientists.html#where-data-science-team-fits"><i class="fa fa-check"></i><b>5.2</b> Where Data Science Team Fits?</a></li>
<li class="chapter" data-level="5.3" data-path="soft-skills-for-data-scientists.html"><a href="soft-skills-for-data-scientists.html#beyond-data-and-analytics"><i class="fa fa-check"></i><b>5.3</b> Beyond Data and Analytics</a></li>
<li class="chapter" data-level="5.4" data-path="soft-skills-for-data-scientists.html"><a href="soft-skills-for-data-scientists.html#data-scientist-as-a-leader"><i class="fa fa-check"></i><b>5.4</b> Data Scientist as a Leader</a></li>
<li class="chapter" data-level="5.5" data-path="soft-skills-for-data-scientists.html"><a href="soft-skills-for-data-scientists.html#three-pillars-of-knowledge"><i class="fa fa-check"></i><b>5.5</b> Three Pillars of Knowledge</a></li>
<li class="chapter" data-level="5.6" data-path="soft-skills-for-data-scientists.html"><a href="soft-skills-for-data-scientists.html#common-pitfalls-of-data-science-projects"><i class="fa fa-check"></i><b>5.6</b> Common Pitfalls of Data Science Projects</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="data-wrangling.html"><a href="data-wrangling.html"><i class="fa fa-check"></i><b>6</b> Data Wrangling</a><ul>
<li class="chapter" data-level="6.1" data-path="data-wrangling.html"><a href="data-wrangling.html#read-and-write-data"><i class="fa fa-check"></i><b>6.1</b> Read and write data</a><ul>
<li class="chapter" data-level="6.1.1" data-path="data-wrangling.html"><a href="data-wrangling.html#readr"><i class="fa fa-check"></i><b>6.1.1</b> <code>readr</code></a></li>
<li class="chapter" data-level="6.1.2" data-path="data-wrangling.html"><a href="data-wrangling.html#data.table-enhanced-data.frame"><i class="fa fa-check"></i><b>6.1.2</b> <code>data.table</code>— enhanced <code>data.frame</code></a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="data-wrangling.html"><a href="data-wrangling.html#summarize-data"><i class="fa fa-check"></i><b>6.2</b> Summarize data</a><ul>
<li class="chapter" data-level="6.2.1" data-path="data-wrangling.html"><a href="data-wrangling.html#apply-lapply-and-sapply-in-base-r"><i class="fa fa-check"></i><b>6.2.1</b> <code>apply()</code>, <code>lapply()</code> and <code>sapply()</code> in base R</a></li>
<li class="chapter" data-level="6.2.2" data-path="data-wrangling.html"><a href="data-wrangling.html#ddply-in-plyr-package"><i class="fa fa-check"></i><b>6.2.2</b> <code>ddply()</code> in <code>plyr</code> package</a></li>
<li class="chapter" data-level="6.2.3" data-path="data-wrangling.html"><a href="data-wrangling.html#dplyr-package"><i class="fa fa-check"></i><b>6.2.3</b> <code>dplyr</code> package</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="data-wrangling.html"><a href="data-wrangling.html#tidy-and-reshape-data"><i class="fa fa-check"></i><b>6.3</b> Tidy and Reshape Data</a><ul>
<li class="chapter" data-level="6.3.1" data-path="data-wrangling.html"><a href="data-wrangling.html#reshape2-package"><i class="fa fa-check"></i><b>6.3.1</b> <code>reshape2</code> package</a></li>
<li class="chapter" data-level="6.3.2" data-path="data-wrangling.html"><a href="data-wrangling.html#tidyr-package"><i class="fa fa-check"></i><b>6.3.2</b> <code>tidyr</code> package</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="data-pre-processing.html"><a href="data-pre-processing.html"><i class="fa fa-check"></i><b>7</b> Data Pre-processing</a><ul>
<li class="chapter" data-level="7.1" data-path="data-pre-processing.html"><a href="data-pre-processing.html#start"><i class="fa fa-check"></i><b>7.1</b> Start</a></li>
<li class="chapter" data-level="7.2" data-path="data-pre-processing.html"><a href="data-pre-processing.html#centering-and-scaling"><i class="fa fa-check"></i><b>7.2</b> <strong>Centering and Scaling</strong></a></li>
<li class="chapter" data-level="7.3" data-path="data-pre-processing.html"><a href="data-pre-processing.html#resolve-skewness"><i class="fa fa-check"></i><b>7.3</b> <strong>Resolve Skewness</strong></a></li>
<li class="chapter" data-level="7.4" data-path="data-pre-processing.html"><a href="data-pre-processing.html#resolve-outliers"><i class="fa fa-check"></i><b>7.4</b> <strong>Resolve Outliers</strong></a></li>
<li class="chapter" data-level="7.5" data-path="data-pre-processing.html"><a href="data-pre-processing.html#missing-values"><i class="fa fa-check"></i><b>7.5</b> <strong>Missing Values</strong></a><ul>
<li class="chapter" data-level="7.5.1" data-path="data-pre-processing.html"><a href="data-pre-processing.html#impute-missing-values-with-medianmode"><i class="fa fa-check"></i><b>7.5.1</b> Impute missing values with median/mode</a></li>
<li class="chapter" data-level="7.5.2" data-path="data-pre-processing.html"><a href="data-pre-processing.html#impute-missing-values-based-on-k-nearest-neighbors"><i class="fa fa-check"></i><b>7.5.2</b> Impute missing values based on K-nearest neighbors</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="data-pre-processing.html"><a href="data-pre-processing.html#collinearity"><i class="fa fa-check"></i><b>7.6</b> <strong>Collinearity</strong></a></li>
<li class="chapter" data-level="7.7" data-path="data-pre-processing.html"><a href="data-pre-processing.html#sparse-variables"><i class="fa fa-check"></i><b>7.7</b> <strong>Sparse Variables</strong></a></li>
<li class="chapter" data-level="7.8" data-path="data-pre-processing.html"><a href="data-pre-processing.html#re-encode-dummy-variables"><i class="fa fa-check"></i><b>7.8</b> <strong>Re-encode Dummy Variables</strong></a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="shiny-intro.html"><a href="shiny-intro.html"><i class="fa fa-check"></i><b>8</b> R-Shiny Introduction</a><ul>
<li class="chapter" data-level="8.1" data-path="shiny-intro.html"><a href="shiny-intro.html#softwares"><i class="fa fa-check"></i><b>8.1</b> Softwares</a></li>
<li class="chapter" data-level="8.2" data-path="shiny-intro.html"><a href="shiny-intro.html#web-development"><i class="fa fa-check"></i><b>8.2</b> Web development</a></li>
<li class="chapter" data-level="8.3" data-path="shiny-intro.html"><a href="shiny-intro.html#shiny"><i class="fa fa-check"></i><b>8.3</b> Shiny</a></li>
<li class="chapter" data-level="8.4" data-path="shiny-intro.html"><a href="shiny-intro.html#resources"><i class="fa fa-check"></i><b>8.4</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="dynamicreproducible-report.html"><a href="dynamicreproducible-report.html"><i class="fa fa-check"></i><b>9</b> Dynamic/Reproducible report</a><ul>
<li class="chapter" data-level="9.1" data-path="dynamicreproducible-report.html"><a href="dynamicreproducible-report.html#what-is-r-markdown"><i class="fa fa-check"></i><b>9.1</b> What is R Markdown?</a></li>
<li class="chapter" data-level="9.2" data-path="dynamicreproducible-report.html"><a href="dynamicreproducible-report.html#how-to-start"><i class="fa fa-check"></i><b>9.2</b> How to Start?</a><ul>
<li class="chapter" data-level="9.2.1" data-path="dynamicreproducible-report.html"><a href="dynamicreproducible-report.html#how-it-work"><i class="fa fa-check"></i><b>9.2.1</b> How It Work?</a></li>
<li class="chapter" data-level="9.2.2" data-path="dynamicreproducible-report.html"><a href="dynamicreproducible-report.html#get-started"><i class="fa fa-check"></i><b>9.2.2</b> Get Started</a></li>
<li class="chapter" data-level="9.2.3" data-path="dynamicreproducible-report.html"><a href="dynamicreproducible-report.html#markdown-basic"><i class="fa fa-check"></i><b>9.2.3</b> Markdown Basic</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="dynamicreproducible-report.html"><a href="dynamicreproducible-report.html#html"><i class="fa fa-check"></i><b>9.3</b> HTML</a><ul>
<li class="chapter" data-level="9.3.1" data-path="dynamicreproducible-report.html"><a href="dynamicreproducible-report.html#create-an-html-document"><i class="fa fa-check"></i><b>9.3.1</b> Create an HTML document</a></li>
<li class="chapter" data-level="9.3.2" data-path="dynamicreproducible-report.html"><a href="dynamicreproducible-report.html#floating-toc"><i class="fa fa-check"></i><b>9.3.2</b> Floating TOC</a></li>
<li class="chapter" data-level="9.3.3" data-path="dynamicreproducible-report.html"><a href="dynamicreproducible-report.html#code-chunks"><i class="fa fa-check"></i><b>9.3.3</b> Code Chunks</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="dynamicreproducible-report.html"><a href="dynamicreproducible-report.html#html5-slides"><i class="fa fa-check"></i><b>9.4</b> HTML5 Slides</a><ul>
<li class="chapter" data-level="9.4.1" data-path="dynamicreproducible-report.html"><a href="dynamicreproducible-report.html#ioslides-presentation"><i class="fa fa-check"></i><b>9.4.1</b> <code>ioslides</code> presentation</a></li>
<li class="chapter" data-level="9.4.2" data-path="dynamicreproducible-report.html"><a href="dynamicreproducible-report.html#slidy-presentation"><i class="fa fa-check"></i><b>9.4.2</b> <code>slidy</code> presentation</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="dynamicreproducible-report.html"><a href="dynamicreproducible-report.html#dashboards"><i class="fa fa-check"></i><b>9.5</b> Dashboards</a><ul>
<li class="chapter" data-level="9.5.1" data-path="dynamicreproducible-report.html"><a href="dynamicreproducible-report.html#layouts"><i class="fa fa-check"></i><b>9.5.1</b> Layouts</a></li>
<li class="chapter" data-level="9.5.2" data-path="dynamicreproducible-report.html"><a href="dynamicreproducible-report.html#components"><i class="fa fa-check"></i><b>9.5.2</b> Components</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="dynamicreproducible-report.html"><a href="dynamicreproducible-report.html#html-widgets-1"><i class="fa fa-check"></i><b>9.6</b> HTML Widgets</a><ul>
<li class="chapter" data-level="9.6.1" data-path="dynamicreproducible-report.html"><a href="dynamicreproducible-report.html#dt-a-wrapper-of-the-javascript-library-datatables"><i class="fa fa-check"></i><b>9.6.1</b> <code>DT</code>: A Wrapper of the JavaScript Library DataTables</a></li>
<li class="chapter" data-level="9.6.2" data-path="dynamicreproducible-report.html"><a href="dynamicreproducible-report.html#leafletinteractive-web-maps-based-on-the-leaflet-javascript-library"><i class="fa fa-check"></i><b>9.6.2</b> <code>Leaflet</code>:Interactive Web-Maps Based on the Leaflet JavaScript Library</a></li>
<li class="chapter" data-level="9.6.3" data-path="dynamicreproducible-report.html"><a href="dynamicreproducible-report.html#dygraphs-interactive-plot-for-time-series-data"><i class="fa fa-check"></i><b>9.6.3</b> <code>dygraphs</code>: interactive plot for time series data</a></li>
<li class="chapter" data-level="9.6.4" data-path="dynamicreproducible-report.html"><a href="dynamicreproducible-report.html#highcharter"><i class="fa fa-check"></i><b>9.6.4</b> <code>highcharter</code></a></li>
<li class="chapter" data-level="9.6.5" data-path="dynamicreproducible-report.html"><a href="dynamicreproducible-report.html#rbokeh-is-a-visualization-library-that-provides-a-flexible-and-powerful-declarative-framework-for-creating-web-based-plots"><i class="fa fa-check"></i><b>9.6.5</b> <code>rbokeh</code> is a visualization library that provides a flexible and powerful declarative framework for creating web-based plots</a></li>
<li class="chapter" data-level="9.6.6" data-path="dynamicreproducible-report.html"><a href="dynamicreproducible-report.html#metricsgraphics-enables-easy-creation-of-d3-scatterplots-line-charts-and-histograms."><i class="fa fa-check"></i><b>9.6.6</b> <code>metricsgraphics</code> enables easy creation of D3 scatterplots, line charts, and histograms.</a></li>
<li class="chapter" data-level="9.6.7" data-path="dynamicreproducible-report.html"><a href="dynamicreproducible-report.html#networkd3-d3-javascript-network-graphs-from-r"><i class="fa fa-check"></i><b>9.6.7</b> <code>networkD3</code>: D3 JavaScript Network Graphs from R</a></li>
<li class="chapter" data-level="9.6.8" data-path="dynamicreproducible-report.html"><a href="dynamicreproducible-report.html#threejs-interactive-3d-scatter-plots-and-globes"><i class="fa fa-check"></i><b>9.6.8</b> <code>threejs</code>: Interactive 3D Scatter Plots and Globes</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="case-study.html"><a href="case-study.html"><i class="fa fa-check"></i><b>10</b> Case Study</a><ul>
<li class="chapter" data-level="10.1" data-path="case-study.html"><a href="case-study.html#case-1-customer-perception-study-for-airline-company"><i class="fa fa-check"></i><b>10.1</b> Case 1: Customer Perception Study for Airline Company</a></li>
<li class="chapter" data-level="10.2" data-path="case-study.html"><a href="case-study.html#case-2-swine-disease-prediction"><i class="fa fa-check"></i><b>10.2</b> Case 2: Swine Disease Prediction</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>11</b> References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">The Science and Art of Data</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="big-data-cloud-platform" class="section level1">
<h1><span class="header-section-number">Chapter 4</span> Big Data Cloud Platform</h1>
<div id="how-data-become-science" class="section level2">
<h2><span class="header-section-number">4.1</span> How Data become Science?</h2>
<p>Data has been a friend of statistician for hundreds of years. Tabulated data are the most familiar format that we use daily. Tabulated data has been stored in pieces of paper, or tapes, or diskettes, or hard drives. Only very recently, with the development of computer hardware, software and algorithms, the volume, variety, and speed of the data suddenly beyond the capacity of traditional statistician. And data becomes a special science with the very first focus on a fundamental question: with huge amount of data, how can we store the data and quick access and process the data. In the past a few years, by utilizing commodity hardware and open source software, a big data ecosystem was created for data storage, data retrieval and parallel computation. Hadoop and Spark have become a popular platform that enable data scientist, statistician and business analyst to access the data and to build models. Programming skills in the big data platform has been the largest gap for statistician to become a successful data scientist. However, with the recent wave of cloud computing, this gap is greatly reduced. Many of the technical details have been pushed to the background and the user interface becomes much easier to learn. Cloud systems also enable quick implementation to the production environment. Now data science is emphasis more on the data itself as well as models and algorithms on top of the data instead of platform and infrastructure.</p>
</div>
<div id="power-of-cluster-of-computers" class="section level2">
<h2><span class="header-section-number">4.2</span> Power of Cluster of Computers</h2>
<p>We are all familiar with our laptop / desktop computers which contain mainly three components to finish computation with data: (1) Hard disk, (2) Memory, and (3) CPU as shown in Figure 41 left. The data and codes are stored in hard disk which has certain features such as relatively slow for read and write and relatively large capacity of around a few TB in today’s market. Memory is relatively fast for read and write but relatively small in capacity in the order of a few dozens of GB in today’s market. CPU is where all the computation is done.</p>
<center>
<img src="http://scientistcafe.com/CE_JSM2017/images/cluster.png" alt="Single computer (left) and a cluster of computers (right)" />
</center>
<p>For statistical software such as R, the amount of data that it can process is limited by the computer’s memory. For a typical computer before year 2000, the memory is less than 1 GB. The memory capacity grows far slower than the availability of the data to analyze. Now it is quite often that we need to analyze data far beyond the capacity of a single computer’s memory, especially in enterprise environment. Meanwhile the computation time is growing faster than linear to solve the same problem (such as regressions) as the data size increases. Using a cluster of computers become a common way to solve big data problem. In Figure 41 (right), a cluster of computers can be viewed as one powerful machine with total memory, hard disk and CPU equivale to the sum of individual computers. It is common to have thousands of nodes for a cluster.</p>
<p>In the past, to use a cluster of computers, users must write special codes such as (such as MPI) to take care of how data is allocated across memory and how the computation is done in a parallel fashion. Luckily with the recent new development, users are leverage a more user-friendly cloud environment for big data analysis. As data is typically beyond the size of one hard disk, the dataset itself is stored across different nodes’ hard disk (i.e. the Hadoop system mentioned below). When we perform analysis, we can assume the needed data is already distributed across many node’s memories in the cluster and algorithm are parallel in nature to leverage corresponding nodes’ CPUs to compute (i.e. the Spark system mentioned below).</p>
</div>
<div id="introduction-of-cloud-environment" class="section level2">
<h2><span class="header-section-number">4.3</span> Introduction of Cloud Environment</h2>
<p>There are many cloud computing environment such as Amazon’s AWS which provides complete list of functions for heavy duty enterprise applications. For example, Netflix runs its business entirely on AWS and Netflix does not own any data centers. For beginners, Databricks provides an easy to use cloud system for learning purpose. Databricks is a company founded by the creators of Apache Spark and it provides a user-friendly web-based notebook environment that can create Hadoop/Spark/GPU cluster on the fly and run R/Python/Scala/SQL. We will use Databricks’ community edition to run demos in this book. Please note the content of this section is adopted from the following web pages:</p>
<ul>
<li><a href="https://docs.databricks.com/user-guide/faq/sparklyr.html" class="uri">https://docs.databricks.com/user-guide/faq/sparklyr.html</a></li>
<li><a href="http://spark.rstudio.com/index.html" class="uri">http://spark.rstudio.com/index.html</a></li>
</ul>
</div>
<div id="open-account-and-create-a-cluster" class="section level2">
<h2><span class="header-section-number">4.4</span> Open Account and Create a Cluster</h2>
<p>Anyone can apply for a community edition for free through <a href="https://databricks.com/try-databricks" class="uri">https://databricks.com/try-databricks</a> and a short YouTube video illustrates the application process can be found <a href="https://youtu.be/vx-3-htFvrg" class="uri">https://youtu.be/vx-3-htFvrg</a>. Another short YouTube video shows how to create a cluster for a cloud computation environment and create a R notebook to run R codes which can be found at <a href="https://youtu.be/0HFujX3t6TU" class="uri">https://youtu.be/0HFujX3t6TU</a>. In fact, you can run Python/R/Scala/SQL cells, as well as markdown cells, in the same notebook by include a keyword at the beginning of each cell that we will discuss later.</p>
</div>
<div id="r-notebook" class="section level2">
<h2><span class="header-section-number">4.5</span> R Notebook</h2>
<p>In last section video, we just created an R notebook. For an R notebook, it contains multiple cells and by default the content within each cell are R scripts. Usually each cell is a well-managed a few lines of codes that accomplish a specific task. For example, Figure 42 shows the default cell for an R notebook for cell 1. We can type in R scripts and comments same as we are using R console. By default, only the result from the last line will be shown following the cell. However, you can use print() function to output results for any lines. If we move the mouse to middle of lower edge of the cell below the results, a “+” symbol will show up and clicking on the symbol will insert a new cell below. When you click any area within a cell, it will make it editable and you will see a few icons on the top right corn of the cell where you can run the cell, as well as add a cell below or above, copy cell, cut cell, high cell etc. One quick way to run the cell is Shift+Enter when the cell is chosen. You will become familiar with the notebook environment quickly.</p>
<center>
<img src="http://scientistcafe.com/CE_JSM2017/images/rnotebook.png" alt="R notebook default cell with R scripts" />
</center>
</div>
<div id="markdown-cells" class="section level2">
<h2><span class="header-section-number">4.6</span> Markdown Cells</h2>
<p>For an R notebook, every cell by default will contain R scripts. But if we put %md, %sql or %python at the first line of a cell, that cell becomes Markdown cell, SQL script cell and Python script cell accordingly. For example, Figure 43 shows a markdown cell with scripts and the actual appearance when exits editing mode. Markdown cell provide a straightforward way to descript what each cell is doing as well as what the entire notebook is about. It is a better way than simple comment within in the code.</p>
<center>
<img src="http://scientistcafe.com/CE_JSM2017/images/markdown_databrick.png" alt="An example of Markdown cell with scripts at top and actual appearance at bottom" />
</center>
</div>
<div id="leverage-hadoop-and-spark-parallel-using-r-notebook" class="section level2">
<h2><span class="header-section-number">4.7</span> Leverage Hadoop and Spark Parallel using R Notebook</h2>
<p>R is a powerful tool for data analysis given the data can be fit into memory. Because of the memory bounded dataset limit, R itself cannot be used directly for big data analysis where the data is likely stored in Hadoop and Spark system. By leverage sparklyr package created by RStudio, we can use Databricks’ R notebook to analyze data stored in Spark system where the data are stored across different nodes and computation are parallel in nature to use the collection of memory units across all nodes. And the process is relative simple. In this section, we will illustrate how to use Databricks’ R notebook for big data analysis on top of Spark environment through <code>sparklyr</code> package.</p>
<div id="library-installation" class="section level3">
<h3><span class="header-section-number">4.7.1</span> Library Installation</h3>
<p>First, we need to install sparklyr package which enables the connection between master or local node to Spark cluster environments. As it will install more than 10 dependencies, it may take more than 5 minutes to finish. Be patient while it is installing! Once the installation finishes, load the sparklyr package as illustrated by the following code:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Installing sparklyr takes a few minutes, </span>
<span class="co"># because it installs +10 dependencies.</span>

if (!<span class="kw">require</span>(<span class="st">&quot;sparklyr&quot;</span>)) {
  <span class="kw">install.packages</span>(<span class="st">&quot;sparklyr&quot;</span>)  
}

<span class="co"># Load sparklyr package.</span>
<span class="kw">library</span>(sparklyr)</code></pre></div>
</div>
<div id="create-connection" class="section level3">
<h3><span class="header-section-number">4.7.2</span> Create Connection</h3>
<p>Once the library is loaded, we need create a Spark Connection to link master / local node to Spark environment. Here we use the “databricks” option for parameter method which is specific for databricks’ system. In enterprise environment, please consult your administrator for details. The created Spark Connection (i.e. sc) will be the pipe that connect master / local / terminal to the Spark Cluster. We can think of the web interface / terminal is running on a master node which has its local memory and CPU. The Spark Connection can be established with:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># create a sparklyr connection </span>
sc &lt;-<span class="st"> </span><span class="kw">spark_connect</span>(<span class="dt">method =</span> <span class="st">&quot;databricks&quot;</span>)</code></pre></div>
</div>
<div id="sample-dataset" class="section level3">
<h3><span class="header-section-number">4.7.3</span> Sample Dataset</h3>
<p>To simplify the learning process, let us use a very familiar dataset: the iris dataset. It is part of the dplyr library and let’s load that library to use the iris data frame. Here the iris dataset is still in the local node where the R notebook is running on. And we can see that the first a few lines of the iris dataset below the code after running:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(dplyr)
<span class="kw">head</span>(iris)</code></pre></div>
</div>
<div id="important---copy-data-to-spark-environment" class="section level3">
<h3><span class="header-section-number">4.7.4</span> IMPORTANT - Copy Data to Spark Environment</h3>
<p>In real applications, your data is usually very big and cannot fit into one hard disk and it is very likely your data is already in Hadoop/Spark ecosystem. You can use SparkDataFrame to analyze your data in Spark system directly. Here, we illustrate how to copy a local dataset to Spark environment and then work on that dataset in the Spark system. As we have already created the Spark Connection sc, it is fairly simple to copy data to spark system by sdf_copy_to() function as below:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">iris_tbl &lt;-<span class="st"> </span><span class="kw">sdf_copy_to</span>(<span class="dt">sc =</span> sc, <span class="dt">x =</span> iris, <span class="dt">overwrite =</span> T)</code></pre></div>
<p>The above one line code copies iris dataset from local node to Spark cluster environment where sc is the Spark Connection we just created; x is the data frame that we want to copy; and overwrite is the option whether we want to overwrite the target object if the same name SparkDataFrame exists in the Spark environment. Finally sdf_copy_to() function will return an R object wrapping the copied SparkDataFrame. So irir_tbl can be used to refer to the iris SparkDataFrame.</p>
<p>To check whether the iris dataset was copied to Spark environment successfully or not, we can use src_tbls( ) function to the Spark Connection (sc):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">src_tbls</span>(sc) ## code to return all the dataframes associated with sc</code></pre></div>
</div>
<div id="analyzing-the-data" class="section level3">
<h3><span class="header-section-number">4.7.5</span> Analyzing the Data</h3>
<p>Now we have successfully copied the iris dataset to the Spark environment as a SparkDataFrame. And iris_tbl is an R object wrapping the iris SparkDataFrame and we can use iris_tbl to refer the iris dataset in the Spark system (i.e. the iris SparkDataFrame). With the sparklyr packages, we can use many functions in dplyr to SparkDataFrame directly through iris_tbl, same as we are applying dplyr functions to a local R data frame in our laptop. For example, we can use %&gt;% operator to pass iris_tbl to count( ) function:</p>
<pre><code>iris_tbl %&gt;% count</code></pre>
<p>or using the head( ) function to return to return the first a few rows in iris_tbl:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">head</span>(iris_tbl)</code></pre></div>
<p>or more advanced data manipulation directly to iris_tbl:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">iris_tbl %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">Sepal_Width =</span> <span class="kw">ROUND</span>(Sepal_Width *<span class="st"> </span><span class="dv">2</span>) /<span class="st"> </span><span class="dv">2</span>) %&gt;%<span class="st"> </span><span class="co"># Bucketizing Sepal_Width</span>
<span class="st">  </span><span class="kw">group_by</span>(Species, Sepal_Width) %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">count =</span> <span class="kw">n</span>(), <span class="dt">Sepal_Length =</span> <span class="kw">mean</span>(Sepal_Length), <span class="dt">stdev =</span> <span class="kw">sd</span>(Sepal_Length))</code></pre></div>
</div>
<div id="collect-results-back-to-master-node" class="section level3">
<h3><span class="header-section-number">4.7.6</span> Collect Results Back to Master Node</h3>
<p>Even though we can run many of the dplyr functions on SparkDataFrame, we cannot apply functions from other packages to SparkDataFrame direction (such as ggplot()). For functions that can only work on local R data frames, we must copy the SparkDataFrame back to the local node. To copy SparkDataFrame back to the local node, we use the collect() function where the argument to it is the name of the SparkDataFrame. The following code collect() the results of a few operations and assign the collected data to iris_summary variable:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">iris_summary &lt;-<span class="st"> </span>
<span class="st">  </span>iris_tbl %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">Sepal_Width =</span> <span class="kw">ROUND</span>(Sepal_Width *<span class="st"> </span><span class="dv">2</span>) /<span class="st"> </span><span class="dv">2</span>) %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>(Species, Sepal_Width) %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">summarize</span>(<span class="dt">count =</span> <span class="kw">n</span>(), <span class="dt">Sepal_Length =</span> <span class="kw">mean</span>(Sepal_Length), <span class="dt">stdev =</span> <span class="kw">sd</span>(Sepal_Length)) %&gt;%<span class="st">  </span>
<span class="st">  </span>collect</code></pre></div>
<p>Now iris_summary is a local variable to the R notebook and we can use all R packages and functions to it. In the following code, we will apply ggplot() to it, exactly the same as a stand along R console:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(ggplot2)
<span class="kw">ggplot</span>(iris_summary, <span class="kw">aes</span>(Sepal_Width, Sepal_Length, <span class="dt">color =</span> Species)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="dt">size =</span> <span class="fl">1.2</span>) +
<span class="st">  </span><span class="kw">geom_errorbar</span>(<span class="kw">aes</span>(<span class="dt">ymin =</span> Sepal_Length -<span class="st"> </span>stdev, <span class="dt">ymax =</span> Sepal_Length +<span class="st"> </span>stdev), <span class="dt">width =</span> <span class="fl">0.05</span>) +
<span class="st">  </span><span class="kw">geom_text</span>(<span class="kw">aes</span>(<span class="dt">label =</span> count), <span class="dt">vjust =</span> -<span class="fl">0.2</span>, <span class="dt">hjust =</span> <span class="fl">1.2</span>, <span class="dt">color =</span> <span class="st">&quot;black&quot;</span>) +
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">legend.position=</span><span class="st">&quot;top&quot;</span>)</code></pre></div>
</div>
<div id="fit-regression-to-sparkdataframe" class="section level3">
<h3><span class="header-section-number">4.7.7</span> Fit Regression to SparkDataFrame</h3>
<p>One of the largest advantage is that, within Spark system, there are already many statistical and machine learning algorithms developed to run parallel across many CPUs with data across many memory units. So, we can easily fit a linear regression for big dataset far beyond the memory limit of one single computer. Below is an illustration of how to fit a linear regression to SparkDataFrame using R notebook:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit1 &lt;-<span class="st">  </span><span class="kw">ml_linear_regression</span>(<span class="dt">x =</span> iris_tbl, <span class="dt">response =</span> <span class="st">&quot;Sepal_Length&quot;</span>, 
                              <span class="dt">features =</span> <span class="kw">c</span>(<span class="st">&quot;Sepal_Width&quot;</span>, <span class="st">&quot;Petal_Length&quot;</span>, <span class="st">&quot;Petal_Width&quot;</span>))
<span class="kw">summary</span>(fit1)</code></pre></div>
<p>In the above code, x is the R object wrapping the SparkDataFrame; response is the y-variable, features is the collection of explanatory variables.</p>
</div>
<div id="fit-a-k-means-cluster" class="section level3">
<h3><span class="header-section-number">4.7.8</span> Fit a K-means Cluster</h3>
<p>Through sparkly package, we can use R notebook to access many Spark Machine Learning Library (MLlib) algorithms such as linear regression, logistic regression, Survival Regression, Generalized Linear Regression, Decision Trees, Random Forests, Gradient-Boosted Trees, Principal Components Analysis, Naive-Bayes, K-Means Clustering and a few other methods. Below codes fit a k-means cluster algorithm:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## Now fit a k-means clustering using iris_tbl data 
## with only two out of four features in iris_tbl
fit2 &lt;-<span class="st"> </span><span class="kw">ml_kmeans</span>(<span class="dt">x =</span> iris_tbl, <span class="dt">centers =</span> <span class="dv">3</span>, 
                  <span class="dt">features =</span> <span class="kw">c</span>(<span class="st">&quot;Petal_Length&quot;</span>, <span class="st">&quot;Petal_Width&quot;</span>))

<span class="co"># print our model fit</span>
<span class="kw">print</span>(fit2)</code></pre></div>
<p>After the k-means model is fit, we can apply the model to predict other datasets through sdf_predict() function. Below code apply the model to iris_tbl again to predict and then the results are collected back to local variable prediction through collect() function:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">prediction =<span class="st"> </span><span class="kw">collect</span>(<span class="kw">sdf_predict</span>(fit2, iris_tbl)) </code></pre></div>
<p>As prediction is a local variable, we can apply any R functions from any libraries to it. For example:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">prediction  %&gt;%
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(Petal_Length, Petal_Width)) +
<span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(Petal_Width, Petal_Length, <span class="dt">col =</span> <span class="kw">factor</span>(prediction +<span class="st"> </span><span class="dv">1</span>)),
             <span class="dt">size =</span> <span class="dv">2</span>, <span class="dt">alpha =</span> <span class="fl">0.5</span>) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">data =</span> fit2$centers, <span class="kw">aes</span>(Petal_Width, Petal_Length),
             <span class="dt">col =</span> scales::<span class="kw">muted</span>(<span class="kw">c</span>(<span class="st">&quot;red&quot;</span>, <span class="st">&quot;green&quot;</span>, <span class="st">&quot;blue&quot;</span>)),
             <span class="dt">pch =</span> <span class="st">&#39;x&#39;</span>, <span class="dt">size =</span> <span class="dv">12</span>) +
<span class="st">  </span><span class="kw">scale_color_discrete</span>(<span class="dt">name =</span> <span class="st">&quot;Predicted Cluster&quot;</span>,
                       <span class="dt">labels =</span> <span class="kw">paste</span>(<span class="st">&quot;Cluster&quot;</span>, <span class="dv">1</span>:<span class="dv">3</span>)) +
<span class="st">  </span><span class="kw">labs</span>(
    <span class="dt">x =</span> <span class="st">&quot;Petal Length&quot;</span>,
    <span class="dt">y =</span> <span class="st">&quot;Petal Width&quot;</span>,
    <span class="dt">title =</span> <span class="st">&quot;K-Means Clustering&quot;</span>,
    <span class="dt">subtitle =</span> <span class="st">&quot;Use Spark.ML to predict cluster membership with the iris dataset.&quot;</span>
  )</code></pre></div>
</div>
<div id="summary" class="section level3">
<h3><span class="header-section-number">4.7.9</span> Summary</h3>
<p>In the above a few sub-sections, we illustrated (1) the relationship between master / local node and Spark Clusters; (2) how to copy a local data frame to a SparkDataFrame (please note if your data is already in Spark environment, there is no need to copy. This is likely to be the case for enterprise environment); (3) how to manipulate SparkDataFrame through dplyr functions with the installation of sparklyr package; (4) how to fit statistical and machine learning models to SparkDataFrame; and (5) how to collect information from SparkDataFrame back to a local data frame for future analysis. These procedures are pretty much covered the basis of big data analysis that a data scientist need to know. The above steps are published as an R notebook: <a href="https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/2961012104553482/3725396058299890/1806228006848429/latest.html" class="uri">https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/2961012104553482/3725396058299890/1806228006848429/latest.html</a></p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="introduction-to-the-data.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="soft-skills-for-data-scientists.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": null,
"download": ["CE_JSM2017.pdf", "CE_JSM2017.epub", "CE_JSM2017.mobi"],
"toc": {
"collapse": "subsection",
"scroll_highlight": true
},
"toolbar": {
"position": "fixed"
},
"search": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
