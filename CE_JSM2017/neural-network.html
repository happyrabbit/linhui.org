<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Introduction to Data Science</title>
  <meta name="description" content="This is the handouts for CE course at JSM 2017">
  <meta name="generator" content="bookdown 0.4 and GitBook 2.6.7">

  <meta property="og:title" content="Introduction to Data Science" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is the handouts for CE course at JSM 2017" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Introduction to Data Science" />
  
  <meta name="twitter:description" content="This is the handouts for CE course at JSM 2017" />
  

<meta name="author" content="Hui Lin and Ming Li">


<meta name="date" content="2017-08-19">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="tree-based-methods.html">
<link rel="next" href="dynamicreproducible-report.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-0.8/htmlwidgets.js"></script>
<script src="libs/datatables-binding-0.2/datatables.js"></script>
<link href="libs/dt-core-1.10.12/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.10.12/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.10.12/js/jquery.dataTables.min.js"></script>
<link href="libs/leaflet-0.7.3/leaflet.css" rel="stylesheet" />
<script src="libs/leaflet-0.7.3/leaflet.js"></script>
<link href="libs/leafletfix-1.0.0/leafletfix.css" rel="stylesheet" />
<script src="libs/leaflet-binding-1.0.1/leaflet.js"></script>
<link href="libs/dygraphs-1.1.1/dygraph.css" rel="stylesheet" />
<script src="libs/dygraphs-1.1.1/dygraph-combined.js"></script>
<script src="libs/moment-2.8.4/moment.js"></script>
<script src="libs/moment-timezone-0.2.5/moment-timezone-with-data.js"></script>
<script src="libs/moment-fquarter-1.0.0/moment-fquarter.min.js"></script>
<script src="libs/dygraphs-binding-1.1.1.4/dygraphs.js"></script>
<script src="libs/proj4js-2.3.15/proj4.js"></script>
<link href="libs/highcharts-5.0.6/css/motion.css" rel="stylesheet" />
<script src="libs/highcharts-5.0.6/highstock.js"></script>
<script src="libs/highcharts-5.0.6/highcharts-3d.js"></script>
<script src="libs/highcharts-5.0.6/highcharts-more.js"></script>
<script src="libs/highcharts-5.0.6/modules/annotations.js"></script>
<script src="libs/highcharts-5.0.6/modules/broken-axis.js"></script>
<script src="libs/highcharts-5.0.6/modules/data.js"></script>
<script src="libs/highcharts-5.0.6/modules/drilldown.js"></script>
<script src="libs/highcharts-5.0.6/modules/exporting.js"></script>
<script src="libs/highcharts-5.0.6/modules/funnel.js"></script>
<script src="libs/highcharts-5.0.6/modules/heatmap.js"></script>
<script src="libs/highcharts-5.0.6/modules/map.js"></script>
<script src="libs/highcharts-5.0.6/modules/no-data-to-display.js"></script>
<script src="libs/highcharts-5.0.6/modules/offline-exporting.js"></script>
<script src="libs/highcharts-5.0.6/modules/solid-gauge.js"></script>
<script src="libs/highcharts-5.0.6/modules/treemap.js"></script>
<script src="libs/highcharts-5.0.6/plugins/annotations.js"></script>
<script src="libs/highcharts-5.0.6/plugins/draggable-legend.js"></script>
<script src="libs/highcharts-5.0.6/plugins/draggable-points.js"></script>
<script src="libs/highcharts-5.0.6/plugins/export-csv.js"></script>
<script src="libs/highcharts-5.0.6/plugins/grouped-categories.js"></script>
<script src="libs/highcharts-5.0.6/plugins/motion.js"></script>
<script src="libs/highcharts-5.0.6/plugins/pattern-fill-v2.js"></script>
<script src="libs/highcharts-5.0.6/plugins/tooltip-delay.js"></script>
<script src="libs/highcharts-5.0.6/custom/reset.js"></script>
<script src="libs/highcharts-5.0.6/custom/symbols-extra.js"></script>
<script src="libs/highcharts-5.0.6/custom/text-symbols.js"></script>
<link href="libs/fontawesome-4.5.0/font-awesome.min.css" rel="stylesheet" />
<link href="libs/htmlwdgtgrid-1/htmlwdgtgrid.css" rel="stylesheet" />
<script src="libs/highchart-binding-0.5.0/highchart.js"></script>
<link href="libs/bokehjs-0.11.1/bokeh.min.css" rel="stylesheet" />
<script src="libs/bokehjs-0.11.1/bokeh.min.js"></script>
<script src="libs/rbokeh-binding-0.4.2/rbokeh.js"></script>
<script src="libs/d3-3.5.12/d3.min.js"></script>
<link href="libs/metrics-graphics-2.7.0/dist/metricsgraphics.css" rel="stylesheet" />
<link href="libs/metrics-graphics-2.7.0/dist/mg_regions.css" rel="stylesheet" />
<script src="libs/metrics-graphics-2.7.0/dist/metricsgraphics.min.js"></script>
<script src="libs/metrics-graphics-2.7.0/dist/mg_regions.js"></script>
<script src="libs/metricsgraphics-binding-0.9.0/metricsgraphics.js"></script>
<script src="libs/forceNetwork-binding-0.2.11/forceNetwork.js"></script>
<script src="libs/threejs-70/three.min.js"></script>
<script src="libs/threejs-70/Detector.js"></script>
<script src="libs/threejs-70/Projector.js"></script>
<script src="libs/threejs-70/CanvasRenderer.js"></script>
<script src="libs/globe-binding-0.2.2/globe.js"></script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Copyright Statement</a></li>
<li class="chapter" data-level="" data-path="about-the-authors.html"><a href="about-the-authors.html"><i class="fa fa-check"></i>About the Authors</a></li>
<li class="chapter" data-level="" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="1" data-path="the-art-of-data-science.html"><a href="the-art-of-data-science.html"><i class="fa fa-check"></i><b>1</b> The art of data science</a><ul>
<li class="chapter" data-level="1.1" data-path="the-art-of-data-science.html"><a href="the-art-of-data-science.html#what-is-data-science"><i class="fa fa-check"></i><b>1.1</b> What is data science?</a></li>
<li class="chapter" data-level="1.2" data-path="the-art-of-data-science.html"><a href="the-art-of-data-science.html#is-it-science-totally"><i class="fa fa-check"></i><b>1.2</b> Is it science? Totally?</a></li>
<li class="chapter" data-level="1.3" data-path="the-art-of-data-science.html"><a href="the-art-of-data-science.html#what-kind-of-questions-can-data-science-solve"><i class="fa fa-check"></i><b>1.3</b> What kind of questions can data science solve?</a><ul>
<li class="chapter" data-level="1.3.1" data-path="the-art-of-data-science.html"><a href="the-art-of-data-science.html#prerequisites"><i class="fa fa-check"></i><b>1.3.1</b> Prerequisites</a></li>
<li class="chapter" data-level="1.3.2" data-path="the-art-of-data-science.html"><a href="the-art-of-data-science.html#problem-type"><i class="fa fa-check"></i><b>1.3.2</b> Problem type</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="the-art-of-data-science.html"><a href="the-art-of-data-science.html#what-are-the-required-skills-for-data-scientist"><i class="fa fa-check"></i><b>1.4</b> What are the required skills for data scientist?</a></li>
<li class="chapter" data-level="1.5" data-path="the-art-of-data-science.html"><a href="the-art-of-data-science.html#types-of-learning"><i class="fa fa-check"></i><b>1.5</b> Types of Learning</a></li>
<li class="chapter" data-level="1.6" data-path="the-art-of-data-science.html"><a href="the-art-of-data-science.html#types-of-algorithm"><i class="fa fa-check"></i><b>1.6</b> Types of Algorithm</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="soft-skills-for-data-scientists.html"><a href="soft-skills-for-data-scientists.html"><i class="fa fa-check"></i><b>2</b> Soft Skills for Data Scientists</a><ul>
<li class="chapter" data-level="2.1" data-path="soft-skills-for-data-scientists.html"><a href="soft-skills-for-data-scientists.html#comparison-between-statistician-and-data-scientist"><i class="fa fa-check"></i><b>2.1</b> Comparison between Statistician and Data Scientist</a></li>
<li class="chapter" data-level="2.2" data-path="soft-skills-for-data-scientists.html"><a href="soft-skills-for-data-scientists.html#where-data-science-team-fits"><i class="fa fa-check"></i><b>2.2</b> Where Data Science Team Fits?</a></li>
<li class="chapter" data-level="2.3" data-path="soft-skills-for-data-scientists.html"><a href="soft-skills-for-data-scientists.html#beyond-data-and-analytics"><i class="fa fa-check"></i><b>2.3</b> Beyond Data and Analytics</a></li>
<li class="chapter" data-level="2.4" data-path="soft-skills-for-data-scientists.html"><a href="soft-skills-for-data-scientists.html#data-scientist-as-a-leader"><i class="fa fa-check"></i><b>2.4</b> Data Scientist as a Leader</a></li>
<li class="chapter" data-level="2.5" data-path="soft-skills-for-data-scientists.html"><a href="soft-skills-for-data-scientists.html#three-pillars-of-knowledge"><i class="fa fa-check"></i><b>2.5</b> Three Pillars of Knowledge</a></li>
<li class="chapter" data-level="2.6" data-path="soft-skills-for-data-scientists.html"><a href="soft-skills-for-data-scientists.html#common-pitfalls-of-data-science-projects"><i class="fa fa-check"></i><b>2.6</b> Common Pitfalls of Data Science Projects</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="introduction-to-the-data.html"><a href="introduction-to-the-data.html"><i class="fa fa-check"></i><b>3</b> Introduction to the data</a><ul>
<li class="chapter" data-level="3.1" data-path="introduction-to-the-data.html"><a href="introduction-to-the-data.html#customer-data-for-clothing-company"><i class="fa fa-check"></i><b>3.1</b> Customer Data for Clothing Company</a></li>
<li class="chapter" data-level="3.2" data-path="introduction-to-the-data.html"><a href="introduction-to-the-data.html#customer-satisfaction-survey-data-from-airline-company"><i class="fa fa-check"></i><b>3.2</b> Customer Satisfaction Survey Data from Airline Company</a></li>
<li class="chapter" data-level="3.3" data-path="introduction-to-the-data.html"><a href="introduction-to-the-data.html#swine-disease-breakout-data"><i class="fa fa-check"></i><b>3.3</b> Swine Disease Breakout Data</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="big-data-cloud-platform.html"><a href="big-data-cloud-platform.html"><i class="fa fa-check"></i><b>4</b> Big Data Cloud Platform</a><ul>
<li class="chapter" data-level="4.1" data-path="big-data-cloud-platform.html"><a href="big-data-cloud-platform.html#how-data-becomes-science"><i class="fa fa-check"></i><b>4.1</b> How Data becomes Science?</a></li>
<li class="chapter" data-level="4.2" data-path="big-data-cloud-platform.html"><a href="big-data-cloud-platform.html#power-of-cluster-of-computers"><i class="fa fa-check"></i><b>4.2</b> Power of Cluster of Computers</a></li>
<li class="chapter" data-level="4.3" data-path="big-data-cloud-platform.html"><a href="big-data-cloud-platform.html#evolution-of-clustering-computing"><i class="fa fa-check"></i><b>4.3</b> Evolution of Clustering Computing</a><ul>
<li class="chapter" data-level="4.3.1" data-path="big-data-cloud-platform.html"><a href="big-data-cloud-platform.html#hadoop"><i class="fa fa-check"></i><b>4.3.1</b> Hadoop</a></li>
<li class="chapter" data-level="4.3.2" data-path="big-data-cloud-platform.html"><a href="big-data-cloud-platform.html#spark"><i class="fa fa-check"></i><b>4.3.2</b> Spark</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="big-data-cloud-platform.html"><a href="big-data-cloud-platform.html#introduction-of-cloud-environment"><i class="fa fa-check"></i><b>4.4</b> Introduction of Cloud Environment</a></li>
<li class="chapter" data-level="4.5" data-path="big-data-cloud-platform.html"><a href="big-data-cloud-platform.html#open-account-and-create-a-cluster"><i class="fa fa-check"></i><b>4.5</b> Open Account and Create a Cluster</a></li>
<li class="chapter" data-level="4.6" data-path="big-data-cloud-platform.html"><a href="big-data-cloud-platform.html#r-notebook"><i class="fa fa-check"></i><b>4.6</b> R Notebook</a></li>
<li class="chapter" data-level="4.7" data-path="big-data-cloud-platform.html"><a href="big-data-cloud-platform.html#markdown-cells"><i class="fa fa-check"></i><b>4.7</b> Markdown Cells</a></li>
<li class="chapter" data-level="4.8" data-path="big-data-cloud-platform.html"><a href="big-data-cloud-platform.html#leverage-hadoop-and-spark-parallel-using-r-notebook"><i class="fa fa-check"></i><b>4.8</b> Leverage Hadoop and Spark Parallel using R Notebook</a><ul>
<li class="chapter" data-level="4.8.1" data-path="big-data-cloud-platform.html"><a href="big-data-cloud-platform.html#library-installation"><i class="fa fa-check"></i><b>4.8.1</b> Library Installation</a></li>
<li class="chapter" data-level="4.8.2" data-path="big-data-cloud-platform.html"><a href="big-data-cloud-platform.html#create-connection"><i class="fa fa-check"></i><b>4.8.2</b> Create Connection</a></li>
<li class="chapter" data-level="4.8.3" data-path="big-data-cloud-platform.html"><a href="big-data-cloud-platform.html#sample-dataset"><i class="fa fa-check"></i><b>4.8.3</b> Sample Dataset</a></li>
<li class="chapter" data-level="4.8.4" data-path="big-data-cloud-platform.html"><a href="big-data-cloud-platform.html#important---copy-data-to-spark-environment"><i class="fa fa-check"></i><b>4.8.4</b> IMPORTANT - Copy Data to Spark Environment</a></li>
<li class="chapter" data-level="4.8.5" data-path="big-data-cloud-platform.html"><a href="big-data-cloud-platform.html#analyzing-the-data"><i class="fa fa-check"></i><b>4.8.5</b> Analyzing the Data</a></li>
<li class="chapter" data-level="4.8.6" data-path="big-data-cloud-platform.html"><a href="big-data-cloud-platform.html#collect-results-back-to-master-node"><i class="fa fa-check"></i><b>4.8.6</b> Collect Results Back to Master Node</a></li>
<li class="chapter" data-level="4.8.7" data-path="big-data-cloud-platform.html"><a href="big-data-cloud-platform.html#fit-regression-to-sparkdataframe"><i class="fa fa-check"></i><b>4.8.7</b> Fit Regression to SparkDataFrame</a></li>
<li class="chapter" data-level="4.8.8" data-path="big-data-cloud-platform.html"><a href="big-data-cloud-platform.html#fit-a-k-means-cluster"><i class="fa fa-check"></i><b>4.8.8</b> Fit a K-means Cluster</a></li>
<li class="chapter" data-level="4.8.9" data-path="big-data-cloud-platform.html"><a href="big-data-cloud-platform.html#summary"><i class="fa fa-check"></i><b>4.8.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="4.9" data-path="big-data-cloud-platform.html"><a href="big-data-cloud-platform.html#databases-and-sql"><i class="fa fa-check"></i><b>4.9</b> Databases and SQL</a><ul>
<li class="chapter" data-level="4.9.1" data-path="big-data-cloud-platform.html"><a href="big-data-cloud-platform.html#database-table-and-view"><i class="fa fa-check"></i><b>4.9.1</b> Database, Table and View</a></li>
<li class="chapter" data-level="4.9.2" data-path="big-data-cloud-platform.html"><a href="big-data-cloud-platform.html#sample-tables"><i class="fa fa-check"></i><b>4.9.2</b> Sample Tables</a></li>
<li class="chapter" data-level="4.9.3" data-path="big-data-cloud-platform.html"><a href="big-data-cloud-platform.html#basic-sql-statement"><i class="fa fa-check"></i><b>4.9.3</b> Basic SQL Statement</a></li>
<li class="chapter" data-level="4.9.4" data-path="big-data-cloud-platform.html"><a href="big-data-cloud-platform.html#simple-select-statement"><i class="fa fa-check"></i><b>4.9.4</b> Simple SELECT Statement</a></li>
<li class="chapter" data-level="4.9.5" data-path="big-data-cloud-platform.html"><a href="big-data-cloud-platform.html#aggregation-functions-and-group-by"><i class="fa fa-check"></i><b>4.9.5</b> Aggregation Functions and GROUP BY</a></li>
<li class="chapter" data-level="4.9.6" data-path="big-data-cloud-platform.html"><a href="big-data-cloud-platform.html#join-multiple-tables"><i class="fa fa-check"></i><b>4.9.6</b> Join Multiple Tables</a></li>
<li class="chapter" data-level="4.9.7" data-path="big-data-cloud-platform.html"><a href="big-data-cloud-platform.html#add-more-content-into-a-table"><i class="fa fa-check"></i><b>4.9.7</b> Add More Content into a Table</a></li>
<li class="chapter" data-level="4.9.8" data-path="big-data-cloud-platform.html"><a href="big-data-cloud-platform.html#advanced-topics-in-database"><i class="fa fa-check"></i><b>4.9.8</b> Advanced Topics in Database</a></li>
</ul></li>
<li class="chapter" data-level="4.10" data-path="big-data-cloud-platform.html"><a href="big-data-cloud-platform.html#other-useful-topics"><i class="fa fa-check"></i><b>4.10</b> Other Useful Topics</a><ul>
<li class="chapter" data-level="4.10.1" data-path="big-data-cloud-platform.html"><a href="big-data-cloud-platform.html#linux-operation-system"><i class="fa fa-check"></i><b>4.10.1</b> Linux Operation System</a></li>
<li class="chapter" data-level="4.10.2" data-path="big-data-cloud-platform.html"><a href="big-data-cloud-platform.html#visualization"><i class="fa fa-check"></i><b>4.10.2</b> Visualization</a></li>
<li class="chapter" data-level="4.10.3" data-path="big-data-cloud-platform.html"><a href="big-data-cloud-platform.html#gpu"><i class="fa fa-check"></i><b>4.10.3</b> GPU</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="data-pre-processing.html"><a href="data-pre-processing.html"><i class="fa fa-check"></i><b>5</b> Data Pre-processing</a><ul>
<li class="chapter" data-level="5.1" data-path="data-pre-processing.html"><a href="data-pre-processing.html#data-cleaning"><i class="fa fa-check"></i><b>5.1</b> Data Cleaning</a></li>
<li class="chapter" data-level="5.2" data-path="data-pre-processing.html"><a href="data-pre-processing.html#missing-values"><i class="fa fa-check"></i><b>5.2</b> Missing Values</a><ul>
<li class="chapter" data-level="5.2.1" data-path="data-pre-processing.html"><a href="data-pre-processing.html#impute-missing-values-with-medianmode"><i class="fa fa-check"></i><b>5.2.1</b> Impute missing values with median/mode</a></li>
<li class="chapter" data-level="5.2.2" data-path="data-pre-processing.html"><a href="data-pre-processing.html#k-nearest-neighbors"><i class="fa fa-check"></i><b>5.2.2</b> K-nearest neighbors</a></li>
<li class="chapter" data-level="5.2.3" data-path="data-pre-processing.html"><a href="data-pre-processing.html#bagging-tree"><i class="fa fa-check"></i><b>5.2.3</b> Bagging Tree</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="data-pre-processing.html"><a href="data-pre-processing.html#centering-and-scaling"><i class="fa fa-check"></i><b>5.3</b> Centering and Scaling</a></li>
<li class="chapter" data-level="5.4" data-path="data-pre-processing.html"><a href="data-pre-processing.html#resolve-skewness"><i class="fa fa-check"></i><b>5.4</b> Resolve Skewness</a></li>
<li class="chapter" data-level="5.5" data-path="data-pre-processing.html"><a href="data-pre-processing.html#resolve-outliers"><i class="fa fa-check"></i><b>5.5</b> Resolve Outliers</a></li>
<li class="chapter" data-level="5.6" data-path="data-pre-processing.html"><a href="data-pre-processing.html#collinearity"><i class="fa fa-check"></i><b>5.6</b> Collinearity</a></li>
<li class="chapter" data-level="5.7" data-path="data-pre-processing.html"><a href="data-pre-processing.html#sparse-variables"><i class="fa fa-check"></i><b>5.7</b> Sparse Variables</a></li>
<li class="chapter" data-level="5.8" data-path="data-pre-processing.html"><a href="data-pre-processing.html#re-encode-dummy-variables"><i class="fa fa-check"></i><b>5.8</b> Re-encode Dummy Variables</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="data-wrangling.html"><a href="data-wrangling.html"><i class="fa fa-check"></i><b>6</b> Data Wrangling</a><ul>
<li class="chapter" data-level="6.1" data-path="data-wrangling.html"><a href="data-wrangling.html#read-and-write-data"><i class="fa fa-check"></i><b>6.1</b> Read and write data</a><ul>
<li class="chapter" data-level="6.1.1" data-path="data-wrangling.html"><a href="data-wrangling.html#readr"><i class="fa fa-check"></i><b>6.1.1</b> <code>readr</code></a></li>
<li class="chapter" data-level="6.1.2" data-path="data-wrangling.html"><a href="data-wrangling.html#data.table-enhanced-data.frame"><i class="fa fa-check"></i><b>6.1.2</b> <code>data.table</code>— enhanced <code>data.frame</code></a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="data-wrangling.html"><a href="data-wrangling.html#summarize-data"><i class="fa fa-check"></i><b>6.2</b> Summarize data</a><ul>
<li class="chapter" data-level="6.2.1" data-path="data-wrangling.html"><a href="data-wrangling.html#apply-lapply-and-sapply-in-base-r"><i class="fa fa-check"></i><b>6.2.1</b> <code>apply()</code>, <code>lapply()</code> and <code>sapply()</code> in base R</a></li>
<li class="chapter" data-level="6.2.2" data-path="data-wrangling.html"><a href="data-wrangling.html#ddply-in-plyr-package"><i class="fa fa-check"></i><b>6.2.2</b> <code>ddply()</code> in <code>plyr</code> package</a></li>
<li class="chapter" data-level="6.2.3" data-path="data-wrangling.html"><a href="data-wrangling.html#dplyr-package"><i class="fa fa-check"></i><b>6.2.3</b> <code>dplyr</code> package</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="data-wrangling.html"><a href="data-wrangling.html#tidy-and-reshape-data"><i class="fa fa-check"></i><b>6.3</b> Tidy and Reshape Data</a><ul>
<li class="chapter" data-level="6.3.1" data-path="data-wrangling.html"><a href="data-wrangling.html#reshape2-package"><i class="fa fa-check"></i><b>6.3.1</b> <code>reshape2</code> package</a></li>
<li class="chapter" data-level="6.3.2" data-path="data-wrangling.html"><a href="data-wrangling.html#tidyr-package"><i class="fa fa-check"></i><b>6.3.2</b> <code>tidyr</code> package</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="measuring-performance.html"><a href="measuring-performance.html"><i class="fa fa-check"></i><b>7</b> Measuring Performance</a><ul>
<li class="chapter" data-level="7.1" data-path="measuring-performance.html"><a href="measuring-performance.html#regression-model-performance"><i class="fa fa-check"></i><b>7.1</b> Regression Model Performance</a></li>
<li class="chapter" data-level="7.2" data-path="measuring-performance.html"><a href="measuring-performance.html#classification-model-performance"><i class="fa fa-check"></i><b>7.2</b> Classification Model Performance</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="model-tuning-technique.html"><a href="model-tuning-technique.html"><i class="fa fa-check"></i><b>8</b> Model Tuning Technique</a><ul>
<li class="chapter" data-level="8.1" data-path="model-tuning-technique.html"><a href="model-tuning-technique.html#systematic-error-and-random-error"><i class="fa fa-check"></i><b>8.1</b> Systematic Error and Random Error</a><ul>
<li class="chapter" data-level="8.1.1" data-path="model-tuning-technique.html"><a href="model-tuning-technique.html#measurement-error-in-the-response"><i class="fa fa-check"></i><b>8.1.1</b> Measurement Error in the Response</a></li>
<li class="chapter" data-level="8.1.2" data-path="model-tuning-technique.html"><a href="model-tuning-technique.html#measurement-error-in-the-independent-variables"><i class="fa fa-check"></i><b>8.1.2</b> Measurement Error in the Independent Variables</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="model-tuning-technique.html"><a href="model-tuning-technique.html#data-splitting-and-resampling"><i class="fa fa-check"></i><b>8.2</b> Data Splitting and Resampling</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="feature-engineering.html"><a href="feature-engineering.html"><i class="fa fa-check"></i><b>9</b> Feature Engineering</a><ul>
<li class="chapter" data-level="9.1" data-path="feature-engineering.html"><a href="feature-engineering.html#feature-construction"><i class="fa fa-check"></i><b>9.1</b> Feature Construction</a></li>
<li class="chapter" data-level="9.2" data-path="feature-engineering.html"><a href="feature-engineering.html#feature-extraction"><i class="fa fa-check"></i><b>9.2</b> Feature Extraction</a></li>
<li class="chapter" data-level="9.3" data-path="feature-engineering.html"><a href="feature-engineering.html#feature-selection"><i class="fa fa-check"></i><b>9.3</b> Feature Selection</a><ul>
<li class="chapter" data-level="9.3.1" data-path="feature-engineering.html"><a href="feature-engineering.html#filter-method"><i class="fa fa-check"></i><b>9.3.1</b> Filter Method</a></li>
<li class="chapter" data-level="9.3.2" data-path="feature-engineering.html"><a href="feature-engineering.html#wrapper-method"><i class="fa fa-check"></i><b>9.3.2</b> Wrapper Method</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="regression-models.html"><a href="regression-models.html"><i class="fa fa-check"></i><b>10</b> Regression Models</a><ul>
<li class="chapter" data-level="10.1" data-path="regression-models.html"><a href="regression-models.html#ordinary-least-squares"><i class="fa fa-check"></i><b>10.1</b> Ordinary Least Squares</a></li>
<li class="chapter" data-level="10.2" data-path="regression-models.html"><a href="regression-models.html#multivariate-adaptive-regression-splines"><i class="fa fa-check"></i><b>10.2</b> Multivariate Adaptive Regression Splines</a></li>
<li class="chapter" data-level="10.3" data-path="regression-models.html"><a href="regression-models.html#generalized-linear-model"><i class="fa fa-check"></i><b>10.3</b> Generalized Linear Model</a></li>
<li class="chapter" data-level="10.4" data-path="regression-models.html"><a href="regression-models.html#pcr-and-pls"><i class="fa fa-check"></i><b>10.4</b> PCR and PLS</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="regularization-methods.html"><a href="regularization-methods.html"><i class="fa fa-check"></i><b>11</b> Regularization Methods</a><ul>
<li class="chapter" data-level="11.1" data-path="regularization-methods.html"><a href="regularization-methods.html#ridge-regression"><i class="fa fa-check"></i><b>11.1</b> Ridge Regression</a></li>
<li class="chapter" data-level="11.2" data-path="regularization-methods.html"><a href="regularization-methods.html#lasso"><i class="fa fa-check"></i><b>11.2</b> LASSO</a></li>
<li class="chapter" data-level="11.3" data-path="regularization-methods.html"><a href="regularization-methods.html#elastic-net"><i class="fa fa-check"></i><b>11.3</b> Elastic Net</a></li>
<li class="chapter" data-level="11.4" data-path="regularization-methods.html"><a href="regularization-methods.html#lasso-generalized-linear-model"><i class="fa fa-check"></i><b>11.4</b> LASSO Generalized Linear Model</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="tree-based-methods.html"><a href="tree-based-methods.html"><i class="fa fa-check"></i><b>12</b> Tree-Based Methods</a><ul>
<li class="chapter" data-level="12.1" data-path="tree-based-methods.html"><a href="tree-based-methods.html#splitting-criteria"><i class="fa fa-check"></i><b>12.1</b> Splitting Criteria</a></li>
<li class="chapter" data-level="12.2" data-path="tree-based-methods.html"><a href="tree-based-methods.html#tree-pruning"><i class="fa fa-check"></i><b>12.2</b> Tree Pruning</a></li>
<li class="chapter" data-level="12.3" data-path="tree-based-methods.html"><a href="tree-based-methods.html#regression-and-decision-tree-basic"><i class="fa fa-check"></i><b>12.3</b> Regression and Decision Tree Basic</a></li>
<li class="chapter" data-level="12.4" data-path="tree-based-methods.html"><a href="tree-based-methods.html#bagging-tree-1"><i class="fa fa-check"></i><b>12.4</b> Bagging Tree</a></li>
<li class="chapter" data-level="12.5" data-path="tree-based-methods.html"><a href="tree-based-methods.html#random-forest"><i class="fa fa-check"></i><b>12.5</b> Random Forest</a></li>
<li class="chapter" data-level="12.6" data-path="tree-based-methods.html"><a href="tree-based-methods.html#gradient-boosted-machine"><i class="fa fa-check"></i><b>12.6</b> Gradient Boosted Machine</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="neural-network.html"><a href="neural-network.html"><i class="fa fa-check"></i><b>13</b> Neural Network</a><ul>
<li class="chapter" data-level="13.1" data-path="neural-network.html"><a href="neural-network.html#projection-pursuit-regression"><i class="fa fa-check"></i><b>13.1</b> Projection Pursuit Regression</a></li>
<li class="chapter" data-level="13.2" data-path="neural-network.html"><a href="neural-network.html#neural-networks"><i class="fa fa-check"></i><b>13.2</b> Neural Networks</a></li>
<li class="chapter" data-level="13.3" data-path="neural-network.html"><a href="neural-network.html#fitting-neural-network"><i class="fa fa-check"></i><b>13.3</b> Fitting Neural Network</a></li>
<li class="chapter" data-level="13.4" data-path="neural-network.html"><a href="neural-network.html#model-training"><i class="fa fa-check"></i><b>13.4</b> Model Training</a></li>
<li class="chapter" data-level="13.5" data-path="neural-network.html"><a href="neural-network.html#computation-in-r"><i class="fa fa-check"></i><b>13.5</b> Computation in R</a><ul>
<li class="chapter" data-level="13.5.1" data-path="neural-network.html"><a href="neural-network.html#general-neural-network"><i class="fa fa-check"></i><b>13.5.1</b> General Neural Network</a></li>
<li class="chapter" data-level="13.5.2" data-path="neural-network.html"><a href="neural-network.html#averaged-neural-network"><i class="fa fa-check"></i><b>13.5.2</b> Averaged Neural Network</a></li>
<li class="chapter" data-level="13.5.3" data-path="neural-network.html"><a href="neural-network.html#model-comparison"><i class="fa fa-check"></i><b>13.5.3</b> Model Comparison</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="dynamicreproducible-report.html"><a href="dynamicreproducible-report.html"><i class="fa fa-check"></i><b>14</b> Dynamic/Reproducible Report</a><ul>
<li class="chapter" data-level="14.1" data-path="dynamicreproducible-report.html"><a href="dynamicreproducible-report.html#what-is-r-markdown"><i class="fa fa-check"></i><b>14.1</b> What is R Markdown?</a></li>
<li class="chapter" data-level="14.2" data-path="dynamicreproducible-report.html"><a href="dynamicreproducible-report.html#how-to-start"><i class="fa fa-check"></i><b>14.2</b> How to Start?</a><ul>
<li class="chapter" data-level="14.2.1" data-path="dynamicreproducible-report.html"><a href="dynamicreproducible-report.html#how-it-works"><i class="fa fa-check"></i><b>14.2.1</b> How It Works?</a></li>
<li class="chapter" data-level="14.2.2" data-path="dynamicreproducible-report.html"><a href="dynamicreproducible-report.html#get-started"><i class="fa fa-check"></i><b>14.2.2</b> Get Started</a></li>
<li class="chapter" data-level="14.2.3" data-path="dynamicreproducible-report.html"><a href="dynamicreproducible-report.html#markdown-basic"><i class="fa fa-check"></i><b>14.2.3</b> Markdown Basic</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="dynamicreproducible-report.html"><a href="dynamicreproducible-report.html#html"><i class="fa fa-check"></i><b>14.3</b> HTML</a><ul>
<li class="chapter" data-level="14.3.1" data-path="dynamicreproducible-report.html"><a href="dynamicreproducible-report.html#create-an-html-document"><i class="fa fa-check"></i><b>14.3.1</b> Create an HTML document</a></li>
<li class="chapter" data-level="14.3.2" data-path="dynamicreproducible-report.html"><a href="dynamicreproducible-report.html#floating-toc"><i class="fa fa-check"></i><b>14.3.2</b> Floating TOC</a></li>
<li class="chapter" data-level="14.3.3" data-path="dynamicreproducible-report.html"><a href="dynamicreproducible-report.html#code-chunks"><i class="fa fa-check"></i><b>14.3.3</b> Code Chunks</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="dynamicreproducible-report.html"><a href="dynamicreproducible-report.html#html5-slides"><i class="fa fa-check"></i><b>14.4</b> HTML5 Slides</a><ul>
<li class="chapter" data-level="14.4.1" data-path="dynamicreproducible-report.html"><a href="dynamicreproducible-report.html#ioslides-presentation"><i class="fa fa-check"></i><b>14.4.1</b> <code>ioslides</code> presentation</a></li>
<li class="chapter" data-level="14.4.2" data-path="dynamicreproducible-report.html"><a href="dynamicreproducible-report.html#slidy-presentation"><i class="fa fa-check"></i><b>14.4.2</b> <code>slidy</code> presentation</a></li>
</ul></li>
<li class="chapter" data-level="14.5" data-path="dynamicreproducible-report.html"><a href="dynamicreproducible-report.html#dashboards"><i class="fa fa-check"></i><b>14.5</b> Dashboards</a><ul>
<li class="chapter" data-level="14.5.1" data-path="dynamicreproducible-report.html"><a href="dynamicreproducible-report.html#layouts"><i class="fa fa-check"></i><b>14.5.1</b> Layouts</a></li>
<li class="chapter" data-level="14.5.2" data-path="dynamicreproducible-report.html"><a href="dynamicreproducible-report.html#components"><i class="fa fa-check"></i><b>14.5.2</b> Components</a></li>
</ul></li>
<li class="chapter" data-level="14.6" data-path="dynamicreproducible-report.html"><a href="dynamicreproducible-report.html#shiny-dashboard"><i class="fa fa-check"></i><b>14.6</b> Shiny Dashboard</a><ul>
<li class="chapter" data-level="14.6.1" data-path="dynamicreproducible-report.html"><a href="dynamicreproducible-report.html#brief-introduction-to-shiny"><i class="fa fa-check"></i><b>14.6.1</b> Brief Introduction to Shiny</a></li>
<li class="chapter" data-level="14.6.2" data-path="dynamicreproducible-report.html"><a href="dynamicreproducible-report.html#using-shiny-with-flexdashboard"><i class="fa fa-check"></i><b>14.6.2</b> Using <code>shiny</code> with <code>flexdashboard</code></a></li>
</ul></li>
<li class="chapter" data-level="14.7" data-path="dynamicreproducible-report.html"><a href="dynamicreproducible-report.html#html-widgets-1"><i class="fa fa-check"></i><b>14.7</b> HTML Widgets</a><ul>
<li class="chapter" data-level="14.7.1" data-path="dynamicreproducible-report.html"><a href="dynamicreproducible-report.html#dt-a-wrapper-of-the-javascript-library-datatables"><i class="fa fa-check"></i><b>14.7.1</b> <code>DT</code>: A Wrapper of the JavaScript Library DataTables</a></li>
<li class="chapter" data-level="14.7.2" data-path="dynamicreproducible-report.html"><a href="dynamicreproducible-report.html#leafletinteractive-web-maps-based-on-the-leaflet-javascript-library"><i class="fa fa-check"></i><b>14.7.2</b> <code>leaflet</code>:Interactive Web-Maps Based on the Leaflet JavaScript Library</a></li>
<li class="chapter" data-level="14.7.3" data-path="dynamicreproducible-report.html"><a href="dynamicreproducible-report.html#dygraphs-interactive-plot-for-time-series-data"><i class="fa fa-check"></i><b>14.7.3</b> <code>dygraphs</code>: interactive plot for time series data</a></li>
<li class="chapter" data-level="14.7.4" data-path="dynamicreproducible-report.html"><a href="dynamicreproducible-report.html#highcharter"><i class="fa fa-check"></i><b>14.7.4</b> <code>highcharter</code></a></li>
<li class="chapter" data-level="14.7.5" data-path="dynamicreproducible-report.html"><a href="dynamicreproducible-report.html#rbokeh-is-a-visualization-library-that-provides-a-flexible-and-powerful-declarative-framework-for-creating-web-based-plots"><i class="fa fa-check"></i><b>14.7.5</b> <code>rbokeh</code> is a visualization library that provides a flexible and powerful declarative framework for creating web-based plots</a></li>
<li class="chapter" data-level="14.7.6" data-path="dynamicreproducible-report.html"><a href="dynamicreproducible-report.html#metricsgraphics-enables-easy-creation-of-d3-scatterplots-line-charts-and-histograms."><i class="fa fa-check"></i><b>14.7.6</b> <code>metricsgraphics</code> enables easy creation of D3 scatterplots, line charts, and histograms.</a></li>
<li class="chapter" data-level="14.7.7" data-path="dynamicreproducible-report.html"><a href="dynamicreproducible-report.html#networkd3-d3-javascript-network-graphs-from-r"><i class="fa fa-check"></i><b>14.7.7</b> <code>networkD3</code>: D3 JavaScript Network Graphs from R</a></li>
<li class="chapter" data-level="14.7.8" data-path="dynamicreproducible-report.html"><a href="dynamicreproducible-report.html#threejs-interactive-3d-scatter-plots-and-globes"><i class="fa fa-check"></i><b>14.7.8</b> <code>threejs</code>: Interactive 3D Scatter Plots and Globes</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="15" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>15</b> References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Data Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="neural-network" class="section level1">
<h1><span class="header-section-number">Chapter 13</span> Neural Network</h1>
<p>Neural Network and its derivatives are so far the most hyped models. Those models sound fancy and mysterious, but it is no more than a button push in the implementation (thanks to all kinds of software). They require nearly no data preprocessing, and the users do not need to understand the reason behind the algorithms. People usually react to things they do not understand in two ways: deification and demonization. The black box model is the first case. In any case, in this chapter will illustrate the basic mathematical background of the neural network, what need to pay attention when using it and how to use R to train the neural network model. Hope it will help to demystify neural network in some degree. Many models are like T stage exaggerated clothes, they look fancy but not necessarily practical in real life.</p>
<div id="projection-pursuit-regression" class="section level2">
<h2><span class="header-section-number">13.1</span> Projection Pursuit Regression</h2>
<p>Before moving onto neural networks, let us start with a broader framework, PPR (Projection Pursuit Regression). It has a form of <strong>additive model</strong> of the derived features rather than the inputs themselves. Another widely used algorithm, AdaBoost, also fits an additive model in a base learner. I will talk more about it in my future post on tree model. [element of statistical learning]</p>
<p>Assume <span class="math inline">\(\mathbf{X^{T}}=(X_1,X_2,\dots,X_p)\)</span> is a vector with <span class="math inline">\(p\)</span> variables. <span class="math inline">\(Y\)</span> is the corresponding response variable. <span class="math inline">\(\mathbf{\omega_{m}},m=1,2,\dots,M\)</span> is parameter vector with <span class="math inline">\(p\)</span> elements.</p>
<p><span class="math display">\[f(\mathbf{X})=\sum_{m=1}^{M}g_{m}(\mathbf{\omega_{m}^{T}X})\]</span></p>
<p>The new feature <span class="math inline">\(\mathbf{V_{m}}=\mathbf{\omega_{m}^{T}X}\)</span> is a linear combination of input variables <span class="math inline">\(\mathbf{X}\)</span>. The additive model is based on the new features. Here <span class="math inline">\(\mathbf{\omega_{m}}\)</span> is a unit vector, and the new feature <span class="math inline">\(\mathbf{v_m}\)</span> is actually the projection of <span class="math inline">\(\mathbf{X}\)</span> on <span class="math inline">\(\mathbf{\omega_{m}}\)</span>. It projects the p-dimensional independent variable space onto the new M-dimensional feature space. This is similar to the principal component analysis except that the principal component is orthogonal projection but it is not necessarily orthogonal here.</p>
<p>I know it is very abstract. Let’s look at some examples. Assume <span class="math inline">\(p=2\)</span>, i.e. there are two variables <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>. If <span class="math inline">\(M=1\)</span>, <span class="math inline">\(\mathbf{\omega^{T}}=(\frac{1}{2},\frac{\sqrt{3}}{2})\)</span>, then the corresponding <span class="math inline">\(v=\frac{1}{2}x_{1}+\frac{\sqrt{3}}{2}x_{2}\)</span>. Let’s try different setings and compare the results:</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(\mathbf{\omega^{T}}=(\frac{1}{2},\frac{\sqrt{3}}{2})\)</span>, <span class="math inline">\(v=\frac{1}{2}x_{1}+\frac{\sqrt{3}}{2}x_{2}\)</span> , <span class="math inline">\(g(v)=\frac{1}{1+e^{-v}}\)</span></p></li>
<li><p><span class="math inline">\(\mathbf{\omega^{T}}=(1,0)\)</span>, <span class="math inline">\(v = x_1\)</span>, <span class="math inline">\(g(v)=(v+5)sin(\frac{1}{\frac{v}{3}+0.1})\)</span></p></li>
<li><p><span class="math inline">\(\mathbf{\omega^{T}}=(0,1)\)</span>, <span class="math inline">\(v = x_2\)</span>, <span class="math inline">\(g(v)=e^{\frac{v^2}{5}}\)</span></p></li>
<li><p><span class="math inline">\(\mathbf{\omega^{T}}=(1,0)\)</span>, <span class="math inline">\(v = x_1\)</span>, <span class="math inline">\(g(v)=(v+0.1)sin(\frac{1}{\frac{v}{3}+0.1})\)</span></p></li>
</ol>
<p>Here is how you can simulate the data and plot it using R:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># use plot3D package to generate 3D plot</span>
<span class="kw">library</span>(plot3D)
<span class="co"># get x1 and x2</span>
<span class="co"># note here x1 and x2 need to be matrix</span>
<span class="co"># if you check the two objects, you will find:</span>
<span class="co"># columns in x1 are identical</span>
<span class="co"># rows in x2 are identical</span>
<span class="co"># mesh() is funciton from plot3D package</span>
<span class="co"># you may need to think a little here</span>
M &lt;-<span class="st"> </span><span class="kw">mesh</span>(<span class="kw">seq</span>(-<span class="fl">13.2</span>, <span class="fl">13.2</span>, <span class="dt">length.out =</span> <span class="dv">50</span>),
          <span class="kw">seq</span>(-<span class="fl">37.4</span>, <span class="fl">37.4</span>, <span class="dt">length.out =</span> <span class="dv">50</span>))
x1 &lt;-<span class="st"> </span>M$x
x2 &lt;-<span class="st"> </span>M$y
## setting 1
<span class="co"># map X using w to get v</span>
v &lt;-<span class="st"> </span>(<span class="dv">1</span>/<span class="dv">2</span>)*x1+(<span class="kw">sqrt</span>(<span class="dv">3</span>)/<span class="dv">2</span>)*x2
<span class="co"># apply g() on v</span>
g1&lt;-<span class="dv">1</span>/(<span class="dv">1</span>+<span class="kw">exp</span>(-v))
<span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">2</span>), <span class="dt">mar =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>))
<span class="kw">surf3D</span>(x1,x2,g1,<span class="dt">colvar =</span> g1, <span class="dt">border =</span> <span class="st">&quot;black&quot;</span>, <span class="dt">colkey =</span> <span class="ot">FALSE</span>, <span class="dt">box =</span> <span class="ot">FALSE</span>, <span class="dt">main =</span> <span class="st">&quot;Setting 1&quot;</span>)
## setting 2
v &lt;-<span class="st"> </span>x1
g2 &lt;-<span class="st"> </span>(v<span class="dv">+5</span>)*<span class="kw">sin</span>(<span class="dv">1</span>/(v/<span class="dv">3</span><span class="fl">+0.1</span>))
<span class="kw">surf3D</span>(x1,x2,g2,<span class="dt">colvar =</span> g2, <span class="dt">border =</span> <span class="st">&quot;black&quot;</span>, <span class="dt">colkey =</span> <span class="ot">FALSE</span>, <span class="dt">box =</span> <span class="ot">FALSE</span>, <span class="dt">main =</span> <span class="st">&quot;Setting 2&quot;</span>)
## setting 3
v &lt;-<span class="st"> </span>x2
g3 &lt;-<span class="st"> </span><span class="kw">exp</span>(v^<span class="dv">2</span>/<span class="dv">5</span>)
<span class="kw">surf3D</span>(x1,x2,g3,<span class="dt">colvar =</span> g3, <span class="dt">border =</span> <span class="st">&quot;black&quot;</span>, <span class="dt">colkey =</span> <span class="ot">FALSE</span>, <span class="dt">box =</span> <span class="ot">FALSE</span>, <span class="dt">main =</span> <span class="st">&quot;Setting 3&quot;</span>)
## setting 4
v &lt;-<span class="st"> </span>x1
g4 &lt;-<span class="st"> </span>(v<span class="fl">+0.1</span>)*<span class="kw">sin</span>(<span class="dv">1</span>/(v/<span class="dv">3</span><span class="fl">+0.1</span>))
<span class="kw">surf3D</span>(x1,x2,g4,<span class="dt">colvar =</span> g4, <span class="dt">border =</span> <span class="st">&quot;black&quot;</span>, <span class="dt">colkey =</span> <span class="ot">FALSE</span>, <span class="dt">box =</span> <span class="ot">FALSE</span>, <span class="dt">main =</span> <span class="st">&quot;Setting 4&quot;</span>)</code></pre></div>
<p><img src="CE_JSM2017_files/figure-html/nnet_simulate_data-1.png" width="672" /></p>
<p>You can see that this framework can be very flexible. In essence, it is to do a non-linear transformation of the linear combination. You can use this way to capture varies of relationships. For example,<span class="math inline">\(x_{1}x_{2}\)</span> can be written as <span class="math inline">\(\frac{(x_{1}+x_{2})^{2}-(x_{1}-x_{2})^{2}}{4}\)</span>, where <span class="math inline">\(M=2\)</span>. All the higher order factors of <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> can be represented similarly. If <span class="math inline">\(M\)</span> is large enough, this frame can approximate any continuous function on <span class="math inline">\(\mathbb{R}^{p}\)</span>. So the model family covers a board area, but with a price. That is the interpretability. Because the number of parameters increases with M and the mode is nested.</p>
<p>Since the advent of the PPR in 1981, it has not been widely used. It is due to the limitation of computational capability then. But this is a new idea which leads to the debut of the neural network model.</p>
</div>
<div id="neural-networks" class="section level2">
<h2><span class="header-section-number">13.2</span> Neural Networks</h2>
<p>Here we introduce one of the most widely used models: single hidden layer back-propagation network. It is a hierarchical regression or classification model with the following structure:</p>
<center>
<img src="http://scientistcafe.com/book/Figure/nnet.png" />
</center>
<p>This is a single hidden layer back-propagation network.</p>
<p>Let <span class="math inline">\(K\)</span> represents the number of variables. In the regression model, there is usually only one response variable. However, in the multi-classification case (the number of classes is greater than 3), the number of response variables is the number of categories. For simplicity, we assume that <span class="math inline">\(K = 1\)</span>,i.e. only one response <span class="math inline">\(Y\)</span>. For the classification problem, the possible value of <span class="math inline">\(Y\)</span> is <span class="math inline">\(0/1\)</span>. We will start from the bottom. Let <span class="math inline">\(\mathbf{X^{T}}=(X_{1},X_{2},\dots,X_{p})\)</span> be a vector of <span class="math inline">\(p\)</span> independent random variables. The first step is from <span class="math inline">\(\mathbf{X}\)</span> to the new feature <span class="math inline">\(\mathbf{Z^{T}}=(Z_1,Z_2,\dots, Z_M)\)</span>.</p>
<p><span class="math display">\[Z_{m}=h(\alpha_{0m}+\mathbf{\alpha_{m}^{T}}X),\ m=1,\dots,M\]</span></p>
<p>The <span class="math inline">\(h(\cdot)\)</span> is called <em>Activation Function</em> , usually <span class="math inline">\(h(v)=\frac{1}{1+e^{-v}}\)</span>. It is the S-shaped function in logical regression that is familiar to many. Let’s expand it a little. Add a factor <span class="math inline">\(s\)</span> in front of <span class="math inline">\(v\)</span> in the formula above, that is: <span class="math inline">\(h(v)=\frac{1}{1+e^{-sv}}\)</span>. Let’s explore how <span class="math inline">\(s\)</span> impact the shape of the curve:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">v &lt;-<span class="st"> </span><span class="kw">seq</span>(-<span class="dv">10</span>,<span class="dv">10</span>,<span class="dt">length=</span><span class="dv">200</span>)
s1 &lt;-<span class="st"> </span><span class="dv">1</span>
s2 &lt;-<span class="st"> </span><span class="fl">0.3</span>
s3 &lt;-<span class="st"> </span><span class="dv">20</span>
h1 &lt;-<span class="st"> </span><span class="dv">1</span>/(<span class="dv">1</span>+<span class="kw">exp</span>(-s1*v))
h2 &lt;-<span class="st"> </span><span class="dv">1</span>/(<span class="dv">1</span>+<span class="kw">exp</span>(-s2*v))
h3 &lt;-<span class="st"> </span><span class="dv">1</span>/(<span class="dv">1</span>+<span class="kw">exp</span>(-s3*v))
<span class="kw">plot</span>(v,h1,<span class="dt">type=</span><span class="st">&quot;l&quot;</span>,<span class="dt">col=</span><span class="dv">1</span>, <span class="dt">ylab=</span><span class="st">&quot;h&quot;</span>, <span class="dt">lty =</span> <span class="dv">1</span>)
<span class="kw">lines</span>(v,h2,<span class="dt">col=</span><span class="dv">2</span>, <span class="dt">lty =</span> <span class="dv">2</span>)
<span class="kw">lines</span>(v,h3,<span class="dt">col=</span><span class="dv">3</span>, <span class="dt">lty =</span> <span class="dv">3</span>)
<span class="kw">legend</span>(<span class="st">&quot;topleft&quot;</span>, <span class="kw">c</span>(<span class="st">&quot;s=1&quot;</span>,<span class="st">&quot;s=0.3&quot;</span>,<span class="st">&quot;s=20&quot;</span>), <span class="dt">lty =</span> <span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>), <span class="dt">col =</span> <span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>))</code></pre></div>
<p><img src="CE_JSM2017_files/figure-html/unnamed-chunk-107-1.png" width="672" /></p>
<p>It is easy to see that the smaller <span class="math inline">\(s\)</span> is, the more linear the line is; the larger <span class="math inline">\(s\)</span> is, the closer the line is to a piecewise function. Understanding the role of <span class="math inline">\(s\)</span> is critical to choose the starting values of the model.</p>
<p>The next step is from <span class="math inline">\(\mathbf{Z}\)</span> to <span class="math inline">\(Y\)</span>:</p>
<p><span class="math display">\[Y=g(\beta_{0}+\mathbf{\beta^{T}Z})=f(\mathbf{X})\]</span></p>
<p>In regression, <span class="math inline">\(g(\cdot)\)</span> is an identical function, i.e.:</p>
<p><span class="math display">\[Y=\beta_{0}+\mathbf{\beta^{T}Z}\]</span></p>
<p>For classification model with <span class="math inline">\(K\)</span> categories, each category corresponds to a liear combination of <span class="math inline">\(\mathbf{Z}\)</span>. Apply softmax function to the linear combination to get <span class="math inline">\(Y_{k}\)</span>:</p>
<p><span class="math display">\[Y_{k}=\frac{e^{\beta_{0k}+\mathbf{\beta_{k}^{T}Z}}}{\Sigma_{l=1}^{K}e^{\beta_{0l}+\mathbf{\beta_{l}^{T}Z}}}\]</span></p>
<p>The idea behind this transformation is very straight forward. After softmax transformation, the value is within <span class="math inline">\([0,1]\)</span>. The nonlinear function <span class="math inline">\(h(\cdot)\)</span> expands the linear model greatly. In some sense, <span class="math inline">\(h(\cdot)\)</span> is to activate a status which is how come the name of the function. The magnitude <span class="math inline">\(\Vert\mathbf{\alpha_{m}}\Vert\)</span> impacts the extent to which the status is activated. From the above plot we can see that when <span class="math inline">\(\Vert\mathbf{\alpha_{m}}\Vert\)</span> is small, the activation function is close to linear. When <span class="math inline">\(\Vert\mathbf{\alpha_{m}}\Vert\)</span> is large, the function is close to piecewise function. Why don’t we use a piecewise function instead? Because the function is non-derivable at the breakpoint which will bring difficulties to the subsequent model fitting. By tuning the parameter, the model can go from close to linear to highly un-linear. Does the function make more sense now?</p>
<p>The single hidden layer back-propagation network here is identical to the PPR model introduced at the begining. The only difference is that <span class="math inline">\(g_m(v)\)</span> in PPR is a non-parametric function, i.e. it is estimated by local smoothing. The <span class="math inline">\(h(\cdot)\)</span> in neural network has a real form. We can use the format of PPR to represent neural network:</p>
<p><span class="math display">\[\beta_{m}h(\alpha_{0m}+\mathbf{\alpha_{m}^{T}X})=\beta_{m}h(\alpha_{0m}+\Vert\mathbf{\alpha_{m}}\Vert(\mathbf{\omega_{m}^{T}X}))=g_{m}(\mathbf{\omega_{m}^{T}X})\]</span></p>
<p>where <span class="math inline">\(\mathbf{\omega_{m}=\frac{\mathbf{\alpha_{m}}}{\Vert\mathbf{\alpha_{m}}\Vert}}\)</span> is a unit vector. The general non-parametric function <span class="math inline">\(g(\cdot)\)</span> in PPR is more complicated than the well-defined parametric funciton <span class="math inline">\(h_{\beta,\alpha_{0},s}(v)=\beta h(\alpha_{0}+sv)\)</span> in neural network. Though the later seems intimidating.</p>
<p>Each of the original input variables <span class="math inline">\(X_1,\dots, X_p\)</span> is like neurons. The function <span class="math inline">\(h(\cdot)\)</span> is like synapses. Why? Consider the function of synapses. Synapse is a junction between two nerve cells. A neuron releases an electrical or chemical signal that passes through a synapse to another neuron. If <span class="math inline">\(h(\cdot)\)</span> is a step function, when the input exceeds a certain value, the function is nonzero. It is an analogy to a synapse. Neuron X sends the signal through the synapse h, when the signal strength surpasses a certain threshold, it will pass to the next neuron Z.</p>
<p>Since the function is non-derivable at the breakpoint, people make a little change to the step function and get the S-shaped line. By controlling the parameters, you can deform the S-shaped curve from approximately linear to highly nonlinear as a resistance switch.</p>
</div>
<div id="fitting-neural-network" class="section level2">
<h2><span class="header-section-number">13.3</span> Fitting Neural Network</h2>
<p>The parameters in the neural network model are the weights:</p>
<p><span class="math display">\[\{\alpha_{0m},\mathbf{\alpha_{m}};m=1,2,\dots,M\},\ M(p+1)\ weight\]</span> <span class="math display">\[\{\beta_{0},\mathbf{\beta}\},\ M+1\ weight\]</span></p>
<p>There are <span class="math inline">\(M(p+1)+M+1\)</span> weights to estimate. We use <span class="math inline">\(\Theta\)</span> to represent parameter set. For regression, to fit a model is to minimize the following sum of squared error:</p>
<p><span class="math display">\[R(\Theta)=\sum_{i=1}^{N}(y_{i}-f(x_{i}))^{2}\]</span></p>
<p>For classification, the goal is to optimize entropy:</p>
<p><span class="math display">\[R(\Theta)=-\sum_{i=1}^{N}\Sigma_{k=1}^{K}y_{ik}log(f_{k}(x_{i}))\]</span></p>
<p>Here <span class="math inline">\(K\)</span> represents the number of category.</p>
<p>If <span class="math inline">\(p=100\)</span> and <span class="math inline">\(M = 30\)</span>, the total number of parameters for the model is <span class="math inline">\(3061\)</span>. As <span class="math inline">\(p\)</span> and <span class="math inline">\(M\)</span> grow, the number of parameters will increase quickly. It is not hard to imagine that it will soon over-fit. A possible solution is to add a penalty to the number of parameters.</p>
<p>Gradient descent is a popular method to minimize <span class="math inline">\(R(\Theta)\)</span>. The “reverse spread” comes from the optimization process. Due to the embedding form of the model, so the gradient can be easily derived by following the chain rule. It is:</p>
<p><span class="math display">\[\frac{df(g(x))}{dx}=\frac{df}{dg}\frac{dg}{dx}\]</span></p>
<p>The approximation process goes from the bottom of the network to the top, and then back from the top to bottom which leads to the name “reverse propagation.” The spread is forward, the reverse is returned. It reduces <span class="math inline">\(R(\Theta)\)</span> gradually during interactions.</p>
<p>Now take the regression as an example to illustrate the process. Assume <span class="math inline">\(z_{mi}=h(\alpha_{0m}+\mathbf{\alpha_{m}^{T}}x_{i})\)</span> and <span class="math inline">\(\mathbf{z_{i}^{T}}=(z_{1i},z_{2i},\dots,z_{Mi})\)</span>, then we have:</p>
<p><span class="math display">\[R(\Theta)=\Sigma_{i=1}^{N}R_{i}=\Sigma_{i=1}^{N}(y_{i}-\beta_{0}-\mathbf{\beta^{T}z_{i}})^{2}\]</span></p>
<p>Derive the above equation by chain rule:</p>
<p><span class="math display">\[\frac{\partial R_{i}}{\partial\beta_{m}}=-2(y_{ik}-\beta_{0}-\mathbf{\mathbf{\beta}^{T}z_{i}})z_{mi}\]</span></p>
<p><span class="math display">\[\frac{\partial R_{i}}{\partial\alpha_{ml}}=-2(y_{ik}-\beta_{0}-\mathbf{\mathbf{\beta}^{T}z_{i}})\beta_{m}h&#39;(\alpha_{0m}+\mathbf{\alpha_{m}^{T}}x_{i})x_{il}\]</span></p>
<p>Given the gradient, the <span class="math inline">\((r+1)^{th}\)</span> iteration is:</p>
<p><span class="math display">\[\beta_{m}^{(r+1)}=\beta_{m}^{(r)}-\gamma_{r}\Sigma_{i=1}^{N}\frac{\partial R_{i}}{\partial\beta_{m}^{(r)}}\]</span></p>
<p><span class="math display">\[\alpha_{ml}^{(r+1)}=\alpha_{ml}^{(r)}-\gamma_{r}\Sigma_{i=1}^{N}\frac{\partial R_{i}}{\partial\alpha_{ml}^{(r)}}\]</span></p>
<p>There is one more parameter <span class="math inline">\(\gamma_{r}\)</span>, aka learning rate. We can simply write the previous gradients as:</p>
<p><span class="math display">\[\frac{\partial R_{i}}{\partial\beta_{m}}=\delta_{i}z_{mi}\]</span></p>
<p><span class="math display">\[\frac{\partial R_{i}}{\partial\alpha_{ml}}=s_{mi}x_{il}\]</span></p>
<p>where <span class="math inline">\(\delta_{i}\)</span> and <span class="math inline">\(s_{mi}\)</span> are called model error in the output level and hidden level respectively. The reverse propagation function is:</p>
<p><span class="math display">\[s_{mi}=-2(y_{ik}-\beta_{0}-\mathbf{\mathbf{\beta}^{T}z_{i}})\beta_{m}h&#39;(\alpha_{0m}+\mathbf{\alpha_{m}^{T}}x_{i})\]</span></p>
<p>The algorithm goes back and forward using the function. The learning rate <span class="math inline">\(\gamma_{r}\)</span> is usually a constant. It can also be a tuning parameter.</p>
</div>
<div id="model-training" class="section level2">
<h2><span class="header-section-number">13.4</span> Model Training</h2>
<p>There are some aspects to pay attention to when training neural networks.</p>
<ul>
<li>Initial value</li>
</ul>
<p>Recall the S-shaped curves shown before. If the weight is close to 0, the model tends to be linear. As there are usually many parameters in the model, by default, the algorithm will select the initial weight to be near 0. In this way, as the iteration progresses, the model is gradually adjusted from linear to non-linear.</p>
<ul>
<li>Overfitting</li>
</ul>
<p>As mentioned before, the parameters of the neural network increased fast which will quickly over-fit. There are two ways to cope with the problem:</p>
<ol style="list-style-type: decimal">
<li>Early termination: end the iteration before reaching the optimal parameter estimates</li>
<li>Penalty function: penalize the number of parameters. When the punishment is large, all the parameters estimates are 0.</li>
</ol>
<p>Use <span class="math inline">\(\lambda\)</span> to indicate the parameter for weight decay. Instead of optimize <span class="math inline">\(R(\Theta)\)</span>, we will optimize <span class="math inline">\(R(\Theta)+\lambda J(\Theta)\)</span> here. <span class="math inline">\(J(\Theta)\)</span> has the following two types:</p>
<p><span class="math display">\[J(\Theta)=\Sigma_{m}\beta_{m}^{2}+\Sigma_{ml}\alpha_{ml}^{2}\]</span></p>
<p>or:</p>
<p><span class="math display">\[J(\Theta)=\Sigma_{m}\frac{\beta_{m}^{2}}{1+\beta_{m}^{2}}+\Sigma_{ml}\frac{\alpha_{ml}^{2}}{1+\alpha_{ml}^{2}}\]</span></p>
<p>The former contracts the weight more intensely.</p>
<ul>
<li>Standardization</li>
</ul>
<p>In penalized models, one should standardize the input variables so that the penalty applies equally instead of applying to different degrees based on the scale of the variables.</p>
<ul>
<li>Number of layers and hidden units</li>
</ul>
<p>Theoretically speaking, the more hidden levels, the better. Model with more the levels has higher ability to capture non-linear relationships in the data. However, the number of parameters also increases. We can control the parameters by adding a penalty. Deep learning is a neural network with multiple layers. Another tuning parameter to decide is the number of hidden.</p>
<ul>
<li>Multiple local extremums</li>
</ul>
<p>Another problem with model fitting is that <span class="math inline">\(R(\Theta)\)</span> is not a perfect convex function, that is, there is more than one local extremum. Which one the algorithm reaches depends on the initial value. This poses instability to model predictions. There are usually two ways to solve: (1) select a series of different initial values, and then average the model weight. (2) Use the bagging method. Fit model using bootstrap samples, and then average the results.</p>
<p>The idea behind these two approaches is first to introduce some randomness at a certain stage, and then stabilize the results by averaging.</p>
</div>
<div id="computation-in-r" class="section level2">
<h2><span class="header-section-number">13.5</span> Computation in R</h2>
<div id="general-neural-network" class="section level3">
<h3><span class="header-section-number">13.5.1</span> General Neural Network</h3>
<p>Swine Disease Breakout Data to show how to train neural network models using <code>caret</code> package.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># run the next line to install DataScienceR package</span>
<span class="co"># devtools::install_github(&quot;happyrabbit/DataScienceR&quot;)</span>
<span class="kw">library</span>(DataScienceR)
<span class="kw">data</span>(<span class="st">&quot;sim1_da1&quot;</span>)
trainx =<span class="st"> </span>dplyr::<span class="kw">select</span>(sim1_da1, -y)
trainy =<span class="st"> </span><span class="kw">paste0</span>(<span class="st">&quot;BREAK&quot;</span>,sim1_da1$y)</code></pre></div>
<p>Set the tuning grid. The tuning parameters are weight, the number of hidden layers and the number of hidden units. <code>decay</code> is the weight decay, and there are three tuning values. <code>size</code> is the number of hidden units.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(caret)
nnetGrid &lt;-<span class="st"> </span><span class="kw">expand.grid</span>(<span class="dt">decay =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="fl">0.01</span>, .<span class="dv">1</span>),
                        <span class="dt">size =</span> <span class="kw">c</span>(<span class="dv">1</span>:<span class="dv">10</span>))
<span class="co"># get the maximum number of hidden units</span>
maxSize &lt;-<span class="st"> </span><span class="kw">max</span>(nnetGrid$size)
<span class="co"># compute the maximum number of parameters</span>
<span class="co"># there are M(p+1)+M+1 parameters in total</span>
numWts &lt;-<span class="st"> </span><span class="dv">1</span>*(maxSize *<span class="st"> </span>(<span class="kw">length</span>(trainx) +<span class="st"> </span><span class="dv">1</span>) +<span class="st"> </span>maxSize +<span class="st"> </span><span class="dv">1</span>)
<span class="co"># set a random seed to ensure repeatability</span>
<span class="kw">set.seed</span>(<span class="dv">2017</span>)</code></pre></div>
<p>You can use <code>trainControl()</code> function to customize the process of selecting tuning/complexity parameters and building the final model. More information about <code>trainControl</code> see “<a href="https://topepo.github.io/caret/model-training-and-tuning.html#custom">Customizing the Tuning Process</a>.”</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ctrl &lt;-<span class="st"> </span><span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>,  <span class="co"># corss-validation</span>
                     <span class="dt">number =</span> <span class="dv">10</span>,  <span class="co"># 10 folds</span>
                     <span class="dt">classProbs =</span> <span class="ot">TRUE</span>, <span class="co"># report class probability</span>
                     <span class="dt">summaryFunction =</span> twoClassSummary <span class="co"># return AUC</span>
)</code></pre></div>
<p>Next, past those control setting to <code>train()</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">nnetTune &lt;-<span class="st"> </span><span class="kw">train</span>(trainx, trainy,
                   <span class="dt">method =</span> <span class="st">&quot;nnet&quot;</span>, <span class="co"># train neural network using `nnet` package </span>
                   <span class="dt">tuneGrid =</span> nnetGrid, <span class="co"># tuning grid</span>
                   <span class="dt">trControl =</span> ctrl, <span class="co"># process customization set before</span>
                   <span class="dt">preProc =</span> <span class="kw">c</span>(<span class="st">&quot;center&quot;</span>, <span class="st">&quot;scale&quot;</span>), <span class="co"># standardize data</span>
                   <span class="dt">trace =</span> <span class="ot">FALSE</span>,  <span class="co"># hide the training trace</span>
                   <span class="dt">MaxNWts =</span> numWts,  <span class="co"># maximum number of weight</span>
                   <span class="dt">maxit =</span> <span class="dv">500</span> <span class="co"># maximum iteration</span>
)</code></pre></div>
<p>The results show that the optimum number of hidden units is 1 and the decay parameter is 0.1：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">nnetTune</code></pre></div>
<pre><code>## Neural Network 
## 
## 800 samples
## 240 predictors
##   2 classes: &#39;BREAK0&#39;, &#39;BREAK1&#39; 
## 
## Pre-processing: centered (240), scaled (240) 
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 720, 720, 720, 720, 720, 720, ... 
## Resampling results across tuning parameters:
## 
##   decay  size  ROC        Sens       Spec     
##   0.00    1    0.8795739  0.8071429  0.8000000
##   0.00    2    0.8847744  0.8190476  0.8263158
##   0.00    3    0.8710526  0.8142857  0.7894737
##   0.00    4    0.8666667  0.8166667  0.8078947
##   0.00    5    0.8128759  0.7547619  0.7552632
##   0.00    6    0.8096491  0.7785714  0.7473684
##   0.00    7    0.8179825  0.7642857  0.7894737
##   0.00    8    0.8029449  0.8023810  0.7526316
##   0.00    9    0.8017544  0.7738095  0.7447368
##   0.00   10    0.8314536  0.7880952  0.7578947
##   0.01    1    0.9323308  0.8738095  0.8236842
##   0.01    2    0.9334586  0.8690476  0.8263158
##   0.01    3    0.9329574  0.8690476  0.8210526
##   0.01    4    0.9121554  0.8428571  0.8000000
##   0.01    5    0.9318922  0.8642857  0.8105263
##   0.01    6    0.9289474  0.8642857  0.8105263
##   0.01    7    0.9305764  0.8690476  0.8131579
##   0.01    8    0.9281328  0.8690476  0.7973684
##   0.01    9    0.9303885  0.8785714  0.7921053
##   0.01   10    0.9320175  0.8666667  0.8184211
##   0.10    1    0.9344612  0.8642857  0.8263158
##   0.10    2    0.9332080  0.8666667  0.8236842
##   0.10    3    0.9325188  0.8690476  0.8184211
##   0.10    4    0.9322055  0.8785714  0.8131579
##   0.10    5    0.9285088  0.8619048  0.8078947
##   0.10    6    0.9311404  0.8714286  0.7921053
##   0.10    7    0.9308271  0.8714286  0.8052632
##   0.10    8    0.9317043  0.8785714  0.8078947
##   0.10    9    0.9327694  0.8690476  0.8052632
##   0.10   10    0.9296366  0.8738095  0.8052632
## 
## ROC was used to select the optimal model using 
##  the largest value.
## The final values used for the model were size = 1
##  and decay = 0.1.</code></pre>
<p>You can directly apply <code>plot()</code> function to get a visualization of the whole process:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(nnetTune)</code></pre></div>
<p><img src="CE_JSM2017_files/figure-html/unnamed-chunk-113-1.png" width="672" /></p>
</div>
<div id="averaged-neural-network" class="section level3">
<h3><span class="header-section-number">13.5.2</span> Averaged Neural Network</h3>
<p>Multiple local minimums will lead to unstable results. A solution is to use bootstrap samples and average the results. We can use the same function <code>train()</code> to train the averaged neural network with <code>method = &quot;avNNet&quot;</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># it takes some time to run</span>
trainx =<span class="st"> </span>dplyr::<span class="kw">select</span>(sim1_da1, -y)
trainy =<span class="st"> </span><span class="kw">paste0</span>(<span class="st">&quot;BREAK&quot;</span>,sim1_da1$y)

avnnetGrid &lt;-<span class="st"> </span><span class="kw">expand.grid</span>(<span class="dt">decay =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="fl">0.01</span>, .<span class="dv">1</span>),
                        <span class="dt">size =</span> <span class="kw">c</span>(<span class="dv">1</span>:<span class="dv">10</span>),
                        <span class="dt">bag =</span> <span class="ot">TRUE</span> <span class="co"># indicate that we will use bootstrap</span>
)
                        
avnnetTune &lt;-<span class="st"> </span><span class="kw">train</span>(trainx, trainy,
                   <span class="dt">method =</span> <span class="st">&quot;avNNet&quot;</span>,
                   <span class="dt">tuneGrid =</span> avnnetGrid,
                   <span class="dt">trControl =</span> ctrl,
                   <span class="dt">preProc =</span> <span class="kw">c</span>(<span class="st">&quot;center&quot;</span>, <span class="st">&quot;scale&quot;</span>),
                   <span class="dt">trace =</span> <span class="ot">FALSE</span>,
                   <span class="dt">MaxNWts =</span> numWts, 
                   <span class="dt">maxit =</span> <span class="dv">500</span>)</code></pre></div>
<p>When the number of hidden units is 10, there are too many parameters in the model, so you will have the following warning:</p>
<pre>
In eval(expr, envir, enclos) :
  model fit failed for Fold10: decay=0.00, size=10, bag=TRUE Error in { : task 1 failed - "too many (2432) weights"
</pre>
<p>But it won’t hurt the model tuning from size=1 to 9. We can see the optimum estimates are <code>size = 3</code> and <code>decay = 0.01</code>：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">avnnetTune</code></pre></div>
<pre><code>## Model Averaged Neural Network 
## 
## 800 samples
## 240 predictors
##   2 classes: &#39;BREAK0&#39;, &#39;BREAK1&#39; 
## 
## Pre-processing: centered (240), scaled (240) 
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 720, 720, 720, 720, 720, 720, ... 
## Resampling results across tuning parameters:
## 
##   decay  size  ROC        Sens       Spec     
##   0.00    1    0.7805138  0.7333333  0.7000000
##   0.00    2    0.7957393  0.7238095  0.7263158
##   0.00    3    0.7912281  0.7380952  0.7000000
##   0.00    4    0.8109962  0.7738095  0.6947368
##   0.00    5    0.8039474  0.7547619  0.6815789
##   0.00    6    0.8094925  0.7452381  0.7263158
##   0.00    7    0.8208647  0.7523810  0.7289474
##   0.00    8    0.8104323  0.7928571  0.6973684
##   0.00    9    0.7916040  0.7380952  0.6947368
##   0.00   10          NaN        NaN        NaN
##   0.01    1    0.9124687  0.8476190  0.8263158
##   0.01    2    0.9010652  0.8357143  0.7973684
##   0.01    3    0.9213659  0.8523810  0.8157895
##   0.01    4    0.9173559  0.8357143  0.8342105
##   0.01    5    0.9153509  0.8309524  0.8210526
##   0.01    6    0.9058271  0.8380952  0.7894737
##   0.01    7    0.9166667  0.8476190  0.8157895
##   0.01    8    0.9065789  0.8309524  0.8157895
##   0.01    9    0.9167293  0.8309524  0.8289474
##   0.01   10          NaN        NaN        NaN
##   0.10    1    0.9125313  0.8380952  0.8078947
##   0.10    2    0.9124687  0.8571429  0.8078947
##   0.10    3    0.9136591  0.8404762  0.8105263
##   0.10    4    0.9124687  0.8285714  0.8157895
##   0.10    5    0.9170426  0.8428571  0.8184211
##   0.10    6    0.9103383  0.8285714  0.8052632
##   0.10    7    0.9119674  0.8547619  0.7921053
##   0.10    8    0.9036341  0.8404762  0.8052632
##   0.10    9    0.9117794  0.8380952  0.8184211
##   0.10   10          NaN        NaN        NaN
## 
## Tuning parameter &#39;bag&#39; was held constant at a value
##  of TRUE
## ROC was used to select the optimal model using 
##  the largest value.
## The final values used for the model were size =
##  3, decay = 0.01 and bag = TRUE.</code></pre>
</div>
<div id="model-comparison" class="section level3">
<h3><span class="header-section-number">13.5.3</span> Model Comparison</h3>
<p>Let’s apply other models on the same dataset and compare the model results.</p>
<ul>
<li>Stochastic Gradient Boosting</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">trainx =<span class="st"> </span>dplyr::<span class="kw">select</span>(sim1_da1, -y)
trainy =<span class="st"> </span>sim1_da1$y

gbmGrid &lt;-<span class="st"> </span><span class="kw">expand.grid</span>(<span class="dt">interaction.depth =</span> <span class="kw">seq</span>(<span class="dv">1</span>, <span class="dv">7</span>, <span class="dt">by =</span> <span class="dv">2</span>), 
                       <span class="dt">n.trees =</span> <span class="kw">seq</span>(<span class="dv">100</span>, <span class="dv">1000</span>, <span class="dt">by =</span> <span class="dv">50</span>),
                       <span class="dt">shrinkage =</span> <span class="kw">c</span>(<span class="fl">0.01</span>, <span class="fl">0.1</span>),
                       <span class="dt">n.minobsinnode =</span> <span class="dv">5</span>)
<span class="kw">set.seed</span>(<span class="dv">2017</span>)
gbmTune &lt;-<span class="st"> </span><span class="kw">train</span>(trainx, trainy,
                 <span class="dt">method=</span><span class="st">&quot;gbm&quot;</span>,
                 <span class="dt">tuneGrid =</span> gbmGrid,
                 <span class="dt">verbose =</span> <span class="ot">FALSE</span>)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(pROC)
pregbm &lt;-<span class="st"> </span><span class="kw">predict</span>(gbmTune, trainx)
<span class="kw">roc</span>(pregbm, trainy)</code></pre></div>
<pre><code>## 
## Call:
## roc.default(response = pregbm, predictor = trainy)
## 
## Data: trainy in 426 controls (pregbm BREAK0) &lt; 374 cases (pregbm BREAK1).
## Area under the curve: 0.9503</code></pre>
<p>The optimum result is <code>AUC=0.9503</code>.</p>
<ul>
<li>Group lasso logistic regression</li>
</ul>
<p>The third model to try is group lasso logistic regression. It adds a group indicator to the lasso logistics regression which allows selecting variables within and across groups.</p>
<p>You can use <code>cv_glasso()</code> function from <code>DataScienceR</code> pacakge to tuning group lasso logistic regression:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># trainx contains the independent variables</span>
trainx =<span class="st"> </span>dplyr::<span class="kw">select</span>(sim1_da1, -y)
<span class="co"># trainy is the response variable</span>
trainy =<span class="st"> </span>sim1_da1$y
<span class="co"># get group index</span>
index &lt;-<span class="st"> </span><span class="kw">gsub</span>(<span class="st">&quot;</span><span class="ch">\\</span><span class="st">..*&quot;</span>, <span class="st">&quot;&quot;</span>, <span class="kw">names</span>(trainx))
<span class="co"># tune over 100 parameter values</span>
nlam &lt;-<span class="st"> </span><span class="dv">100</span>
<span class="co"># set the model response</span>
<span class="co"># - `link`：return value of link function</span>
<span class="co"># - `response`：return the probability response (note:  the real pobability number doesn&#39;t make sense but the rank does)</span>
<span class="co"># number of cross-validation folds</span>
kfold &lt;-<span class="st"> </span><span class="dv">10</span>
cv_fit &lt;-<span class="st"> </span><span class="kw">cv_glasso</span>(trainx, trainy, <span class="dt">nlam =</span> nlam, <span class="dt">kfold =</span> kfold)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">cv_fit$lambda.max.auc </code></pre></div>
<div class="sourceCode"><pre class="sourceCode html"><code class="sourceCode html">   lambda       auc 
1.0009576 0.9453094 </code></pre></div>
<p>The optimum tuning parameter is <code>lambda=1</code>，the optimum <code>AUC=0.945</code>. It is a little better than neural network. But it runs much faster and is interpretable. Here is a summary of the three models:</p>
<pre><code>##  nnet   gbm lasso 
##  0.93  0.95  0.95</code></pre>
<p>The three models perform similarly on this dataset. Stochastic gradient boosting and lasso is slightly better than neural network. Considering the computation time and model interpretability, I will choose group lasso logistic regression.</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="tree-based-methods.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="dynamicreproducible-report.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": null,
"download": null,
"toc": {
"collapse": "subsection",
"scroll_highlight": true
},
"search": true,
"toolbar": {
"position": "fixed"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
