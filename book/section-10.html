<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>数据科学家：R语言</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="This is my first book on data science">
  <meta name="generator" content="bookdown 0.1 and GitBook 2.6.7">

  <meta property="og:title" content="数据科学家：R语言" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is my first book on data science" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="数据科学家：R语言" />
  
  <meta name="twitter:description" content="This is my first book on data science" />
  

<meta name="author" content="林荟">

<meta name="date" content="2016-09-07">

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="section-9.html">
<link rel="next" href="section-11.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">数据科学家：R语言</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> 介绍</a></li>
<li class="chapter" data-level="2" data-path="section-2.html"><a href="section-2.html"><i class="fa fa-check"></i><b>2</b> 数据科学</a><ul>
<li class="chapter" data-level="2.1" data-path="section-2.html"><a href="section-2.html#section-2.1"><i class="fa fa-check"></i><b>2.1</b> 什么是数据科学？</a></li>
<li class="chapter" data-level="2.2" data-path="section-2.html"><a href="section-2.html#section-2.2"><i class="fa fa-check"></i><b>2.2</b> 什么是数据科学家？</a></li>
<li class="chapter" data-level="2.3" data-path="section-2.html"><a href="section-2.html#section-2.3"><i class="fa fa-check"></i><b>2.3</b> 数据科学家需要的技能</a></li>
<li class="chapter" data-level="2.4" data-path="section-2.html"><a href="section-2.html#section-2.4"><i class="fa fa-check"></i><b>2.4</b> 数据科学可以解决什么问题？</a><ul>
<li class="chapter" data-level="2.4.1" data-path="section-2.html"><a href="section-2.html#section-2.4.1"><i class="fa fa-check"></i><b>2.4.1</b> 前提要求</a></li>
<li class="chapter" data-level="2.4.2" data-path="section-2.html"><a href="section-2.html#section-2.4.2"><i class="fa fa-check"></i><b>2.4.2</b> 问题种类</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="section-3.html"><a href="section-3.html"><i class="fa fa-check"></i><b>3</b> 数据集模拟和背景介绍</a><ul>
<li class="chapter" data-level="3.1" data-path="section-3.html"><a href="section-3.html#section-3.1"><i class="fa fa-check"></i><b>3.1</b> 服装消费者数据</a></li>
<li class="chapter" data-level="3.2" data-path="section-3.html"><a href="section-3.html#section-3.2"><i class="fa fa-check"></i><b>3.2</b> 航空公司满意度调查</a></li>
<li class="chapter" data-level="3.3" data-path="section-3.html"><a href="section-3.html#section-3.3"><i class="fa fa-check"></i><b>3.3</b> 生猪疫情风险预测数据</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="section-4.html"><a href="section-4.html"><i class="fa fa-check"></i><b>4</b> 数据分析一般流程</a><ul>
<li class="chapter" data-level="4.1" data-path="section-4.html"><a href="section-4.html#section-4.1"><i class="fa fa-check"></i><b>4.1</b> 问题到数据</a></li>
<li class="chapter" data-level="4.2" data-path="section-4.html"><a href="section-4.html#section-4.2"><i class="fa fa-check"></i><b>4.2</b> 数据到信息</a></li>
<li class="chapter" data-level="4.3" data-path="section-4.html"><a href="section-4.html#section-4.3"><i class="fa fa-check"></i><b>4.3</b> 信息到行动</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="section-5.html"><a href="section-5.html"><i class="fa fa-check"></i><b>5</b> 数据预处理</a><ul>
<li class="chapter" data-level="5.1" data-path="section-5.html"><a href="section-5.html#section-5.1"><i class="fa fa-check"></i><b>5.1</b> 介绍</a></li>
<li class="chapter" data-level="5.2" data-path="section-5.html"><a href="section-5.html#section-5.2"><i class="fa fa-check"></i><b>5.2</b> 数据清理</a></li>
<li class="chapter" data-level="5.3" data-path="section-5.html"><a href="section-5.html#section-5.3"><i class="fa fa-check"></i><b>5.3</b> 缺失值填补</a><ul>
<li class="chapter" data-level="5.3.1" data-path="section-5.html"><a href="section-5.html#section-5.3.1"><i class="fa fa-check"></i><b>5.3.1</b> 中位数或众数填补</a></li>
<li class="chapter" data-level="5.3.2" data-path="section-5.html"><a href="section-5.html#k-"><i class="fa fa-check"></i><b>5.3.2</b> K-近邻填补</a></li>
<li class="chapter" data-level="5.3.3" data-path="section-5.html"><a href="section-5.html#section-5.3.3"><i class="fa fa-check"></i><b>5.3.3</b> 袋状树填补</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="section-5.html"><a href="section-5.html#section-5.4"><i class="fa fa-check"></i><b>5.4</b> 中心化和标量化</a></li>
<li class="chapter" data-level="5.5" data-path="section-5.html"><a href="section-5.html#section-5.5"><i class="fa fa-check"></i><b>5.5</b> 有偏分布</a></li>
<li class="chapter" data-level="5.6" data-path="section-5.html"><a href="section-5.html#section-5.6"><i class="fa fa-check"></i><b>5.6</b> 处理离群点</a></li>
<li class="chapter" data-level="5.7" data-path="section-5.html"><a href="section-5.html#section-5.7"><i class="fa fa-check"></i><b>5.7</b> 共线性</a></li>
<li class="chapter" data-level="5.8" data-path="section-5.html"><a href="section-5.html#section-5.8"><i class="fa fa-check"></i><b>5.8</b> 稀疏变量</a></li>
<li class="chapter" data-level="5.9" data-path="section-5.html"><a href="section-5.html#section-5.9"><i class="fa fa-check"></i><b>5.9</b> 编码名义变量</a></li>
<li class="chapter" data-level="5.10" data-path="section-5.html"><a href="section-5.html#section-5.10"><i class="fa fa-check"></i><b>5.10</b> 本章总结</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="section-6.html"><a href="section-6.html"><i class="fa fa-check"></i><b>6</b> 数据整合和整形</a><ul>
<li class="chapter" data-level="6.1" data-path="section-6.html"><a href="section-6.html#section-6.1"><i class="fa fa-check"></i><b>6.1</b> 数据整合</a><ul>
<li class="chapter" data-level="6.1.1" data-path="section-6.html"><a href="section-6.html#baseapply"><i class="fa fa-check"></i><b>6.1.1</b> base包：apply()</a></li>
<li class="chapter" data-level="6.1.2" data-path="section-6.html"><a href="section-6.html#plyrddply"><i class="fa fa-check"></i><b>6.1.2</b> plyr包：ddply()函数</a></li>
<li class="chapter" data-level="6.1.3" data-path="section-6.html"><a href="section-6.html#dplyr"><i class="fa fa-check"></i><b>6.1.3</b> dplyr包</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="section-6.html"><a href="section-6.html#section-6.2"><i class="fa fa-check"></i><b>6.2</b> 数据整形</a><ul>
<li class="chapter" data-level="6.2.1" data-path="section-6.html"><a href="section-6.html#reshape2"><i class="fa fa-check"></i><b>6.2.1</b> <code>reshape2</code>包</a></li>
<li class="chapter" data-level="6.2.2" data-path="section-6.html"><a href="section-6.html#tidyr"><i class="fa fa-check"></i><b>6.2.2</b> <code>tidyr</code>包</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="section-6.html"><a href="section-6.html#-1"><i class="fa fa-check"></i><b>6.3</b> 本章总结</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="section-7.html"><a href="section-7.html"><i class="fa fa-check"></i><b>7</b> 基础建模技术</a><ul>
<li class="chapter" data-level="7.1" data-path="section-7.html"><a href="section-7.html#section-7.1"><i class="fa fa-check"></i><b>7.1</b> 有监督和无监督</a></li>
<li class="chapter" data-level="7.2" data-path="section-7.html"><a href="section-7.html#section-7.2"><i class="fa fa-check"></i><b>7.2</b> 误差及其来源</a><ul>
<li class="chapter" data-level="7.2.1" data-path="section-7.html"><a href="section-7.html#section-7.2.1"><i class="fa fa-check"></i><b>7.2.1</b> 系统误差和随机误差</a></li>
<li class="chapter" data-level="7.2.2" data-path="section-7.html"><a href="section-7.html#section-7.2.2"><i class="fa fa-check"></i><b>7.2.2</b> 应变量误差</a></li>
<li class="chapter" data-level="7.2.3" data-path="section-7.html"><a href="section-7.html#section-7.2.3"><i class="fa fa-check"></i><b>7.2.3</b> 自变量误差</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="section-7.html"><a href="section-7.html#section-7.3"><i class="fa fa-check"></i><b>7.3</b> 数据划分和再抽样</a><ul>
<li class="chapter" data-level="7.3.1" data-path="section-7.html"><a href="section-7.html#section-7.3.1"><i class="fa fa-check"></i><b>7.3.1</b> 划分训练集和测试集</a></li>
<li class="chapter" data-level="7.3.2" data-path="section-7.html"><a href="section-7.html#section-7.3.2"><i class="fa fa-check"></i><b>7.3.2</b> 重抽样</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="section-7.html"><a href="section-7.html#-2"><i class="fa fa-check"></i><b>7.4</b> 本章总结</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="section-8.html"><a href="section-8.html"><i class="fa fa-check"></i><b>8</b> 模型评估度量</a><ul>
<li class="chapter" data-level="8.1" data-path="section-8.html"><a href="section-8.html#section-8.1"><i class="fa fa-check"></i><b>8.1</b> 回归模型评估度量</a></li>
<li class="chapter" data-level="8.2" data-path="section-8.html"><a href="section-8.html#section-8.2"><i class="fa fa-check"></i><b>8.2</b> 分类模型评估度量</a><ul>
<li class="chapter" data-level="8.2.1" data-path="section-8.html"><a href="section-8.html#kappa"><i class="fa fa-check"></i><b>8.2.1</b> Kappa统计量</a></li>
<li class="chapter" data-level="8.2.2" data-path="section-8.html"><a href="section-8.html#roc"><i class="fa fa-check"></i><b>8.2.2</b> ROC曲线</a></li>
<li class="chapter" data-level="8.2.3" data-path="section-8.html"><a href="section-8.html#section-8.2.3"><i class="fa fa-check"></i><b>8.2.3</b> 提升图</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="section-8.html"><a href="section-8.html#-3"><i class="fa fa-check"></i><b>8.3</b> 本章总结</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="section-9.html"><a href="section-9.html"><i class="fa fa-check"></i><b>9</b> 特征工程</a><ul>
<li class="chapter" data-level="9.1" data-path="section-9.html"><a href="section-9.html#section-9.1"><i class="fa fa-check"></i><b>9.1</b> 特征构建</a></li>
<li class="chapter" data-level="9.2" data-path="section-9.html"><a href="section-9.html#section-9.2"><i class="fa fa-check"></i><b>9.2</b> 特征提取</a><ul>
<li class="chapter" data-level="9.2.1" data-path="section-9.html"><a href="section-9.html#section-9.2.1"><i class="fa fa-check"></i><b>9.2.1</b> 初步探索数据</a></li>
<li class="chapter" data-level="9.2.2" data-path="section-9.html"><a href="section-9.html#section-9.2.2"><i class="fa fa-check"></i><b>9.2.2</b> 主成分分析</a></li>
<li class="chapter" data-level="9.2.3" data-path="section-9.html"><a href="section-9.html#section-9.2.3"><i class="fa fa-check"></i><b>9.2.3</b> 探索性因子分析</a></li>
<li class="chapter" data-level="9.2.4" data-path="section-9.html"><a href="section-9.html#section-9.2.4"><i class="fa fa-check"></i><b>9.2.4</b> 高维标度化</a></li>
<li class="chapter" data-level="9.2.5" data-path="section-9.html"><a href="section-9.html#section-9.2.5"><i class="fa fa-check"></i><b>9.2.5</b> 知识扩展</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="section-9.html"><a href="section-9.html#section-9.3"><i class="fa fa-check"></i><b>9.3</b> 特征选择</a><ul>
<li class="chapter" data-level="9.3.1" data-path="section-9.html"><a href="section-9.html#section-9.3.1"><i class="fa fa-check"></i><b>9.3.1</b> 过滤法</a></li>
<li class="chapter" data-level="9.3.2" data-path="section-9.html"><a href="section-9.html#section-9.3.2"><i class="fa fa-check"></i><b>9.3.2</b> 绕封法</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="section-10.html"><a href="section-10.html"><i class="fa fa-check"></i><b>10</b> 线性回归极其衍生</a><ul>
<li class="chapter" data-level="10.1" data-path="section-10.html"><a href="section-10.html#section-10.1"><i class="fa fa-check"></i><b>10.1</b> 普通线性回归</a><ul>
<li class="chapter" data-level="10.1.1" data-path="section-10.html"><a href="section-10.html#section-10.1.1"><i class="fa fa-check"></i><b>10.1.1</b> 最小二乘线性模型</a></li>
<li class="chapter" data-level="10.1.2" data-path="section-10.html"><a href="section-10.html#section-10.1.2"><i class="fa fa-check"></i><b>10.1.2</b> 回归诊断</a></li>
<li class="chapter" data-level="10.1.3" data-path="section-10.html"><a href="section-10.html#section-10.1.3"><i class="fa fa-check"></i><b>10.1.3</b> 离群点，高杠杆点和强影响点</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="section-10.html"><a href="section-10.html#section-10.2"><i class="fa fa-check"></i><b>10.2</b> 收缩方法</a><ul>
<li class="chapter" data-level="10.2.1" data-path="section-10.html"><a href="section-10.html#section-10.2.1"><i class="fa fa-check"></i><b>10.2.1</b> 岭回归</a></li>
<li class="chapter" data-level="10.2.2" data-path="section-10.html"><a href="section-10.html#lasso"><i class="fa fa-check"></i><b>10.2.2</b> Lasso</a></li>
<li class="chapter" data-level="10.2.3" data-path="section-10.html"><a href="section-10.html#section-10.2.3"><i class="fa fa-check"></i><b>10.2.3</b> 弹性网络</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="section-10.html"><a href="section-10.html#lasso"><i class="fa fa-check"></i><b>10.3</b> 知识扩展：Lasso的变量选择功能</a></li>
<li class="chapter" data-level="10.4" data-path="section-10.html"><a href="section-10.html#glmnet"><i class="fa fa-check"></i><b>10.4</b> 广义线性模型压缩方法及<code>glmnet</code>包</a></li>
<li class="chapter" data-level="10.5" data-path="section-10.html"><a href="section-10.html#section-10.5"><i class="fa fa-check"></i><b>10.5</b> 分层线性回归</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="section-11.html"><a href="section-11.html"><i class="fa fa-check"></i><b>11</b> 树模型</a><ul>
<li class="chapter" data-level="11.1" data-path="section-11.html"><a href="section-11.html#section-11.1"><i class="fa fa-check"></i><b>11.1</b> 基本树模型</a></li>
<li class="chapter" data-level="11.2" data-path="section-11.html"><a href="section-11.html#section-11.2"><i class="fa fa-check"></i><b>11.2</b> 装袋树</a></li>
<li class="chapter" data-level="11.3" data-path="section-11.html"><a href="section-11.html#section-11.3"><i class="fa fa-check"></i><b>11.3</b> 随机森林</a></li>
<li class="chapter" data-level="11.4" data-path="section-11.html"><a href="section-11.html#section-11.4"><i class="fa fa-check"></i><b>11.4</b> 其它树话题</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="section-12.html"><a href="section-12.html"><i class="fa fa-check"></i><b>12</b> 聚类判别分析</a><ul>
<li class="chapter" data-level="12.1" data-path="section-12.html"><a href="section-12.html#section-12.1"><i class="fa fa-check"></i><b>12.1</b> 聚类分析</a></li>
<li class="chapter" data-level="12.2" data-path="section-12.html"><a href="section-12.html#section-12.2"><i class="fa fa-check"></i><b>12.2</b> 判别分析</a><ul>
<li class="chapter" data-level="12.2.1" data-path="section-12.html"><a href="section-12.html#section-12.2.1"><i class="fa fa-check"></i><b>12.2.1</b> 逻辑回归</a></li>
<li class="chapter" data-level="12.2.2" data-path="section-12.html"><a href="section-12.html#section-12.2.2"><i class="fa fa-check"></i><b>12.2.2</b> 线性判别分析</a></li>
<li class="chapter" data-level="12.2.3" data-path="section-12.html"><a href="section-12.html#section-12.2.3"><i class="fa fa-check"></i><b>12.2.3</b> 最小二乘判别分析</a></li>
<li class="chapter" data-level="12.2.4" data-path="section-12.html"><a href="section-12.html#section-12.2.4"><i class="fa fa-check"></i><b>12.2.4</b> 朴素贝叶斯</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="section-12.html"><a href="section-12.html#section-12.3"><i class="fa fa-check"></i><b>12.3</b> 案例：客户分组</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="section-13.html"><a href="section-13.html"><i class="fa fa-check"></i><b>13</b> 关联法则分析</a><ul>
<li class="chapter" data-level="13.1" data-path="section-13.html"><a href="section-13.html#section-13.1"><i class="fa fa-check"></i><b>13.1</b> 关联法则简介</a></li>
<li class="chapter" data-level="13.2" data-path="section-13.html"><a href="section-13.html#section-13.2"><i class="fa fa-check"></i><b>13.2</b> 案例：商业购物篮分析</a></li>
<li class="chapter" data-level="13.3" data-path="section-13.html"><a href="section-13.html#section-13.3"><i class="fa fa-check"></i><b>13.3</b> 关联法则可视化</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="section-14.html"><a href="section-14.html"><i class="fa fa-check"></i><b>14</b> 数据可视化和结果展示</a><ul>
<li class="chapter" data-level="14.1" data-path="section-14.html"><a href="section-14.html#r-markdown"><i class="fa fa-check"></i><b>14.1</b> R Markdown</a><ul>
<li class="chapter" data-level="14.1.1" data-path="section-14.html"><a href="section-14.html#r-markdown"><i class="fa fa-check"></i><b>14.1.1</b> 什么是R Markdown?</a></li>
<li class="chapter" data-level="14.1.2" data-path="section-14.html"><a href="section-14.html#how-to-start"><i class="fa fa-check"></i><b>14.1.2</b> How to Start?</a></li>
<li class="chapter" data-level="14.1.3" data-path="section-14.html"><a href="section-14.html#interactive-r-markdown-document"><i class="fa fa-check"></i><b>14.1.3</b> Interactive R Markdown Document</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="15" data-path="section-15.html"><a href="section-15.html"><i class="fa fa-check"></i><b>15</b> 数据科学的科学</a></li>
<li class="chapter" data-level="16" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>16</b> References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">数据科学家：R语言</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="section-10" class="section level1">
<h1><span class="header-section-number">第10章</span> 线性回归极其衍生</h1>
<p>本章主要讲线性回归和它的衍生，顺序由易到难。先介绍普通线性回归（也称为最小二乘回归），这是非常简单的（可能是最简单的）有监督方法，相对于其它方法，普通线性回归可谓历史悠久，声名远扬。很多非理工科专业的小伙伴也都听过或者用过该模型。虽然和很多我们将要介绍的更新的模型比起来，普通线性回归太过低端，但它依旧是有用并且被广泛使用，此外，很多新模型其实是普通线性模型的衍生。因此理解普通线性模型对理解后面更加复杂的模型非常重要。之后我们会讲到两个收缩方法：岭回归和Lasso回归。和普通最小二乘估计相比，收缩方法可以将参数估计向0“收缩”，当观测量少时（相对于变量个数而言），这种方法有助于减少估计方差，稳定参数估计。接下来我们会介绍分层线性回归，和贝叶斯框架下的分层线性回归。R有强大的拟合线性模型的功能。我们先回顾一些基本知识，展示如何用R展示拟合相应模型，但是本章不会介绍所有实践中需要知道的知识。我们鼓励想进一步了解模型的读者参考我们在介绍该部分时列出的参考资料。本章中我们还是使用服装消费者数据解释线性模型。我们需要回答类似这样的问题：“那些变量是总消费量（线上和实体店消费额之和）的驱动因子？”这个问题的答案可以帮助公司知道需要将钱投到产品的哪个方面（如服装的设计，服装质量等）。</p>
<p>这里特别要注意的一点是，驱动因子不意味着原因。线性模型只假设变量之间存在<strong>关联性</strong>。如果某汽车客户问卷调查结果显示满意度和价格之间正相关，难道商家为了提高消费者满意度而刻意提高汽车价格？貌似不符合常识。更可能的情况是因为价格更高的汽车质量也更好，客户真正满意的是汽车的质量。因果关系在分析实践中是个很大的坑，在解释结果的时候一定要小心再小心，一定要将问题放在相应的语境中。</p>
<div id="section-10.1" class="section level2">
<h2><span class="header-section-number">10.1</span> 普通线性回归</h2>
<p>虽然最小二乘线性回归看起来太过简单粗暴，但现在很多更复杂的模型其基本形式也是线性的。比如逻辑回归，就是对因变量的均值进行逻辑变换后再拟合线性模型。通常我们都将神经网络模型归于非线性模型，但神经网络中的每个潜变量都是某些预测变量的线性组合。日光之下，并无新事。在大量新技术不断涌入更新换代的今天，人的思维更加容易见树不见林。很多事物本质上是有相似性的，找到光怪陆离的表象下的实质是一种重要的能力。在学习了很多不同的方法之后应该退后一步，看看这些方法的演变联系，对背后的知识进行提取抽象，触及本质，时不时停下来问自己：这些模型背后的根本思想是什么？ 在R实现这些模型也类似，不同的模型在R中的表达方法都是模仿线性模型的拟合语句。因此，只要理解了如何使用R拟合，解释和诊断线性模型，你能够举一反三的应用其它更加复杂的模型。本节主要介绍用R中<code>lm()</code>拟合最小二乘线性模型，以及该函数中的不同选项。然后我们将会讲到线性模型的诊断方法，它们用于检测模型的假设是否成立，或者我们拟合的结果是否充分。</p>
<div id="section-10.1.1" class="section level3">
<h3><span class="header-section-number">10.1.1</span> 最小二乘线性模型</h3>
<p>在线性模型中，</p>
<p><span class="math display">\[f(\mathbf{X})=\mathbf{X}\mathbf{\beta}=\beta_{0}+\sum_{j=1}^{p}\mathbf{x_{.j}}\beta_{j}\]</span></p>
<p>其中<span class="math inline">\(\mathbf{\beta}\)</span>是长度为<span class="math inline">\(p+1\)</span>的参数向量。这里的数学公式表达和之前6.1中介绍的一致。最小二乘估计就是选择<span class="math inline">\(\mathbf{\beta^{T}}=(\beta_{0},\beta_{1},...,\beta_{p})\)</span>最小化下面残差平方和：</p>
<p><span class="math display">\[RSS(\beta)=\sum_{i=1}^{N}(y_{i}-f(\mathbf{x_{i.}}))^{2}=\sum_{i=1}^{N}(y_{i}-\beta_{0}-\sum_{j=1}^{p}x_{ij}\beta_{j})^{2}\]</span></p>
<p>我们还是从载入数据开始。</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">dat&lt;-<span class="kw">read.csv</span>(<span class="st">&quot;/Users/happyrabbit/Documents/GitHub/DataScientistR/Data/SegData.csv&quot;</span>)</code></pre></div>
<p>在我们开始之前，还需要对数据进行一些清理，删除错误的样本观测，消费金额不能为负数。</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">dat&lt;-<span class="kw">subset</span>(dat,store_exp&gt;<span class="dv">0</span> &amp;<span class="st"> </span>online_exp&gt;<span class="dv">0</span>)</code></pre></div>
<p>我们将10个问卷调查变量当作自变量。</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">modeldat&lt;-dat[,<span class="kw">grep</span>(<span class="st">&quot;Q&quot;</span>,<span class="kw">names</span>(dat))]</code></pre></div>
<p>将实体店消费量和在线消费之和当作应变量。</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># 得到总消费量=实体店消费+在线消费</span>
modeldat$total_exp&lt;-dat$store_exp+dat$online_exp</code></pre></div>
<p>我们先检查一下数据，看是不是有缺失值或者离群点：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># 这里没有展示输出结果</span>
<span class="kw">summary</span>(modeldat)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))
<span class="kw">hist</span>(modeldat$total_exp,<span class="dt">main=</span><span class="st">&quot;&quot;</span>,<span class="dt">xlab=</span><span class="st">&quot;total_exp&quot;</span>)
<span class="kw">boxplot</span>(modeldat$total_exp)</code></pre></div>
<p><img src="DS_R_files/figure-html/checkplot-1.png" width="80%" style="display: block; margin: auto;" /></p>
<p>数据集<code>modeldat</code>中没有缺失值，但是明显有离群点，而且应变量<code>total_exp</code>分布明显偏离正态。我们删除离群点，然后对应变量进行对数变换。</p>
<p>我们用之前数据预处理章节介绍的Z分值的方法查找并删除离群点。这里不重复解释，不明白的读者可以返回复习相应的章节。</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">y&lt;-modeldat$total_exp
<span class="co"># 求Z分值</span>
zs&lt;-(y-<span class="kw">mean</span>(y))/<span class="kw">mad</span>(y)
<span class="co"># 找到Z分值大于3.5的离群点，删除这些观测</span>
modeldat&lt;-modeldat[-<span class="kw">which</span>(zs&gt;<span class="fl">3.5</span>),]</code></pre></div>
<p>这里我们先不对应变量进行对数变换，之后在回归函数的公式里对应变量进行变换。接下来检查变量的共线性：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(corrplot)
correlation&lt;-<span class="kw">cor</span>(modeldat[,<span class="kw">grep</span>(<span class="st">&quot;Q&quot;</span>,<span class="kw">names</span>(modeldat))])
<span class="kw">corrplot.mixed</span>(correlation,<span class="dt">order=</span><span class="st">&quot;hclust&quot;</span>,<span class="dt">tl.pos=</span><span class="st">&quot;lt&quot;</span>,<span class="dt">upper=</span><span class="st">&quot;ellipse&quot;</span>)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:corplotlm"></span>
<img src="DS_R_files/figure-html/corplotlm-1.png" alt="自变量相关矩阵图" width="80%" />
<p class="caption">
Figure 10.1: 自变量相关矩阵图
</p>
</div>
<p>由图<a href="section-10.html#fig:corplotlm">10.1</a> 可以看到，变量之间有很强的相关性。我们用之前在预处理章节中提到的删除高度相关变量的算法，设置阈值为0.75：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(caret)
highcor&lt;-<span class="kw">findCorrelation</span>(correlation,<span class="dt">cutoff=</span>.<span class="dv">75</span>)
modeldat&lt;-modeldat[,-highcor]</code></pre></div>
<p>现在我们可以拟合线性模型。标准的模型公式表达是在“~”号的左边指定因变量，右边指定自变量。“.”表示数据集<code>modeldat</code>中除了因变量之外的所有变量都被当作自变量。这里我们没有考虑交互效应，如果要添加<code>Q1</code>和<code>Q2</code>的交互效应，只要在“~”右边加上“Q1*Q2”即可。注意下面的代码中我们对原始变量进行了对数变换（<code>log(total_exp)</code>）。</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lmfit&lt;-<span class="kw">lm</span>(<span class="kw">log</span>(total_exp)~.,<span class="dt">data=</span>modeldat)
<span class="kw">summary</span>(lmfit)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = log(total_exp) ~ ., data = modeldat)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1.17494 -0.13719  0.01284  0.14163  0.56227 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  8.098314   0.054286 149.177  &lt; 2e-16 ***
## Q1          -0.145340   0.008823 -16.474  &lt; 2e-16 ***
## Q2           0.102275   0.019492   5.247 1.98e-07 ***
## Q3           0.254450   0.018348  13.868  &lt; 2e-16 ***
## Q6          -0.227684   0.011520 -19.764  &lt; 2e-16 ***
## Q8          -0.090706   0.016497  -5.498 5.15e-08 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.2262 on 805 degrees of freedom
## Multiple R-squared:  0.8542, Adjusted R-squared:  0.8533 
## F-statistic: 943.4 on 5 and 805 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>从模型结果总结中我们可以看到各个自变量的参数估计（<code>Estimate</code>列）、标准差（<code>Std. Error</code>），t统计量（<code>t value</code>）和p值（<code>Pr(&gt;|t|)</code>）。在输出的底部包含了残差标准误，即RMSE（<code>Residual standard error</code>），<span class="math inline">\(R^2\)</span>（<code>Multiple R-squared</code>）和调整后的<span class="math inline">\(R^2\)</span>（<code>Adjusted R-squared</code>），模型的F统计量（<code>F-statistic</code>）以及相应F检验的显著性p值（<code>p-value</code>）。</p>
<ul>
<li><strong>关于p值的讨论</strong></li>
</ul>
<p>谈到p值，不能不提美国统计协会在2016年2月发表的关于P值的声明 “Position on p-values: context, process, and purpose” <span class="citation">(Ronald L. Wassersteina <a href="#ref-ASA_P">2016</a>)</span> ，统计之都有一篇对该声明的中文总结<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>。关于p值弊端的讨论在统计学领域已经不是新鲜事。其中一些抨击言辞比较激烈的是 Siegfried：</p>
<blockquote>
<p>这是科学中最肮脏的秘密：使用统计假设检验的“科学方法”建立在一个脆弱的基础之上。——ScienceNews（Siegfried, 2010）</p>
</blockquote>
<blockquote>
<p>假设检验中用到的统计方法……比Facebook隐私条款的缺陷还多。——ScienceNews（Siegfried, 2014）</p>
</blockquote>
<p>尽管争议已经持续了很久，但这是第一次统计协会对该话题给出郑重的声明，其主要目的不是解决该问题，而是对这些批评和讨论作一个回应，讨论发表一些关于p值的普遍共识，唤起大家对科学研究可重复性的重要性。声明中概括了关于p值的6个准则：</p>
<blockquote>
<ol style="list-style-type: decimal">
<li>P值可以表达的是数据与一个给定模型不匹配的程度。</li>
<li>P值并不能衡量某条假设为真的概率，或是数据仅由随机因素产生的概率。</li>
<li>科学结论、商业决策或政策制定不应该仅依赖于P值是否超过一个给定的阈值。</li>
<li>合理的推断过程需要完整的报告和透明度。</li>
<li>P值或统计显著性并不衡量影响的大小或结果的重要性。</li>
<li>P值就其本身而言，并不是一个非常好的对模型或假设所含证据大小的衡量。</li>
</ol>
</blockquote>
<p>在文章末尾列举了一些其他替代手段，其中之一就是报告置信区间而非p值。 回到当前的例子，这里我们不去讨论参数估计的对应p值，而是使用各个参数估计的置信区间。在R中可以用下面代码得到置信区间：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">confint</span>(lmfit,<span class="dt">level=</span><span class="fl">0.9</span>)</code></pre></div>
<pre><code>##                     5 %        95 %
## (Intercept)  8.00891811  8.18771037
## Q1          -0.15986889 -0.13081186
## Q2           0.07017625  0.13437445
## Q3           0.22423576  0.28466333
## Q6          -0.24665434 -0.20871330
## Q8          -0.11787330 -0.06353905</code></pre>
<p>上面的输出就是参数的90%置信区间。其中<code>level=0.9</code>将置信度设置为0.9。</p>
<p>拟合线性模型是非常容易的，以致于很多分析师拟合了模型之后不考虑模型是否合理，直接撰写结果报告。其实我们可以很容易用R从不同方面检查模型的拟合情况和假设条件。下面的几个小节中我们将介绍一些常用的线性模型诊断方法。</p>
</div>
<div id="section-10.1.2" class="section level3">
<h3><span class="header-section-number">10.1.2</span> 回归诊断</h3>
<p>拟合线性模型是非常容易的，以致于很多分析师拟合了模型之后不考虑模型是否合理，直接撰写结果报告。其实我们可以很容易用R从不同方面检查模型的拟合情况和假设条件。下面的几个小节中我们将介绍一些常用的线性模型诊断方法。我们希望需要最小二乘估计（OLS）同时也是最优线性无偏估计（BLUE）。换句话说，我们希望得到的估计的期望即为真实值（无偏），且最小化残差方差（最优）。根据高斯-马尔可夫定理（Gauss-Markov theorem），OLS在下面条件满足时是BLUE:</p>
<ol style="list-style-type: decimal">
<li>自变量（<span class="math inline">\(\mathbf{x_{.j}}\)</span>）和随机误差（<span class="math inline">\(\mathbf{\epsilon}\)</span>）不相关，即：<span class="math inline">\(cov(\mathbf{x_{.j},\epsilon})=0\)</span> 对 <span class="math inline">\(\forall j=j\in1...p\)</span></li>
<li>随机误差均值为0：<span class="math inline">\(E(\mathbf{\epsilon|X})=0\)</span></li>
<li>随机误差方差一致且相互独立：<span class="math inline">\(Var(\mathbf{\epsilon})=\sigma^{2}I\)</span>，其中<span class="math inline">\(\sigma\)</span>是正实数，<span class="math inline">\(I\)</span>是<span class="math inline">\(n\times n\)</span>的单位矩阵</li>
</ol>
<p>下面介绍4种图形诊断。</p>
<ol style="list-style-type: decimal">
<li><p>残差图（Residuals vs Fitted）</p>
残差图分析法是一种直观、方便的分析方法。它以残差<span class="math inline">\(\epsilon_{i}\)</span>为纵坐标，以样本拟合值为横坐标画散点图（也可以绘制横坐标为任意自变量的残差散点图）。正常情况下残差分布应该是随机的。我们要检查残差图的如下几个方面：
<ul>
<li>残差是否在0附近分布</li>
<li>残差分布是否随机，如果呈现出某种特定分布模式（如：随横坐标的增大而增大或减小）的话，说明当前模型关系的假设不充分</li>
<li>残差是否存在异方差性，比如随着拟合值增大残差分布方差增加，这就说明残差分布有异方差性。如前所述，当存在异方差时，参数估计值虽然是无偏的，但不是最小方差线性无偏估计。由于参数的显著性检验是基于残差分布假设的，所以在该假设不成立的情况下该检验也将失效。如果你用该回归方程来预测新样本，效果很可能极不理想。</li>
</ul></li>
<li><p>Q-Q图（Norm Q-Q）</p>
<p>Q-Q图是一种正态分布检测。对于标准状态分布，Q-Q图上的点分布在Y=X直线上，点偏离直线越远说明样本偏离正态分布越远。</p></li>
<li><p>标准化残差方根散点图（Scale-Location）</p>
<p>和残差图类似，横坐标依旧是样本拟合值，纵坐标变为了标准化残差的绝对值开方。</p></li>
<li><p>Cook距离图（Cook’s distance）</p>
<p>该图用于判断观测值是否有异常点。一般认为 当D&lt;0.5时认为不是异常值点；当D&gt;0.5时认为是异常值点。</p></li>
</ol>
<p>对回归结果应用<code>plot()</code>函数可以得到不同的图形诊断。</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))
<span class="kw">plot</span>(lmfit,<span class="dt">which=</span><span class="dv">1</span>)
<span class="kw">plot</span>(lmfit,<span class="dt">which=</span><span class="dv">2</span>)
<span class="kw">plot</span>(lmfit,<span class="dt">which=</span><span class="dv">3</span>)
<span class="kw">plot</span>(lmfit,<span class="dt">which=</span><span class="dv">4</span>)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:errorplot"></span>
<img src="DS_R_files/figure-html/errorplot-1.png" alt="一般线性回归残差图" width="80%" />
<p class="caption">
Figure 10.2: 一般线性回归残差图
</p>
</div>
<p>从回归的四个图形结果（图<a href="section-10.html#fig:errorplot">10.2</a>）来看：</p>
<ul>
<li>残差图：数据点都基本均匀地分布在直线y=0的两侧, 无明显趋势，满足线性假设。</li>
<li>标准Q-Q图：图上的点基本都在y=x直线附件，可认为残差近似服从正态分布；</li>
<li>标准化残差方根散点图：若满足不变方差假设，则在该图中水平线周围的点应随机分布，最高点为残差最大值点。该图显示基本符合方差齐性的要求。</li>
<li>Cook距离图：最大的Cook距离为0.05左右，可以认为没有异常值点。</li>
</ul>
</div>
<div id="section-10.1.3" class="section level3">
<h3><span class="header-section-number">10.1.3</span> 离群点，高杠杆点和强影响点</h3>
<p>关于一般线性回归，最好检查下是否有观测会强烈影响线性模型拟合结果。如果一个或者几个观测对模型结果有决定性的影响，那么用这些观测得到的模型是具有误导性的。这里我们介绍这三类观测点的检测：离群点，高杠杆点和强影响点。</p>
<ul>
<li>离群点</li>
</ul>
<p>刚才介绍的Cook距离图，以及之前讲到的Z分值都可以用来检测线性模型中的离群点。注意，Z分值仅仅是针对应变量观测而言，和使用的模型无关，即其并未考虑模型的拟合情况。下面我们用<code>car</code>包<span class="citation">(John Fox and Weisberg <a href="#ref-car">2011</a>)</span>中的<code>outlierTest()</code>函数对拟合模型对象检测是否存在离群点，和Z分值方法鉴别的离群点不同，这里的离群点指的是<strong>那些模型预测效果不佳的观测点</strong>，通常有很大的、或正或负的残差，正残差说明模型低估了响应值，负残差说明高佑了响应值。这里使用的是Bonferroni离群点检验，该检验也可作用于广义线性模型。对于一般线性模型使用的是t检验，对于广义线性模型使用的是正态检验。关于该检验相关知识见 <span class="citation">(Williams <a href="#ref-Williams1987">1987</a>; J. Fox <a href="#ref-fox2008">2008</a>; Cook and Weisberg <a href="#ref-cookd">1982</a>; S. Weisberg <a href="#ref-weisberg14">2014</a>)</span>。</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(car)
<span class="kw">outlierTest</span>(lmfit) <span class="co">#Bonferroni离群点检验</span></code></pre></div>
<pre><code>##      rstudent unadjusted p-value Bonferonni p
## 960 -5.295504          1.533e-07   0.00012432</code></pre>
<p><code>outlierTest()</code>函数是根据单个最大（或正或负）残差值的显著性来判断是否有离群点，若不显著，则说明数据集中没有离群点，若显著，则建议删除该离群点，然后再检验是否还有其他离群点存在。这里我们删除第960个被认为是离群点的观测。</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">outlierTest</span>(lmfit)</code></pre></div>
<pre><code>##      rstudent unadjusted p-value Bonferonni p
## 960 -5.295504          1.533e-07   0.00012432</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># 这里数据modeldat的行名是原数据集的行号，所以是字符类型</span>
<span class="co"># 找到相应的观测</span>
idex&lt;-<span class="kw">which</span>(<span class="kw">row.names</span>(modeldat)==<span class="st">&quot;960&quot;</span>)
<span class="co"># 删除离群观测</span>
modeldat=modeldat[-idex,]</code></pre></div>
<p>接下来我们再拟合一次模型然后检测看看是否还有离群点：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lmfit&lt;-<span class="kw">lm</span>(<span class="kw">log</span>(total_exp)~.,<span class="dt">data=</span>modeldat)
<span class="kw">outlierTest</span>(lmfit)</code></pre></div>
<pre><code>## 
## No Studentized residuals with Bonferonni p &lt; 0.05
## Largest |rstudent|:
##      rstudent unadjusted p-value Bonferonni p
## 155 -3.818112         0.00014483      0.11731</code></pre>
<p>可以看到现在没有检测出显著离群点。</p>
<p>高杠杆值点是与其他预测变量有关的离群点，即它们是由许多异常的预测变量组合起来的，与响应变量值没有关系。 高杠杆值的观测点可通过帽子矩阵的值（hat statistic）判断。对于一个给定的数据集，帽子均值为<span class="math inline">\(p/n\)</span>，其中p是模型估计的参数数目（包含截距项），n是样本量。一般来说，若观测点的帽子值大于帽子均值的2或3倍，则可认定为高杠杆值点。</p>
<!--
hat.plot<-function(fit){  
  p<-length(coefficients(fit))  
  n<-length(fitted(fit))  
  plot(hatvalues(fit),main="Index Plot of Hat Values")  
  abline(h=c(2,3)*p/n,col="red",lty=2)  
  identify(1:n,hatvalues(fit),names(hatvalues(fit)))  
}  
hat.plot(model) 

此图中可以看到1号点是高杠杆值点。

强影响点

强影响点，即对模型参数估计值影响有些比例失衡的点。例如，当移除 模型的一个观测点时模型会发生巨大的改变，那么需要检测一下数据中是否存在强影响点。Cook距离，或称为D统计量。Cook’s D值大于4/(n-k-1)，则表明它是强影响点，其中n为样本量大小，k是预测变量数目（有助于鉴别强影响点，但并不提供关于这些点如何影响模型的信息）。
对回归的影响点。根据Cook距离，13号点可能是个强影响点。

帽子统计量、DFFITS准测、Cook统计量和COVRATIO准则在R软件可分别通过hatvalues(),dffits(),cooks.distance()和covration()函数计算。influence.measures()可对一次获得这四个统计量的结果。 影响分析综合分析

influencePlot(model) 
#car包中的influencePlot（）函数，可将离群点、
#杠杆点和强影响点的信息整合到一幅图形中
influence.measures(model)

纵坐标超过2或小于-2的州可被认为是离群点，水平轴超过0.2或0.3的州有高杠杆值（通常为预测值的组合）。圆圈大小与影响成比例，圆圈很大的点可能是对模型估计造成的不成比例影响的强影响点。influence.measures()的inf用×标注异常值。
-->
</div>
</div>
<div id="section-10.2" class="section level2">
<h2><span class="header-section-number">10.2</span> 收缩方法</h2>
<p>之前特征工程的章节中讲到各种变量选择方法，其中我们对内嵌法没有详细展开。内嵌法是将特征选择的过程内嵌如建模的过程，它是学习器自身自主选择特征，这里我们要讲的收缩方法就属于内嵌法。我们可以通过对模型参数进行限制或者规范化来达到变量选择的效果，这些方法能将一些参数估计朝着0收缩。使用收缩方法提高模型拟合表现的原理可能不那么显而易见，但是收缩方法的效果是非常好的，这也是我最常使用的方法，尤其是项目要求使用可以解释的模型时。收缩方法不仅仅限于线性回归，之后我们在讲判别分析时会介绍将lasso用于逻辑回归。最常用的收缩方法是岭回归（ridge regression）、lasso以及弹性网络（elastic net）。弹性网络结合了岭回归和lasso中的罚函数，可以说是它们的一般化版本。</p>
<p>对于一般线性回归，在标准模型假设下最小二乘估计号称是方差最小无偏估计，这里的方差最小是在所有线性无偏估计当中最小。之间在介绍误差来源时讲过MSE是方差和偏差的一个组合。当预测变量存在高度相关时，估计量的方差可能会非常大，偏差的微小增加可能使得方差大幅度下降，因此对于回归模型中的多重共线性问题，有偏模型也可能得到具有竞争力的MSE取值。构建有偏回归模型的一种方法是在误差平方和的基础上加上一个惩罚项。</p>
<div id="section-10.2.1" class="section level3">
<h3><span class="header-section-number">10.2.1</span> 岭回归</h3>
<p>回顾最小二乘模型，它的目的是寻找参数的估计，以使得误差平方（RSS）和达到最小：</p>
<p><span class="math display">\[RSS=\Sigma_{i=1}^{n}(y_{i}-\beta_{0}-\Sigma_{j=1}^{p}\beta_{j}x_{ij})^{2}\]</span></p>
<p>岭回归<span class="citation">(H. A <a href="#ref-Hoerl1970">1970</a>)</span>和最小二乘回归有类似之处，不同在于优化的方程略有变化。岭回归寻找的是优化下面方程的<span class="math inline">\(\hat{\beta}^{R}\)</span>：</p>
<p><span class="math display">\[\Sigma_{i=1}^{n}(y_{i}-\beta_{0}-\Sigma_{j=1}^{p}\beta_{j}x_{ij})^{2}+\lambda\Sigma_{j=1}^{p}\beta_{j}^{2}=RSS+\lambda\Sigma_{j=1}^{p}\beta_{j}^{2}\]</span></p>
<p>其中<span class="math inline">\(\lambda &gt;0\)</span>是需要额外估计的调优参数。上式是在两个不同的准则间权衡。和最小二乘回归类似，岭回归考虑了最小化RSS，但其还有一个称为收缩惩罚的项<span class="math inline">\(\lambda\Sigma_{j=1}^{p}\beta_{j}^{2}\)</span>，该项在参数<span class="math inline">\(\beta_{1},...,\beta_{p}\)</span>趋近于0的时候变小，因此有向0收缩参数估计的用处。调优参数<span class="math inline">\(\lambda\)</span>用来调整这两个部分对最后参数估计的影响。当<span class="math inline">\(\lambda=0\)</span>时，惩罚项对结果没有影响，这时岭回归等同于最小二乘回归。当<span class="math inline">\(\lambda\rightarrow\infty\)</span>时，惩罚项的影响增大，岭回归系数估计趋近于0。这里惩罚只针对<span class="math inline">\(\beta_{1},...,\beta_{p}\)</span>，对截距项<span class="math inline">\(\beta_{0}\)</span>并没有惩罚。每个<span class="math inline">\(\lambda\)</span>值都对应一组参数估计，通过尝试不同的调优参数值，找到最优的模型。通常使用交互校验来选择调优参数，关于交互校验，我在之前基础建模技术那一章节已经讲过了。</p>
<p>有许多R函数可以进行岭回归。<code>MASS</code>包中的<code>lm.ridge()</code>函数，以及<code>elasticnet</code>包中的<code>enet()</code>函数，如果你知道调优参数的值，可以直接使用这两个函数拟合岭回归模型。如果你要对参数进行调优，最方便的函数是<code>caret</code>包中的<code>train()</code>函数。还是以服装消费者数据为例展示这些函数的使用，自变量为10个问卷调查问题以及年龄、性别、收入和房产情况，应变量为总体花销（在线花销和实体店花销的总和）。</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">dat&lt;-<span class="kw">read.csv</span>(<span class="st">&quot;/Users/happyrabbit/Documents/GitHub/DataScientistR/Data/SegData.csv&quot;</span>)
<span class="co"># 对数据进行一些清理，删除错误的样本观测，消费金额不能为负数</span>
dat&lt;-<span class="kw">subset</span>(dat,store_exp&gt;<span class="dv">0</span> &amp;<span class="st"> </span>online_exp&gt;<span class="dv">0</span>)
<span class="co"># 将10个问卷调查变量当作自变量</span>
trainx&lt;-dat[,<span class="kw">grep</span>(<span class="st">&quot;Q&quot;</span>,<span class="kw">names</span>(dat))]
<span class="co"># 将实体店消费量和在线消费之和当作应变量</span>
<span class="co"># 得到总消费量=实体店消费+在线消费</span>
trainy&lt;-dat$store_exp+dat$online_exp</code></pre></div>
<p>先用<code>train()</code>函数对参数进行调优。首先设置交互校验和参数调优范围，这里我们使用10层交互校验。岭回归在回归系数的平方和前加调优参数，因此最好对自变量进行标准化，这条准则适用于所有罚函数包含回归系数的方法。这里因为10个问题分值的范围是一致的，是否标准化并不太影响分析结果，但保险起见，建议大家在用此类方法前都先进行标准化：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ctrl &lt;-<span class="st"> </span><span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="dt">number =</span> <span class="dv">10</span>)
ridgeGrid &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">.lambda =</span> <span class="kw">seq</span>(<span class="dv">0</span>, .<span class="dv">1</span>, <span class="dt">length =</span> <span class="dv">20</span>))
<span class="kw">set.seed</span>(<span class="dv">100</span>)

ridgeRegTune &lt;-<span class="st"> </span><span class="kw">train</span>(trainx, trainy,
                      <span class="dt">method =</span> <span class="st">&quot;ridge&quot;</span>,
                      ## 用不同的罚函数值来拟合模型
                      <span class="dt">tuneGrid =</span> ridgeGrid,
                      <span class="dt">trControl =</span> ctrl,
                      ## 中心化和标度化变量
                      <span class="dt">preProc =</span> <span class="kw">c</span>(<span class="st">&quot;center&quot;</span>, <span class="st">&quot;scale&quot;</span>))
ridgeRegTune</code></pre></div>
<pre><code>## Ridge Regression 
## 
## 999 samples
##  10 predictor
## 
## Pre-processing: centered (10), scaled (10) 
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 899, 899, 899, 899, 899, 899, ... 
## Resampling results across tuning parameters:
## 
##   lambda       RMSE      Rsquared 
##   0.000000000  1763.347  0.7896421
##   0.005263158  1762.683  0.7898448
##   0.010526316  1762.469  0.7899844
##   0.015789474  1762.589  0.7900829
##   0.021052632  1762.975  0.7901534
##   0.026315789  1763.584  0.7902040
##   0.031578947  1764.388  0.7902401
##   0.036842105  1765.368  0.7902652
##   0.042105263  1766.510  0.7902819
##   0.047368421  1767.804  0.7902919
##   0.052631579  1769.243  0.7902967
##   0.057894737  1770.821  0.7902971
##   0.063157895  1772.532  0.7902938
##   0.068421053  1774.373  0.7902876
##   0.073684211  1776.339  0.7902788
##   0.078947368  1778.429  0.7902677
##   0.084210526  1780.638  0.7902547
##   0.089473684  1782.964  0.7902399
##   0.094736842  1785.406  0.7902237
##   0.100000000  1787.959  0.7902060
## 
## RMSE was used to select the optimal model using  the smallest value.
## The final value used for the model was lambda = 0.01052632.</code></pre>
<p>训练出的模型调优参数为0.01，对应RMSE和<span class="math inline">\(R^{2}\)</span>分别为1762.469和80%。从交互校验的RMSE结果图中可以看到，随着调优参数增加，RMSE有一个略微减小然后增加的过程。</p>
<p><img src="DS_R_files/figure-html/unnamed-chunk-162-1.png" width="672" /></p>
<p>训练出调优参数之后，很多函数都可以用来拟合岭回归。这里展示如何使用<code>elasticnet</code>包中的<code>enet()</code>函数：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ridgefit =<span class="st"> </span><span class="kw">enet</span>(<span class="dt">x =</span> <span class="kw">as.matrix</span>(trainx), <span class="dt">y =</span> trainy, <span class="dt">lambda =</span> <span class="fl">0.01</span>,
                <span class="co"># 这里设置将自变量标准化</span>
                <span class="dt">normalize =</span> <span class="ot">TRUE</span>)</code></pre></div>
<p>这里注意<code>ridgefit</code>只指定了岭回归的罚函数，由于弹性网模型同时具有岭回归和lasso罚函数，我们需要进一步通过<code>predict()</code>函数得到相应的拟合系数和拟合结果。针对<code>enet</code>对象的<code>predict()</code>函数可以通过参数<code>s</code>和<code>mode</code>来指定在lasso罚参数下的拟合，这里我们需要“屏蔽”lasso的罚函数参数，这可以通过设置<code>s = 1</code> 和 <code>mode = &quot;fraction&quot;</code>得到。我们在之后讲lasso的时候会进一步讲解：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ridgePred &lt;-<span class="st"> </span><span class="kw">predict</span>(ridgefit, <span class="dt">newx =</span> <span class="kw">as.matrix</span>(trainx), 
                     <span class="dt">s =</span> <span class="dv">1</span>, <span class="dt">mode =</span> <span class="st">&quot;fraction&quot;</span>, <span class="dt">type =</span> <span class="st">&quot;fit&quot;</span>)</code></pre></div>
<p>注意上面<code>type = &quot;fit&quot;</code>返回的结果是一个列表，其中<code>fit</code>项包含预测结果：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">names</span>(ridgePred)</code></pre></div>
<pre><code>## [1] &quot;s&quot;        &quot;fraction&quot; &quot;mode&quot;     &quot;fit&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">head</span>(ridgePred$fit)</code></pre></div>
<pre><code>##         1         2         3         4         5         6 
## 1290.4697  224.1595  591.4406 1220.6384  853.3572  908.2040</code></pre>
<p>如果要得到参数拟合结果，需要在<code>predict()</code>函数中设定<code>type=&quot;coefficients&quot;</code>：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ridgeCoef&lt;-<span class="kw">predict</span>(ridgefit,<span class="dt">newx =</span> <span class="kw">as.matrix</span>(trainx), 
                   <span class="dt">s=</span><span class="dv">1</span>, <span class="dt">mode=</span><span class="st">&quot;fraction&quot;</span>, <span class="dt">type=</span><span class="st">&quot;coefficients&quot;</span>)</code></pre></div>
<p>返回的结果依旧是一个列表，其中<code>coefficients</code>项包含各个变量的参数估计：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># 这里不显示结果</span>
RidgeCoef=ridgeCoef$coefficients</code></pre></div>
<p>岭回归和原始最小二乘回归相比优势在于偏差和方差之间的权衡。之前讲过，一般线性回归中的最小二乘估计在无偏估计中是最优的，但是通常估计方差会很大。这意味着训练集数据的微小变化可能导致参数估计较大的变化。而岭回归估计就是通过牺牲一点点“无偏性”，换取估计方差的减小。因此，岭回归适合在普通最小二乘回归参数估计方差很大的情况下使用。</p>
</div>
<div id="lasso" class="section level3">
<h3><span class="header-section-number">10.2.2</span> Lasso</h3>
<p>虽然岭回归可以将参数估计值向0进行收缩，但对于任何调优参数值，它都不能将系数取值变为严格的0。尽管某些参数估计值变得非常小以至于可以忽略，但事实上岭回归并没有进行变量选择。这可能对预测精确度来说不是问题，但却对模型解释提出了挑战，尤其在变量个数大的时候。一种流行的用来替代岭回归的模型是“最小绝对收缩与选择算子”模型，通常被称为lasso<span class="citation">(R <a href="#ref-Tibshirani1996">1996</a>)</span>。这个模型使用了与岭回归类似的惩罚项，lasso的回归参数估计<span class="math inline">\(\hat{\beta}_{\lambda}^{L}\)</span>最小化如下方程：</p>
<p><span class="math display">\[\Sigma_{i=1}^{n}(y_{i}-\beta_{0}-\Sigma_{j=1}^{p}\beta_{j}x_{ij})^{2}+\lambda\Sigma_{j=1}^{p}|\beta_{j}|=RSS+\lambda\Sigma_{j=1}^{p}|\beta_{j}|\]</span></p>
<p>lasso和岭回归很相似，唯一的不同在于罚函数。岭回归中的<span class="math inline">\(\beta_{j}^{2}\)</span>在lasso中变为<span class="math inline">\(|\beta_{j}|\)</span>。用统计术语讲就是岭回归是在结构风险最小化的正则化因子上使用模型参数向量的二阶范数形式，lasso使用的是一阶范数形式。lasso不仅将参数估计向0收缩，当调优参数足够大时，一些参数估计将直接缩减为零，这可以达到特征提取的作用。这样一来，lasso回归的结果更易于解释。和其它有调优参数的模型类似，lasso也需要通过交互校验进行参数调优。下面我们展示在R中如何进行调优和拟合。</p>
<p>先用<code>train()</code>函数对参数进行调优。首先设置交互校验和参数调优范围，这里我们使用10层交互校验。建议大家在用此类方法前都先进行标准化：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ctrl &lt;-<span class="st"> </span><span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="dt">number =</span> <span class="dv">10</span>)
lassoGrid &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">fraction =</span> <span class="kw">seq</span>(.<span class="dv">8</span>, <span class="dv">1</span>, <span class="dt">length =</span> <span class="dv">20</span>))
<span class="kw">set.seed</span>(<span class="dv">100</span>)
lassoTune &lt;-<span class="st"> </span><span class="kw">train</span>(trainx, trainy,
                      <span class="dt">method =</span> <span class="st">&quot;lars&quot;</span>,
                      ## 用不同的罚函数值来拟合模型
                      <span class="dt">tuneGrid =</span> lassoGrid,
                      <span class="dt">trControl =</span> ctrl,
                      ## 中心化和标度化变量
                      <span class="dt">preProc =</span> <span class="kw">c</span>(<span class="st">&quot;center&quot;</span>, <span class="st">&quot;scale&quot;</span>))
lassoTune</code></pre></div>
<pre><code>## Least Angle Regression 
## 
## 999 samples
##  10 predictor
## 
## Pre-processing: centered (10), scaled (10) 
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 899, 899, 899, 899, 899, 899, ... 
## Resampling results across tuning parameters:
## 
##   fraction   RMSE      Rsquared 
##   0.8000000  1778.817  0.7872676
##   0.8105263  1776.714  0.7875679
##   0.8210526  1774.842  0.7878315
##   0.8315789  1773.090  0.7880823
##   0.8421053  1771.293  0.7883477
##   0.8526316  1769.551  0.7886105
##   0.8631579  1767.924  0.7888577
##   0.8736842  1766.442  0.7890909
##   0.8842105  1765.127  0.7893044
##   0.8947368  1763.960  0.7894958
##   0.9052632  1762.974  0.7896597
##   0.9157895  1762.165  0.7897985
##   0.9263158  1761.540  0.7899101
##   0.9368421  1761.212  0.7899582
##   0.9473684  1761.076  0.7899771
##   0.9578947  1761.121  0.7899701
##   0.9684211  1761.350  0.7899381
##   0.9789474  1761.839  0.7898641
##   0.9894737  1762.492  0.7897668
##   1.0000000  1763.347  0.7896421
## 
## RMSE was used to select the optimal model using  the smallest value.
## The final value used for the model was fraction = 0.9473684.</code></pre>
<p>训练出的模型调优参数为0.95，对应RMSE和<span class="math inline">\(R^{2}\)</span>分别为1761和79%，和之间岭回归几乎相同。从交互校验的RMSE结果图中可以看到，随着调优参数增加，RMSE有一个减小然后增加的过程。</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(lassoTune)</code></pre></div>
<p><img src="DS_R_files/figure-html/unnamed-chunk-169-1.png" width="672" /></p>
<p>lasso模型可以用许多不同的函数进行拟合。<code>lars</code>包中的<code>lars()</code>函数，<code>elasticnet</code>包的<code>enet()</code>函数，<code>glmnet</code>包的<code>glmnet()</code>函数都可以拟合lasso，它们的语法非常相似。我们还是使用<code>enet()</code>函数，其要求自变量必须是一个矩阵对象，因此要将数据框<code>trainx</code>转换成矩阵。此外，预测变量在建模之前需要中心化和标准化，函数中的<code>normalize</code>参数可以自动完成这一过程<code>lambda</code>参数控制了岭回归的罚参数，因此将该值设为0即为拟合lasso模型。</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lassoModel&lt;-<span class="st"> </span><span class="kw">enet</span>(<span class="dt">x =</span> <span class="kw">as.matrix</span>(trainx), <span class="dt">y =</span> trainy, <span class="dt">lambda =</span> <span class="dv">0</span>, <span class="dt">normalize =</span> <span class="ot">TRUE</span>)</code></pre></div>
<p>lasso模型在进行预测之前不需要进行指定：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lassoFit &lt;-<span class="st"> </span><span class="kw">predict</span>(lassoModel, <span class="dt">newx =</span> <span class="kw">as.matrix</span>(trainx),<span class="dt">s =</span> .<span class="dv">95</span>, <span class="dt">mode =</span> <span class="st">&quot;fraction&quot;</span>,<span class="dt">type =</span> <span class="st">&quot;fit&quot;</span>)</code></pre></div>
<p>类似，这里<code>type = &quot;fit&quot;</code>返回的结果是一个列表，其中<code>fit</code>项包含预测结果：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">head</span>(lassoFit$fit)</code></pre></div>
<pre><code>##         1         2         3         4         5         6 
## 1371.6160  308.6984  702.2026 1225.5508  832.0466 1028.9785</code></pre>
<p>如果要得到参数拟合结果，需要在<code>predict()</code>函数中设定<code>type=&quot;coefficients&quot;</code>：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lassoCoef&lt;-<span class="kw">predict</span>(lassoModel,<span class="dt">newx =</span> <span class="kw">as.matrix</span>(trainx),<span class="dt">s=</span><span class="fl">0.95</span>, <span class="dt">mode=</span><span class="st">&quot;fraction&quot;</span>, <span class="dt">type=</span><span class="st">&quot;coefficients&quot;</span>)</code></pre></div>
<p>返回的结果依旧是一个列表，其中<code>coefficients</code>项包含各个变量的参数估计：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># 这里不显示结果</span>
LassoCoef=lassoCoef$coefficients</code></pre></div>
<p>这类规则化方法的研究现在非常活跃。很多学者将lasso模型嫁接到其它方法上，比如线性判别<span class="citation">(Clemmensen L <a href="#ref-Clem2011">2011</a>)</span>，偏最小二乘回归<span class="citation">(C. H and S <a href="#ref-chun2010">2010</a>)</span>。但由于一阶范数不是连续可导的，lasso回归的计算过程更复杂。很多学者对相应的优化算法进行了研究，其中最重要的改进是Bradley Efron等<span class="citation">(Efron B <a href="#ref-efron2014">2014</a>)</span>提出的最小角回归(Least Angle Regression［LARS］)算法，该算法很好地解决Lasso的计算问题，尤其在维度高的时候。</p>
</div>
<div id="section-10.2.3" class="section level3">
<h3><span class="header-section-number">10.2.3</span> 弹性网络</h3>
<p>弹性网络是lasso的一般化版本<span class="citation">(Z. H and T <a href="#ref-zou2005">2005</a>)</span>，该模型结合了两种罚函数，参数估计最小化如下方程：</p>
<p><span class="math display">\[\Sigma_{i=1}^{n}(y_{i}-\hat{y}_{i})^{2}+\lambda_{1}\Sigma_{j=1}^{p}\beta_{j}^{2}+\lambda_{2}\Sigma_{j=1}^{p}|\beta_{j}|\]</span></p>
<p>lasso对应的估计方差较大，而岭回归又没有特征选择的功能，弹性网络的优点在于利用了岭回归的罚函数，同时又有lasso的特征选择功能。Zou和Hastie<span class="citation">(Z. H and T <a href="#ref-zou2005">2005</a>)</span>指出该模型能够更有效的处理成组的高度相关变量。</p>
<p>还是先用<code>train()</code>函数对参数进行调优。首先设置交互校验和参数调优范围，这里我们使用10层交互校验，并且对变量标准化：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">enetGrid &lt;-<span class="st"> </span><span class="kw">expand.grid</span>(<span class="dt">.lambda =</span> <span class="kw">seq</span>(<span class="dv">0</span>,<span class="fl">0.2</span>,<span class="dt">length=</span><span class="dv">20</span>), <span class="dt">.fraction =</span> <span class="kw">seq</span>(.<span class="dv">8</span>, <span class="dv">1</span>, <span class="dt">length =</span> <span class="dv">20</span>))
<span class="kw">set.seed</span>(<span class="dv">100</span>)
enetTune &lt;-<span class="st"> </span><span class="kw">train</span>(trainx, trainy,
                  <span class="dt">method =</span> <span class="st">&quot;enet&quot;</span>,
                  <span class="dt">tuneGrid =</span> enetGrid,
                  <span class="dt">trControl =</span> ctrl,
                  <span class="dt">preProc =</span> <span class="kw">c</span>(<span class="st">&quot;center&quot;</span>, <span class="st">&quot;scale&quot;</span>))</code></pre></div>
<p>训练出的模型对应的lasso调优参数为0.958，岭回归调优参数为0.01。相应RMSE和<span class="math inline">\(R^{2}\)</span>分别为1760和79%，在这里这三种方法的效果并没有很大的不同。这里展示如何使用<code>elasticnet</code>包中的<code>enet()</code>函数：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">enetfit =<span class="st"> </span><span class="kw">enet</span>(<span class="dt">x =</span> <span class="kw">as.matrix</span>(trainx), <span class="dt">y =</span> trainy, <span class="dt">lambda =</span> <span class="fl">0.01</span>,
                <span class="co"># 这里设置将自变量标准化</span>
                <span class="dt">normalize =</span> <span class="ot">TRUE</span>)</code></pre></div>
<p>和之前一样，<code>enetfit</code>只指定了岭回归的罚函数，由于弹性网模型同时具有岭回归和lasso罚函数，我们需要进一步通过<code>predict()</code>函数得到相应的拟合系数和拟合结果。针对<code>enet</code>对象的<code>predict()</code>函数可以通过参数<code>s</code>和<code>mode</code>来指定在lasso罚参数下的拟合，和之前不同的是，这里我们对应的lasso的罚函数参数设置为<code>s = 0.958</code> 和 <code>mode = &quot;fraction&quot;</code>：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">enetPred &lt;-<span class="st"> </span><span class="kw">predict</span>(enetfit, <span class="dt">newx =</span> <span class="kw">as.matrix</span>(trainx), 
                     <span class="dt">s =</span> <span class="fl">0.958</span>, <span class="dt">mode =</span> <span class="st">&quot;fraction&quot;</span>, <span class="dt">type =</span> <span class="st">&quot;fit&quot;</span>)</code></pre></div>
<p>注意上面<code>type = &quot;fit&quot;</code>返回的结果是一个列表，其中<code>fit</code>项包含预测结果。如果要得到参数拟合结果，需要在<code>predict()</code>函数中设定<code>type=&quot;coefficients&quot;</code>：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">enetCoef&lt;-<span class="kw">predict</span>(ridgefit,<span class="dt">newx =</span> <span class="kw">as.matrix</span>(trainx), 
                   <span class="dt">s=</span><span class="fl">0.958</span>, <span class="dt">mode=</span><span class="st">&quot;fraction&quot;</span>, <span class="dt">type=</span><span class="st">&quot;coefficients&quot;</span>)</code></pre></div>
<p>返回的结果依旧是一个列表，其中<code>coefficients</code>项包含各个变量的参数估计。</p>
</div>
</div>
<div id="lasso" class="section level2">
<h2><span class="header-section-number">10.3</span> 知识扩展：Lasso的变量选择功能</h2>
<p>可能有人会问从岭回归到lasso，只是罚函数从二阶范数变成一阶范数，为什么lasso就能够将参数估计收缩成0而岭回归不能呢？要回答这个问题，我们先看下lasso和岭回归分别对应的另一版本的等价优化方程。对于lasso而言，优化下面两个方程是等价的：</p>
<p><span class="math display">\[\Sigma_{i=1}^{n}(y_{i}-\beta_{0}-\Sigma_{j=1}^{p}\beta_{j}x_{ij})^{2}+\lambda\Sigma_{j=1}^{p}|\beta_{j}|=RSS+\lambda\Sigma_{j=1}^{p}|\beta_{j}|\]</span></p>
<p><span class="math display">\[\underset{\beta}{min}\left\{ \Sigma_{i=1}^{n}\left(y_{i}-\beta_{0}-\Sigma_{j=1}^{p}\beta_{j}x_{ij}\right)^{2}\right\} ,\ \Sigma_{j=1}^{p}|\beta_{j}|\leq s\]</span></p>
<p>也就是说，对每个调优参数<span class="math inline">\(\lambda\)</span>的取值，都存在相应的<span class="math inline">\(s\)</span>值，使得上面两个方程优化后得到的参数估计相同。类似的，对于岭回归，下面两个方程等价：</p>
<p><span class="math display">\[\underset{\beta}{min}\left\{ \Sigma_{i=1}^{n}\left(y_{i}-\beta_{0}-\Sigma_{j=1}^{p}\beta_{j}x_{ij}\right)^{2}\right\} ,\ \Sigma_{j=1}^{p}\beta_{j}^{2}\leq s\]</span></p>
<p><span class="math display">\[\Sigma_{i=1}^{n}(y_{i}-\beta_{0}-\Sigma_{j=1}^{p}\beta_{j}x_{ij})^{2}+\lambda\Sigma_{j=1}^{p}\beta_{j}^{2}=RSS+\lambda\Sigma_{j=1}^{p}\beta_{j}^{2}\]</span></p>
</div>
<div id="glmnet" class="section level2">
<h2><span class="header-section-number">10.4</span> 广义线性模型压缩方法及<code>glmnet</code>包</h2>
</div>
<div id="section-10.5" class="section level2">
<h2><span class="header-section-number">10.5</span> 分层线性回归</h2>
<p>我们已经讨论过简单线性模型，下面讨论该模型的扩展分层线性模型。什么时候需要使用分层模型呢？需要考虑分层结构的常见情况有两种。（1）当你的数据有嵌套结构，这是指一些观测可能属于更高一层级单位。比如在教育学分析当中通常需要研究学生的学习情况，同一所学校或者同一班级的学生相似度更高，这里我们就需要考虑学生学习情况的个体观测可能嵌套在班级，或者学校这个更高一层级单位中。市场营销中，地理位置通常是一个更高的层极单位。北上广的消费者和一些二三线城市的消费者可能不同，东南沿海和东北地区的消费者差异可能更大。这些情况中我们都需要考虑分层结构。（2）还有一种情况是针对纵向数据。比如对一些人年收入连续10年的观测，那么研究观测就该考虑个人的随机效应。类似的还有我们的航空公司满意度调查数据，每个受访者针对每项对3个航空公司进行评分，这里也需要考虑受访者的个体随机效应，可能的情况是有的人倾向于给高分，有的人倾向于给低分，这样的倾向和问卷问题以及哪家航空公司无关。</p>
<!--
## 贝叶斯线性回归

## 贝叶斯分层线性回归
-->

</div>
</div>
<h3> References</h3>
<div id="refs" class="references">
<div id="ref-ASA_P">
<p>Ronald L. Wassersteina, Nicole A. Lazara. 2016. “Position on P-Values: Context, Process, and Purpose.”</p>
</div>
<div id="ref-car">
<p>Fox, John, and Sanford Weisberg. 2011. “An R Companion to Applied Regression.” <em>Sage</em>.</p>
</div>
<div id="ref-Williams1987">
<p>Williams, D. A. 1987. “Generalized Linear Model Diagnostics Using the Deviance and Single Case Deletions.” <em>Applied Statistics</em> 36: 181–191.</p>
</div>
<div id="ref-fox2008">
<p>Fox, J. 2008. “Applied Regression Analysis and Generalized Linear Models.” <em>Sage</em>.</p>
</div>
<div id="ref-cookd">
<p>Cook, R. D., and S Weisberg. 1982. <em>Residuals and Influence in Regression</em>. Chapman; Hall.</p>
</div>
<div id="ref-weisberg14">
<p>Weisberg, S. 2014. <em>Applied Linear Regression</em>. <em>Applied Linear Regression</em>. Fourth Edition. Wiley.</p>
</div>
<div id="ref-Hoerl1970">
<p>A, Hoerl. 1970. “Ridge Regression: Biased Estimation for Nonorthogonal Problems.” <em>Technometrics</em> 12 (1): 55–67.</p>
</div>
<div id="ref-Tibshirani1996">
<p>R, Tibshirani. 1996. “Regression Shrinkage and Selection via the Lasso.” <em>Journal of the Royal Statistical Society Series B (Methodological)</em> 58 (1): 267–88.</p>
</div>
<div id="ref-Clem2011">
<p>Clemmensen L, Witten D, Hastie T. 2011. “Sparse Discriminant Analysis.” <em>Technometrics</em> 53 (4): 406–13.</p>
</div>
<div id="ref-chun2010">
<p>H, Chun, and Kele S. 2010. “Sparse Partial Least Squares Regression for Simultaneous Dimension Reduction and Variable Selection.” <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em> 72 (1): 3–25.</p>
</div>
<div id="ref-efron2014">
<p>Efron B, Johnstone I, Hastie T. 2014. “Least Angle Regression.” <em>The Annals of Statistics</em> 32 (2): 407–99.</p>
</div>
<div id="ref-zou2005">
<p>H, Zou, and Hastie T. 2005. “Regularization and Variable Selection via the Elastic Net.” <em>Journal of the Royal Statistical Society, Series B,</em> 67 (2): 301–20.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p><a href="http://cos.name/2016/03/asa-statement-on-p-value/" class="uri">http://cos.name/2016/03/asa-statement-on-p-value/</a><a href="section-10.html#fnref1">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="section-9.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="section-11.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/10-xianxinghuigui.Rmd",
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
