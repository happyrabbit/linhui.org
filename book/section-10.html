<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>数据科学家：R语言</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="This is my first book on data science">
  <meta name="generator" content="bookdown 0.1 and GitBook 2.6.7">

  <meta property="og:title" content="数据科学家：R语言" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is my first book on data science" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="数据科学家：R语言" />
  
  <meta name="twitter:description" content="This is my first book on data science" />
  

<meta name="author" content="林荟">

<meta name="date" content="2016-12-11">

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="section-9.html">
<link rel="next" href="section-11.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">数据科学家：R语言</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> 介绍</a></li>
<li class="chapter" data-level="2" data-path="section-2.html"><a href="section-2.html"><i class="fa fa-check"></i><b>2</b> 数据科学</a><ul>
<li class="chapter" data-level="2.1" data-path="section-2.html"><a href="section-2.html#section-2.1"><i class="fa fa-check"></i><b>2.1</b> 什么是数据科学？</a></li>
<li class="chapter" data-level="2.2" data-path="section-2.html"><a href="section-2.html#section-2.2"><i class="fa fa-check"></i><b>2.2</b> 什么是数据科学家？</a></li>
<li class="chapter" data-level="2.3" data-path="section-2.html"><a href="section-2.html#section-2.3"><i class="fa fa-check"></i><b>2.3</b> 数据科学家需要的技能</a></li>
<li class="chapter" data-level="2.4" data-path="section-2.html"><a href="section-2.html#section-2.4"><i class="fa fa-check"></i><b>2.4</b> 数据科学可以解决什么问题？</a><ul>
<li class="chapter" data-level="2.4.1" data-path="section-2.html"><a href="section-2.html#section-2.4.1"><i class="fa fa-check"></i><b>2.4.1</b> 前提要求</a></li>
<li class="chapter" data-level="2.4.2" data-path="section-2.html"><a href="section-2.html#section-2.4.2"><i class="fa fa-check"></i><b>2.4.2</b> 问题种类</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="section-3.html"><a href="section-3.html"><i class="fa fa-check"></i><b>3</b> 数据集模拟和背景介绍</a><ul>
<li class="chapter" data-level="3.1" data-path="section-3.html"><a href="section-3.html#section-3.1"><i class="fa fa-check"></i><b>3.1</b> 服装消费者数据</a></li>
<li class="chapter" data-level="3.2" data-path="section-3.html"><a href="section-3.html#section-3.2"><i class="fa fa-check"></i><b>3.2</b> 航空公司满意度调查</a></li>
<li class="chapter" data-level="3.3" data-path="section-3.html"><a href="section-3.html#section-3.3"><i class="fa fa-check"></i><b>3.3</b> 生猪疫情风险预测数据</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="section-4.html"><a href="section-4.html"><i class="fa fa-check"></i><b>4</b> 数据分析一般流程</a><ul>
<li class="chapter" data-level="4.1" data-path="section-4.html"><a href="section-4.html#section-4.1"><i class="fa fa-check"></i><b>4.1</b> 问题到数据</a></li>
<li class="chapter" data-level="4.2" data-path="section-4.html"><a href="section-4.html#section-4.2"><i class="fa fa-check"></i><b>4.2</b> 数据到信息</a></li>
<li class="chapter" data-level="4.3" data-path="section-4.html"><a href="section-4.html#section-4.3"><i class="fa fa-check"></i><b>4.3</b> 信息到行动</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="section-5.html"><a href="section-5.html"><i class="fa fa-check"></i><b>5</b> 数据预处理</a><ul>
<li class="chapter" data-level="5.1" data-path="section-5.html"><a href="section-5.html#section-5.1"><i class="fa fa-check"></i><b>5.1</b> 介绍</a></li>
<li class="chapter" data-level="5.2" data-path="section-5.html"><a href="section-5.html#section-5.2"><i class="fa fa-check"></i><b>5.2</b> 数据清理</a></li>
<li class="chapter" data-level="5.3" data-path="section-5.html"><a href="section-5.html#section-5.3"><i class="fa fa-check"></i><b>5.3</b> 缺失值填补</a><ul>
<li class="chapter" data-level="5.3.1" data-path="section-5.html"><a href="section-5.html#section-5.3.1"><i class="fa fa-check"></i><b>5.3.1</b> 中位数或众数填补</a></li>
<li class="chapter" data-level="5.3.2" data-path="section-5.html"><a href="section-5.html#k-"><i class="fa fa-check"></i><b>5.3.2</b> K-近邻填补</a></li>
<li class="chapter" data-level="5.3.3" data-path="section-5.html"><a href="section-5.html#section-5.3.3"><i class="fa fa-check"></i><b>5.3.3</b> 袋状树填补</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="section-5.html"><a href="section-5.html#section-5.4"><i class="fa fa-check"></i><b>5.4</b> 中心化和标量化</a></li>
<li class="chapter" data-level="5.5" data-path="section-5.html"><a href="section-5.html#section-5.5"><i class="fa fa-check"></i><b>5.5</b> 有偏分布</a></li>
<li class="chapter" data-level="5.6" data-path="section-5.html"><a href="section-5.html#section-5.6"><i class="fa fa-check"></i><b>5.6</b> 处理离群点</a></li>
<li class="chapter" data-level="5.7" data-path="section-5.html"><a href="section-5.html#section-5.7"><i class="fa fa-check"></i><b>5.7</b> 共线性</a></li>
<li class="chapter" data-level="5.8" data-path="section-5.html"><a href="section-5.html#section-5.8"><i class="fa fa-check"></i><b>5.8</b> 稀疏变量</a></li>
<li class="chapter" data-level="5.9" data-path="section-5.html"><a href="section-5.html#section-5.9"><i class="fa fa-check"></i><b>5.9</b> 编码名义变量</a></li>
<li class="chapter" data-level="5.10" data-path="section-5.html"><a href="section-5.html#section-5.10"><i class="fa fa-check"></i><b>5.10</b> 本章总结</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="section-6.html"><a href="section-6.html"><i class="fa fa-check"></i><b>6</b> 数据操作</a><ul>
<li class="chapter" data-level="6.1" data-path="section-6.html"><a href="section-6.html#section-6.1"><i class="fa fa-check"></i><b>6.1</b> 数据读写</a><ul>
<li class="chapter" data-level="6.1.1" data-path="section-6.html"><a href="section-6.html#tibble"><i class="fa fa-check"></i><b>6.1.1</b> 取代传统数据框的<code>tibble</code>对象</a></li>
<li class="chapter" data-level="6.1.2" data-path="section-6.html"><a href="section-6.html#readr"><i class="fa fa-check"></i><b>6.1.2</b> 高效数据读写：<code>readr</code>包</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="section-6.html"><a href="section-6.html#section-6.2"><i class="fa fa-check"></i><b>6.2</b> 数据整合</a><ul>
<li class="chapter" data-level="6.2.1" data-path="section-6.html"><a href="section-6.html#baseapply"><i class="fa fa-check"></i><b>6.2.1</b> base包：apply()</a></li>
<li class="chapter" data-level="6.2.2" data-path="section-6.html"><a href="section-6.html#plyrddply"><i class="fa fa-check"></i><b>6.2.2</b> plyr包：ddply()函数</a></li>
<li class="chapter" data-level="6.2.3" data-path="section-6.html"><a href="section-6.html#dplyr"><i class="fa fa-check"></i><b>6.2.3</b> dplyr包</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="section-6.html"><a href="section-6.html#section-6.3"><i class="fa fa-check"></i><b>6.3</b> 数据整形</a><ul>
<li class="chapter" data-level="6.3.1" data-path="section-6.html"><a href="section-6.html#reshape2"><i class="fa fa-check"></i><b>6.3.1</b> <code>reshape2</code>包</a></li>
<li class="chapter" data-level="6.3.2" data-path="section-6.html"><a href="section-6.html#tidyr"><i class="fa fa-check"></i><b>6.3.2</b> <code>tidyr</code>包</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="section-6.html"><a href="section-6.html#-1"><i class="fa fa-check"></i><b>6.4</b> 本章总结</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="section-7.html"><a href="section-7.html"><i class="fa fa-check"></i><b>7</b> 基础建模技术</a><ul>
<li class="chapter" data-level="7.1" data-path="section-7.html"><a href="section-7.html#section-7.1"><i class="fa fa-check"></i><b>7.1</b> 有监督和无监督</a></li>
<li class="chapter" data-level="7.2" data-path="section-7.html"><a href="section-7.html#section-7.2"><i class="fa fa-check"></i><b>7.2</b> 误差及其来源</a><ul>
<li class="chapter" data-level="7.2.1" data-path="section-7.html"><a href="section-7.html#section-7.2.1"><i class="fa fa-check"></i><b>7.2.1</b> 系统误差和随机误差</a></li>
<li class="chapter" data-level="7.2.2" data-path="section-7.html"><a href="section-7.html#section-7.2.2"><i class="fa fa-check"></i><b>7.2.2</b> 应变量误差</a></li>
<li class="chapter" data-level="7.2.3" data-path="section-7.html"><a href="section-7.html#section-7.2.3"><i class="fa fa-check"></i><b>7.2.3</b> 自变量误差</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="section-7.html"><a href="section-7.html#section-7.3"><i class="fa fa-check"></i><b>7.3</b> 数据划分和再抽样</a><ul>
<li class="chapter" data-level="7.3.1" data-path="section-7.html"><a href="section-7.html#section-7.3.1"><i class="fa fa-check"></i><b>7.3.1</b> 划分训练集和测试集</a></li>
<li class="chapter" data-level="7.3.2" data-path="section-7.html"><a href="section-7.html#section-7.3.2"><i class="fa fa-check"></i><b>7.3.2</b> 重抽样</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="section-7.html"><a href="section-7.html#-2"><i class="fa fa-check"></i><b>7.4</b> 本章总结</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="section-8.html"><a href="section-8.html"><i class="fa fa-check"></i><b>8</b> 模型评估度量</a><ul>
<li class="chapter" data-level="8.1" data-path="section-8.html"><a href="section-8.html#section-8.1"><i class="fa fa-check"></i><b>8.1</b> 回归模型评估度量</a></li>
<li class="chapter" data-level="8.2" data-path="section-8.html"><a href="section-8.html#section-8.2"><i class="fa fa-check"></i><b>8.2</b> 分类模型评估度量</a><ul>
<li class="chapter" data-level="8.2.1" data-path="section-8.html"><a href="section-8.html#kappa"><i class="fa fa-check"></i><b>8.2.1</b> Kappa统计量</a></li>
<li class="chapter" data-level="8.2.2" data-path="section-8.html"><a href="section-8.html#roc"><i class="fa fa-check"></i><b>8.2.2</b> ROC曲线</a></li>
<li class="chapter" data-level="8.2.3" data-path="section-8.html"><a href="section-8.html#section-8.2.3"><i class="fa fa-check"></i><b>8.2.3</b> 提升图</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="section-8.html"><a href="section-8.html#-3"><i class="fa fa-check"></i><b>8.3</b> 本章总结</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="section-9.html"><a href="section-9.html"><i class="fa fa-check"></i><b>9</b> 特征工程</a><ul>
<li class="chapter" data-level="9.1" data-path="section-9.html"><a href="section-9.html#section-9.1"><i class="fa fa-check"></i><b>9.1</b> 特征构建</a></li>
<li class="chapter" data-level="9.2" data-path="section-9.html"><a href="section-9.html#section-9.2"><i class="fa fa-check"></i><b>9.2</b> 特征提取</a><ul>
<li class="chapter" data-level="9.2.1" data-path="section-9.html"><a href="section-9.html#section-9.2.1"><i class="fa fa-check"></i><b>9.2.1</b> 初步探索数据</a></li>
<li class="chapter" data-level="9.2.2" data-path="section-9.html"><a href="section-9.html#section-9.2.2"><i class="fa fa-check"></i><b>9.2.2</b> 主成分分析</a></li>
<li class="chapter" data-level="9.2.3" data-path="section-9.html"><a href="section-9.html#section-9.2.3"><i class="fa fa-check"></i><b>9.2.3</b> 探索性因子分析</a></li>
<li class="chapter" data-level="9.2.4" data-path="section-9.html"><a href="section-9.html#section-9.2.4"><i class="fa fa-check"></i><b>9.2.4</b> 高维标度化</a></li>
<li class="chapter" data-level="9.2.5" data-path="section-9.html"><a href="section-9.html#section-9.2.5"><i class="fa fa-check"></i><b>9.2.5</b> 知识扩展</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="section-9.html"><a href="section-9.html#section-9.3"><i class="fa fa-check"></i><b>9.3</b> 特征选择</a><ul>
<li class="chapter" data-level="9.3.1" data-path="section-9.html"><a href="section-9.html#section-9.3.1"><i class="fa fa-check"></i><b>9.3.1</b> 过滤法</a></li>
<li class="chapter" data-level="9.3.2" data-path="section-9.html"><a href="section-9.html#section-9.3.2"><i class="fa fa-check"></i><b>9.3.2</b> 绕封法</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="section-10.html"><a href="section-10.html"><i class="fa fa-check"></i><b>10</b> 线性回归极其衍生</a><ul>
<li class="chapter" data-level="10.1" data-path="section-10.html"><a href="section-10.html#section-10.1"><i class="fa fa-check"></i><b>10.1</b> 普通线性回归</a><ul>
<li class="chapter" data-level="10.1.1" data-path="section-10.html"><a href="section-10.html#section-10.1.1"><i class="fa fa-check"></i><b>10.1.1</b> 最小二乘线性模型</a></li>
<li class="chapter" data-level="10.1.2" data-path="section-10.html"><a href="section-10.html#section-10.1.2"><i class="fa fa-check"></i><b>10.1.2</b> 回归诊断</a></li>
<li class="chapter" data-level="10.1.3" data-path="section-10.html"><a href="section-10.html#section-10.1.3"><i class="fa fa-check"></i><b>10.1.3</b> 离群点，高杠杆点和强影响点</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="section-10.html"><a href="section-10.html#section-10.2"><i class="fa fa-check"></i><b>10.2</b> 收缩方法</a><ul>
<li class="chapter" data-level="10.2.1" data-path="section-10.html"><a href="section-10.html#section-10.2.1"><i class="fa fa-check"></i><b>10.2.1</b> 岭回归</a></li>
<li class="chapter" data-level="10.2.2" data-path="section-10.html"><a href="section-10.html#lasso"><i class="fa fa-check"></i><b>10.2.2</b> Lasso</a></li>
<li class="chapter" data-level="10.2.3" data-path="section-10.html"><a href="section-10.html#section-10.2.3"><i class="fa fa-check"></i><b>10.2.3</b> 弹性网络</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="section-10.html"><a href="section-10.html#lasso"><i class="fa fa-check"></i><b>10.3</b> 知识扩展：Lasso的变量选择功能</a></li>
<li class="chapter" data-level="10.4" data-path="section-10.html"><a href="section-10.html#section-10.4"><i class="fa fa-check"></i><b>10.4</b> 主成分和偏最小二乘回归</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="section-11.html"><a href="section-11.html"><i class="fa fa-check"></i><b>11</b> 广义线性模型压缩方法</a><ul>
<li class="chapter" data-level="11.1" data-path="section-11.html"><a href="section-11.html#glmnet"><i class="fa fa-check"></i><b>11.1</b> 初识<code>glmnet</code></a></li>
<li class="chapter" data-level="11.2" data-path="section-11.html"><a href="section-11.html#section-11.2"><i class="fa fa-check"></i><b>11.2</b> 收缩线性回归</a></li>
<li class="chapter" data-level="11.3" data-path="section-11.html"><a href="section-11.html#section-11.3"><i class="fa fa-check"></i><b>11.3</b> 逻辑回归</a><ul>
<li class="chapter" data-level="11.3.1" data-path="section-11.html"><a href="section-11.html#section-11.3.1"><i class="fa fa-check"></i><b>11.3.1</b> 普通逻辑回归</a></li>
<li class="chapter" data-level="11.3.2" data-path="section-11.html"><a href="section-11.html#section-11.3.2"><i class="fa fa-check"></i><b>11.3.2</b> 收缩逻辑回归</a></li>
<li class="chapter" data-level="11.3.3" data-path="section-10.html"><a href="section-10.html#lasso"><i class="fa fa-check"></i><b>11.3.3</b> 知识扩展：群组lasso逻辑回归</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="section-11.html"><a href="section-11.html#section-11.4"><i class="fa fa-check"></i><b>11.4</b> 收缩多项回归</a></li>
<li class="chapter" data-level="11.5" data-path="section-11.html"><a href="section-11.html#section-11.5"><i class="fa fa-check"></i><b>11.5</b> 泊松收缩回归</a></li>
<li class="chapter" data-level="11.6" data-path="section-11.html"><a href="section-11.html#-4"><i class="fa fa-check"></i><b>11.6</b> 本章总结</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="section-12.html"><a href="section-12.html"><i class="fa fa-check"></i><b>12</b> 树模型</a><ul>
<li class="chapter" data-level="12.1" data-path="section-12.html"><a href="section-12.html#section-12.1"><i class="fa fa-check"></i><b>12.1</b> 分裂准则</a></li>
<li class="chapter" data-level="12.2" data-path="section-12.html"><a href="section-12.html#section-12.2"><i class="fa fa-check"></i><b>12.2</b> 树的修剪</a></li>
<li class="chapter" data-level="12.3" data-path="section-12.html"><a href="section-12.html#section-12.3"><i class="fa fa-check"></i><b>12.3</b> 回归树和决策树</a><ul>
<li class="chapter" data-level="12.3.1" data-path="section-12.html"><a href="section-12.html#section-12.3.1"><i class="fa fa-check"></i><b>12.3.1</b> 回归树</a></li>
<li class="chapter" data-level="12.3.2" data-path="section-12.html"><a href="section-12.html#section-12.3.2"><i class="fa fa-check"></i><b>12.3.2</b> 决策树</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="section-12.html"><a href="section-12.html#section-12.4"><i class="fa fa-check"></i><b>12.4</b> 装袋树</a></li>
<li class="chapter" data-level="12.5" data-path="section-12.html"><a href="section-12.html#section-12.5"><i class="fa fa-check"></i><b>12.5</b> 随机森林</a></li>
<li class="chapter" data-level="12.6" data-path="section-12.html"><a href="section-12.html#section-12.6"><i class="fa fa-check"></i><b>12.6</b> 助推法</a></li>
<li class="chapter" data-level="12.7" data-path="section-12.html"><a href="section-12.html#section-12.7"><i class="fa fa-check"></i><b>12.7</b> 知识扩展：助推法的可加模型框架</a></li>
<li class="chapter" data-level="12.8" data-path="section-12.html"><a href="section-12.html#section-12.8"><i class="fa fa-check"></i><b>12.8</b> 知识扩展：助推树的数学框架</a><ul>
<li class="chapter" data-level="12.8.1" data-path="section-12.html"><a href="section-12.html#section-12.8.1"><i class="fa fa-check"></i><b>12.8.1</b> 数学表达</a></li>
<li class="chapter" data-level="12.8.2" data-path="section-12.html"><a href="section-12.html#section-12.8.2"><i class="fa fa-check"></i><b>12.8.2</b> 梯度助推数值优化</a></li>
</ul></li>
<li class="chapter" data-level="12.9" data-path="section-12.html"><a href="section-12.html#-5"><i class="fa fa-check"></i><b>12.9</b> 本章总结</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="section-13.html"><a href="section-13.html"><i class="fa fa-check"></i><b>13</b> 深度学习</a><ul>
<li class="chapter" data-level="13.1" data-path="section-6.html"><a href="section-6.html#-1"><i class="fa fa-check"></i><b>13.1</b> 介绍</a></li>
<li class="chapter" data-level="13.2" data-path="section-13.html"><a href="section-13.html#r"><i class="fa fa-check"></i><b>13.2</b> R中深度学习包</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>14</b> References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">数据科学家：R语言</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="section-10" class="section level1">
<h1><span class="header-section-number">第10章</span> 线性回归极其衍生</h1>
<p>本章主要讲线性回归和它的衍生，顺序由易到难。先介绍普通线性回归（也称为最小二乘回归），这是非常简单的（可能是最简单的）有监督方法，相对于其它方法，普通线性回归可谓历史悠久，声名远扬。很多非理工科专业的小伙伴也都听过或者用过该模型。虽然和很多我们将要介绍的更新的模型比起来，普通线性回归太过低端，但它依旧是有用并且被广泛使用，此外，很多新模型其实是普通线性模型的衍生。因此理解普通线性模型对理解后面更加复杂的模型非常重要。之后我们会讲到两个收缩方法：岭回归和Lasso回归。和普通最小二乘估计相比，收缩方法可以将参数估计向0“收缩”，当观测量少时（相对于变量个数而言），这种方法有助于减少估计方差，稳定参数估计。接下来我们会介绍分层线性回归，和贝叶斯框架下的分层线性回归。R有强大的拟合线性模型的功能。我们先回顾一些基本知识，展示如何用R展示拟合相应模型，但是本章不会介绍所有实践中需要知道的知识。我们鼓励想进一步了解模型的读者参考我们在介绍该部分时列出的参考资料。本章中我们还是使用服装消费者数据解释线性模型。我们需要回答类似这样的问题：“那些变量是总消费量（线上和实体店消费额之和）的驱动因子？”这个问题的答案可以帮助公司知道需要将钱投到产品的哪个方面（如服装的设计，服装质量等）。</p>
<p>这里特别要注意的一点是，驱动因子不意味着原因。线性模型只假设变量之间存在<strong>关联性</strong>。如果某汽车客户问卷调查结果显示满意度和价格之间正相关，难道商家为了提高消费者满意度而刻意提高汽车价格？貌似不符合常识。更可能的情况是因为价格更高的汽车质量也更好，客户真正满意的是汽车的质量。因果关系在分析实践中是个很大的坑，在解释结果的时候一定要小心再小心，一定要将问题放在相应的语境中。</p>
<div id="section-10.1" class="section level2">
<h2><span class="header-section-number">10.1</span> 普通线性回归</h2>
<p>虽然最小二乘线性回归看起来太过简单粗暴，但现在很多更复杂的模型其基本形式也是线性的。比如逻辑回归，就是对因变量的均值进行逻辑变换后再拟合线性模型。通常我们都将神经网络模型归于非线性模型，但神经网络中的每个潜变量都是某些预测变量的线性组合。日光之下，并无新事。在大量新技术不断涌入更新换代的今天，人的思维更加容易见树不见林。很多事物本质上是有相似性的，找到光怪陆离的表象下的实质是一种重要的能力。在学习了很多不同的方法之后应该退后一步，看看这些方法的演变联系，对背后的知识进行提取抽象，触及本质，时不时停下来问自己：这些模型背后的根本思想是什么？ 在R实现这些模型也类似，不同的模型在R中的表达方法都是模仿线性模型的拟合语句。因此，只要理解了如何使用R拟合，解释和诊断线性模型，你能够举一反三的应用其它更加复杂的模型。本节主要介绍用R中<code>lm()</code>拟合最小二乘线性模型，以及该函数中的不同选项。然后我们将会讲到线性模型的诊断方法，它们用于检测模型的假设是否成立，或者我们拟合的结果是否充分。</p>
<div id="section-10.1.1" class="section level3">
<h3><span class="header-section-number">10.1.1</span> 最小二乘线性模型</h3>
<p>在线性模型中，</p>
<p><span class="math display">\[f(\mathbf{X})=\mathbf{X}\mathbf{\beta}=\beta_{0}+\sum_{j=1}^{p}\mathbf{x_{.j}}\beta_{j}\]</span></p>
<p>其中<span class="math inline">\(\mathbf{\beta}\)</span>是长度为<span class="math inline">\(p+1\)</span>的参数向量。这里的数学公式表达和之前6.1中介绍的一致。最小二乘估计就是选择<span class="math inline">\(\mathbf{\beta^{T}}=(\beta_{0},\beta_{1},...,\beta_{p})\)</span>最小化下面残差平方和：</p>
<p><span class="math display">\[RSS(\beta)=\sum_{i=1}^{N}(y_{i}-f(\mathbf{x_{i.}}))^{2}=\sum_{i=1}^{N}(y_{i}-\beta_{0}-\sum_{j=1}^{p}x_{ij}\beta_{j})^{2}\]</span></p>
<p>我们还是从载入数据开始。</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">dat&lt;-<span class="kw">read.csv</span>(<span class="st">&quot;https://raw.githubusercontent.com/happyrabbit/DataScientistR/master/Data/SegData.csv&quot;</span>)</code></pre></div>
<p>在我们开始之前，还需要对数据进行一些清理，删除错误的样本观测，消费金额不能为负数。</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">dat&lt;-<span class="kw">subset</span>(dat,store_exp&gt;<span class="dv">0</span> &amp;<span class="st"> </span>online_exp&gt;<span class="dv">0</span>)</code></pre></div>
<p>我们将10个问卷调查变量当作自变量。</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">modeldat&lt;-dat[,<span class="kw">grep</span>(<span class="st">&quot;Q&quot;</span>,<span class="kw">names</span>(dat))]</code></pre></div>
<p>将实体店消费量和在线消费之和当作应变量。</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># 得到总消费量=实体店消费+在线消费</span>
modeldat$total_exp&lt;-dat$store_exp+dat$online_exp</code></pre></div>
<p>我们先检查一下数据，看是不是有缺失值或者离群点：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># 这里没有展示输出结果</span>
<span class="kw">summary</span>(modeldat)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))
<span class="kw">hist</span>(modeldat$total_exp,<span class="dt">main=</span><span class="st">&quot;&quot;</span>,<span class="dt">xlab=</span><span class="st">&quot;total_exp&quot;</span>)
<span class="kw">boxplot</span>(modeldat$total_exp)</code></pre></div>
<p><img src="DS_R_files/figure-html/checkplot-1.png" width="80%" style="display: block; margin: auto;" /></p>
<p>数据集<code>modeldat</code>中没有缺失值，但是明显有离群点，而且应变量<code>total_exp</code>分布明显偏离正态。我们删除离群点，然后对应变量进行对数变换。</p>
<p>我们用之前数据预处理章节介绍的Z分值的方法查找并删除离群点。这里不重复解释，不明白的读者可以返回复习相应的章节。</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">y&lt;-modeldat$total_exp
<span class="co"># 求Z分值</span>
zs&lt;-(y-<span class="kw">mean</span>(y))/<span class="kw">mad</span>(y)
<span class="co"># 找到Z分值大于3.5的离群点，删除这些观测</span>
modeldat&lt;-modeldat[-<span class="kw">which</span>(zs&gt;<span class="fl">3.5</span>),]</code></pre></div>
<p>这里我们先不对应变量进行对数变换，之后在回归函数的公式里对应变量进行变换。接下来检查变量的共线性：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(corrplot)
correlation&lt;-<span class="kw">cor</span>(modeldat[,<span class="kw">grep</span>(<span class="st">&quot;Q&quot;</span>,<span class="kw">names</span>(modeldat))])
<span class="kw">corrplot.mixed</span>(correlation,<span class="dt">order=</span><span class="st">&quot;hclust&quot;</span>,<span class="dt">tl.pos=</span><span class="st">&quot;lt&quot;</span>,<span class="dt">upper=</span><span class="st">&quot;ellipse&quot;</span>)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:corplotlm"></span>
<img src="DS_R_files/figure-html/corplotlm-1.png" alt="自变量相关矩阵图" width="80%" />
<p class="caption">
Figure 10.1: 自变量相关矩阵图
</p>
</div>
<p>由图<a href="section-10.html#fig:corplotlm">10.1</a> 可以看到，变量之间有很强的相关性。我们用之前在预处理章节中提到的删除高度相关变量的算法，设置阈值为0.75：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(caret)
highcor&lt;-<span class="kw">findCorrelation</span>(correlation,<span class="dt">cutoff=</span>.<span class="dv">75</span>)
modeldat&lt;-modeldat[,-highcor]</code></pre></div>
<p>现在我们可以拟合线性模型。标准的模型公式表达是在“~”号的左边指定因变量，右边指定自变量。“.”表示数据集<code>modeldat</code>中除了因变量之外的所有变量都被当作自变量。这里我们没有考虑交互效应，如果要添加<code>Q1</code>和<code>Q2</code>的交互效应，只要在“~”右边加上“Q1*Q2”即可。注意下面的代码中我们对原始变量进行了对数变换（<code>log(total_exp)</code>）。</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lmfit&lt;-<span class="kw">lm</span>(<span class="kw">log</span>(total_exp)~.,<span class="dt">data=</span>modeldat)
<span class="kw">summary</span>(lmfit)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = log(total_exp) ~ ., data = modeldat)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1.17494 -0.13719  0.01284  0.14163  0.56227 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  8.098314   0.054286 149.177  &lt; 2e-16 ***
## Q1          -0.145340   0.008823 -16.474  &lt; 2e-16 ***
## Q2           0.102275   0.019492   5.247 1.98e-07 ***
## Q3           0.254450   0.018348  13.868  &lt; 2e-16 ***
## Q6          -0.227684   0.011520 -19.764  &lt; 2e-16 ***
## Q8          -0.090706   0.016497  -5.498 5.15e-08 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.2262 on 805 degrees of freedom
## Multiple R-squared:  0.8542, Adjusted R-squared:  0.8533 
## F-statistic: 943.4 on 5 and 805 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>从模型结果总结中我们可以看到各个自变量的参数估计（<code>Estimate</code>列）、标准差（<code>Std. Error</code>），t统计量（<code>t value</code>）和p值（<code>Pr(&gt;|t|)</code>）。在输出的底部包含了残差标准误，即RMSE（<code>Residual standard error</code>），<span class="math inline">\(R^2\)</span>（<code>Multiple R-squared</code>）和调整后的<span class="math inline">\(R^2\)</span>（<code>Adjusted R-squared</code>），模型的F统计量（<code>F-statistic</code>）以及相应F检验的显著性p值（<code>p-value</code>）。</p>
<ul>
<li><strong>关于p值的讨论</strong></li>
</ul>
<p>谈到p值，不能不提美国统计协会在2016年2月发表的关于P值的声明 “Position on p-values: context, process, and purpose” <span class="citation">[<a href="#ref-ASA_P">51</a>]</span> ，统计之都有一篇对该声明的中文总结<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>。关于p值弊端的讨论在统计学领域已经不是新鲜事。其中一些抨击言辞比较激烈的是 Siegfried：</p>
<blockquote>
<p>这是科学中最肮脏的秘密：使用统计假设检验的“科学方法”建立在一个脆弱的基础之上。——ScienceNews（Siegfried, 2010）</p>
</blockquote>
<blockquote>
<p>假设检验中用到的统计方法……比Facebook隐私条款的缺陷还多。——ScienceNews（Siegfried, 2014）</p>
</blockquote>
<p>尽管争议已经持续了很久，但这是第一次统计协会对该话题给出郑重的声明，其主要目的不是解决该问题，而是对这些批评和讨论作一个回应，讨论发表一些关于p值的普遍共识，唤起大家对科学研究可重复性的重要性。声明中概括了关于p值的6个准则：</p>
<blockquote>
<ol style="list-style-type: decimal">
<li>P值可以表达的是数据与一个给定模型不匹配的程度。</li>
<li>P值并不能衡量某条假设为真的概率，或是数据仅由随机因素产生的概率。</li>
<li>科学结论、商业决策或政策制定不应该仅依赖于P值是否超过一个给定的阈值。</li>
<li>合理的推断过程需要完整的报告和透明度。</li>
<li>P值或统计显著性并不衡量影响的大小或结果的重要性。</li>
<li>P值就其本身而言，并不是一个非常好的对模型或假设所含证据大小的衡量。</li>
</ol>
</blockquote>
<p>在文章末尾列举了一些其他替代手段，其中之一就是报告置信区间而非p值。 回到当前的例子，这里我们不去讨论参数估计的对应p值，而是使用各个参数估计的置信区间。在R中可以用下面代码得到置信区间：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">confint</span>(lmfit,<span class="dt">level=</span><span class="fl">0.9</span>)</code></pre></div>
<pre><code>##                     5 %        95 %
## (Intercept)  8.00891811  8.18771037
## Q1          -0.15986889 -0.13081186
## Q2           0.07017625  0.13437445
## Q3           0.22423576  0.28466333
## Q6          -0.24665434 -0.20871330
## Q8          -0.11787330 -0.06353905</code></pre>
<p>上面的输出就是参数的90%置信区间。其中<code>level=0.9</code>将置信度设置为0.9。</p>
<p>拟合线性模型是非常容易的，以致于很多分析师拟合了模型之后不考虑模型是否合理，直接撰写结果报告。其实我们可以很容易用R从不同方面检查模型的拟合情况和假设条件。下面的几个小节中我们将介绍一些常用的线性模型诊断方法。</p>
</div>
<div id="section-10.1.2" class="section level3">
<h3><span class="header-section-number">10.1.2</span> 回归诊断</h3>
<p>拟合线性模型是非常容易的，以致于很多分析师拟合了模型之后不考虑模型是否合理，直接撰写结果报告。其实我们可以很容易用R从不同方面检查模型的拟合情况和假设条件。下面的几个小节中我们将介绍一些常用的线性模型诊断方法。我们希望需要最小二乘估计（OLS）同时也是最优线性无偏估计（BLUE）。换句话说，我们希望得到的估计的期望即为真实值（无偏），且最小化残差方差（最优）。根据高斯-马尔可夫定理（Gauss-Markov theorem），OLS在下面条件满足时是BLUE:</p>
<ol style="list-style-type: decimal">
<li>自变量（<span class="math inline">\(\mathbf{x_{.j}}\)</span>）和随机误差（<span class="math inline">\(\mathbf{\epsilon}\)</span>）不相关，即：<span class="math inline">\(cov(\mathbf{x_{.j},\epsilon})=0\)</span> 对 <span class="math inline">\(\forall j=j\in1...p\)</span></li>
<li>随机误差均值为0：<span class="math inline">\(E(\mathbf{\epsilon|X})=0\)</span></li>
<li>随机误差方差一致且相互独立：<span class="math inline">\(Var(\mathbf{\epsilon})=\sigma^{2}I\)</span>，其中<span class="math inline">\(\sigma\)</span>是正实数，<span class="math inline">\(I\)</span>是<span class="math inline">\(n\times n\)</span>的单位矩阵</li>
</ol>
<p>下面介绍4种图形诊断。</p>
<ol style="list-style-type: decimal">
<li><p>残差图（Residuals vs Fitted）</p>
残差图分析法是一种直观、方便的分析方法。它以残差<span class="math inline">\(\epsilon_{i}\)</span>为纵坐标，以样本拟合值为横坐标画散点图（也可以绘制横坐标为任意自变量的残差散点图）。正常情况下残差分布应该是随机的。我们要检查残差图的如下几个方面：
<ul>
<li>残差是否在0附近分布</li>
<li>残差分布是否随机，如果呈现出某种特定分布模式（如：随横坐标的增大而增大或减小）的话，说明当前模型关系的假设不充分</li>
<li>残差是否存在异方差性，比如随着拟合值增大残差分布方差增加，这就说明残差分布有异方差性。如前所述，当存在异方差时，参数估计值虽然是无偏的，但不是最小方差线性无偏估计。由于参数的显著性检验是基于残差分布假设的，所以在该假设不成立的情况下该检验也将失效。如果你用该回归方程来预测新样本，效果很可能极不理想。</li>
</ul></li>
<li><p>Q-Q图（Norm Q-Q）</p>
<p>Q-Q图是一种正态分布检测。对于标准状态分布，Q-Q图上的点分布在Y=X直线上，点偏离直线越远说明样本偏离正态分布越远。</p></li>
<li><p>标准化残差方根散点图（Scale-Location）</p>
<p>和残差图类似，横坐标依旧是样本拟合值，纵坐标变为了标准化残差的绝对值开方。</p></li>
<li><p>Cook距离图（Cook’s distance）</p>
<p>该图用于判断观测值是否有异常点。一般认为 当D&lt;0.5时认为不是异常值点；当D&gt;0.5时认为是异常值点。</p></li>
</ol>
<p>对回归结果应用<code>plot()</code>函数可以得到不同的图形诊断。</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))
<span class="kw">plot</span>(lmfit,<span class="dt">which=</span><span class="dv">1</span>)
<span class="kw">plot</span>(lmfit,<span class="dt">which=</span><span class="dv">2</span>)
<span class="kw">plot</span>(lmfit,<span class="dt">which=</span><span class="dv">3</span>)
<span class="kw">plot</span>(lmfit,<span class="dt">which=</span><span class="dv">4</span>)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:errorplot"></span>
<img src="DS_R_files/figure-html/errorplot-1.png" alt="一般线性回归残差图" width="80%" />
<p class="caption">
Figure 10.2: 一般线性回归残差图
</p>
</div>
<p>从回归的四个图形结果（图<a href="section-10.html#fig:errorplot">10.2</a>）来看：</p>
<ul>
<li>残差图：数据点都基本均匀地分布在直线y=0的两侧, 无明显趋势，满足线性假设。</li>
<li>标准Q-Q图：图上的点基本都在y=x直线附件，可认为残差近似服从正态分布；</li>
<li>标准化残差方根散点图：若满足不变方差假设，则在该图中水平线周围的点应随机分布，最高点为残差最大值点。该图显示基本符合方差齐性的要求。</li>
<li>Cook距离图：最大的Cook距离为0.05左右，可以认为没有异常值点。</li>
</ul>
</div>
<div id="section-10.1.3" class="section level3">
<h3><span class="header-section-number">10.1.3</span> 离群点，高杠杆点和强影响点</h3>
<p>关于一般线性回归，最好检查下是否有观测会强烈影响线性模型拟合结果。如果一个或者几个观测对模型结果有决定性的影响，那么用这些观测得到的模型是具有误导性的。这里我们介绍这三类观测点的检测：离群点，高杠杆点和强影响点。</p>
<ul>
<li>离群点</li>
</ul>
<p>刚才介绍的Cook距离图，以及之前讲到的Z分值都可以用来检测线性模型中的离群点。注意，Z分值仅仅是针对应变量观测而言，和使用的模型无关，即其并未考虑模型的拟合情况。下面我们用<code>car</code>包<span class="citation">[<a href="#ref-car">52</a>]</span>中的<code>outlierTest()</code>函数对拟合模型对象检测是否存在离群点，和Z分值方法鉴别的离群点不同，这里的离群点指的是<strong>那些模型预测效果不佳的观测点</strong>，通常有很大的、或正或负的残差，正残差说明模型低估了响应值，负残差说明高佑了响应值。这里使用的是Bonferroni离群点检验，该检验也可作用于广义线性模型。对于一般线性模型使用的是t检验，对于广义线性模型使用的是正态检验。关于该检验相关知识见 <span class="citation">[<a href="#ref-Williams1987">53</a>–<a href="#ref-weisberg14">56</a>]</span>。</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(car)
<span class="kw">outlierTest</span>(lmfit) <span class="co">#Bonferroni离群点检验</span></code></pre></div>
<pre><code>##      rstudent unadjusted p-value Bonferonni p
## 960 -5.295504          1.533e-07   0.00012432</code></pre>
<p><code>outlierTest()</code>函数是根据单个最大（或正或负）残差值的显著性来判断是否有离群点，若不显著，则说明数据集中没有离群点，若显著，则建议删除该离群点，然后再检验是否还有其他离群点存在。这里我们删除第960个被认为是离群点的观测。</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">outlierTest</span>(lmfit)</code></pre></div>
<pre><code>##      rstudent unadjusted p-value Bonferonni p
## 960 -5.295504          1.533e-07   0.00012432</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># 这里数据modeldat的行名是原数据集的行号，所以是字符类型</span>
<span class="co"># 找到相应的观测</span>
idex&lt;-<span class="kw">which</span>(<span class="kw">row.names</span>(modeldat)==<span class="st">&quot;960&quot;</span>)
<span class="co"># 删除离群观测</span>
modeldat=modeldat[-idex,]</code></pre></div>
<p>接下来我们再拟合一次模型然后检测看看是否还有离群点：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lmfit&lt;-<span class="kw">lm</span>(<span class="kw">log</span>(total_exp)~.,<span class="dt">data=</span>modeldat)
<span class="kw">outlierTest</span>(lmfit)</code></pre></div>
<pre><code>## 
## No Studentized residuals with Bonferonni p &lt; 0.05
## Largest |rstudent|:
##      rstudent unadjusted p-value Bonferonni p
## 155 -3.818112         0.00014483      0.11731</code></pre>
<p>可以看到现在没有检测出显著离群点。</p>
<p>高杠杆值点是与其他预测变量有关的离群点，即它们是由许多异常的预测变量组合起来的，与响应变量值没有关系。 高杠杆值的观测点可通过帽子矩阵的值（hat statistic）判断。对于一个给定的数据集，帽子均值为<span class="math inline">\(p/n\)</span>，其中p是模型估计的参数数目（包含截距项），n是样本量。一般来说，若观测点的帽子值大于帽子均值的2或3倍，则可认定为高杠杆值点。</p>
<!--
hat.plot<-function(fit){  
  p<-length(coefficients(fit))  
  n<-length(fitted(fit))  
  plot(hatvalues(fit),main="Index Plot of Hat Values")  
  abline(h=c(2,3)*p/n,col="red",lty=2)  
  identify(1:n,hatvalues(fit),names(hatvalues(fit)))  
}  
hat.plot(model) 

此图中可以看到1号点是高杠杆值点。

强影响点

强影响点，即对模型参数估计值影响有些比例失衡的点。例如，当移除 模型的一个观测点时模型会发生巨大的改变，那么需要检测一下数据中是否存在强影响点。Cook距离，或称为D统计量。Cook’s D值大于4/(n-k-1)，则表明它是强影响点，其中n为样本量大小，k是预测变量数目（有助于鉴别强影响点，但并不提供关于这些点如何影响模型的信息）。
对回归的影响点。根据Cook距离，13号点可能是个强影响点。

帽子统计量、DFFITS准测、Cook统计量和COVRATIO准则在R软件可分别通过hatvalues(),dffits(),cooks.distance()和covration()函数计算。influence.measures()可对一次获得这四个统计量的结果。 影响分析综合分析

influencePlot(model) 
#car包中的influencePlot（）函数，可将离群点、
#杠杆点和强影响点的信息整合到一幅图形中
influence.measures(model)

纵坐标超过2或小于-2的州可被认为是离群点，水平轴超过0.2或0.3的州有高杠杆值（通常为预测值的组合）。圆圈大小与影响成比例，圆圈很大的点可能是对模型估计造成的不成比例影响的强影响点。influence.measures()的inf用×标注异常值。
-->
</div>
</div>
<div id="section-10.2" class="section level2">
<h2><span class="header-section-number">10.2</span> 收缩方法</h2>
<p>之前特征工程的章节中讲到各种变量选择方法，其中我们对内嵌法没有详细展开。内嵌法是将特征选择的过程内嵌如建模的过程，它是学习器自身自主选择特征，这里我们要讲的收缩方法就属于内嵌法。我们可以通过对模型参数进行限制或者规范化来达到变量选择的效果，这些方法能将一些参数估计朝着0收缩。使用收缩方法提高模型拟合表现的原理可能不那么显而易见，但是收缩方法的效果是非常好的，这也是我最常使用的方法，尤其是项目要求使用可以解释的模型时。收缩方法不仅仅限于线性回归，之后我们在讲判别分析时会介绍将lasso用于逻辑回归。最常用的收缩方法是岭回归（ridge regression）、lasso以及弹性网络（elastic net）。弹性网络结合了岭回归和lasso中的罚函数，可以说是它们的一般化版本。</p>
<p>对于一般线性回归，在标准模型假设下最小二乘估计号称是方差最小无偏估计，这里的方差最小是在所有线性无偏估计当中最小。之间在介绍误差来源时讲过MSE是方差和偏差的一个组合。当预测变量存在高度相关时，估计量的方差可能会非常大，偏差的微小增加可能使得方差大幅度下降，因此对于回归模型中的多重共线性问题，有偏模型也可能得到具有竞争力的MSE取值。构建有偏回归模型的一种方法是在误差平方和的基础上加上一个惩罚项。</p>
<div id="section-10.2.1" class="section level3">
<h3><span class="header-section-number">10.2.1</span> 岭回归</h3>
<p>回顾最小二乘模型，它的目的是寻找参数的估计，以使得误差平方（RSS）和达到最小：</p>
<p><span class="math display">\[RSS=\Sigma_{i=1}^{n}(y_{i}-\beta_{0}-\Sigma_{j=1}^{p}\beta_{j}x_{ij})^{2}\]</span></p>
<p>岭回归<span class="citation">[<a href="#ref-Hoerl1970">57</a>]</span>和最小二乘回归有类似之处，不同在于优化的方程略有变化。岭回归寻找的是优化下面方程的<span class="math inline">\(\hat{\beta}^{R}\)</span>：</p>
<p><span class="math display">\[\Sigma_{i=1}^{n}(y_{i}-\beta_{0}-\Sigma_{j=1}^{p}\beta_{j}x_{ij})^{2}+\lambda\Sigma_{j=1}^{p}\beta_{j}^{2}=RSS+\lambda\Sigma_{j=1}^{p}\beta_{j}^{2}\]</span></p>
<p>其中<span class="math inline">\(\lambda &gt;0\)</span>是需要额外估计的调优参数。上式是在两个不同的准则间权衡。和最小二乘回归类似，岭回归考虑了最小化RSS，但其还有一个称为收缩惩罚的项<span class="math inline">\(\lambda\Sigma_{j=1}^{p}\beta_{j}^{2}\)</span>，该项在参数<span class="math inline">\(\beta_{1},...,\beta_{p}\)</span>趋近于0的时候变小，因此有向0收缩参数估计的用处。调优参数<span class="math inline">\(\lambda\)</span>用来调整这两个部分对最后参数估计的影响。当<span class="math inline">\(\lambda=0\)</span>时，惩罚项对结果没有影响，这时岭回归等同于最小二乘回归。当<span class="math inline">\(\lambda\rightarrow\infty\)</span>时，惩罚项的影响增大，岭回归系数估计趋近于0。这里惩罚只针对<span class="math inline">\(\beta_{1},...,\beta_{p}\)</span>，对截距项<span class="math inline">\(\beta_{0}\)</span>并没有惩罚。每个<span class="math inline">\(\lambda\)</span>值都对应一组参数估计，通过尝试不同的调优参数值，找到最优的模型。通常使用交互校验来选择调优参数，关于交互校验，我在之前基础建模技术那一章节已经讲过了。</p>
<p>有许多R函数可以进行岭回归。<code>MASS</code>包中的<code>lm.ridge()</code>函数，以及<code>elasticnet</code>包中的<code>enet()</code>函数，如果你知道调优参数的值，可以直接使用这两个函数拟合岭回归模型。如果你要对参数进行调优，最方便的函数是<code>caret</code>包中的<code>train()</code>函数。还是以服装消费者数据为例展示这些函数的使用，自变量为10个问卷调查问题以及年龄、性别、收入和房产情况，应变量为总体花销（在线花销和实体店花销的总和）。</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">dat&lt;-<span class="kw">read.csv</span>(<span class="st">&quot;/Users/happyrabbit/Documents/GitHub/DataScientistR/Data/SegData.csv&quot;</span>)
<span class="co"># 对数据进行一些清理，删除错误的样本观测，消费金额不能为负数</span>
dat&lt;-<span class="kw">subset</span>(dat,store_exp&gt;<span class="dv">0</span> &amp;<span class="st"> </span>online_exp&gt;<span class="dv">0</span>)
<span class="co"># 将10个问卷调查变量当作自变量</span>
trainx&lt;-dat[,<span class="kw">grep</span>(<span class="st">&quot;Q&quot;</span>,<span class="kw">names</span>(dat))]
<span class="co"># 将实体店消费量和在线消费之和当作应变量</span>
<span class="co"># 得到总消费量=实体店消费+在线消费</span>
trainy&lt;-dat$store_exp+dat$online_exp</code></pre></div>
<p>先用<code>train()</code>函数对参数进行调优。首先设置交互校验和参数调优范围，这里我们使用10层交互校验。岭回归在回归系数的平方和前加调优参数，因此最好对自变量进行标准化，这条准则适用于所有罚函数包含回归系数的方法。这里因为10个问题分值的范围是一致的，是否标准化并不太影响分析结果，但保险起见，建议大家在用此类方法前都先进行标准化：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ctrl &lt;-<span class="st"> </span><span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="dt">number =</span> <span class="dv">10</span>)
ridgeGrid &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">.lambda =</span> <span class="kw">seq</span>(<span class="dv">0</span>, .<span class="dv">1</span>, <span class="dt">length =</span> <span class="dv">20</span>))
<span class="kw">set.seed</span>(<span class="dv">100</span>)

ridgeRegTune &lt;-<span class="st"> </span><span class="kw">train</span>(trainx, trainy,
                      <span class="dt">method =</span> <span class="st">&quot;ridge&quot;</span>,
                      ## 用不同的罚函数值来拟合模型
                      <span class="dt">tuneGrid =</span> ridgeGrid,
                      <span class="dt">trControl =</span> ctrl,
                      ## 中心化和标度化变量
                      <span class="dt">preProc =</span> <span class="kw">c</span>(<span class="st">&quot;center&quot;</span>, <span class="st">&quot;scale&quot;</span>))</code></pre></div>
<pre><code>## Loading required package: elasticnet</code></pre>
<pre><code>## Loading required package: lars</code></pre>
<pre><code>## Loaded lars 1.2</code></pre>
<pre><code>## 
## Attaching package: &#39;lars&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:psych&#39;:
## 
##     error.bars</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ridgeRegTune</code></pre></div>
<pre><code>## Ridge Regression 
## 
## 999 samples
##  10 predictor
## 
## Pre-processing: centered (10), scaled (10) 
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 899, 899, 899, 899, 899, 899, ... 
## Resampling results across tuning parameters:
## 
##   lambda       RMSE      Rsquared 
##   0.000000000  1763.347  0.7896421
##   0.005263158  1762.683  0.7898448
##   0.010526316  1762.469  0.7899844
##   0.015789474  1762.589  0.7900829
##   0.021052632  1762.975  0.7901534
##   0.026315789  1763.584  0.7902040
##   0.031578947  1764.388  0.7902401
##   0.036842105  1765.368  0.7902652
##   0.042105263  1766.510  0.7902819
##   0.047368421  1767.804  0.7902919
##   0.052631579  1769.243  0.7902967
##   0.057894737  1770.821  0.7902971
##   0.063157895  1772.532  0.7902938
##   0.068421053  1774.373  0.7902876
##   0.073684211  1776.339  0.7902788
##   0.078947368  1778.429  0.7902677
##   0.084210526  1780.638  0.7902547
##   0.089473684  1782.964  0.7902399
##   0.094736842  1785.406  0.7902237
##   0.100000000  1787.959  0.7902060
## 
## RMSE was used to select the optimal model using  the smallest value.
## The final value used for the model was lambda = 0.01052632.</code></pre>
<p>训练出的模型调优参数为0.01，对应RMSE和<span class="math inline">\(R^{2}\)</span>分别为1762.469和80%。从交互校验的RMSE结果图中可以看到，随着调优参数增加，RMSE有一个略微减小然后增加的过程。</p>
<p><img src="DS_R_files/figure-html/unnamed-chunk-181-1.png" width="672" /></p>
<p>训练出调优参数之后，很多函数都可以用来拟合岭回归。这里展示如何使用<code>elasticnet</code>包中的<code>enet()</code>函数：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ridgefit =<span class="st"> </span><span class="kw">enet</span>(<span class="dt">x =</span> <span class="kw">as.matrix</span>(trainx), <span class="dt">y =</span> trainy, <span class="dt">lambda =</span> <span class="fl">0.01</span>,
                <span class="co"># 这里设置将自变量标准化</span>
                <span class="dt">normalize =</span> <span class="ot">TRUE</span>)</code></pre></div>
<p>这里注意<code>ridgefit</code>只指定了岭回归的罚函数，由于弹性网模型同时具有岭回归和lasso罚函数，我们需要进一步通过<code>predict()</code>函数得到相应的拟合系数和拟合结果。针对<code>enet</code>对象的<code>predict()</code>函数可以通过参数<code>s</code>和<code>mode</code>来指定在lasso罚参数下的拟合，这里我们需要“屏蔽”lasso的罚函数参数，这可以通过设置<code>s = 1</code> 和 <code>mode = &quot;fraction&quot;</code>得到。我们在之后讲lasso的时候会进一步讲解：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ridgePred &lt;-<span class="st"> </span><span class="kw">predict</span>(ridgefit, <span class="dt">newx =</span> <span class="kw">as.matrix</span>(trainx), 
                     <span class="dt">s =</span> <span class="dv">1</span>, <span class="dt">mode =</span> <span class="st">&quot;fraction&quot;</span>, <span class="dt">type =</span> <span class="st">&quot;fit&quot;</span>)</code></pre></div>
<p>注意上面<code>type = &quot;fit&quot;</code>返回的结果是一个列表，其中<code>fit</code>项包含预测结果：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">names</span>(ridgePred)</code></pre></div>
<pre><code>## [1] &quot;s&quot;        &quot;fraction&quot; &quot;mode&quot;     &quot;fit&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">head</span>(ridgePred$fit)</code></pre></div>
<pre><code>##         1         2         3         4         5         6 
## 1290.4697  224.1595  591.4406 1220.6384  853.3572  908.2040</code></pre>
<p>如果要得到参数拟合结果，需要在<code>predict()</code>函数中设定<code>type=&quot;coefficients&quot;</code>：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ridgeCoef&lt;-<span class="kw">predict</span>(ridgefit,<span class="dt">newx =</span> <span class="kw">as.matrix</span>(trainx), 
                   <span class="dt">s=</span><span class="dv">1</span>, <span class="dt">mode=</span><span class="st">&quot;fraction&quot;</span>, <span class="dt">type=</span><span class="st">&quot;coefficients&quot;</span>)</code></pre></div>
<p>返回的结果依旧是一个列表，其中<code>coefficients</code>项包含各个变量的参数估计：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># 这里不显示结果</span>
RidgeCoef=ridgeCoef$coefficients</code></pre></div>
<p>岭回归和原始最小二乘回归相比优势在于偏差和方差之间的权衡。之前讲过，一般线性回归中的最小二乘估计在无偏估计中是最优的，但是通常估计方差会很大。这意味着训练集数据的微小变化可能导致参数估计较大的变化。而岭回归估计就是通过牺牲一点点“无偏性”，换取估计方差的减小。因此，岭回归适合在普通最小二乘回归参数估计方差很大的情况下使用。</p>
</div>
<div id="lasso" class="section level3">
<h3><span class="header-section-number">10.2.2</span> Lasso</h3>
<p>虽然岭回归可以将参数估计值向0进行收缩，但对于任何调优参数值，它都不能将系数取值变为严格的0。尽管某些参数估计值变得非常小以至于可以忽略，但事实上岭回归并没有进行变量选择。这可能对预测精确度来说不是问题，但却对模型解释提出了挑战，尤其在变量个数大的时候。一种流行的用来替代岭回归的模型是“最小绝对收缩与选择算子”模型，通常被称为lasso<span class="citation">[<a href="#ref-Tibshirani1996">58</a>]</span>。这个模型使用了与岭回归类似的惩罚项，lasso的回归参数估计<span class="math inline">\(\hat{\beta}_{\lambda}^{L}\)</span>最小化如下方程：</p>
<p><span class="math display">\[\Sigma_{i=1}^{n}(y_{i}-\beta_{0}-\Sigma_{j=1}^{p}\beta_{j}x_{ij})^{2}+\lambda\Sigma_{j=1}^{p}|\beta_{j}|=RSS+\lambda\Sigma_{j=1}^{p}|\beta_{j}|\]</span></p>
<p>lasso和岭回归很相似，唯一的不同在于罚函数。岭回归中的<span class="math inline">\(\beta_{j}^{2}\)</span>在lasso中变为<span class="math inline">\(|\beta_{j}|\)</span>。用统计术语讲就是岭回归是在结构风险最小化的正则化因子上使用模型参数向量的二阶范数形式，lasso使用的是一阶范数形式。lasso不仅将参数估计向0收缩，当调优参数足够大时，一些参数估计将直接缩减为零，这可以达到特征提取的作用。这样一来，lasso回归的结果更易于解释。和其它有调优参数的模型类似，lasso也需要通过交互校验进行参数调优。下面我们展示在R中如何进行调优和拟合。</p>
<p>先用<code>train()</code>函数对参数进行调优。首先设置交互校验和参数调优范围，这里我们使用10层交互校验。建议大家在用此类方法前都先进行标准化：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ctrl &lt;-<span class="st"> </span><span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="dt">number =</span> <span class="dv">10</span>)
lassoGrid &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">fraction =</span> <span class="kw">seq</span>(.<span class="dv">8</span>, <span class="dv">1</span>, <span class="dt">length =</span> <span class="dv">20</span>))
<span class="kw">set.seed</span>(<span class="dv">100</span>)
lassoTune &lt;-<span class="st"> </span><span class="kw">train</span>(trainx, trainy,
                      <span class="dt">method =</span> <span class="st">&quot;lars&quot;</span>,
                      ## 用不同的罚函数值来拟合模型
                      <span class="dt">tuneGrid =</span> lassoGrid,
                      <span class="dt">trControl =</span> ctrl,
                      ## 中心化和标度化变量
                      <span class="dt">preProc =</span> <span class="kw">c</span>(<span class="st">&quot;center&quot;</span>, <span class="st">&quot;scale&quot;</span>))
lassoTune</code></pre></div>
<pre><code>## Least Angle Regression 
## 
## 999 samples
##  10 predictor
## 
## Pre-processing: centered (10), scaled (10) 
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 899, 899, 899, 899, 899, 899, ... 
## Resampling results across tuning parameters:
## 
##   fraction   RMSE      Rsquared 
##   0.8000000  1778.817  0.7872676
##   0.8105263  1776.714  0.7875679
##   0.8210526  1774.842  0.7878315
##   0.8315789  1773.090  0.7880823
##   0.8421053  1771.293  0.7883477
##   0.8526316  1769.551  0.7886105
##   0.8631579  1767.924  0.7888577
##   0.8736842  1766.442  0.7890909
##   0.8842105  1765.127  0.7893044
##   0.8947368  1763.960  0.7894958
##   0.9052632  1762.974  0.7896597
##   0.9157895  1762.165  0.7897985
##   0.9263158  1761.540  0.7899101
##   0.9368421  1761.212  0.7899582
##   0.9473684  1761.076  0.7899771
##   0.9578947  1761.121  0.7899701
##   0.9684211  1761.350  0.7899381
##   0.9789474  1761.839  0.7898641
##   0.9894737  1762.492  0.7897668
##   1.0000000  1763.347  0.7896421
## 
## RMSE was used to select the optimal model using  the smallest value.
## The final value used for the model was fraction = 0.9473684.</code></pre>
<p>训练出的模型调优参数为0.95，对应RMSE和<span class="math inline">\(R^{2}\)</span>分别为1761和79%，和之间岭回归几乎相同。从交互校验的RMSE结果图中可以看到，随着调优参数增加，RMSE有一个减小然后增加的过程。</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(lassoTune)</code></pre></div>
<p><img src="DS_R_files/figure-html/unnamed-chunk-188-1.png" width="672" /></p>
<p>lasso模型可以用许多不同的函数进行拟合。<code>lars</code>包中的<code>lars()</code>函数，<code>elasticnet</code>包的<code>enet()</code>函数，<code>glmnet</code>包的<code>glmnet()</code>函数都可以拟合lasso，它们的语法非常相似。我们还是使用<code>enet()</code>函数，其要求自变量必须是一个矩阵对象，因此要将数据框<code>trainx</code>转换成矩阵。此外，预测变量在建模之前需要中心化和标准化，函数中的<code>normalize</code>参数可以自动完成这一过程<code>lambda</code>参数控制了岭回归的罚参数，因此将该值设为0即为拟合lasso模型。</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lassoModel&lt;-<span class="st"> </span><span class="kw">enet</span>(<span class="dt">x =</span> <span class="kw">as.matrix</span>(trainx), <span class="dt">y =</span> trainy, <span class="dt">lambda =</span> <span class="dv">0</span>, <span class="dt">normalize =</span> <span class="ot">TRUE</span>)</code></pre></div>
<p>lasso模型在进行预测之前不需要进行指定：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lassoFit &lt;-<span class="st"> </span><span class="kw">predict</span>(lassoModel, <span class="dt">newx =</span> <span class="kw">as.matrix</span>(trainx),<span class="dt">s =</span> .<span class="dv">95</span>, <span class="dt">mode =</span> <span class="st">&quot;fraction&quot;</span>,<span class="dt">type =</span> <span class="st">&quot;fit&quot;</span>)</code></pre></div>
<p>类似，这里<code>type = &quot;fit&quot;</code>返回的结果是一个列表，其中<code>fit</code>项包含预测结果：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">head</span>(lassoFit$fit)</code></pre></div>
<pre><code>##         1         2         3         4         5         6 
## 1371.6160  308.6984  702.2026 1225.5508  832.0466 1028.9785</code></pre>
<p>如果要得到参数拟合结果，需要在<code>predict()</code>函数中设定<code>type=&quot;coefficients&quot;</code>：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lassoCoef&lt;-<span class="kw">predict</span>(lassoModel,<span class="dt">newx =</span> <span class="kw">as.matrix</span>(trainx),<span class="dt">s=</span><span class="fl">0.95</span>, <span class="dt">mode=</span><span class="st">&quot;fraction&quot;</span>, <span class="dt">type=</span><span class="st">&quot;coefficients&quot;</span>)</code></pre></div>
<p>返回的结果依旧是一个列表，其中<code>coefficients</code>项包含各个变量的参数估计：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># 这里不显示结果</span>
LassoCoef=lassoCoef$coefficients</code></pre></div>
<p>这类规则化方法的研究现在非常活跃。很多学者将lasso模型嫁接到其它方法上，比如线性判别<span class="citation">[<a href="#ref-Clem2011">59</a>]</span>，偏最小二乘回归<span class="citation">[<a href="#ref-chun2010">60</a>]</span>。但由于一阶范数不是连续可导的，lasso回归的计算过程更复杂。很多学者对相应的优化算法进行了研究，其中最重要的改进是Bradley Efron等<span class="citation">[<a href="#ref-efron2014">61</a>]</span>提出的最小角回归(Least Angle Regression［LARS］)算法，该算法很好地解决Lasso的计算问题，尤其在维度高的时候。</p>
</div>
<div id="section-10.2.3" class="section level3">
<h3><span class="header-section-number">10.2.3</span> 弹性网络</h3>
<p>弹性网络是lasso的一般化版本<span class="citation">[<a href="#ref-zou2005">62</a>]</span>，该模型结合了两种罚函数，参数估计最小化如下方程：</p>
<p><span class="math display">\[\Sigma_{i=1}^{n}(y_{i}-\hat{y}_{i})^{2}+\lambda_{1}\Sigma_{j=1}^{p}\beta_{j}^{2}+\lambda_{2}\Sigma_{j=1}^{p}|\beta_{j}|\]</span></p>
<p>lasso对应的估计方差较大，而岭回归又没有特征选择的功能，弹性网络的优点在于利用了岭回归的罚函数，同时又有lasso的特征选择功能。Zou和Hastie<span class="citation">[<a href="#ref-zou2005">62</a>]</span>指出该模型能够更有效的处理成组的高度相关变量。</p>
<p>还是先用<code>train()</code>函数对参数进行调优。首先设置交互校验和参数调优范围，这里我们使用10层交互校验，并且对变量标准化：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">enetGrid &lt;-<span class="st"> </span><span class="kw">expand.grid</span>(<span class="dt">.lambda =</span> <span class="kw">seq</span>(<span class="dv">0</span>,<span class="fl">0.2</span>,<span class="dt">length=</span><span class="dv">20</span>), <span class="dt">.fraction =</span> <span class="kw">seq</span>(.<span class="dv">8</span>, <span class="dv">1</span>, <span class="dt">length =</span> <span class="dv">20</span>))
<span class="kw">set.seed</span>(<span class="dv">100</span>)
enetTune &lt;-<span class="st"> </span><span class="kw">train</span>(trainx, trainy,
                  <span class="dt">method =</span> <span class="st">&quot;enet&quot;</span>,
                  <span class="dt">tuneGrid =</span> enetGrid,
                  <span class="dt">trControl =</span> ctrl,
                  <span class="dt">preProc =</span> <span class="kw">c</span>(<span class="st">&quot;center&quot;</span>, <span class="st">&quot;scale&quot;</span>))</code></pre></div>
<p>训练出的模型对应的lasso调优参数为0.958，岭回归调优参数为0.01。相应RMSE和<span class="math inline">\(R^{2}\)</span>分别为1760和79%，在这里这三种方法的效果并没有很大的不同。这里展示如何使用<code>elasticnet</code>包中的<code>enet()</code>函数：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">enetfit =<span class="st"> </span><span class="kw">enet</span>(<span class="dt">x =</span> <span class="kw">as.matrix</span>(trainx), <span class="dt">y =</span> trainy, <span class="dt">lambda =</span> <span class="fl">0.01</span>,
                <span class="co"># 这里设置将自变量标准化</span>
                <span class="dt">normalize =</span> <span class="ot">TRUE</span>)</code></pre></div>
<p>和之前一样，<code>enetfit</code>只指定了岭回归的罚函数，由于弹性网模型同时具有岭回归和lasso罚函数，我们需要进一步通过<code>predict()</code>函数得到相应的拟合系数和拟合结果。针对<code>enet</code>对象的<code>predict()</code>函数可以通过参数<code>s</code>和<code>mode</code>来指定在lasso罚参数下的拟合，和之前不同的是，这里我们对应的lasso的罚函数参数设置为<code>s = 0.958</code> 和 <code>mode = &quot;fraction&quot;</code>：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">enetPred &lt;-<span class="st"> </span><span class="kw">predict</span>(enetfit, <span class="dt">newx =</span> <span class="kw">as.matrix</span>(trainx), 
                     <span class="dt">s =</span> <span class="fl">0.958</span>, <span class="dt">mode =</span> <span class="st">&quot;fraction&quot;</span>, <span class="dt">type =</span> <span class="st">&quot;fit&quot;</span>)</code></pre></div>
<p>注意上面<code>type = &quot;fit&quot;</code>返回的结果是一个列表，其中<code>fit</code>项包含预测结果。如果要得到参数拟合结果，需要在<code>predict()</code>函数中设定<code>type=&quot;coefficients&quot;</code>：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">enetCoef&lt;-<span class="kw">predict</span>(ridgefit,<span class="dt">newx =</span> <span class="kw">as.matrix</span>(trainx), 
                   <span class="dt">s=</span><span class="fl">0.958</span>, <span class="dt">mode=</span><span class="st">&quot;fraction&quot;</span>, <span class="dt">type=</span><span class="st">&quot;coefficients&quot;</span>)</code></pre></div>
<p>返回的结果依旧是一个列表，其中<code>coefficients</code>项包含各个变量的参数估计。</p>
</div>
</div>
<div id="lasso" class="section level2">
<h2><span class="header-section-number">10.3</span> 知识扩展：Lasso的变量选择功能</h2>
<p>可能有人会问从岭回归到lasso，只是罚函数从二阶范数变成一阶范数，为什么lasso就能够将参数估计收缩成0而岭回归不能呢？要回答这个问题，我们先看下lasso和岭回归分别对应的另一版本的等价优化方程。对于lasso而言，优化下面两个方程是等价的：</p>
<p><span class="math display">\[\Sigma_{i=1}^{n}(y_{i}-\beta_{0}-\Sigma_{j=1}^{p}\beta_{j}x_{ij})^{2}+\lambda\Sigma_{j=1}^{p}|\beta_{j}|=RSS+\lambda\Sigma_{j=1}^{p}|\beta_{j}|\]</span></p>
<p><span class="math display">\[\underset{\beta}{min}\left\{ \Sigma_{i=1}^{n}\left(y_{i}-\beta_{0}-\Sigma_{j=1}^{p}\beta_{j}x_{ij}\right)^{2}\right\} ,\ \Sigma_{j=1}^{p}|\beta_{j}|\leq s\]</span></p>
<p>也就是说，对每个调优参数<span class="math inline">\(\lambda\)</span>的取值，都存在相应的<span class="math inline">\(s\)</span>值，使得上面两个方程优化后得到的参数估计相同。类似的，对于岭回归，下面两个方程等价：</p>
<p><span class="math display">\[\Sigma_{i=1}^{n}(y_{i}-\beta_{0}-\Sigma_{j=1}^{p}\beta_{j}x_{ij})^{2}+\lambda\Sigma_{j=1}^{p}\beta_{j}^{2}=RSS+\lambda\Sigma_{j=1}^{p}\beta_{j}^{2}\]</span></p>
<p><span class="math display">\[\underset{\beta}{min}\left\{ \Sigma_{i=1}^{n}\left(y_{i}-\beta_{0}-\Sigma_{j=1}^{p}\beta_{j}x_{ij}\right)^{2}\right\} ,\ \Sigma_{j=1}^{p}\beta_{j}^{2}\leq s\]</span></p>
<p>当p＝2时，lasso的参数估计是所有满足<span class="math inline">\(|\beta_{1}|+|\beta_{2}|\leq s\)</span>的<span class="math inline">\(\beta_{1}\)</span>和<span class="math inline">\(\beta_{2}\)</span>取值中最小化<span class="math inline">\(RSS\)</span>的。岭回归是估计所有满足<span class="math inline">\(\beta_{1}^{2}+\beta_{2}^{2}\leq s\)</span>的参数取值中最小化<span class="math inline">\(RSS\)</span>的。当s很大时，相应的限制条件几乎是无效的，只要参数估计能够最小化<span class="math inline">\(RSS\)</span>即使绝对值很大也没有问题。只要s所定义的区域包含最小二乘解，那么收缩方法得出的参数估计和一般最小二乘回归就相同。相反，如果s很小，那么可能的参数取值范围就很有限。</p>
<p>了解这样的等价表达之后，我们再看看岭回归和lasso的不同之处。</p>
<p align="center">
<img src="http://scientistcafe.com/book/Figure/LassoRidge.png" width="50%"  />
</p>
<p>左边是lasso对应的误差等位线和正方形限制区域，右边是岭回归对应的等位线和圆形限制区域。 上面图中围绕在<span class="math inline">\(\hat{\beta}\)</span>周围的椭圆表示有相同RSS的参数估计。随着椭圆的扩大，对应的RSS增加。lasso和岭回归的估计值就是在一定的限制区域下，椭圆不断扩张的过程中和限制区域的第一个接触点。大家想想看，如果有某个参数的估计是0的话，那么这个接触点该在哪里？一定在某条坐标轴上。由于岭回归的限制区域是圆形，所以真正的触点无法落在坐标轴上，可能无限接近，但就是到不了。这就是求之而不可得的数学诠释。所以岭回归无法将参数收缩成0，而lasso可以。</p>
<p>上面是2个参数的情况。如果参数个数是3的话，那么lasso的限制区域就是一个三位空间的多面体，而岭回归的限制区域就是个球。参数个数再增加的话，就得让科幻小说家来描述了。希望大家理解lasso可以进行变量选择，而岭回归不行的几何解释。</p>
</div>
<div id="section-10.4" class="section level2">
<h2><span class="header-section-number">10.4</span> 主成分和偏最小二乘回归</h2>
<p>在实际应用中，自变量之间通常是彼此相关的，包含相似的信息。比如我们在建模技术那章用消费记录变量建立关于消费者收入的预测模型，其中实体店花销（<code>store_exp</code>），在线花销（<code>online_exp</code>），实体店交易次数（<code>store_trans</code>），在线交易次数（<code>online_trans</code>）这几个变量之间有不同程度的相关性，尤其是交易次数和对应花销相关性较高。如果自变量之间相关性较大， 那么多元线性回归的最小二乘估计将会很不稳定。有的时候，自变量的数目会高于观测，这时通常意义上的最小二乘将同样无法得到一组唯一的最小化SSE的回归系数。在这些情况下进行回归，通常的解决办法是对自变量进行预处理，包括数据预处理章节讲到的删除高度相关的自变量算法，移除高度相关的预测变量可以保证它们两两间的相关系数不超过一个给定的阈值。然而，这一过程并不能保证某些预测变量的线性组合与其他变量是不相关的。如果情况如此，那么最小二乘的解将依然是不稳定的。因此特别需要注意的是，移除两两之间高度相关的预测变量并不能保证一个稳定的解。你也可以参考特征工程那章讲到的特征提取方法，比如对预测变量进行主成分分析（PCA），让后用主成分进行回归。这种方法得到的主成分是彼此不相关的。但存在的弊端是得到的新自变量是原变量的线性组合，因此使得模型难以解释。</p>
<p>对自变量进行主成分分析，然后用得到得主成分进行回归的方法称为<strong>主成分回归（PCR）</strong><span class="citation">[<a href="#ref-Massy1965">63</a>]</span>。 这项技术在自变量内在强相关或者变量个数大于观测个数时可以使用。然而，尽管这样先降维然后回归的方法可以在理论上建立回归模型，但结果可能并不理想。因为PCA降维得到的新变量并不一定能很好地解释因变量，因为这个过程是无监督的，在降维的时候并没有考虑到因变量的信息，它只是简单地追踪自变量空间的方差。如果自变量自身的变异恰好与因变量相关，那么PCR或许能很好地识别出它们之间的关系。 然而，如果自变量自身方差大，但其变化并不影响因变量，那么PCR或许就无法识别出真实存在的关系。还有需要注意的一点是，虽然PCR是降维的方法，但它并不是变量选择的方法，因为它是所有的自变量的线性组合。</p>
<p><strong>偏最小二乘回归（PLS）</strong>可以说是PCR的有监督版本。像PCR，PLS也是一种降维方法，而且也受变量方差的影响，因此通常需要在建模前对变量进行标准化。假设我们有随机变量组成的向量<span class="math inline">\(\mathbf{X}=[X_{1},X_{2},...,X_{p}]^{T}\)</span>，其对应的协方差矩阵为<span class="math inline">\(\Sigma\)</span>。PLS也需要通过对原变量进行线性组合找到<strong>彼此不相关</strong>的新变量 <span class="math inline">\((Z_{1} , Z_{2} , \ldots , Z_{m})\)</span>来代替原变量，且当<span class="math inline">\(m=p\)</span>时，PLS的结果和普通最小二乘的结果相同。这和PCA类似，且生成的线性组合通常被称为主成分或者潜变量。但其不同在于，PCA的线性组合将最大程度地概括预测变量空间的变异性， 而PLS的线性组合则是最大化其与因变量的协方差。PLS在构建自变量线性组合的过程中考虑了自变量和应变量的关系，赋予和应变量相关性高的自变量更高的权重。PLS是在两个目标之间达到一个平衡：对预测变量空间进行降维， 以及保持预测变量与因变量之间的预测关系。</p>
<div class="figure">
<img src="http://scientistcafe.com/book/Figure/pls.png" alt="PLS流程图" />
<p class="caption">PLS流程图</p>
</div>
<p>PLS源自于Herman Wold的非线性迭代偏最小二乘（NIPALS）算法<span class="citation">[<a href="#ref-wold1966">64</a>, <a href="#ref-wold1982">65</a>]</span>。 该算法被用来对那些参数非线性的模型进行线性化。之后NIPALS方法被应用在变量之间彼此相关的回归问题中，称其为“PLS”<span class="citation">[<a href="#ref-wold1983">66</a>]</span>。简单来说，NIPALS算法会迭代地寻找高度相关的预测变量与因变量之间潜在的关系。 对于单因变量问题而言，算法的每一次迭代都会评价自变量（<span class="math inline">\(\mathbf{X}\)</span>）与因变量（<span class="math inline">\(\mathbf{y}\)</span>）之间的关系，并将这种关系用一个权重向量（<span class="math inline">\(\mathbf{\varphi}\)</span>）表达，该向量被称为一个方向。接下来，自变量将被正交地投影到这个方向上，从而生成这个方向上的分值（<span class="math inline">\(\mathbf{z}\)</span>），接着用这个分值生成载荷（<span class="math inline">\(\mathbf{\theta}\)</span>），载荷代表了得分向量与原自变量之间的相关系数。每经过一次迭代，自变量和因变量将被“缩减”，意思是它们将分别减去当前对自变量和因变量结构的估计值。缩减后的自变量和因变量信息将被用来生成下一组权重、得分和载荷。PLS的具体算法如下（这里我们默认将所有的<span class="math inline">\(\mathbf{x}\)</span>标准化为均值0方差1的向量）：</p>
<ol style="list-style-type: decimal">
<li>设置初始值<span class="math inline">\(\mathbf{\hat{y}^{0}}=\bar{y}\mathbf{1}\)</span>，<span class="math inline">\(\mathbf{x_j^{0}}=\mathbf{x_j},\ j=1,\dots,p\)</span>；</li>
<li>对<span class="math inline">\(m=1,2,...,p\)</span>:
<ul>
<li><span class="math inline">\(\mathbf{z_{m}}=\Sigma_{j=1}^p \hat{\varphi}_{mj}\mathbf{x_j^{m-1}}\)</span>，其中<span class="math inline">\(\hat{\varphi}_{mj}=&lt;\mathbf{x_j^{m-1},y}&gt;\)</span>；</li>
<li><span class="math inline">\(\mathbf{\hat{\theta}_m}=\frac{&lt;\mathbf{z_{m},y}&gt;}{&lt;\mathbf{z_{m},z_{m}}&gt;}\)</span>；</li>
<li><span class="math inline">\(\mathbf{\hat{y}^m}=\mathbf{\hat{y}^{m-1}}+\hat{\theta}_{m}\mathbf{z_m}\)</span>；</li>
<li>每个<span class="math inline">\(\mathbf{x_j^{m-1}}\)</span>对<span class="math inline">\(\mathbf{z_{m}}\)</span>的方向正交化：<span class="math inline">\(\mathbf{x_{j}^{m}}=\mathbb{x_j^{m-1}}-\frac{&lt;\mathbf{z_m,x_j^{m-1}}&gt;\mathbf{z_m}}{&lt;\mathbf{z_m,z_m}&gt;},\ j=1,2,\dots, p\)</span>；</li>
</ul></li>
<li>输出一系列的<span class="math inline">\(\mathbf{\hat{y}^{m}}, m=1,\dots, p\)</span>。由于所有的<span class="math inline">\(\mathbf{z_m}\)</span>都是<span class="math inline">\(\mathbf{x_j}\)</span>的线性组合，所以<span class="math inline">\(\mathbf{\hat{y}^{m}}=\mathbf{X}\hat{\beta}^{pls}(m)\)</span>也是<span class="math inline">\(\mathbf{x_j}\)</span>的线性组合。</li>
</ol>
<p>下面我们以服装消费者数据中的10个问卷调查回复为自变量（<code>Q1-Q10</code>），收入（<code>income</code>）为应变量展示如何用R中的<code>caret</code>包训练PCR和PLS模型。</p>
<p>先载入相应的包和数据，并且对数据进行适当预处理：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(lattice)
<span class="kw">library</span>(caret)
<span class="kw">library</span>(dplyr)
<span class="kw">library</span>(elasticnet)
<span class="kw">library</span>(lars)
<span class="co"># 载入数据</span>
sim.dat&lt;-<span class="kw">read.csv</span>(<span class="st">&quot;/Users/happyrabbit/Documents/GitHub/DataScientistR/Data/SegData.csv&quot;</span>)
ymad&lt;-<span class="kw">mad</span>(<span class="kw">na.omit</span>(sim.dat$income))
<span class="co"># 计算Z分值</span>
zs&lt;-(sim.dat$income-<span class="kw">mean</span>(<span class="kw">na.omit</span>(sim.dat$income)))/ymad
<span class="co"># which(na.omit(zs&gt;3.5)) 找到利群点</span>
<span class="co"># which(is.na(zs)) 找到缺失值</span>
idex&lt;-<span class="kw">c</span>(<span class="kw">which</span>(<span class="kw">na.omit</span>(zs&gt;<span class="fl">3.5</span>)),<span class="kw">which</span>(<span class="kw">is.na</span>(zs)))
<span class="co"># 删除含有离群点和缺失值的行</span>
sim.dat&lt;-sim.dat[-idex,]</code></pre></div>
<p>选取10个问卷调查变量作为自变量矩阵<code>xtrain</code>，收入作为应变量存在<code>ytrain</code>中：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">xtrain=dplyr::<span class="kw">select</span>(sim.dat, Q1:Q10)
ytrain=sim.dat$income</code></pre></div>
<p>设置随机种子，以及交互校验的方式为10层交互校验：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">100</span>)
ctrl &lt;-<span class="st"> </span><span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="dt">number=</span><span class="dv">10</span>)</code></pre></div>
<p>先训练PLS模型，这里的调优参数为使用潜变量的个数，因为这里最多只有10个变量，设置调优参数值为1-10：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">plsTune &lt;-<span class="st"> </span><span class="kw">train</span>(xtrain, ytrain,
                 <span class="dt">method =</span> <span class="st">&quot;pls&quot;</span>,
                 <span class="co"># 设置调优参数值</span>
                 <span class="dt">tuneGrid =</span> <span class="kw">expand.grid</span>(<span class="dt">.ncomp =</span> <span class="dv">1</span>:<span class="dv">10</span>),
                 <span class="dt">trControl =</span> ctrl)
plsTune</code></pre></div>
<pre><code>## Partial Least Squares 
## 
## 772 samples
##  10 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 695, 694, 694, 694, 694, 695, ... 
## Resampling results across tuning parameters:
## 
##   ncomp  RMSE      Rsquared 
##    1     27761.47  0.6487324
##    2     24493.98  0.7280445
##    3     23222.55  0.7561373
##    4     23068.86  0.7595387
##    5     23034.38  0.7604738
##    6     23035.39  0.7604228
##    7     23034.19  0.7604444
##    8     23034.85  0.7604287
##    9     23034.69  0.7604318
##   10     23034.70  0.7604316
## 
## RMSE was used to select the optimal model using  the smallest value.
## The final value used for the model was ncomp = 7.</code></pre>
<p>可以看到，调优的结果是选取7个主成分。但如果大家注意输出模型中成分数目对应的<code>RMSE</code>的值可以发现，其实在成分数目5之后<code>RMSE</code>变化就不太大了。</p>
<p>从PLS调优过程中还可以得到变量的重要性排序，我们可以通过下面代码得到各个变量的重要性并且对其可视化：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">plsImp &lt;-<span class="st"> </span><span class="kw">varImp</span>(plsTune, <span class="dt">scale =</span> <span class="ot">FALSE</span>)
<span class="kw">plot</span>(plsImp, <span class="dt">top =</span> <span class="dv">10</span>, <span class="dt">scales =</span> <span class="kw">list</span>(<span class="dt">y =</span> <span class="kw">list</span>(<span class="dt">cex =</span> .<span class="dv">95</span>)))</code></pre></div>
<p><img src="DS_R_files/figure-html/unnamed-chunk-201-1.png" width="672" /></p>
<p>可以看到，Q1、Q2、Q3和Q6的重要性明显高于其它变量。下面我们对主成分回归进行调优，调优参数仍旧是模型中成分的数目：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># 设置随机种子</span>
<span class="kw">set.seed</span>(<span class="dv">100</span>)
pcrTune &lt;-<span class="st"> </span><span class="kw">train</span>(<span class="dt">x =</span> xtrain, <span class="dt">y =</span> ytrain,
                 <span class="dt">method =</span> <span class="st">&quot;pcr&quot;</span>,
                 <span class="co"># 设置调优参数值</span>
                 <span class="dt">tuneGrid =</span> <span class="kw">expand.grid</span>(<span class="dt">.ncomp =</span> <span class="dv">1</span>:<span class="dv">10</span>),
                 <span class="dt">trControl =</span> ctrl)
pcrTune                  </code></pre></div>
<pre><code>## Principal Component Analysis 
## 
## 772 samples
##  10 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 695, 694, 694, 694, 694, 695, ... 
## Resampling results across tuning parameters:
## 
##   ncomp  RMSE      Rsquared  
##    1     45915.48  0.03427177
##    2     32495.72  0.51800996
##    3     23280.82  0.75489255
##    4     23307.53  0.75429987
##    5     23215.51  0.75599048
##    6     23191.55  0.75656906
##    7     23174.19  0.75701726
##    8     23164.03  0.75748073
##    9     23058.54  0.76014078
##   10     23034.70  0.76043161
## 
## RMSE was used to select the optimal model using  the smallest value.
## The final value used for the model was ncomp = 10.</code></pre>
<p>结果建议的调优参数的个数是10，同样的，如果大家观察输出中<code>RMSE</code>的变化可以发现，其实在成分数目为5之后变化就开始趋于平缓了。这里的两种情况下，选择5个成分用于建模都是合理的。我们比较下两个模型调优的过程：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># 将PLS模型调优返回对象中需要的模型调优部分信息存在对象plsResamples里</span>
plsResamples &lt;-<span class="st"> </span>plsTune$results
plsResamples$Model &lt;-<span class="st"> &quot;PLS&quot;</span>
<span class="co"># 将PCR模型调优返回对象中需要的模型调优部分信息存在对象plsResamples里</span>
pcrResamples &lt;-<span class="st"> </span>pcrTune$results
pcrResamples$Model &lt;-<span class="st"> &quot;PCR&quot;</span>
<span class="co"># 合并二者得到用于作图的数据集</span>
plsPlotData &lt;-<span class="st"> </span><span class="kw">rbind</span>(plsResamples, pcrResamples)
<span class="co"># 用lattice包中的xyplot()函数对两个模型结果可视化比较</span>
<span class="kw">xyplot</span>(RMSE ~<span class="st"> </span>ncomp,
       <span class="dt">data =</span> plsPlotData,
       <span class="co">#aspect = 1,</span>
       <span class="dt">xlab =</span> <span class="st">&quot;# Components&quot;</span>,
       <span class="dt">ylab =</span> <span class="st">&quot;RMSE (Cross-Validation)&quot;</span>,
       <span class="dt">auto.key =</span> <span class="kw">list</span>(<span class="dt">columns =</span> <span class="dv">2</span>),
       <span class="dt">groups =</span> Model,
       <span class="dt">type =</span> <span class="kw">c</span>(<span class="st">&quot;o&quot;</span>, <span class="st">&quot;g&quot;</span>))</code></pre></div>
<p><img src="DS_R_files/figure-html/unnamed-chunk-203-1.png" width="672" /></p>
<p>之前我们通过观察<code>RMSE</code>得出结论可以取前5个成分，这里的图形表明用前3个成分给出的模型效果和用更多成分给出的模型效果已经没有太大差别了。</p>
<!--## 分层线性回归

我们已经讨论过简单线性模型，下面讨论该模型的扩展分层线性模型。什么时候需要使用分层模型呢？需要考虑分层结构的常见情况有两种。（1）当你的数据有嵌套结构，这是指一些观测可能属于更高一层级单位。比如在教育学分析当中通常需要研究学生的学习情况，同一所学校或者同一班级的学生相似度更高，这里我们就需要考虑学生学习情况的个体观测可能嵌套在班级，或者学校这个更高一层级单位中。市场营销中，地理位置通常是一个更高的层极单位。北上广的消费者和一些二三线城市的消费者可能不同，东南沿海和东北地区的消费者差异可能更大。这些情况中我们都需要考虑分层结构。（2）还有一种情况是针对纵向数据。比如对一些人年收入连续10年的观测，那么研究观测就该考虑个人的随机效应。类似的还有我们的航空公司满意度调查数据，每个受访者针对每项对3个航空公司进行评分，这里也需要考虑受访者的个体随机效应，可能的情况是有的人倾向于给高分，有的人倾向于给低分，这样的倾向和问卷问题以及哪家航空公司无关。

## 贝叶斯线性回归

## 贝叶斯分层线性回归
-->

</div>
</div>
<h3> References</h3>
<div id="refs" class="references">
<div id="ref-ASA_P">
<p>51. Ronald L. Wassersteina, N.A.L.: Position on p-values: Context, process, and purpose. (2016).</p>
</div>
<div id="ref-car">
<p>52. Fox, J., Weisberg, S.: An r companion to applied regression. Sage. (2011).</p>
</div>
<div id="ref-Williams1987">
<p>53. Williams, D.A.: Generalized linear model diagnostics using the deviance and single case deletions. Applied Statistics. 36, 181–191. (1987).</p>
</div>
<div id="ref-weisberg14">
<p>56. Weisberg, S.: Applied linear regression. Wiley (2014).</p>
</div>
<div id="ref-Hoerl1970">
<p>57. A, H.: Ridge regression: Biased estimation for nonorthogonal problems. Technometrics. 12, 55–67 (1970).</p>
</div>
<div id="ref-Tibshirani1996">
<p>58. R, T.: Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society Series B (Methodological). 58, 267–288 (1996).</p>
</div>
<div id="ref-Clem2011">
<p>59. Clemmensen L, W.D., Hastie T: Sparse discriminant analysis. Technometrics. 53, 406–413 (2011).</p>
</div>
<div id="ref-chun2010">
<p>60. H, C., S, K.: Sparse partial least squares regression for simultaneous dimension reduction and variable selection. Journal of the Royal Statistical Society: Series B (Statistical Methodology). 72, 3–25 (2010).</p>
</div>
<div id="ref-efron2014">
<p>61. Efron B, J.I., Hastie T: Least angle regression. The Annals of Statistics. 32, 407–499 (2014).</p>
</div>
<div id="ref-zou2005">
<p>62. H, Z., T, H.: Regularization and variable selection via the elastic net. Journal of the Royal Statistical Society, Series B, 67, 301–320 (2005).</p>
</div>
<div id="ref-Massy1965">
<p>63. W, M.: Principal components regression in exploratory sta- tistical research. Journal of the American Statistical Association. 60, 234–246 (1965).</p>
</div>
<div id="ref-wold1966">
<p>64. H, W.: Multivariate analyses. Presented at the (1966).</p>
</div>
<div id="ref-wold1982">
<p>65. H, W.: Systems under indirect observation: Causal- ity, structure, prediction. Presented at the (1982).</p>
</div>
<div id="ref-wold1983">
<p>66. Wold S, W.H., Martens H: The multivariate calibration problem in chemistry solved by the pls method, (1983).</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p><a href="http://cos.name/2016/03/asa-statement-on-p-value/" class="uri">http://cos.name/2016/03/asa-statement-on-p-value/</a><a href="section-10.html#fnref1">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="section-9.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="section-11.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/10-xianxinghuigui.Rmd",
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
