<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>数据科学家：R语言</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="This is my first book on data science">
  <meta name="generator" content="bookdown 0.1 and GitBook 2.6.7">

  <meta property="og:title" content="数据科学家：R语言" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is my first book on data science" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="数据科学家：R语言" />
  
  <meta name="twitter:description" content="This is my first book on data science" />
  

<meta name="author" content="林荟">

<meta name="date" content="2016-12-11">

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="section-10.html">
<link rel="next" href="section-12.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">数据科学家：R语言</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> 介绍</a></li>
<li class="chapter" data-level="2" data-path="section-2.html"><a href="section-2.html"><i class="fa fa-check"></i><b>2</b> 数据科学</a><ul>
<li class="chapter" data-level="2.1" data-path="section-2.html"><a href="section-2.html#section-2.1"><i class="fa fa-check"></i><b>2.1</b> 什么是数据科学？</a></li>
<li class="chapter" data-level="2.2" data-path="section-2.html"><a href="section-2.html#section-2.2"><i class="fa fa-check"></i><b>2.2</b> 什么是数据科学家？</a></li>
<li class="chapter" data-level="2.3" data-path="section-2.html"><a href="section-2.html#section-2.3"><i class="fa fa-check"></i><b>2.3</b> 数据科学家需要的技能</a></li>
<li class="chapter" data-level="2.4" data-path="section-2.html"><a href="section-2.html#section-2.4"><i class="fa fa-check"></i><b>2.4</b> 数据科学可以解决什么问题？</a><ul>
<li class="chapter" data-level="2.4.1" data-path="section-2.html"><a href="section-2.html#section-2.4.1"><i class="fa fa-check"></i><b>2.4.1</b> 前提要求</a></li>
<li class="chapter" data-level="2.4.2" data-path="section-2.html"><a href="section-2.html#section-2.4.2"><i class="fa fa-check"></i><b>2.4.2</b> 问题种类</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="section-3.html"><a href="section-3.html"><i class="fa fa-check"></i><b>3</b> 数据集模拟和背景介绍</a><ul>
<li class="chapter" data-level="3.1" data-path="section-3.html"><a href="section-3.html#section-3.1"><i class="fa fa-check"></i><b>3.1</b> 服装消费者数据</a></li>
<li class="chapter" data-level="3.2" data-path="section-3.html"><a href="section-3.html#section-3.2"><i class="fa fa-check"></i><b>3.2</b> 航空公司满意度调查</a></li>
<li class="chapter" data-level="3.3" data-path="section-3.html"><a href="section-3.html#section-3.3"><i class="fa fa-check"></i><b>3.3</b> 生猪疫情风险预测数据</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="section-4.html"><a href="section-4.html"><i class="fa fa-check"></i><b>4</b> 数据分析一般流程</a><ul>
<li class="chapter" data-level="4.1" data-path="section-4.html"><a href="section-4.html#section-4.1"><i class="fa fa-check"></i><b>4.1</b> 问题到数据</a></li>
<li class="chapter" data-level="4.2" data-path="section-4.html"><a href="section-4.html#section-4.2"><i class="fa fa-check"></i><b>4.2</b> 数据到信息</a></li>
<li class="chapter" data-level="4.3" data-path="section-4.html"><a href="section-4.html#section-4.3"><i class="fa fa-check"></i><b>4.3</b> 信息到行动</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="section-5.html"><a href="section-5.html"><i class="fa fa-check"></i><b>5</b> 数据预处理</a><ul>
<li class="chapter" data-level="5.1" data-path="section-5.html"><a href="section-5.html#section-5.1"><i class="fa fa-check"></i><b>5.1</b> 介绍</a></li>
<li class="chapter" data-level="5.2" data-path="section-5.html"><a href="section-5.html#section-5.2"><i class="fa fa-check"></i><b>5.2</b> 数据清理</a></li>
<li class="chapter" data-level="5.3" data-path="section-5.html"><a href="section-5.html#section-5.3"><i class="fa fa-check"></i><b>5.3</b> 缺失值填补</a><ul>
<li class="chapter" data-level="5.3.1" data-path="section-5.html"><a href="section-5.html#section-5.3.1"><i class="fa fa-check"></i><b>5.3.1</b> 中位数或众数填补</a></li>
<li class="chapter" data-level="5.3.2" data-path="section-5.html"><a href="section-5.html#k-"><i class="fa fa-check"></i><b>5.3.2</b> K-近邻填补</a></li>
<li class="chapter" data-level="5.3.3" data-path="section-5.html"><a href="section-5.html#section-5.3.3"><i class="fa fa-check"></i><b>5.3.3</b> 袋状树填补</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="section-5.html"><a href="section-5.html#section-5.4"><i class="fa fa-check"></i><b>5.4</b> 中心化和标量化</a></li>
<li class="chapter" data-level="5.5" data-path="section-5.html"><a href="section-5.html#section-5.5"><i class="fa fa-check"></i><b>5.5</b> 有偏分布</a></li>
<li class="chapter" data-level="5.6" data-path="section-5.html"><a href="section-5.html#section-5.6"><i class="fa fa-check"></i><b>5.6</b> 处理离群点</a></li>
<li class="chapter" data-level="5.7" data-path="section-5.html"><a href="section-5.html#section-5.7"><i class="fa fa-check"></i><b>5.7</b> 共线性</a></li>
<li class="chapter" data-level="5.8" data-path="section-5.html"><a href="section-5.html#section-5.8"><i class="fa fa-check"></i><b>5.8</b> 稀疏变量</a></li>
<li class="chapter" data-level="5.9" data-path="section-5.html"><a href="section-5.html#section-5.9"><i class="fa fa-check"></i><b>5.9</b> 编码名义变量</a></li>
<li class="chapter" data-level="5.10" data-path="section-5.html"><a href="section-5.html#section-5.10"><i class="fa fa-check"></i><b>5.10</b> 本章总结</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="section-6.html"><a href="section-6.html"><i class="fa fa-check"></i><b>6</b> 数据操作</a><ul>
<li class="chapter" data-level="6.1" data-path="section-6.html"><a href="section-6.html#section-6.1"><i class="fa fa-check"></i><b>6.1</b> 数据读写</a><ul>
<li class="chapter" data-level="6.1.1" data-path="section-6.html"><a href="section-6.html#tibble"><i class="fa fa-check"></i><b>6.1.1</b> 取代传统数据框的<code>tibble</code>对象</a></li>
<li class="chapter" data-level="6.1.2" data-path="section-6.html"><a href="section-6.html#readr"><i class="fa fa-check"></i><b>6.1.2</b> 高效数据读写：<code>readr</code>包</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="section-6.html"><a href="section-6.html#section-6.2"><i class="fa fa-check"></i><b>6.2</b> 数据整合</a><ul>
<li class="chapter" data-level="6.2.1" data-path="section-6.html"><a href="section-6.html#baseapply"><i class="fa fa-check"></i><b>6.2.1</b> base包：apply()</a></li>
<li class="chapter" data-level="6.2.2" data-path="section-6.html"><a href="section-6.html#plyrddply"><i class="fa fa-check"></i><b>6.2.2</b> plyr包：ddply()函数</a></li>
<li class="chapter" data-level="6.2.3" data-path="section-6.html"><a href="section-6.html#dplyr"><i class="fa fa-check"></i><b>6.2.3</b> dplyr包</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="section-6.html"><a href="section-6.html#section-6.3"><i class="fa fa-check"></i><b>6.3</b> 数据整形</a><ul>
<li class="chapter" data-level="6.3.1" data-path="section-6.html"><a href="section-6.html#reshape2"><i class="fa fa-check"></i><b>6.3.1</b> <code>reshape2</code>包</a></li>
<li class="chapter" data-level="6.3.2" data-path="section-6.html"><a href="section-6.html#tidyr"><i class="fa fa-check"></i><b>6.3.2</b> <code>tidyr</code>包</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="section-6.html"><a href="section-6.html#-1"><i class="fa fa-check"></i><b>6.4</b> 本章总结</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="section-7.html"><a href="section-7.html"><i class="fa fa-check"></i><b>7</b> 基础建模技术</a><ul>
<li class="chapter" data-level="7.1" data-path="section-7.html"><a href="section-7.html#section-7.1"><i class="fa fa-check"></i><b>7.1</b> 有监督和无监督</a></li>
<li class="chapter" data-level="7.2" data-path="section-7.html"><a href="section-7.html#section-7.2"><i class="fa fa-check"></i><b>7.2</b> 误差及其来源</a><ul>
<li class="chapter" data-level="7.2.1" data-path="section-7.html"><a href="section-7.html#section-7.2.1"><i class="fa fa-check"></i><b>7.2.1</b> 系统误差和随机误差</a></li>
<li class="chapter" data-level="7.2.2" data-path="section-7.html"><a href="section-7.html#section-7.2.2"><i class="fa fa-check"></i><b>7.2.2</b> 应变量误差</a></li>
<li class="chapter" data-level="7.2.3" data-path="section-7.html"><a href="section-7.html#section-7.2.3"><i class="fa fa-check"></i><b>7.2.3</b> 自变量误差</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="section-7.html"><a href="section-7.html#section-7.3"><i class="fa fa-check"></i><b>7.3</b> 数据划分和再抽样</a><ul>
<li class="chapter" data-level="7.3.1" data-path="section-7.html"><a href="section-7.html#section-7.3.1"><i class="fa fa-check"></i><b>7.3.1</b> 划分训练集和测试集</a></li>
<li class="chapter" data-level="7.3.2" data-path="section-7.html"><a href="section-7.html#section-7.3.2"><i class="fa fa-check"></i><b>7.3.2</b> 重抽样</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="section-7.html"><a href="section-7.html#-2"><i class="fa fa-check"></i><b>7.4</b> 本章总结</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="section-8.html"><a href="section-8.html"><i class="fa fa-check"></i><b>8</b> 模型评估度量</a><ul>
<li class="chapter" data-level="8.1" data-path="section-8.html"><a href="section-8.html#section-8.1"><i class="fa fa-check"></i><b>8.1</b> 回归模型评估度量</a></li>
<li class="chapter" data-level="8.2" data-path="section-8.html"><a href="section-8.html#section-8.2"><i class="fa fa-check"></i><b>8.2</b> 分类模型评估度量</a><ul>
<li class="chapter" data-level="8.2.1" data-path="section-8.html"><a href="section-8.html#kappa"><i class="fa fa-check"></i><b>8.2.1</b> Kappa统计量</a></li>
<li class="chapter" data-level="8.2.2" data-path="section-8.html"><a href="section-8.html#roc"><i class="fa fa-check"></i><b>8.2.2</b> ROC曲线</a></li>
<li class="chapter" data-level="8.2.3" data-path="section-8.html"><a href="section-8.html#section-8.2.3"><i class="fa fa-check"></i><b>8.2.3</b> 提升图</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="section-8.html"><a href="section-8.html#-3"><i class="fa fa-check"></i><b>8.3</b> 本章总结</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="section-9.html"><a href="section-9.html"><i class="fa fa-check"></i><b>9</b> 特征工程</a><ul>
<li class="chapter" data-level="9.1" data-path="section-9.html"><a href="section-9.html#section-9.1"><i class="fa fa-check"></i><b>9.1</b> 特征构建</a></li>
<li class="chapter" data-level="9.2" data-path="section-9.html"><a href="section-9.html#section-9.2"><i class="fa fa-check"></i><b>9.2</b> 特征提取</a><ul>
<li class="chapter" data-level="9.2.1" data-path="section-9.html"><a href="section-9.html#section-9.2.1"><i class="fa fa-check"></i><b>9.2.1</b> 初步探索数据</a></li>
<li class="chapter" data-level="9.2.2" data-path="section-9.html"><a href="section-9.html#section-9.2.2"><i class="fa fa-check"></i><b>9.2.2</b> 主成分分析</a></li>
<li class="chapter" data-level="9.2.3" data-path="section-9.html"><a href="section-9.html#section-9.2.3"><i class="fa fa-check"></i><b>9.2.3</b> 探索性因子分析</a></li>
<li class="chapter" data-level="9.2.4" data-path="section-9.html"><a href="section-9.html#section-9.2.4"><i class="fa fa-check"></i><b>9.2.4</b> 高维标度化</a></li>
<li class="chapter" data-level="9.2.5" data-path="section-9.html"><a href="section-9.html#section-9.2.5"><i class="fa fa-check"></i><b>9.2.5</b> 知识扩展</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="section-9.html"><a href="section-9.html#section-9.3"><i class="fa fa-check"></i><b>9.3</b> 特征选择</a><ul>
<li class="chapter" data-level="9.3.1" data-path="section-9.html"><a href="section-9.html#section-9.3.1"><i class="fa fa-check"></i><b>9.3.1</b> 过滤法</a></li>
<li class="chapter" data-level="9.3.2" data-path="section-9.html"><a href="section-9.html#section-9.3.2"><i class="fa fa-check"></i><b>9.3.2</b> 绕封法</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="section-10.html"><a href="section-10.html"><i class="fa fa-check"></i><b>10</b> 线性回归极其衍生</a><ul>
<li class="chapter" data-level="10.1" data-path="section-10.html"><a href="section-10.html#section-10.1"><i class="fa fa-check"></i><b>10.1</b> 普通线性回归</a><ul>
<li class="chapter" data-level="10.1.1" data-path="section-10.html"><a href="section-10.html#section-10.1.1"><i class="fa fa-check"></i><b>10.1.1</b> 最小二乘线性模型</a></li>
<li class="chapter" data-level="10.1.2" data-path="section-10.html"><a href="section-10.html#section-10.1.2"><i class="fa fa-check"></i><b>10.1.2</b> 回归诊断</a></li>
<li class="chapter" data-level="10.1.3" data-path="section-10.html"><a href="section-10.html#section-10.1.3"><i class="fa fa-check"></i><b>10.1.3</b> 离群点，高杠杆点和强影响点</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="section-10.html"><a href="section-10.html#section-10.2"><i class="fa fa-check"></i><b>10.2</b> 收缩方法</a><ul>
<li class="chapter" data-level="10.2.1" data-path="section-10.html"><a href="section-10.html#section-10.2.1"><i class="fa fa-check"></i><b>10.2.1</b> 岭回归</a></li>
<li class="chapter" data-level="10.2.2" data-path="section-10.html"><a href="section-10.html#lasso"><i class="fa fa-check"></i><b>10.2.2</b> Lasso</a></li>
<li class="chapter" data-level="10.2.3" data-path="section-10.html"><a href="section-10.html#section-10.2.3"><i class="fa fa-check"></i><b>10.2.3</b> 弹性网络</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="section-10.html"><a href="section-10.html#lasso"><i class="fa fa-check"></i><b>10.3</b> 知识扩展：Lasso的变量选择功能</a></li>
<li class="chapter" data-level="10.4" data-path="section-10.html"><a href="section-10.html#section-10.4"><i class="fa fa-check"></i><b>10.4</b> 主成分和偏最小二乘回归</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="section-11.html"><a href="section-11.html"><i class="fa fa-check"></i><b>11</b> 广义线性模型压缩方法</a><ul>
<li class="chapter" data-level="11.1" data-path="section-11.html"><a href="section-11.html#glmnet"><i class="fa fa-check"></i><b>11.1</b> 初识<code>glmnet</code></a></li>
<li class="chapter" data-level="11.2" data-path="section-11.html"><a href="section-11.html#section-11.2"><i class="fa fa-check"></i><b>11.2</b> 收缩线性回归</a></li>
<li class="chapter" data-level="11.3" data-path="section-11.html"><a href="section-11.html#section-11.3"><i class="fa fa-check"></i><b>11.3</b> 逻辑回归</a><ul>
<li class="chapter" data-level="11.3.1" data-path="section-11.html"><a href="section-11.html#section-11.3.1"><i class="fa fa-check"></i><b>11.3.1</b> 普通逻辑回归</a></li>
<li class="chapter" data-level="11.3.2" data-path="section-11.html"><a href="section-11.html#section-11.3.2"><i class="fa fa-check"></i><b>11.3.2</b> 收缩逻辑回归</a></li>
<li class="chapter" data-level="11.3.3" data-path="section-10.html"><a href="section-10.html#lasso"><i class="fa fa-check"></i><b>11.3.3</b> 知识扩展：群组lasso逻辑回归</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="section-11.html"><a href="section-11.html#section-11.4"><i class="fa fa-check"></i><b>11.4</b> 收缩多项回归</a></li>
<li class="chapter" data-level="11.5" data-path="section-11.html"><a href="section-11.html#section-11.5"><i class="fa fa-check"></i><b>11.5</b> 泊松收缩回归</a></li>
<li class="chapter" data-level="11.6" data-path="section-11.html"><a href="section-11.html#-4"><i class="fa fa-check"></i><b>11.6</b> 本章总结</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="section-12.html"><a href="section-12.html"><i class="fa fa-check"></i><b>12</b> 树模型</a><ul>
<li class="chapter" data-level="12.1" data-path="section-12.html"><a href="section-12.html#section-12.1"><i class="fa fa-check"></i><b>12.1</b> 分裂准则</a></li>
<li class="chapter" data-level="12.2" data-path="section-12.html"><a href="section-12.html#section-12.2"><i class="fa fa-check"></i><b>12.2</b> 树的修剪</a></li>
<li class="chapter" data-level="12.3" data-path="section-12.html"><a href="section-12.html#section-12.3"><i class="fa fa-check"></i><b>12.3</b> 回归树和决策树</a><ul>
<li class="chapter" data-level="12.3.1" data-path="section-12.html"><a href="section-12.html#section-12.3.1"><i class="fa fa-check"></i><b>12.3.1</b> 回归树</a></li>
<li class="chapter" data-level="12.3.2" data-path="section-12.html"><a href="section-12.html#section-12.3.2"><i class="fa fa-check"></i><b>12.3.2</b> 决策树</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="section-12.html"><a href="section-12.html#section-12.4"><i class="fa fa-check"></i><b>12.4</b> 装袋树</a></li>
<li class="chapter" data-level="12.5" data-path="section-12.html"><a href="section-12.html#section-12.5"><i class="fa fa-check"></i><b>12.5</b> 随机森林</a></li>
<li class="chapter" data-level="12.6" data-path="section-12.html"><a href="section-12.html#section-12.6"><i class="fa fa-check"></i><b>12.6</b> 助推法</a></li>
<li class="chapter" data-level="12.7" data-path="section-12.html"><a href="section-12.html#section-12.7"><i class="fa fa-check"></i><b>12.7</b> 知识扩展：助推法的可加模型框架</a></li>
<li class="chapter" data-level="12.8" data-path="section-12.html"><a href="section-12.html#section-12.8"><i class="fa fa-check"></i><b>12.8</b> 知识扩展：助推树的数学框架</a><ul>
<li class="chapter" data-level="12.8.1" data-path="section-12.html"><a href="section-12.html#section-12.8.1"><i class="fa fa-check"></i><b>12.8.1</b> 数学表达</a></li>
<li class="chapter" data-level="12.8.2" data-path="section-12.html"><a href="section-12.html#section-12.8.2"><i class="fa fa-check"></i><b>12.8.2</b> 梯度助推数值优化</a></li>
</ul></li>
<li class="chapter" data-level="12.9" data-path="section-12.html"><a href="section-12.html#-5"><i class="fa fa-check"></i><b>12.9</b> 本章总结</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="section-13.html"><a href="section-13.html"><i class="fa fa-check"></i><b>13</b> 深度学习</a><ul>
<li class="chapter" data-level="13.1" data-path="section-6.html"><a href="section-6.html#-1"><i class="fa fa-check"></i><b>13.1</b> 介绍</a></li>
<li class="chapter" data-level="13.2" data-path="section-13.html"><a href="section-13.html#r"><i class="fa fa-check"></i><b>13.2</b> R中深度学习包</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>14</b> References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">数据科学家：R语言</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="section-11" class="section level1">
<h1><span class="header-section-number">第11章</span> 广义线性模型压缩方法</h1>
<p>之前只是对线性回归使用罚函数。不难理解，这样的罚函数可以用于很多其它回回归函数的优化上，比如逻辑回归，泊松回归等。<code>glmnet</code>包能够通过<strong>罚极大似然函数</strong>拟合广义线性回归，也就是在似然函数上加上罚函数，和之间在RSS上加罚函数类似。之前的线性回归的情况是广义线性回归的一个特例。和之前一样，罚函数的选择可以是一阶范数和二阶范数的一个组合。<code>glmnet</code>包可以对一系列调优参数值同时计算参数估计。除了线性回归外，该包可以拟合的广义线性模型还有：逻辑回归、多项式回归，泊松回归，cox回归。<code>glmnet</code>包的作者是Jerome Friedman、Trevor Hastie、Rob Tibshirani和Noah Simon，当前的R包由Trevor Hastie维护。该包还有一个matlab版本。</p>
<p>广义线性模型压缩方法可以表达成优化下面方程：</p>
<p><span class="math display">\[\underset{\beta_{0},\mathbf{\beta}}{min}\frac{1}{N}\Sigma_{i=1}^{N}w_{i}l(y_{i},\beta_{0}+\mathbf{\beta^{T}x_{i}})+\lambda[(1-\alpha)\parallel\mathbf{\beta}\parallel_{2}^{2}/2+\alpha\parallel\mathbf{\beta}\parallel_{1}]\]</span></p>
<p>其中需要对一定范围内的<span class="math inline">\(\lambda\)</span>值进行调优。其中：</p>
<p><span class="math display">\[l(y_{i},\beta_{0}+\mathbf{\beta^{T}x_{i}})=-log[\mathcal{L}(y_{i},\beta_{0}+\mathbf{\beta^{T}x_{i}})]\]</span></p>
<p>也就是似然函数<span class="math inline">\(\mathcal{L}(y_{i},\beta_{0}+\mathbf{\beta^{T}x_{i}})\)</span>取对数后再加负号，最大化似然函数即等价于最小化<span class="math inline">\(l(y_{i},\beta_{0}+\mathbf{\beta^{T}x_{i}})\)</span>。参数<span class="math inline">\(\alpha\)</span>控制了弹性网络罚函数，即在岭回归（<span class="math inline">\(\alpha=0\)</span>）和lasso（<span class="math inline">\(\alpha=1\)</span>）之间权衡。<span class="math inline">\(\lambda\)</span>控制了罚函数的总体权重，其值越大，罚函数相对于似然函数的权重越高。</p>
<p>之前我们已经讲过，岭回归的罚函数能够将参数估计向0收缩，但是不能收缩为0。而lasso的罚函数能够将参数严格收缩为0，因而具有变量选择功能。弹性网络的罚函数结合了这两者。这里的<span class="math inline">\(\alpha\)</span>也是需要估计的参数。<code>glmnet</code>包使用的是循环坐标下降法（cyclical coordinate descent），这是一种非梯度优化算法。算法每次针对一个参数优化目标方程，固定所有其它参数，然后转向另外一个参数，如此循环直到收敛。</p>
<div id="glmnet" class="section level2">
<h2><span class="header-section-number">11.1</span> 初识<code>glmnet</code></h2>
<p>在介绍具体不同的广义线性模型压缩方法之前，先让大家熟悉一下这个R包的基本使用方式。我会简单的介绍下其中的主要函数，功能，和输出。这样大家对这个包能做什么有个大致的概念。后面的小节会分别介绍不同模型。</p>
<p>默认设置下的模型是高斯线性回归或者最小二乘模型，也就是之前几个小节介绍的模型，只是参数化的方式略有不同，但都是RSS加上一个罚函数。所以我们还是从之前服装消费者数据集中的自变量和应变量开始：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(glmnet)
dat &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;/Users/happyrabbit/Documents/GitHub/DataScientistR/Data/SegData.csv&quot;</span>)
<span class="co"># 对数据进行一些清理，删除错误的样本观测，消费金额不能为负数</span>
dat &lt;-<span class="st"> </span><span class="kw">subset</span>(dat, store_exp &gt;<span class="st"> </span><span class="dv">0</span> &amp;<span class="st"> </span>online_exp &gt;<span class="st"> </span><span class="dv">0</span>)
<span class="co"># 将10个问卷调查变量当作自变量</span>
trainx &lt;-<span class="st"> </span>dat[, <span class="kw">grep</span>(<span class="st">&quot;Q&quot;</span>, <span class="kw">names</span>(dat))]
<span class="co"># 将实体店消费量和在线消费之和当作应变量</span>
<span class="co"># 得到总消费量=实体店消费+在线消费</span>
trainy &lt;-<span class="st"> </span>dat$store_exp +<span class="st"> </span>dat$online_exp</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">glmfit =<span class="st"> </span><span class="kw">glmnet</span>(<span class="kw">as.matrix</span>(trainx), trainy)</code></pre></div>
<p>这里函数<code>glmnet()</code>返回的对象<code>glmfit</code>中含有所有之后可能进一步会用到的模型拟合信息。大家并不需要手动的检查<code>glmfit</code>中都有那些信息，然后提取相应的部分，而是可以通过<code>plot()</code>、<code>coef()</code>、<code>predict()</code>这类耳熟能详的函数来得到相应的信息。比如我们可以用如下方式绘制lasso的参数选择路径图：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(glmfit, <span class="dt">label =</span> T)</code></pre></div>
<p><img src="DS_R_files/figure-html/unnamed-chunk-206-1.png" width="672" /></p>
<p>图中每种颜色的线代表对应一个自变量，展示的是随着lasso罚函数（也就是一阶范数，有时也称为<span class="math inline">\(l_{1}-norm\)</span>）对应调优参数<span class="math inline">\(\lambda\)</span>变化，各个变量对应的参数估计路径（注：当<span class="math inline">\(\alpha=1\)</span>时，优化方程里就只有lasso罚函数，这是默认设置）。图中有上下两个x轴标度，下x轴是<span class="math inline">\(\lambda\)</span>变化对应最优解的一阶范数值（也就是<span class="math inline">\(\parallel\mathbf{\beta}\parallel_{1}\)</span>），上x轴是相应<span class="math inline">\(\lambda\)</span>值对应的非0参数估计个数，也就是lasso模型的自由度。我们可以查看路径的具体每一步信息：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">print</span>(glmfit)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode html"><code class="sourceCode html">Call:  glmnet(x = as.matrix(trainx), y = trainy) 

      Df   %Dev   Lambda
 [1,]  0 0.0000 3042.000
 [2,]  2 0.1038 2771.000
 [3,]  2 0.1919 2525.000
 [4,]  2 0.2650 2301.000
 [5,]  3 0.3264 2096.000
 [6,]  3 0.3894 1910.000
 [7,]  3 0.4417 1741.000
 [8,]  3 0.4852 1586.000
 [9,]  3 0.5212 1445.000
[10,]  3 0.5512 1317.000
[11,]  3 0.5760 1200.000
[12,]  3 0.5967 1093.000
[13,]  3 0.6138  996.000
[14,]  3 0.6280  907.500
...</code></pre></div>
<p>这里第一列<code>Df</code>表示非零估计的参数个数，<code>%Dev</code>解释的方差百分比，以及<code>Lambda</code>调优参数<span class="math inline">\(\lambda\)</span>的取值。虽然在默认设置下，glmnet会尝试100个不同的<span class="math inline">\(\lambda\)</span>取值，但如果随着<span class="math inline">\(\lambda\)</span>的减小，<code>%Dev</code>百分比只发生微小变化的时候，算法也会提前停止，上面的例子算法就只计算了68个不同的调优参数取值。我们也可以通过指定一个<span class="math inline">\(\lambda\)</span>的取值来得到对应的参数估计，其中<code>s=</code>用来指定调优参数值：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">coef</span>(glmfit, <span class="dt">s =</span> <span class="dv">1200</span>)</code></pre></div>
<pre><code>## 11 x 1 sparse Matrix of class &quot;dgCMatrix&quot;
##                     1
## (Intercept) 2255.2221
## Q1          -390.9214
## Q2           653.6437
## Q3           624.4068
## Q4             .     
## Q5             .     
## Q6             .     
## Q7             .     
## Q8             .     
## Q9             .     
## Q10            .</code></pre>
<p>在<span class="math inline">\(\lambda=1200\)</span>时，只有3个变量（<code>Q1</code>、<code>Q2</code>和<code>Q3</code>）的参数估计非0。你也可以用新数据对一个或多个<span class="math inline">\(\lambda\)</span>值进行预测。我们随机抽取3个观测作为新数据，然后用<code>predict()</code>函数得到针对多个<span class="math inline">\(\lambda\)</span>值的预测：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">newdat =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">sample</span>(<span class="dv">1</span>:<span class="dv">9</span>, <span class="dv">30</span>, <span class="dt">replace =</span> T), <span class="dt">nrow =</span> <span class="dv">3</span>)
<span class="kw">predict</span>(glmfit, newdat, <span class="dt">s =</span> <span class="kw">c</span>(<span class="dv">1741</span>, <span class="dv">2000</span>))</code></pre></div>
<pre><code>##             1        2
## [1,] 3337.144 3394.986
## [2,] 6559.382 6186.969
## [3,] 6831.266 6765.357</code></pre>
<p>结果中每列分别对应一个<span class="math inline">\(\lambda\)</span>取值的预测。这里需要通过交互校验进行参数（<span class="math inline">\(\lambda\)</span>）调优。<code>glmnet</code>包中的<code>cv.glmnet()</code>可以实现这一目标。</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">cvfit =<span class="st"> </span><span class="kw">cv.glmnet</span>(<span class="kw">as.matrix</span>(trainx), trainy)</code></pre></div>
<p><code>cv.glmnet()</code>会返回一个列表，其中包括交互校验过程的结果，我们将该结果存在<code>cvfit</code>这个对象里。 我们可以对交互校验结果可视化：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(cvfit)</code></pre></div>
<p><img src="DS_R_files/figure-html/unnamed-chunk-210-1.png" width="672" /></p>
<p>红色的点是不同<span class="math inline">\(\lambda\)</span>取值对应的交互校验均方误差，灰色的线是相应置信区间。两条虚线表示选中的两个调优参数。左边的那个调优参数值对应的是最小的交互校验均方误差，右边的那个调优参数值是离最小均方误差一个标准差的调优参数值。我们可以通过下面代码查看根据两种不同规则选中的调优参数值：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># 最小均方误差对应的参数值</span>
cvfit$lambda.min</code></pre></div>
<pre><code>## [1] 7.893144</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># 一个标准差原则下对应的参数值</span>
cvfit$lambda.1se</code></pre></div>
<pre><code>## [1] 1199.688</code></pre>
<p>我们也可以按如下方式查看不同调优参数值对应的回归参数（注意这里不是调优参数估计）估计：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># 一个标准差原则下对应的回归参数估计</span>
<span class="kw">coef</span>(cvfit, <span class="dt">s =</span> <span class="st">&quot;lambda.1se&quot;</span>)</code></pre></div>
<pre><code>## 11 x 1 sparse Matrix of class &quot;dgCMatrix&quot;
##                     1
## (Intercept) 2255.3136
## Q1          -391.0562
## Q2           653.7079
## Q3           624.5119
## Q4             .     
## Q5             .     
## Q6             .     
## Q7             .     
## Q8             .     
## Q9             .     
## Q10            .</code></pre>
</div>
<div id="section-11.2" class="section level2">
<h2><span class="header-section-number">11.2</span> 收缩线性回归</h2>
<p>普通线性回归是广义线性回归框架下的一种特殊情况。这里将要介绍的是之前章节中线性回归的收缩方法的另外一种实现方式。之后我们会介绍广义框架下更多模型的收缩方法：逻辑回归，多项回归和泊松回归。</p>
<p>线性回归有两种，一种是我们已经介绍过的属于高斯（<code>gaussian</code>）家族的模型，其中应变量是一个向量。另外一种是多元高斯（<code>multivariate gaussian</code>），也就是多元响应变量的情况，这时应变量是一个矩阵，参数也是矩阵。我们着重介绍用<code>glmnet</code>包实现普通高斯收缩回归。</p>
<p>假定自变量观测<span class="math inline">\(\mathbf{x_{i}}\in \mathbb{R}^{p}\)</span>，应变量<span class="math inline">\(y_{i} \in \mathbb{R},\ i=1,\dots,n\)</span>。这里的收缩线性回归目标是找到能够优化下面方程的参数估计，这和前一章参数化的方式略有不同，但本质是相同的：</p>
<p><span class="math display">\[\underset{(\beta_{0},\mathbf{\beta})\in \mathbb{R}^{p+1}}{min}\frac{1}{2n}\Sigma_{i=1}^{n}(y_{i}-\beta_{0}-\mathbf{x_{i}^{T} \beta)}^2+\lambda [(1-\alpha)]\Vert\beta\Vert_2^2/2+\alpha\Vert\beta\Vert_1\]</span> 其中<span class="math inline">\(\lambda&gt;0\)</span>是总体的复杂度参数，<span class="math inline">\(0\leq\alpha\leq1\)</span>是权衡lasso（<span class="math inline">\(\alpha=1\)</span>）和ridge（<span class="math inline">\(\alpha=0\)</span>）罚函数的参数。<code>glmnet</code>提供了定义各种参数设置的选项。下面是一些通常需要用到的参数设置：</p>
<ul>
<li><p><code>alpha</code>：上面优化函数中的<span class="math inline">\(\alpha\)</span>，默认设置是<span class="math inline">\(\alpha=1\)</span>，也就是lasso回归，你可以将其设置为0进行岭回归。<span class="math inline">\(\alpha\in[0,1]\)</span>。</p></li>
<li><p><code>weights</code>： 每个观测的权重，默认设置下每个观测的权重都是1，权重总和就是参数个数n。你也可以自定义每个观测的权重，但是<code>glmnet</code>包会自动将你设置的权重标准化，使得权重之和总是n。</p></li>
<li><p><code>nlambda</code>：调优参数<span class="math inline">\(\lambda\)</span>的取值个数，默认设置是100。函数会自行生成一个含有<code>nlambda</code>个<span class="math inline">\(\lambda\)</span>取值的向量进行调优。这些值的选取基于两个量：<code>lambda.max</code>和<code>lambda.min.ratio</code>。前者是最大的lambda值，在<span class="math inline">\(\alpha\)</span>不为0的情况下，一阶范数罚<span class="math inline">\(\Vert\beta\Vert_1\)</span>使得存在一个<span class="math inline">\(\lambda\)</span>取值时所有的参数估计都收缩为0，也就是模型中只有截距项。这个取值就是<code>lambda.max</code>。当<span class="math inline">\(\alpha＝0\)</span>的时候，<code>lambda.max</code>将是无穷大，因此在这种情况下，函数会自动选择一个很小的<span class="math inline">\(\alpha\)</span>值用来计算<code>lambda.max</code>。具体背后的数学原理，可以参考上一章的“知识扩展：Lasso的变量选择功能”小节。<code>lambda.min.ratio</code>是向量中最小的<span class="math inline">\(\lambda\)</span>取值与最大<span class="math inline">\(\lambda\)</span>取值的比例。如果<code>lambda.min.ratio＝0</code>，表明调优参数<span class="math inline">\(\lambda\)</span>的取值向量分布从0到<code>lambda.max</code>。</p></li>
<li><p><code>lambda</code>：如果不用设定<code>nlambda</code>的方式，你也可以通过设定<code>lambda</code>这个参数自己定义调优参数值向量。</p></li>
<li><p><code>standardize</code>：用来告诉函数是否标准化自变量的逻辑值。默认设置为<code>standardize=TRUE</code>。</p></li>
</ul>
<p>作为例子，这里设置<code>alpha=0.2</code>，<code>nlamdba=10</code>。在实际应用中，通常会尝试100-150个不同的<span class="math inline">\(\lambda\)</span>取值，这里为了避免过多的输出，只设置20个值。</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># 这里要用as.matrix(xtrain)将自变量输入转化成矩阵</span>
fit =<span class="st"> </span><span class="kw">glmnet</span>(<span class="kw">as.matrix</span>(trainx), trainy, <span class="dt">alpha =</span> <span class="fl">0.2</span>, <span class="dt">nlambda =</span> <span class="dv">20</span>)
<span class="co"># 这里digits=2限制了输出中的小数位数</span>
<span class="kw">print</span>(fit, <span class="dt">digits =</span> <span class="dv">4</span>)</code></pre></div>
<pre><code>## 
## Call:  glmnet(x = as.matrix(trainx), y = trainy, alpha = 0.2, nlambda = 20) 
## 
##       Df   %Dev    Lambda
##  [1,]  0 0.0000 15210.000
##  [2,]  4 0.2502  9366.000
##  [3,]  6 0.4590  5768.000
##  [4,]  7 0.5848  3552.000
##  [5,]  9 0.6502  2188.000
##  [6,]  9 0.6823  1347.000
##  [7,]  9 0.6967   829.700
##  [8,]  9 0.7033   511.000
##  [9,]  9 0.7064   314.700
## [10,]  9 0.7080   193.800
## [11,]  9 0.7088   119.300
## [12,]  9 0.7093    73.500
## [13,]  9 0.7095    45.270
## [14,]  9 0.7096    27.880
## [15,]  9 0.7096    17.170
## [16,]  9 0.7096    10.570
## [17,] 10 0.7096     6.511
## [18,] 10 0.7096     4.010</code></pre>
<p>关于输出中各列代表什么，参考之前的解释。大家可能会发现，之前设置了20个调优参数值，这里只输出了18个。其原因在于算法中设置了停止条件。根据默认设置，在下面两种情况下计算会停止：</p>
<ol style="list-style-type: decimal">
<li>解释的方差百分比（<code>%Dev</code>）的变化小于<span class="math inline">\(10^{-5}\)</span>时</li>
<li>解释的方差百分比本身大于<span class="math inline">\(0.999\)</span></li>
</ol>
<p>这里停止是因为遇到了第一种情况，<span class="math inline">\(\lambda\)</span>的取值变化几乎不会对解释的方差百分比造成影响。更多关于算法的控制条件，可以键入<code>help(&quot;glmnet.control&quot;)</code>。</p>
<p>我们可以用<code>plot()</code>绘制拟合过程的参数估计路径图。图的x坐标轴有3个设置：</p>
<ol style="list-style-type: decimal">
<li><code>norm</code>：参数估计的一阶范数，这是默认设置</li>
<li><code>lambda</code>：<span class="math inline">\(log(\lambda)\)</span>值</li>
<li><code>dev</code>：解释方差的百分比，即之前结果输出中的<code>%Dev</code></li>
</ol>
<p>我们之前展示了默认设置下的参数估计路径图（<code>norm</code>）。现在绘制另外两种情况。设置<code>xvar = &quot;lambda&quot;</code>可以得到对应<span class="math inline">\(log(\lambda)\)</span>的路径图：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(fit, <span class="dt">xvar =</span> <span class="st">&quot;lambda&quot;</span>, <span class="dt">label =</span> T)</code></pre></div>
<p><img src="DS_R_files/figure-html/unnamed-chunk-214-1.png" width="672" /></p>
<p>可以看到，随着<span class="math inline">\(\lambda\)</span>值的增大，参数逐步向0收缩。<code>Q1</code>、<code>Q2</code>、<code>Q3</code>和<code>Q8</code>的参数估计都差不多在最后才收缩为0。说明这几个变量对解释应变量最重要。下面我们看看对于解释方差的百分比的路径图：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(fit, <span class="dt">xvar =</span> <span class="st">&quot;dev&quot;</span>, <span class="dt">label =</span> T)</code></pre></div>
<p><img src="DS_R_files/figure-html/unnamed-chunk-215-1.png" width="672" /></p>
<p>横坐标为解释方差的百分比时的路径图和之前的不太一样。非0参数个数越多，解释的方差百分比就越大。到最右端的时候，解释方差变化很小，但是参数估计确急剧增大。由图中我们可以看出，<code>Q7</code>对模型拟合几乎起不了作用。观察这些图能够帮助我们将注意力放在一些重要的变量上。</p>
<p>如果确定了调优参数<span class="math inline">\(\lambda\)</span>的取值，我们便可以进一步得到相应的参数估计并预测新数据。在之前“初识<code>glmnet</code>”那一小节中，我们展示了在默认设置（<code>alpha = 1</code>，即只有lasso罚函数的情况下）下如何得到调优参数<span class="math inline">\(\lambda\)</span>的某个取值下的拟合结果。这里和之前的情况稍有不同，因为我们设置<code>alpha=0.2</code>，也就是说现在lasso和岭回归的罚函数同时存在。我们可以在<code>fit=glmnet(as.matrix(trainx),trainy,alpha=0.2,nlambda=20)</code>结果的基础上得到相应<span class="math inline">\(\lambda\)</span>取值的调优参数。从输出结果看，若考虑解释方差的变化情况，<span class="math inline">\(\lambda \in [829.7, 1347]\)</span>之间比较妥当。这里我们选择<span class="math inline">\(\lambda ＝ 1000\)</span>，这里为了展示一些函数的用法，特意选择了不在调优过的<span class="math inline">\(\lambda\)</span>取值中的数，也可以通过下面这行简短的代码查看你想要的取值是不是已经调优拟合过：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">any</span>(fit$lambda ==<span class="st"> </span><span class="dv">1000</span>)</code></pre></div>
<pre><code>## [1] FALSE</code></pre>
<p>可以看到返回的是<code>FALSE</code>，即<span class="math inline">\(\lambda ＝ 1000\)</span>这个取值不在调优取值内。于是有两种方法可以得到相应的参数估计。第一种是重新用<span class="math inline">\(\lambda ＝ 1000\)</span>更新拟合模型，这时我们想要得到的是确切的拟合值，可以设置<code>exact = T</code>，再次强调注意在函数中<span class="math inline">\(\lambda\)</span>对应的设置是<code>s</code>：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">coef.exact =<span class="st"> </span><span class="kw">coef</span>(fit, <span class="dt">s =</span> <span class="dv">1000</span>, <span class="dt">exact =</span> T)</code></pre></div>
<p>你也可以不要再次拟合模型，那么我们可以设置<code>exact = F</code>（这是函数的默认设置），这样函数会用插值法得到相应的近似结果：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">coef.apprx =<span class="st"> </span><span class="kw">coef</span>(fit, <span class="dt">s =</span> <span class="dv">1000</span>, <span class="dt">exact =</span> F)</code></pre></div>
<p>我们可以看看这两种情况下参数估计有何不同：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">cbind2</span>(coef.exact, coef.apprx)</code></pre></div>
<pre><code>## 11 x 2 sparse Matrix of class &quot;dgCMatrix&quot;
##                      1          1
## (Intercept) 1208.75790 1177.37262
## Q1          -485.44857 -483.29175
## Q2           852.41149  850.50376
## Q3           645.89607  645.89528
## Q4            59.16109   60.82823
## Q5           161.48707  163.98503
## Q6           188.02162  192.23258
## Q7             .          .      
## Q8          -196.29622 -193.76827
## Q9           196.35592  198.42621
## Q10         -129.38656 -132.20608</code></pre>
<p>输出的左边那列是确切拟合，右边的是插值近似，可以看到它们非常相近，所以通常情况下直接用线性插值就可以，不一定要重新拟合模型。同样我们可以在新的数据集上预测结果：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># 和之前一样，我们抽取一个小样本最为新自变量观测</span>
newdat =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">sample</span>(<span class="dv">1</span>:<span class="dv">9</span>, <span class="dv">30</span>, <span class="dt">replace =</span> T), <span class="dt">nrow =</span> <span class="dv">3</span>)
<span class="kw">predict</span>(fit, newdat, <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>, <span class="dt">s =</span> <span class="dv">1000</span>)</code></pre></div>
<pre><code>##             1
## [1,] 6664.514
## [2,] 9799.426
## [3,] 2430.454</code></pre>
<p>这里的<code>type</code>选项设置有3种，上面的<code>type = &quot;response&quot;</code>也就是直接得到预测的应变量估计。如果<code>type = &quot;coefficients&quot;</code>，等价与之前的<code>coef.apprx = coef(fit, s = 1000, exact = F)</code>，得到的是线性插值参数估计：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">predict</span>(fit, newdat, <span class="dt">type =</span> <span class="st">&quot;coefficients&quot;</span>, <span class="dt">s =</span> <span class="dv">1000</span>)</code></pre></div>
<pre><code>## 11 x 1 sparse Matrix of class &quot;dgCMatrix&quot;
##                      1
## (Intercept) 1177.37262
## Q1          -483.29175
## Q2           850.50376
## Q3           645.89528
## Q4            60.82823
## Q5           163.98503
## Q6           192.23258
## Q7             .      
## Q8          -193.76827
## Q9           198.42621
## Q10         -132.20608</code></pre>
<p><code>type = &quot;nonzero&quot;</code> 将会返回一个向量告诉你那些自变量的参数估计非0：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">predict</span>(fit, newdat, <span class="dt">type =</span> <span class="st">&quot;nonzero&quot;</span>, <span class="dt">s =</span> <span class="dv">1000</span>)</code></pre></div>
<pre><code>##   X1
## 1  1
## 2  2
## 3  3
## 4  4
## 5  5
## 6  6
## 7  8
## 8  9
## 9 10</code></pre>
<p>此外，你也可以自定义k层交互校验的层数。<code>glmnet()</code>中的参数在<code>cv.glmnet()</code>中都有，后者还多了参数<code>nfolds</code>用来定义层级的数目，<code>foldid</code>让用户自定义层级，以及<code>type.measure</code>评估标准：方差（<code>deviance</code>）或者绝对误差均值（<code>mae</code>）。不同类型的的模型有不同的评估标准。比如<code>type.measure=&quot;class&quot;</code>仅仅适用于二项回归和多项逻辑回归，其使用的是误判率。<code>type.measure=&quot;auc&quot;</code>只针对二分类逻辑回归，使用的是ROC线下面积。刚才提到的绝对误差均值<code>type.measure=&quot;mae&quot;</code>可以用与除了cox模型之外的其它所有模型。我们会在之后介绍其它模型。对于高斯族类的模型，默认设置是<code>type.measure=&quot;deviance&quot;</code>。这里我们修改下设置，用绝对误差均值：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">cvfit =<span class="st"> </span><span class="kw">cv.glmnet</span>(<span class="kw">as.matrix</span>(trainx), trainy, <span class="dt">type.measure =</span> <span class="st">&quot;mae&quot;</span>, <span class="dt">nfolds =</span> <span class="dv">20</span>)</code></pre></div>
<p>这里顺便提一下，<code>cv.glmnet()</code>函数能够支持并行计算，这用到<code>doMC</code>包。</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># 抽取一个大样本来展示并行计算对效率的提高</span>
X =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rnorm</span>(<span class="fl">1e5</span>*<span class="dv">200</span>), <span class="fl">1e5</span>, <span class="dv">200</span>)
Y =<span class="st"> </span><span class="kw">rnorm</span>(<span class="fl">1e5</span>)
<span class="co"># 不用并行计算</span>
<span class="kw">system.time</span>(<span class="kw">cv.glmnet</span>(X, Y))</code></pre></div>
<div class="sourceCode"><pre class="sourceCode html"><code class="sourceCode html">   user  system elapsed 
 26.476   1.423  27.918 </code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(doMC)
<span class="co"># 我的电脑是4核的，所以设置cores = 4</span>
<span class="kw">registerDoMC</span>(<span class="dt">cores =</span> <span class="dv">4</span>)
<span class="co"># 用并行计算</span>
<span class="kw">system.time</span>(<span class="kw">cv.glmnet</span>(X, Y, <span class="dt">parallel =</span> T))</code></pre></div>
<div class="sourceCode"><pre class="sourceCode html"><code class="sourceCode html">   user  system elapsed 
 15.574   1.047  12.603 </code></pre></div>
<p>大家可以看到用并行计算在数据量大的时候能够节省好多时间。函数<code>coef()</code>和<code>predict()</code>用于<code>cv.glmnet</code>对象的方式和<code>glmnet</code>相似，只是前者多了两个关于<span class="math inline">\(\lambda\)</span>参数值<code>s</code>的字符串设置：<code>lambda.min</code>（对应最小均方误差的<span class="math inline">\(\lambda\)</span>值）和<code>lambda.1se</code>（对应最小均方误差1个标准差的<span class="math inline">\(\lambda\)</span>值）：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># 这里不展示输出结果</span>
cvfit =<span class="st"> </span><span class="kw">cv.glmnet</span>(<span class="kw">as.matrix</span>(trainx), trainy, <span class="dt">type.measure =</span> <span class="st">&quot;mse&quot;</span>, <span class="dt">nfolds =</span> <span class="dv">20</span>)
<span class="co"># 最小均方误差对应的参数值</span>
cvfit$lambda.min
<span class="co"># 预测新样本</span>
<span class="kw">predict</span>(cvfit, <span class="dt">newx =</span> newdat, <span class="dt">s=</span> <span class="st">&quot;lambda.min&quot;</span>)
<span class="co"># 得到参数估计</span>
<span class="kw">coef</span>(cvfit, <span class="dt">s =</span> <span class="st">&quot;lambda.min&quot;</span>)</code></pre></div>
<p>如果你不仅想要选择<span class="math inline">\(\lambda\)</span>，还要尝试不同的<span class="math inline">\(\alpha\)</span>的参数值，可以自己指定交互校验的样本分层情况。</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># 自定义层级</span>
foldid =<span class="st"> </span><span class="kw">sample</span>(<span class="dv">1</span>:<span class="dv">10</span>, <span class="dt">size =</span> <span class="kw">length</span>(trainy), <span class="dt">replace =</span> T)
<span class="co"># 尝试3个不同的alpha取值：1、0.5、0.2和0</span>
cv1 =<span class="st"> </span><span class="kw">cv.glmnet</span>(<span class="kw">as.matrix</span>(trainx), trainy, <span class="dt">foldid =</span> foldid, <span class="dt">alpha =</span> <span class="dv">1</span>)
cv<span class="fl">.2</span> =<span class="st"> </span><span class="kw">cv.glmnet</span>(<span class="kw">as.matrix</span>(trainx), trainy, <span class="dt">foldid =</span> foldid, <span class="dt">alpha =</span> .<span class="dv">2</span>)
cv<span class="fl">.5</span> =<span class="st"> </span><span class="kw">cv.glmnet</span>(<span class="kw">as.matrix</span>(trainx), trainy, <span class="dt">foldid =</span> foldid, <span class="dt">alpha =</span> .<span class="dv">5</span>)
cv0 =<span class="st"> </span><span class="kw">cv.glmnet</span>(<span class="kw">as.matrix</span>(trainx), trainy, <span class="dt">foldid =</span> foldid, <span class="dt">alpha =</span> <span class="dv">0</span>)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(<span class="kw">log</span>(cv1$lambda), cv1$cvm, <span class="dt">pch =</span> <span class="dv">19</span>, <span class="dt">col =</span> <span class="dv">2</span>, <span class="dt">xlab =</span> <span class="st">&quot;log(Lambda)&quot;</span>, <span class="dt">ylab =</span> cv1$name)
<span class="kw">points</span>(<span class="kw">log</span>(cv<span class="fl">.5</span>$lambda), cv<span class="fl">.5</span>$cvm, <span class="dt">col =</span> <span class="dv">1</span>)
<span class="kw">points</span>(<span class="kw">log</span>(cv<span class="fl">.2</span>$lambda), cv<span class="fl">.2</span>$cvm, <span class="dt">col =</span> <span class="dv">3</span>)
<span class="kw">points</span>(<span class="kw">log</span>(cv0$lambda), cv0$cvm, <span class="dt">col =</span> <span class="dv">4</span>)
<span class="kw">legend</span>(<span class="st">&quot;topleft&quot;</span>, <span class="dt">legend =</span> <span class="kw">c</span>(<span class="st">&quot;alpha = 1&quot;</span>, <span class="st">&quot;alpha = 0.5&quot;</span>, <span class="st">&quot;alpha = 0.2&quot;</span>, <span class="st">&quot;alpha = 0&quot;</span>), 
       <span class="dt">pch =</span> <span class="dv">19</span>, <span class="dt">col =</span> <span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">4</span>))</code></pre></div>
<p><img src="DS_R_files/figure-html/unnamed-chunk-225-1.png" width="672" /></p>
<p>可以看到，不同 <code>alpha</code>的取值对应的最优均方误差几乎相同，只是在<span class="math inline">\(\lambda\)</span>值大的时候（图的右端），<span class="math inline">\(\alpha\)</span>值的变化会对模型拟合结果产生影响。</p>
<p>在实际应用中，基于问题的背景需要对参数进行一些限制。回顾下面三个问卷调查的问题（小伙伴们可以到“数据集模拟和背景介绍”那一章中查看所有问卷调查的问题）：</p>
<p>-（<code>Q3</code>）：品牌的知名度对我来说非常重要 -（<code>Q8</code>）：价格对我来说很重要</p>
<p>其中问题<code>Q3</code>和消费金额应该是正相关的，对品牌知名度在意的人，很可能是土豪或者国民老公一类的，反正相对不差钱，衣着对于这部分群体而言满足的需求已经从马斯洛的需求金字塔底端升级到顶端。而<code>Q8</code>的回复和消费金额应该是负相关的，对价格越是在意的人，花销很可能越小，屌丝群体，动不动就得卖肾，生活不易，且花且珍惜。好啦，人艰不拆，我们从残酷的现实世界回到美好的数据世界。在这里，我们需要限定参数估计的区间，<code>Q3</code>的参数估计是正数，<code>Q8</code>的参数估计是负数。在<code>glmnet()</code>函数中，我们可以对每个参数设置可能的限制区间，如果不知道区间的，可以用很小或者很大的上下限。这里需要赋予一个长度为10的向量，其中每个元素对应一个变量的上（或下）限值：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># 先得到一个下限向量，设置成负无穷大</span>
<span class="co"># 这时等于没有下限</span>
lower.limits &lt;-<span class="st"> </span><span class="kw">rep</span>(-<span class="ot">Inf</span>, <span class="kw">ncol</span>(trainx))
<span class="co"># 在将需要的下限值加入</span>
<span class="co"># Q3的估计是正数，所以下限为0</span>
lower.limits[<span class="dv">3</span>] &lt;-<span class="st"> </span><span class="dv">0</span>
<span class="co"># 类似的设置上限向量</span>
upper.limits &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="ot">Inf</span>, <span class="kw">ncol</span>(trainx))
upper.limits[<span class="dv">8</span>] &lt;-<span class="st"> </span><span class="dv">0</span>
boundfit =<span class="st"> </span><span class="kw">glmnet</span>(<span class="kw">as.matrix</span>(trainx), trainy, <span class="dt">lower.limits =</span> lower.limits, <span class="dt">upper.limits =</span> upper.limits)</code></pre></div>
<p>注意，这里下限<code>lower.limits</code>的值必须是负数或者0，上限<code>upper.limits</code>的值必须是正数或者0。</p>
</div>
<div id="section-11.3" class="section level2">
<h2><span class="header-section-number">11.3</span> 逻辑回归</h2>
<p>逻辑回归是非常流行的判别方法，尤其对于二分类问题。我们以生猪疫情数据为例来讲解模型以及实践代码。关于数据的模拟和背景介绍，可以参考之前“数据集模拟和背景介绍”章节中相应小节。研究目标是对农场爆发疫情建模。假设样本量为<span class="math inline">\(n\)</span>，对应<span class="math inline">\(G\)</span>个自变量（也就是农场问卷调查的问题），每个问题对应3个选项（A、B和C），因此每个分类自变量将转化为3个虚拟变量，不知道大家是不是记得，我们在数据模拟的部分讲过选项C作为基准选项，因为某个回复一定是A、B和C中的一种，所以有一列包含重复信息，我们任意选择一个作为基准列从自变量矩阵中删去。假设第<span class="math inline">\(i\)</span>个受访农场（<span class="math inline">\(i=1,\dots,n\)</span>）对应第<span class="math inline">\(g\)</span>个问题（<span class="math inline">\(g=1,\dots,G\)</span>）回复编码后的虚拟变量观测是<span class="math inline">\(\mathbf{x_{ig}}\)</span>（如果第1个农场在第2个问题中选择B，那么对应的观测就是<span class="math inline">\(\mathbf{x_{12}}=(0,1)^{T}\)</span>），这种情况下第<span class="math inline">\(g\)</span>个问题对应的自由度就是2，我们将自由度一般化的记为<span class="math inline">\(df_g\)</span>。第<span class="math inline">\(i\)</span>个农场对应疫情发生实际观测情况就是应变量<span class="math inline">\(y_i \in \{0,1\}\)</span>，其中1代表发生疫情，0代表没有发生疫情。第<span class="math inline">\(i\)</span>个农场疫情发生的概率是<span class="math inline">\(\theta_i\in [0,1]\)</span>。</p>
<div id="section-11.3.1" class="section level3">
<h3><span class="header-section-number">11.3.1</span> 普通逻辑回归</h3>
<p>基于上述数学符号定义，对于普通逻辑回归：</p>
<p><span class="math display">\[y_{i}\sim Bounoulli(\theta_{i})\]</span></p>
<p><span class="math display">\[log\left(\frac{\theta_{i}}{1-\theta_{i}}\right)=\eta_{\mathbf{\beta}}(x_{i})=\beta_{0}+\sum_{g=1}^{G}\mathbf{x_{i,g}}^{T}\mathbf{\mathbf{\beta_{g}}}\]</span></p>
<p>其中 <span class="math inline">\(\beta_{0}\)</span> 是截距项，且<span class="math inline">\(\mathbf{\beta_{g}}\)</span>是第<span class="math inline">\(g\)</span>个问题观测转化成的虚拟变量对应的参数估计。在当前例子中，每个问题有3个选项，所以每个问题对应3-1=2个虚拟变量（其中选项C是基准项，之前已经讲过了），<span class="math inline">\(\mathbf{\beta_{g}}\)</span>就是长度为2的向量。</p>
<!--Construction of risk scoring systems using logistic regression usually consists of two steps: selection among
the $G$ risk factors, and estimation of the selected factor parameters. For model selection, significance has been used 
as a criterion for inclusion and exclusion of risk factors \cite{Emmanuel2005,Kimberly2003,Rhatigan2010}. Some researchers use univariate logistic regression
to screen factors by significance before putting them into a multivariate logistic regression model \cite{Emmanuel2005,Rhatigan2010}, whereas others \cite{Kimberly2003} don't.
-->
<p>一般逻辑回归就是通过最大化下面的极大似然函数的对数估计<span class="math inline">\(\mathbf{\beta}=(\beta_{0}^{T},\mathbf{\beta_{1}}^{T},\mathbf{\beta_{2}}^{T},...,\mathbf{\beta_{G}}^{T})^{T}\)</span>：</p>
<p><span class="math display">\[ 
\begin{eqnarray*}
l(\mathbf{\beta})&amp;=&amp;log[\prod_{i=1}^{n}\theta_{i}^{y_{i}}(1-\theta_{i})^{1-y_{i}}]\\
&amp;=&amp;\sum_{i=1}^{n}\{y_{i}log(\theta_{i})+(1-y_{i})log(1-\theta_{i})\}\\
&amp;=&amp;\sum_{i=1}^{n}\{\ y_{i}\eta_{\mathbf{\beta}}(\mathbf{x_{i}})-log[1+exp(\eta_{\mathbf{\beta}}(\mathbf{x_{i}}))]\ \}.
\end{eqnarray*}
\]</span></p>
<p>在逻辑回归中，当变量个数相对观测较大时，很容易发生完全分离或者准完全分离的现象，这时候没有唯一的极大似然估计，因此参数估计的方差极大。关于逻辑回归这样的对数线性模型参数极大似然估计的存在性，唯一性的讨论可以参考<span class="citation">[<a href="#ref-Wed1976">67</a>]</span>和<span class="citation">[<a href="#ref-albert1984">68</a>]</span>。这种情况时常出现，比如疾病预测数据：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(MASS)
dat &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;/Users/happyrabbit/Documents/GitHub/DataScientistR/Data/sim1_da1.csv&quot;</span>)
fit &lt;-<span class="st"> </span><span class="kw">glm</span>(y~., dat, <span class="dt">family =</span> <span class="st">&quot;binomial&quot;</span>)</code></pre></div>
<pre class="http"><code>glm.fit: algorithm did not convergeglm.fit: fitted probabilities numerically 0 or 1 occurred</code></pre>
<p>大家可以看到，函数报错说无法收敛，这里就是发生了完全分离的情况。这时收缩方法就可以解决这个问题。</p>
</div>
<div id="section-11.3.2" class="section level3">
<h3><span class="header-section-number">11.3.2</span> 收缩逻辑回归</h3>
<p>我们可以类似的在逻辑回归的似然函数后添加罚函数来收缩参数估计：</p>
<p><span class="math display">\[
\underset{\mathbf{\beta}\in \mathbb{R}^{p+1}}{min} -\sum_{i=1}^{n}\{\ y_{i}\eta_{\mathbf{\beta}}(\mathbf{x_{i}})-log[1+exp(\eta_{\mathbf{\beta}}(\mathbf{x_{i}}))]\ \}+\lambda [(1-\alpha) \parallel \mathbf{\beta}\parallel _{2}^{2}/2] + \alpha \parallel \mathbf{\beta}\parallel _{1} ]
\]</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">dat &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;/Users/happyrabbit/Documents/GitHub/DataScientistR/Data/sim1_da1.csv&quot;</span>)
trainx =<span class="st"> </span>dplyr::<span class="kw">select</span>(dat, -y)
trainy =<span class="st"> </span>dat$y
fit &lt;-<span class="st"> </span><span class="kw">glmnet</span>(<span class="kw">as.matrix</span>(trainx), trainy, <span class="dt">family =</span> <span class="st">&quot;binomial&quot;</span>)</code></pre></div>
<p>可以看到，这里没有错误信息。和之前类似，我们可以绘制参数收缩的路径图，提取某个<span class="math inline">\(\lambda\)</span>取值对应的参数估计，并且进行预测。比如我们可以绘制下面的解释方差比例的参数路径图，绘制图形的语法和之前高斯的情况一样：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(fit, <span class="dt">xvar =</span> <span class="st">&quot;dev&quot;</span>)</code></pre></div>
<p><img src="DS_R_files/figure-html/unnamed-chunk-228-1.png" width="672" /></p>
<p>预测函数<code>predict()</code>和之前的高斯族情况有所不同，主要在于<code>type</code>选项。在二项应变量的情况下，函数的<code>type</code>选项有如下几种：</p>
<ul>
<li><code>link</code>：返回链结函数的拟合值</li>
<li><code>response</code>：返回拟合的概率值</li>
<li><code>class</code>：返回预测的类别（0/1）值</li>
<li><code>coefficients</code>：返回相应的参数估计</li>
<li><code>nonzero</code>：返回估计非0的参数指针向量（即告诉你模型选择了哪些参数）</li>
</ul>
<p>函数默认的这些估计针对的是因子型应变量中第二个层级。比如这里的应变量<code>trainy</code>对应的因子层级是：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">levels</span>(<span class="kw">as.factor</span>(trainy))</code></pre></div>
<pre><code>## [1] &quot;0&quot; &quot;1&quot;</code></pre>
<p>这里的预测概率针对的是第二个因子层级，也就是“1”的概率。这里用3个观测行和两个<span class="math inline">\(\lambda\)</span>的取值为例，展示<code>predict()</code>函数的用法：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">newdat =<span class="st"> </span><span class="kw">as.matrix</span>(trainx[<span class="dv">1</span>:<span class="dv">3</span>, ])
<span class="kw">predict</span>(fit, newdat, <span class="dt">type =</span> <span class="st">&quot;link&quot;</span>, <span class="dt">s =</span> <span class="kw">c</span>(<span class="fl">2.833e-02</span>, <span class="fl">3.110e-02</span>))</code></pre></div>
<pre><code>##            1          2
## 1  0.1943472  0.1442796
## 2 -0.9913159 -1.0076600
## 3 -0.5840566 -0.5496074</code></pre>
<p>上面输出中第1列对应的是<span class="math inline">\(\lambda=0.02833\)</span>时3个样本的链结函数预测值。第2列对应的是<span class="math inline">\(\lambda=0.0311\)</span>时的链结函数预测值。类似的，大家可以自己改变<code>type</code>的设置看结果输出。对逻辑回归我们也可以类似的使用<code>cv.glmnet()</code>函数通过交互校验对参数进行调优。参数和高斯的情况基本相同，不同的地方在于<code>type.measure</code>参数的设置。因为这里应变量是分类变量而非连续变量，在之前的“模型评估度量”章节里详细的介绍过应变量为分类和连续时模型评估方法的差异。在分类情况下，模型评估方法的常用设置<code>type.measure</code>有：</p>
<ul>
<li><code>class</code>：计算误判率</li>
<li><code>auc</code>：仅对于二分类的情况，计算ROC曲线下面积</li>
</ul>
<p>例如：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># 老用as.matrix(trainx)确实有些烦人，但是函数要求时矩阵格式。</span>
<span class="co"># 小伙伴可以在一开始选择将trainx直接转化成矩阵格式。</span>
<span class="co"># 这里不这么做的原因是矩阵格式下有的数据框的操作又无法进行</span>
<span class="co"># 且一些数据框的行列信息可能在转化过程中丢失。</span>
<span class="co"># 所以在每次拟合模型的时候临时转化而不更改原数据框</span>
cvfit =<span class="st"> </span><span class="kw">cv.glmnet</span>(<span class="kw">as.matrix</span>(trainx), trainy, <span class="dt">family =</span> <span class="st">&quot;binomial&quot;</span>, <span class="dt">type.measure =</span> <span class="st">&quot;class&quot;</span>)
<span class="kw">plot</span>(cvfit)</code></pre></div>
<p><img src="DS_R_files/figure-html/unnamed-chunk-231-1.png" width="672" /></p>
<p>上面使用的是误判率作为标准，进行10层交互校验。同样你可以得到对应最小误判率的<span class="math inline">\(\lambda\)</span>取值，以及距离最小误判率一个标准差的<span class="math inline">\(\lambda\)</span>取值：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">cvfit$lambda.min</code></pre></div>
<pre><code>## [1] 8.856624e-05</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">cvfit$lambda.1se</code></pre></div>
<pre><code>## [1] 0.001315181</code></pre>
<p>至于获取参数估计以及对新样本进行预测之前已经讲过，在此就不赘述。</p>
</div>
<div id="lasso" class="section level3">
<h3><span class="header-section-number">11.3.3</span> 知识扩展：群组lasso逻辑回归</h3>
<p>下面我进一步讲一个相对较新的方法：群组lasso逻辑回归。该方法最早由Meier等人在2008年提出的<span class="citation">[<a href="#ref-Meier2008">69</a>]</span>。它在普通lasso逻辑回归的基础上加上了变量的分组信息。比如在疾病预测的例子中，每个问题的回复对应两个虚拟变量，来自同一个问卷调查问题的虚拟变量在反映的信息上有相似性，可以归为一组。这样分组在当前应用例子中是必须的。因为增加分组信息之后，模型不仅可以在组内选择变量，而且可以对不同的组进行选择，也就是说属于同一组的变量参数同时为0或者同时不为0。此例子中，不同组代表不同的问题。这里变量选择的目标主要是从问卷中选择重要的对疾病爆发有预测性的问题，而非问题中的具体选项，所以群组lasso逻辑回归对解决这样的问题非常有效。群组逻辑回归最小化下面方程：</p>
<p><span class="math display">\[
S_{\lambda}(\mathbf{\beta})=-l(\mathbf{\beta})+\lambda\sum_{g=1}^{G}s(df_{g})\parallel\mathbf{\beta_{g}}\parallel_{2}
\]</span> 其中<span class="math inline">\(\lambda\)</span>是调优参数，<span class="math inline">\(s(\cdot)\)</span>调整罚函数大小的系数，<span class="citation">[<a href="#ref-Meier2008">69</a>]</span>最初提出<span class="math inline">\(s(df_g)=df_g^{0.5}\)</span>，因为这样能够保证每个组参数估计对应的罚函数值和该组含有的变量数目同阶。<span class="math inline">\(l(\mathbf{\beta})\)</span> 是普通逻辑回归的似然函数。这里需要对<span class="math inline">\(\lambda\)</span>进行调优，范围在0到<span class="math inline">\(\lambda_{max}\)</span>之间。这里的<span class="math inline">\(\lambda_{max}\)</span>定义如下<span class="citation">[<a href="#ref-Meier2008">69</a>]</span>：</p>
<p><span class="math display">\[
\lambda_{max}=\underset{g\in\{1,...,G\}}{max}\left\{\frac{1}{s(df_{g})}\parallel \mathbf{x}_{g}^{T}(\mathbf{y}-\bar{\mathbf{y}})\parallel_{2}\right\},
\]</span></p>
<p>当<span class="math inline">\(\lambda=\lambda_{max}\)</span>时，模型中只有截距项存在，也就是说除了<span class="math inline">\(\beta_0\)</span>以外所有其它变量参数估计都是0。当<span class="math inline">\(\lambda=0\)</span>时，模型等价于普通逻辑回归。我们通常取如下m个<span class="math inline">\(\lambda\)</span>值进行调优：</p>
<p><span class="math display">\[\{0.96\lambda_{max},0.96^{2}\lambda_{max},0.96^{3}\lambda_{max},...,0.96^{m}\lambda_{max}\}\]</span></p>
<p>这里的m表示你想要调优的参数个数，通常在100到150之间比较合适。参数的选择还是需要交互校验。有3种调优准则，其中之一是已经讲过的AUC，在此不赘述。除了AUC之外，还有两种准则：对数似然函数值<span class="citation">[<a href="#ref-Meier2008">69</a>]</span>和最大相关系数。每个调优参数对应的对数似然函数值是其在不同交互校验拟合得到的对数似然函数的平均，这个很容易理解。最大相关系数来自Yeo和Burge的论文，其定义如下<span class="citation">[<a href="#ref-Yeo2004">70</a>]</span>：</p>
<p><span class="math display">\[
\rho_{max}=max\{\rho_{\tau}|\tau\in(0,1)\},
\]</span></p>
<p>其中<span class="math inline">\(\tau\in(0,1)\)</span> 是划分预测概率的截断点，概率大于<span class="math inline">\(\tau\)</span>的判定为1，否者为0。<span class="math inline">\(\rho_\tau\)</span>是观测到的真实应变量和相应截断点<span class="math inline">\(\tau\)</span>下得到的预测结果的Pearson相关系数。我正在开发的一个R包中有实现群组lasso逻辑回归调优拟合的函数。可以通过下面代码安装该包：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">devtools::<span class="kw">install_github</span>(<span class="st">&quot;happyrabbit/DataScienceR&quot;</span>)</code></pre></div>
<p>安装好了之后载入该包：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(DataScienceR)</code></pre></div>
<pre><code>## 
## Attaching package: &#39;DataScienceR&#39;</code></pre>
<pre><code>## The following object is masked _by_ &#39;.GlobalEnv&#39;:
## 
##     multiplot</code></pre>
<p>该包中含有这里使用的疾病预测数据，只需要用下面代码载入数据：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data</span>(<span class="st">&quot;sim1_da1&quot;</span>)</code></pre></div>
<p>包中的函数<code>cv_glasso()</code>可以用来对不同的参数进行调优：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># sim1_da1中最后一列y是应变量，其余的都是自变量</span>
<span class="co"># trainx是自变量矩阵，去除应变量列</span>
trainx =<span class="st"> </span>dplyr::<span class="kw">select</span>(sim1_da1, -y)
<span class="co"># 将应变量存在trainy中</span>
trainy =<span class="st"> </span>sim1_da1$y
<span class="co"># 得到关于群组的指针</span>
index &lt;-<span class="st"> </span><span class="kw">gsub</span>(<span class="st">&quot;</span><span class="ch">\\</span><span class="st">..*&quot;</span>, <span class="st">&quot;&quot;</span>, <span class="kw">names</span>(trainx))</code></pre></div>
<p>可以看到，每个问题对应的虚拟变量属于同一组：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">index[<span class="dv">1</span>:<span class="dv">50</span>]</code></pre></div>
<pre><code>##  [1] &quot;Q1&quot;  &quot;Q1&quot;  &quot;Q2&quot;  &quot;Q2&quot;  &quot;Q3&quot;  &quot;Q3&quot;  &quot;Q4&quot;  &quot;Q4&quot;  &quot;Q5&quot;  &quot;Q5&quot;  &quot;Q6&quot; 
## [12] &quot;Q6&quot;  &quot;Q7&quot;  &quot;Q7&quot;  &quot;Q8&quot;  &quot;Q8&quot;  &quot;Q9&quot;  &quot;Q9&quot;  &quot;Q10&quot; &quot;Q10&quot; &quot;Q11&quot; &quot;Q11&quot;
## [23] &quot;Q12&quot; &quot;Q12&quot; &quot;Q13&quot; &quot;Q13&quot; &quot;Q14&quot; &quot;Q14&quot; &quot;Q15&quot; &quot;Q15&quot; &quot;Q16&quot; &quot;Q16&quot; &quot;Q17&quot;
## [34] &quot;Q17&quot; &quot;Q18&quot; &quot;Q18&quot; &quot;Q19&quot; &quot;Q19&quot; &quot;Q20&quot; &quot;Q20&quot; &quot;Q21&quot; &quot;Q21&quot; &quot;Q22&quot; &quot;Q22&quot;
## [45] &quot;Q23&quot; &quot;Q23&quot; &quot;Q24&quot; &quot;Q24&quot; &quot;Q25&quot; &quot;Q25&quot;</code></pre>
<p>下面我们设置调优参数，<code>nlam</code>是调优参数个数，也就是<span class="math inline">\(\{0.96\lambda_{max},0.96^{2}\lambda_{max},0.96^{3}\lambda_{max},...,0.96^{m}\lambda_{max}\}\)</span>中的m。调优过程中会有很多输出，这里省略这些输出：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># 对100个调优参数值进行调优</span>
nlam &lt;-<span class="st"> </span><span class="dv">100</span>
<span class="co"># 设置调优过程中模型的预测类型</span>
<span class="co"># - `link`：返回链结函数的拟合值</span>
<span class="co"># - `response`：返回拟合的概率值</span>
<span class="co"># </span>
<span class="co"># 这和之前的</span>
type =<span class="st"> &quot;link&quot;</span>
<span class="co"># number of cross-validation folds</span>
kfold &lt;-<span class="st"> </span><span class="dv">10</span>
cv_fit &lt;-<span class="st"> </span><span class="kw">cv_glasso</span>(trainx, trainy, <span class="dt">nlam =</span> nlam, <span class="dt">kfold =</span> kfold)
<span class="co"># 只展示部分结果</span>
<span class="kw">str</span>(cv_fit)</code></pre></div>
<p>这里由于篇幅所限，只展示部分<code>cv_fit</code>的结果：</p>
<div class="sourceCode"><pre class="sourceCode html"><code class="sourceCode html">...
 $ auc               : num [1:100] 0.573 0.567 0.535 0.484 0.514 ...
 $ log_likelihood    : num [1:100] -554 -554 -553 -553 -552 ...
 $ maxrho            : num [1:100] -0.0519 0.00666 0.04631 0.0486 0.06269 ...
 $ lambda.max.auc    : Named num [1:2] 0.922 0.94
  ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;lambda&quot; &quot;auc&quot;
 $ lambda.1se.auc    : Named num [1:2] 16.74 0.81
  ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;&quot; &quot;se.auc&quot;
 $ lambda.max.loglike: Named num [1:2] 1.77 -248.86
  ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;lambda&quot; &quot;loglike&quot;
 $ lambda.1se.loglike: Named num [1:2] 9.45 -360.13
  ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;lambda&quot; &quot;se.loglike&quot;
 $ lambda.max.maxco  : Named num [1:2] 0.922 0.708
  ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;lambda&quot; &quot;maxco&quot;
 $ lambda.1se.maxco  : Named num [1:2] 14.216 0.504
  ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;lambda&quot; &quot;se.maxco&quot;</code></pre></div>
<p>结果中</p>
<ul>
<li><code>$ auc</code>对应100个调优参数值对应的AUC值</li>
<li><code>$ log_likelihood</code>是调优参数对应的对数似然函数值</li>
<li><code>$ maxrho</code>是调优参数对应的最大相关性</li>
<li><code>$ lambda.max.auc</code> 对应两个值，最优化auc的<span class="math inline">\(\lambda\)</span>取值以及该取值下的auc值</li>
<li><code>$ lambda.1se.auc</code> 对应的两个值分别是离最大auc值1个标准差的值，和其对应的<span class="math inline">\(\lambda\)</span>值</li>
<li><code>$ lambda.max.loglike</code> 对应两个值，最优化对数似然函数的<span class="math inline">\(\lambda\)</span>取值以及该取值下的对数似然函数值</li>
<li><code>$ lambda.1se.loglike</code> 对应的两个值分别是离最大对数似然函数值1个标准差的值，和其对应的<span class="math inline">\(\lambda\)</span>值</li>
<li><code>$ lambda.max.maxco</code> 对应两个值，最优化最大相关性的<span class="math inline">\(\lambda\)</span>取值以及该取值下的最大相关性</li>
<li><code>$ lambda.1se.maxco</code> 对应的两个值分别是离最优最大相关性1个标准差的值，和其对应的<span class="math inline">\(\lambda\)</span>值</li>
</ul>
<p>一般情况下我们主要用auc值作为标准，但是可以比较不同标准对应的调优参数选择，如果一致，我们会对调优参数的选择更有信心。如果差别很大，那我们可能需要进一步考虑参数选择的过程是不是稳定。那么什么时候用最优化相应准则的调优参数，什么时候用1标准差调优参数呢？以auc为例，如果auc从最优值到1标准差值变化非常平缓，比如从0.8降低到0.78，但放松标准可以极大的改变选择的调优参数，如果我们牺牲一点auc，模型中的参数数量可能大量降低（这种情况我经常遇到），那模型的解释性大大提高，这时就可以适当放松标准。</p>
<p>也可以对交互校验拟合结果绘图：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(cv_fit)</code></pre></div>
<p><img src="DS_R_files/figure-html/unnamed-chunk-238-1.png" width="672" /></p>
<p>其中横坐标是调优参数值，纵坐标是auc。两条虚线分别对应的是最优化auc的<span class="math inline">\(\lambda\)</span>取值和1标准差<span class="math inline">\(\lambda\)</span>取值。一旦确定调优参数的值，可以用<code>fitglasso()</code>重新拟合相应模型，比如我们取最优化auc的调优参数值<span class="math inline">\(\lambda=0.922\)</span>：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fitgl &lt;-<span class="st"> </span><span class="kw">fitglasso</span>(trainx, trainy, <span class="dt">lambda =</span> <span class="fl">0.922</span>, <span class="dt">na_action =</span> na.pass)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode html"><code class="sourceCode html">Lambda: 0.922  nr.var: 229 </code></pre></div>
<p>类似的可以使用<code>coef()</code>函数：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">coef</span>(fitgl)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode html"><code class="sourceCode html">                  0.922
Intercept -5.318039e+01
Q1.A       1.756672e+00
Q1.B       1.719050e+00
Q2.A       2.169919e+00
Q2.B       6.939251e-01
Q3.A       2.102014e+00
Q3.B       1.358941e+00
Q4.A       1.561528e+00
Q4.B       5.539396e-01
...</code></pre></div>
<p>对新样本预测有一些不同，得用<code>predict_glasso()</code>函数：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">prey &lt;-<span class="st"> </span><span class="kw">predict_glasso</span>(fitgl, trainx)</code></pre></div>
<!--
### 疾病预测案例

Risk scoring systems for predicting disease are widely used in medicine. 
Such scoring systems are usually derived from multivariate logistic regression models with disease 
as the response variable. Typical approaches in the literature select potential explanatory variables (risk factors) based on variable significance , with risk scores of selected variables assigned based on estimated regression coefficients \cite{Emmanuel2005,Kimberly2003,Rhatigan2010}. However, when the number of potential explanatory variables is large, such
approaches may fail to  produce a risk scoring system with the greatest power for predicting disease.

This paper is motivated by the need to develop a risk  scoring system for
porcine reproductive and respiratory syndrome (PRRS) based on survey data. PRRS, caused by the
PRRS virus, is a major disease, production and financial problem for
swine producers in nearly every country. PRRS costs the United States
swine industry around \$560 million annually \cite{Neumann2005}.
PRRS outbreaks in China caused pork prices to increase by 85 percent
in 2006 \cite{Li Y2007}. For breeding herds, costs of clinical outbreaks
of PRRS result from lost production due to abortion, mummies, stillborns,
pre-wean mortality and sow deaths and increased costs for treatment
and control. Performance of observational studies to better understand the relative importance of
risk factors for PRRS outbreaks have been limited by the availability
of good data on a large set of farms over a relatively long period
of time.

In human medicine, large datasets of information on risk factors,
prevalence, incidence and clinical outcomes of disease are common. In veterinary medicine, until
recently, there have been no parallel efforts to create epidemiological
databases on a similar scale. The American Association of Swine Veterinarians (AASV) Production
Animal Disease Risk Assessment Program (PADRAP) is a program through which a set of web-based risk
assessment surveys are delivered(please visit: http://vdpambi.vdl.iastate.edu/padrap/default.aspx). It is used by veterinarians who are members of the AASV. Each of the surveys consists of a set of questions
about potential risk factors for clinical outbreaks of PRRS in swine. Each question may have up to 
6 possible responses. Members of the AASV
use PADRAP to help producers systematically assess biosecurity factors
that may be associated with clinical outcomes. As assessments are
performed by veterinarians they are added to the database of completed
assessments. 

Version 2 of the PRRS Risk Assessment for the Breeding Herd survey
was introduced in 2005. The survey instrument was developed using
expert opinion with the aid of the PRRS Risk Assessment
Working Group composed of 21 veterinarians and researchers with expertise
in PRRS. Initial estimates of the risk scores associated with each response were 
based on the consensus of expert opinion and equal weight is assigned to each question.

The aim of this study is to use the survey data that has been collected to develop
a risk scoring system with 127 survey questions (categorical explanatory variables) 
that outperforms the current risk scoring system based on expert opinion when multivariate
logistic regression is used in similar studies with variables selected 
by significance.  \textquotedblleft{} Quasi-complete-separation\textquotedblright{} may result when there are
a large number of explanatory variables which makes estimation
of the coefficients unstable. To stabilize the estimation of parameter
coefficients, one popular approach is the lasso algorithm with $l_{1}$-norm penalty
proposed by Tibshirani \cite{Tibshirani (1996)}. Since the lasso algorithm can estimate some 
variable coefficients to be 0, it can also be used as a variable selection
tool. For models with categorical survey questions (explanatory variables), however, original lasso algorithm
 only selects individual dummy variables instead of sets of the dummy variables grouped by 
 question in the survey. Another disadvantage of applying lasso to grouped variables is that the estimates are
 affected by the way dummy variables are encoded.
Thus the group lasso \cite{Yuan2007} method has been proposed to enable variable selection in linear regression models
on groups of variables, instead of on single variables. For logistic regression models, the group lasso algorithm was first studied by  
Kim et al. \cite{Kim2006}. They proposed a gradient descent algorithm to
solve the corresponding constrained problem, which does, however, depend
on unknown constants. Meier et al. \cite{Meier2008} proposed a new algorithm
that could work directly on the penalized problem and its convergence
property does not depend on unknown constants. The algorithm is especially
suitable for high-dimensional problems. It can also be  applied
to solve the corresponding convex optimization
problem in generalized linear models. The logistic group lasso involves selection of a penalty (tuning)
 parameter $\lambda$ which can be determined by cross-validation.
The group lasso estimator proposed by Meier et al. \cite{Meier2008}
for logistic regression has been shown to be statistically consistent, even with large number of 
categorical predictors.
 
 In this paper, we propose to use the logistic group lasso algorithm to construct risk scoring systems for predicting clinical PRRS outbreaks in swine herds. The paper is organized as follows. In Section 2, we introduce the group lasso method for logistic regression to construct risk scoring system for clinical PRRS outbreaks. The penalty parameter $\lambda$ for group lasso is selected through leave-one-out cross validation, using the criterion of the area under the receiver operating characteristic curve (AUC). Section 3 presents a simulation study to evaluate the performance of each method. In Section 4, we  discuss the application to the PRRS survey data from 896 swine breeding herd sites in the United States and Canada. We show our scoring system for PRRS is superior to both the current scoring system based on expert opinion and that developed by using logistic regression with model selection based on variable significance.  Finally results and conclusions presented in Section 5 and 6.

#### Models for risk scoring systems


%### Choosing $\lambda$ through leave-one-out cross validation}

Here we consider selection of the tuning parameter $\lambda$ from a multiplicative grid of 148
 values $\{0.96\lambda_{max},0.96^{2}\lambda_{max},0.96^{3}\lambda_{max},...,0.96^{148}\lambda_{max}\}$, as in Meier et al. \cite{Meier2008}. Here $\lambda_{max}$ is defined as

\begin{equation}
\lambda_{max}=\underset{g\in\{1,...,G\}}{max}\left\{\frac{1}{s(df_{g})}\parallel \bx_{g}^{T}(\by-\bar{\by})\parallel_{2}\right\},
\end{equation}
such that when $\lambda=\lambda_{max}$, only the intercept is
in the model. When $\lambda$ goes to $0$, the model is equivalent to regular
logistic regression. 

The optimal value of $\lambda$ is determined through leave-one-out cross validation, which is a special case of K-fold cross-validation with K being equal to $n$, the number of observations in the sample. In each fold, leave-one-out cross validation uses a single observation from the original sample as the validation data, and the remaining observations as the training data. This step is repeated until each  observation in the sample is used once as the validation data. Predicted probabilities of disease are calculated from cross-validation and are compared to true observed disease status to assess the predictive power of model. 

In this paper, we assess the predictive power of each model through ROC analysis. ROC curve is a graph of pairs
of the true positive rate (sensitivity) and false positive rate (1-specificity)
 that result as the cutoff value for the predicted probability of disease is varied. 
%The cutoff value varied in order from minimum to maximum of the resulted score.
Theoretically, cutoff values can be any values on the real line. The practical cutoff values are determined from resulting scores based on our data. The value of AUC is used as the criterion to evaluate the predictive power of the logistic model. AUC as well as the confidence interval are estimated through the approach proposed by DeLong et al. \cite{DeLong1988}. The AUC can be interpreted as the probability that a random diseased individual has larger predicted probability of disease than a random non-diseased individual \cite{Pepe2003} and it has been used to assess predictive power of risk scoring systems \cite{Emmanuel2005,Kimberly2003,Rhatigan2010}. We calculate the AUCs for cross-validations of all $\lambda$s, and the value of $\lambda$ with the largest AUC is chosen as the $\lambda$ used in constructing the final scoring system.



\section{Simulation Study}

A simulation study to 
%use simulation to 
demonstrate group lasso logistic
regression  and compare it to ordinary forward stepwise logistic regression is performed.

\section{Application to PRRS Data}

In this section, we apply the proposed method to construct a scoring system for PRRS survey data of swine breeding herd sites in the United States and Canada. 

#### Data Description

Surveys in the database completed between March 2005 and March 2009
are candidates for inclusion in the analysis. To avoid multiple surveys
from a single swine breeding herd site, the study dataset is limited to responses obtained
from the most recently completed survey for each site. Surveys meeting
these criteria are extracted from the database, and identity
information is removed. Incomplete surveys are excluded.

%The outcome of interest is whether a breeding herd site reported
%a clinical PRRS outbreak in the 3 years prior to when the assessment
%was completed. 
The outcome of interest is whether a site is positive or not. 
Positive sites are sites with clinical PRRS outbreak in the 3 years prior to when the assessment was completed, negative sites otherwise. 
The information to determine the outcome was obtained from the survey.
A clinical PRRS outbreak is described in the survey
as an increase in one or more reproductive performance measures that
exceeds normal variation with diagnostic confirmation of PRRS virus involvement. 

Of the 896 sites in the United States and Canada included in the
study, 499 (56\%) became positive during the past 3 years. 127 survey questions
are considered potential explanatory variables in the analysis. The
survey questions are first converted to dummy indicator variables. All of the responses for 
each survey question  are defined as a group of variables.

#### Application of logistic group lasso

First, leave-one-out cross validation is used to choose tuning parameter $\lambda$, as described in Section 2.3.
% In each fold of cross-validation, one of the 896 farms is excluded and the other 895 farms are used as a training data set on which the
% group lasso logistic regression is applied. The resulting model is used to calculate a predicted probability for PRRS outbreak for the excluded farm. This procedure is repeated for all 896 farms. 

For each $\lambda$ in the grid $\{0.96\lambda_{max},0.96^{2}\lambda_{max},0.96^{3}\lambda_{max},...,0.96^{148}\lambda_{max}\}$,  the values of three evaluation criteria  are calculated based on cross validation.  The penalty parameter for final risk scoring system is selected to be the one that optimizes AUC. 

%### Comparison with other risk scoring systems}

The logistic group lasso based scoring system is compared with two other systems: 

\begin{enumerate}

\item The current risk scoring system used in versions 2 of the PRRS risk assessment for the breeding herd that is based on expert opinion,
\item A risk scoring system based on multivariate logistic regression model  selected by variable significance.

\end{enumerate}

We construct the significance based logistic model by following 
the method used by Van Zee et al. \cite{Kimberly2003}. Specifically, we use forward stepwise variable selection to construct the logistic regression model with 0.05 significant level. Leave-one-out cross validation is applied to the model construction by variable significance, in the same manner as described for logistic group lasso. 

ROC curves are plotted for the three risk scoring systems. A point estimate as well as the 95\% confidence interval for the AUC are provided. The estimated AUCs are compared by using the nonparametric approach of DeLong et al. \cite{DeLong1988} and p-values are calculated.


#### Determination of penalty parameter $\lambda$ 

The AUC, maximum correlation coefficient and log-likelihood are calculated based on leave-one-out cross validation and are plotted against the penalty parameter $\lambda$ in Figure \ref{Flo:penalty}. The trends for all three criteria are similar with a sharp increase for small values of $\lambda$ and gradual decrease after reaching the maximum. The optimal
values of $\lambda$ selected to maximize the three criteria  are 11.72, 4.22 and 11.72  for AUC, maximum correlation coefficient and log-likelihood respectively.

\begin{figure}
\resizebox*{15cm}{!}{\includegraphics{ThreeCri}}%
\caption{Three criteria for choice of penalty parameter $\lambda$}
\label{Flo:penalty}
\end{figure}

#### Logistic group lasso based PRRS risk scoring system

The penalty parameter maximizing AUC (i.e. $\lambda=11.72$) from the leave-one-out cross validation is used for the
group lasso estimation of the logistic regression parameters.  Figure~\ref{histogram} show the distributions of the predicted probabilities based on cross validation for both negative and positive farms. It can be observed that the predicted probability for positive farms is larger than that of negative farms in stochastic order. The actual risk score  can take the value of the predicted probability, the linear predictor in the logistic regression model, or any strictly increasing function of the predicted probability. This is because the ROC curve for a predictor is invariate to such transformation. 

\begin{figure}
\resizebox*{15cm}{!}{\includegraphics{histograms}}%
\caption{Distributions of estimated probabilities for both negative and positive groups}
\label{histogram}
\end{figure}

In the resulting scoring system, 74 out of 127 survey questions are estimated with 0 coefficients and are excluded from the system. PADRAP questions target internal risks (bio-management of virus already present) and external risks (bio-exclusion of virus not present). A summary of the number of questions included in the final risk scoring system in each category of risk factors in the PRRS Risk Assessment for the Breeding Herd is shown in Table 1. 

\begin{table}
\caption{Summary of number of questions in the final risk scoring system by category of risk factors }
\begin{center} \begin{tabular}{lrrrr} \hline Category of risk factors & \multicolumn{2}{r}{Questions} & \multicolumn{2}{r}{Dummy Variables}  \\ \hline INTERNAL RISKS & Included & Total & Included & Total \\ Circulation Risk & & & & \\ Characteristics of the herd & 3 & 4 & 9 & 11 \\ Characteristics of the site & 0 & 2 & 0 & 5 \\ Management practices & 0 & 2 & 0 & 9 \\--------- &-------&------&------&------\\
\textbf{\textit{Total}} & \textbf{\textit{3}}  & \textbf{\textit{8}} & \textbf{\textit{9}} & \textbf{\textit{25}}\\ & & & &

\\ EXTERNAL RISKS & & & & \\ Pig Related & & & & \\ Entry of replacement animals into the breeding herd & 4 & 12 & 18 & 40 \\ Entry of semen into the breeding herd & 13 & 31 & 47 & 104 \\ & & & & \\ Non-Pig Related & & & & \\ Transportation of live animals & 13 & 29 & 38 & 71 \\ Transportation of feed & 1 & 1 & 2 & 2 \\ Employee and service vehicles & 1 & 2 & 3 & 6 \\ Disposal of dead animals and waste management & 2 & 8 & 3 & 10 \\ Employees and visitors & 5 & 9 & 15 & 19 \\ Entry of supplies & 1 & 1 & 3 & 3 \\ Facilities & 0 & 4 & 0 & 11 \\ Biovectors & 1 & 1 & 2 & 1 \\ Density of pig farms in the area & 3 & 3 & 10 & 10 \\ Neighboring pig farms & 3 & 13 & 12 & 28 \\ Distance to pork industry infrastructure & 2 & 4 & 5 & 11 \\ Topography and forestation of surrounding area & 1 & 1 & 3 & 3 \\ --------- &-------&------&------&------\\
\textbf{\textit{Total}} & \textbf{\textit{50}}  & \textbf{\textit{119}} & \textbf{\textit{161}} &\textbf{\textit{319}}\\\hline \end{tabular} \end{center} 
\end{table}

Three out of eight questions regarding internal risk factors remain in the scoring system, and they are all factors concerning characteristics of the herd. Fifty questions remain in external risk factor section out of the total 119 questions. 
%Sixty nine questions are excluded out of the external risk factors section and fifty remain.
In the external risks section, all of the 14 categories have at least one question remaining in the final scoring system, except all 4 questions concerning facilities are excluded. Several categories have  a large number of questions removed. In particular,  8 of 12 (66.7\%) questions concerning entry of animals into the breeding herd, 18 of 31 (58.1\%) questions concerning entry of semen into the breeding herd, 16 of 29 (55.2\%) questions concerning transportation of live animals, and 10 of 13 (76.9\%) questions concerning neighboring pig farms are excluded.

#### Comparison among risk scoring systems

The ROC curves for the three risk scoring systems are plotted in Figure  \ref{ROC}. The ROC curves for the two scoring systems based on logistic regression analyses of the data are constructed using the results of leave-one-out cross validation. The ROC curve of logistic group lasso apparently dominates the other two scoring systems. 


\begin{figure}
\includegraphics{roc_plot_diff_model}
\caption{ROC curves for three risk scoring systems}
\label{ROC}
\end{figure}

Point and 95\% interval estimates of AUC are reported in Table 2. The risk scoring system based on the has the largest  AUC = 0.848. This AUC estimate is significantly higher than those based on either expert opinion   (AUC = 0.696, p-value $<$ 0.001) or logistic regression model selected by variable significance (AUC = 0.807, p-value $<$ 0.001).


\begin{table}
\caption{AUC estimations for three risk scoring systems}
\begin{center} \begin{tabular}{lcc} \hline Model Names & AUC & 95\% CI \\ \hline Group Lasso & 0.848 & (0.822, 0.873) \\ Significance Based Method & 0.807 & (0.773, 0.841) \\ Expert Opinion & 0.696 & (0.661, 0.731) \\ \hline \end{tabular} \end{center}
\end{table}


Results for the simulation study  are shown in Table 3. The mean AUC is increasing with the value of $\gamma$ for both methods. The Wilcoxon signed-rank test result in the last column of Table 3 shows that AUC's from 
group lasso are significant larger than those from logistic regression,  especially for $\gamma \geq 0.25$.

\providecommand{\tabularnewline}{\\}

\makeatother

\begin{table}
\caption{Simulation study result with various values of coefficient $\gamma$ with  mean and standard deviation for both method, mean difference and p value from Wilcox signed rank test }
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline 
Coefficient $\gamma$  & Group Lasso (mean$\pm$sd ) & Logistic Regression (mean$\pm$sd )  & Difference  & p value \tabularnewline
\hline 
0.1  & $0.57\pm0.03$ & $0.54\pm0.06$ & $0.03$ & $0.040$\tabularnewline
\hline 
0.25 & $0.71\pm0.02$ & $0.64\pm0.04$ & $0.07$ & $<0.001$\tabularnewline
\hline 
0.5  & $0.91\pm0.03$ & $0.78\pm0.03$ & $0.13$ & $<0.001$\tabularnewline
\hline 
1 & $0.92\pm0.01$ & $0.82\pm0.02$ & $0.1$0 & $<0.001$\tabularnewline
\hline 
2 & $0.95\pm0.01$ & $0.84\pm0.02$ & $0.11$ & $<0.001$\tabularnewline
\hline 
\end{tabular}
 \end{center}
\end{table}

#### Discussion

The risk scoring system for disease developed using the logistic group lasso algorithm significantly improves upon the current risk scoring system based on expert opinion for predicting whether a swine breeding site experienced a PRRS outbreak. 
%We introduce the logistic group lasso algorithm to develop  risk scoring systems for diseases. Choice of penalty parameter $\lambda$ is determined by leave-one-out cross validation with criterion %of AUC.
%We apply our method to construct a new risk scoring system for PRRS outbreak in swine farms. Our scoring system significantly improves the current risk scoring system based on expert
%opinion  with respect to informing us about the contribution of certain category to the probability of outbreak. 
The simuation study explores the performance of the scoring systems with different settings of coefficients. The logistic group lasso based scoring system is superior to the scoring system constructed through logistic regression selected by variable significance. 

 One advantage of group lasso is that it can be used as variable selection
tool by setting 0 coefficients to parameters. It not only helps to
find important explanatory factors in predicting the response variables
but also identifies questions that could be removed from the survey
without affecting the survey's ability for classifying herds according
to whether they report clinical PRRS outbreaks in the previous 3 years.

Seventy-four of the 127 questions analyzed are excluded from the final risk scoring system based on logistic group lasso. The analysis and results demonstrate how a program like PADRAP, that is supported by a professional association and used by a community of veterinarians, can generate valuable data that contributes to our understanding of the relative importance of risk factors and areas of risk factors for clinical outcomes.  The results may also be used to decrease the reliance upon expert opinion to identify questions that should remain in the survey and those that may be eliminated to iteratively increase the value of the program and the data.
-->
</div>
</div>
<div id="section-11.4" class="section level2">
<h2><span class="header-section-number">11.4</span> 收缩多项回归</h2>
<p>对类数目超过2的情况需要使用多项回归，比如服装消费者分为：价格敏感（Price），炫耀性消费（Conspicuous），质量（Quality），风格（Style）这四类。这里一般性的假设应变量有K个类别，<span class="math inline">\(\mathbb{G}=\{1,2,\dots,K\}\)</span>。这里的目标是找到样本属于每个类别的概率：</p>
<p><span class="math display">\[Pr(G=k|\mathbf{X=x})=\frac{e^{\beta_{0k}+\mathbf{\beta_{k}^{T}\mathbf{x}}}}{\Sigma_{l=1}^{K}e^{\beta_{0l}+\mathbf{\beta_{l}^{T}x}}}\]</span></p>
<p>这里的应变量可以转化成虚拟变量的矩阵，每一列代表一个类别。假设<span class="math inline">\(\mathbf{Y}\)</span>是应变量转化成的<span class="math inline">\(N\times K\)</span>的0/1矩阵，其中<span class="math inline">\(y_{il}=I(g_{i}=l)\)</span>，也就是某个观测<span class="math inline">\(i\)</span>从属的类别<span class="math inline">\(l\)</span>的位置上观测是1，其余是0。此时弹性网络模型优化的是如下方程：</p>
<p><span class="math display">\[l({\beta_{0k}, \mathbf{\beta_{k}}_{1}^{K}})= - \left[ \frac{1}{N}\Sigma_{i=1}^N \left( \Sigma_{k=1}^K y_{il}(\beta_{0k} + \mathbf{x_i^T\beta_{k}}) - log(\Sigma_{k=1}^{K}e^{\beta_{0k}+\mathbf{x_i^T\beta_{k}}}) \right) \right]+\lambda\left[  \frac{(1-\alpha)\parallel \beta \parallel_F^2}{2} + \alpha \Sigma_{j=1}^p \parallel \beta_j \parallel_q\right]\]</span></p>
<p>这个式子有些复杂，稍微解释一下。其实思路和之前是一样的，上式的前半部分是应变量矩阵中对应<span class="math inline">\(l\)</span>类那列的对数似然函数取负号，第二部分是罚函数。第一部分<span class="math inline">\(y_{il}(\beta_{0k} + \mathbf{x_i^T\beta_{k}})\)</span>中，<span class="math inline">\(y_{il}\)</span>和<span class="math inline">\(\mathbf{x_i^T\beta_{k}}\)</span>是取乘积因为<span class="math inline">\(y_{il}\)</span>的取值是0/1，否则应该是<span class="math inline">\(log(y_{il})+\beta_{0k} + \mathbf{x_i^T\beta_{k}}\)</span>的形式。此外，罚函数中<span class="math inline">\(\parallel\beta\parallel_{F}^{2}\)</span>的<span class="math inline">\(\beta\)</span>是一个矩阵而非向量，因为这里每个类别都对应一个参数向量，所以所有类别的参数向量集结成了一个矩阵<span class="math inline">\(\beta\)</span>。罚函数中的另外一项<span class="math inline">\(\parallel \beta_j \parallel_q\)</span>中的q可以有两个取值：<span class="math inline">\(q\in \{1, 2\}\)</span>。当<span class="math inline">\(q=1\)</span>时，是对每个类对应的参数应用lasso罚，当<span class="math inline">\(q=2\)</span>时，相当于是群组lasso罚，和之前“知识扩展：群组lasso逻辑回归”这个小节的方法非常类似，这时观测j对应的所有K个群组的参数同时为0或者同时不为0。</p>
<p>这里严格使用标准的牛顿迭代算法优化起来非常繁琐。<code>glmnet</code>包使用了一种叫做偏牛顿优化算法（Partial Newton Algorithm）的方法。该方法对对数似然函数进行偏二项逼近，即对某一类别每次只允许对应的参数<span class="math inline">\((\beta_{0k}, \mathbf{\beta_{k}})\)</span>变化，其余参数保持不变。关于该算法更加详细的介绍，可以参考Noah Simon, Jerome Friedman和Trevor Hastie的论文<span class="citation">[<a href="#ref-Noah2013">71</a>]</span>。</p>
<p>下面还是用服装消费者数据展示如何通过<code>glmnet</code>包实现该方法，这里自变量依旧和之前一样是问卷调查的问题，应变量变成是消费者类别：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(glmnet)
dat &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;/Users/happyrabbit/Documents/GitHub/DataScientistR/Data/SegData.csv&quot;</span>)
<span class="co"># 将10个问卷调查变量当作自变量</span>
trainx &lt;-<span class="st"> </span>dat[, <span class="kw">grep</span>(<span class="st">&quot;Q&quot;</span>, <span class="kw">names</span>(dat))]
<span class="co"># 将消费者类别当作应变量</span>
trainy &lt;-<span class="st"> </span>dat$segment</code></pre></div>
<p>对于多项回归中有一个关于罚函数的选项<code>type.multinomial</code>，这是用来指定在上面提到过的<span class="math inline">\(\parallel \beta_j \parallel_q\)</span>部分。如果<code>type.multinomial = &quot;group&quot;</code>对应的就是<span class="math inline">\(q=2\)</span>的情况，也就是群组lasso。默认设置是非群组，也就是<span class="math inline">\(q=1\)</span>。</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit &lt;-<span class="st"> </span><span class="kw">glmnet</span>(<span class="kw">as.matrix</span>(trainx), trainy, <span class="dt">family =</span> <span class="st">&quot;multinomial&quot;</span>)</code></pre></div>
<p>可以对拟合对象进行绘图：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(fit, <span class="dt">xvar =</span> <span class="st">&quot;lambda&quot;</span>, <span class="dt">label =</span> T, <span class="dt">type.coef =</span> <span class="st">&quot;2norm&quot;</span>)</code></pre></div>
<p><img src="DS_R_files/figure-html/unnamed-chunk-241-1.png" width="672" /></p>
<p>其中参数有：</p>
<ul>
<li><code>xvar</code>和<code>label</code>和之前相同，用来指定横坐标的变量和是否标出自变量名字</li>
<li><code>type.coef</code>和之前不同，如果<code>type.coef = &quot;coef&quot;</code>，那么对因变量的每个类别都会返回一幅图，此例子中会返回4幅图，各自对应一个类别的参数估计。如果<code>type.coef = &quot;2norm&quot;</code>，那么只会返回1幅图，纵坐标是所有类别参数的二阶矩</li>
</ul>
<p>也可以用<code>cv.glment()</code>函数进行交互校验：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">cvfit &lt;-<span class="st"> </span><span class="kw">cv.glmnet</span>(<span class="kw">as.matrix</span>(trainx), trainy, <span class="dt">family =</span> <span class="st">&quot;multinomial&quot;</span>)
<span class="kw">plot</span>(cvfit)</code></pre></div>
<p><img src="DS_R_files/figure-html/unnamed-chunk-242-1.png" width="672" /></p>
<p>同样你可以得到最优的lambda取值然后用该取值拟合模型：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">cvfit$lambda.min</code></pre></div>
<pre><code>## [1] 0.0005155172</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">newdat =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">sample</span>(<span class="dv">1</span>:<span class="dv">9</span>, <span class="dv">60</span>, <span class="dt">replace =</span> T), <span class="dt">nrow =</span> <span class="dv">6</span>)
<span class="kw">predict</span>(cvfit, newdat, <span class="dt">s =</span> <span class="st">&quot;lambda.min&quot;</span>, <span class="dt">type =</span> <span class="st">&quot;class&quot;</span>)</code></pre></div>
<pre><code>##      1            
## [1,] &quot;Conspicuous&quot;
## [2,] &quot;Conspicuous&quot;
## [3,] &quot;Quality&quot;    
## [4,] &quot;Quality&quot;    
## [5,] &quot;Style&quot;      
## [6,] &quot;Style&quot;</code></pre>
</div>
<div id="section-11.5" class="section level2">
<h2><span class="header-section-number">11.5</span> 泊松收缩回归</h2>
<p>关于广义收缩方法中我们要介绍的最后一个是泊松模型。泊松回归处理的是应变量为计数的情况，比如消费者光顾实体店的次数。或者其它应变量非负，并且均值和方差成比例的情况。这里对应变量的要求是大致服从泊松分布的假设。泊松分布也是指数分布家族中的一员。通常对应变量进行对数变换：</p>
<p><span class="math display">\[log (\mu_{x})=\beta_{0}+\mathbf{\beta^{T}x}\]</span></p>
<p>这时的对数似然函数是：</p>
<p><span class="math display">\[l(\mathbf{\beta|X,y})=\Sigma_{i=1}^{N}\left(y_i(\beta_0+\mathbf{\beta^Tx_i})-e^{\beta_0+\mathbf{\beta^Tx_i}} \right)\]</span></p>
<p>和之前类似，我们需要优化的加上罚函数后的式子：</p>
<p><span class="math display">\[\underset{\beta_{0},\mathbf{\beta}}{min}\left\{ -\frac{1}{N}l(\mathbf{\beta|X,y})+\lambda\left((1-\alpha)\Sigma_{i=1}^{N}\frac{\mathbf{\beta_{i}^{2}}}{2}+\alpha\parallel\mathbf{\beta}\parallel_{1}\right)\right\} \]</span></p>
<p>下面继续用服装消费者数据展示如何通过<code>glmnet</code>包实现该方法，这里自变量依旧和之前一样是问卷调查的问题，应变量变成是实体店消费次数：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(glmnet)
dat &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;/Users/happyrabbit/Documents/GitHub/DataScientistR/Data/SegData.csv&quot;</span>)
<span class="co"># 将10个问卷调查变量当作自变量</span>
trainx &lt;-<span class="st"> </span>dat[, <span class="kw">grep</span>(<span class="st">&quot;Q&quot;</span>, <span class="kw">names</span>(dat))]
<span class="co"># 实体店消费次数当作应变量</span>
trainy &lt;-<span class="st"> </span>dat$store_trans
<span class="co"># 拟合泊松模型</span>
fit &lt;-<span class="st"> </span><span class="kw">glmnet</span>(<span class="kw">as.matrix</span>(trainx), trainy, <span class="dt">family =</span> <span class="st">&quot;poisson&quot;</span>)</code></pre></div>
<p>可以类似的使用<code>plot()</code>函数检查参数收缩的轨迹：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(fit, <span class="dt">label =</span> T)</code></pre></div>
<p><img src="DS_R_files/figure-html/unnamed-chunk-245-1.png" width="672" /></p>
<p>可以看到（Q2）：我喜欢买同一个品牌的服装和（Q6）：我喜欢在实体店购买的参数估计到很后面才收缩为0，之间表明是否喜欢实体店购买和真实实体店消费次数之间有关是很符合逻辑的，有意思的是，是否喜欢买同一个品牌的服装也和实体店消费次数有关。如果知道具体问题和实体店购买次数之间关系的正负性需要得到相应的回归系数。我们可以类似用<code>cv.glment()</code>函数进行交互校验：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">cvfit &lt;-<span class="st"> </span><span class="kw">cv.glmnet</span>(<span class="kw">as.matrix</span>(trainx), trainy, <span class="dt">family =</span> <span class="st">&quot;poisson&quot;</span>)
<span class="kw">plot</span>(cvfit)</code></pre></div>
<p><img src="DS_R_files/figure-html/unnamed-chunk-246-1.png" width="672" /></p>
<p>这里用来调优的准则是泊松离差。同样你可以得到最优的lambda取值然后用该取值拟合模型：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">coef</span>(fit, <span class="dt">s=</span>cvfit$lambda.min)</code></pre></div>
<pre><code>## 11 x 1 sparse Matrix of class &quot;dgCMatrix&quot;
##                        1
## (Intercept)  0.134542659
## Q1          -0.002966470
## Q2           0.112332909
## Q3           .          
## Q4          -0.001188666
## Q5           0.128028742
## Q6           0.318738648
## Q7          -0.023964782
## Q8           0.027572033
## Q9           0.020892375
## Q10          .</code></pre>
<p>可见，那些喜欢买同一个服装品牌的，明确表明喜欢实体店购买以及有明确风格偏好的人会更多的光顾实体店。</p>
</div>
<div id="-4" class="section level2">
<h2><span class="header-section-number">11.6</span> 本章总结</h2>
<p>这里我们系统性的介绍了常用的广义回归收缩模型，以及如何用R实施这些模型。<code>glmnet</code>包能够通过<strong>罚极大似然函数</strong>拟合广义线性回归，也就是在似然函数上加上罚函数，和之间在RSS上加罚函数类似。之前的线性回归的情况是广义线性回归的一个特例。和之前一样，罚函数的选择可以是一阶范数和二阶范数的一个组合。<code>glmnet</code>包可以对一系列调优参数值同时计算参数估计。</p>
<p>我们介绍了线性回归，逻辑回归、多项式回归和泊松回归的收缩方法。此外对于逻辑回归，我们补充介绍了群组逻辑回归，该方法在自变量有明显群组效应的时候非常有效。</p>

</div>
</div>
<h3> References</h3>
<div id="refs" class="references">
<div id="ref-Wed1976">
<p>67. Wedderburn, R.W.M.: On the existence and uniqueness of the maximum likelihood estimates for certain generalized linear models. Biometrika. 63, 27–32 (1976).</p>
</div>
<div id="ref-albert1984">
<p>68. A, A., J, A.A.: On the existence of the maximum likelihood estimates in logistic regression models. Biometrika. 71, 1–10 (1984).</p>
</div>
<div id="ref-Meier2008">
<p>69. L. Meier, S. van de Geer, Buhlmann, P.: The group lasso for logistic regression. J. R. Stat. Soc. Ser. B Stat. Methodol. 70, 53–71 (2008).</p>
</div>
<div id="ref-Yeo2004">
<p>70. Yeo, G., Burge, C.: Maximum entropy modeling of short sequence motifs with applications to rna splicing signals. Journal of Computational Biology. 475–494 (2004).</p>
</div>
<div id="ref-Noah2013">
<p>71. Noah Simon, J.F., Hastie, T.: A blockwise descent algorithm for group-penalized multiresponse and multinomial regression. arXiv:1311.6529.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="section-10.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="section-12.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/11-glmnet.Rmd",
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
