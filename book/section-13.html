<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>数据科学家：R语言</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="This is my first book on data science">
  <meta name="generator" content="bookdown 0.1 and GitBook 2.6.7">

  <meta property="og:title" content="数据科学家：R语言" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is my first book on data science" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="数据科学家：R语言" />
  
  <meta name="twitter:description" content="This is my first book on data science" />
  

<meta name="author" content="林荟">

<meta name="date" content="2016-10-04">

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="section-12.html">
<link rel="next" href="section-14.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">数据科学家：R语言</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> 介绍</a></li>
<li class="chapter" data-level="2" data-path="section-2.html"><a href="section-2.html"><i class="fa fa-check"></i><b>2</b> 数据科学</a><ul>
<li class="chapter" data-level="2.1" data-path="section-2.html"><a href="section-2.html#section-2.1"><i class="fa fa-check"></i><b>2.1</b> 什么是数据科学？</a></li>
<li class="chapter" data-level="2.2" data-path="section-2.html"><a href="section-2.html#section-2.2"><i class="fa fa-check"></i><b>2.2</b> 什么是数据科学家？</a></li>
<li class="chapter" data-level="2.3" data-path="section-2.html"><a href="section-2.html#section-2.3"><i class="fa fa-check"></i><b>2.3</b> 数据科学家需要的技能</a></li>
<li class="chapter" data-level="2.4" data-path="section-2.html"><a href="section-2.html#section-2.4"><i class="fa fa-check"></i><b>2.4</b> 数据科学可以解决什么问题？</a><ul>
<li class="chapter" data-level="2.4.1" data-path="section-2.html"><a href="section-2.html#section-2.4.1"><i class="fa fa-check"></i><b>2.4.1</b> 前提要求</a></li>
<li class="chapter" data-level="2.4.2" data-path="section-2.html"><a href="section-2.html#section-2.4.2"><i class="fa fa-check"></i><b>2.4.2</b> 问题种类</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="section-3.html"><a href="section-3.html"><i class="fa fa-check"></i><b>3</b> 数据集模拟和背景介绍</a><ul>
<li class="chapter" data-level="3.1" data-path="section-3.html"><a href="section-3.html#section-3.1"><i class="fa fa-check"></i><b>3.1</b> 服装消费者数据</a></li>
<li class="chapter" data-level="3.2" data-path="section-3.html"><a href="section-3.html#section-3.2"><i class="fa fa-check"></i><b>3.2</b> 航空公司满意度调查</a></li>
<li class="chapter" data-level="3.3" data-path="section-3.html"><a href="section-3.html#section-3.3"><i class="fa fa-check"></i><b>3.3</b> 生猪疫情风险预测数据</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="section-4.html"><a href="section-4.html"><i class="fa fa-check"></i><b>4</b> 数据分析一般流程</a><ul>
<li class="chapter" data-level="4.1" data-path="section-4.html"><a href="section-4.html#section-4.1"><i class="fa fa-check"></i><b>4.1</b> 问题到数据</a></li>
<li class="chapter" data-level="4.2" data-path="section-4.html"><a href="section-4.html#section-4.2"><i class="fa fa-check"></i><b>4.2</b> 数据到信息</a></li>
<li class="chapter" data-level="4.3" data-path="section-4.html"><a href="section-4.html#section-4.3"><i class="fa fa-check"></i><b>4.3</b> 信息到行动</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="section-5.html"><a href="section-5.html"><i class="fa fa-check"></i><b>5</b> 数据预处理</a><ul>
<li class="chapter" data-level="5.1" data-path="section-5.html"><a href="section-5.html#section-5.1"><i class="fa fa-check"></i><b>5.1</b> 介绍</a></li>
<li class="chapter" data-level="5.2" data-path="section-5.html"><a href="section-5.html#section-5.2"><i class="fa fa-check"></i><b>5.2</b> 数据清理</a></li>
<li class="chapter" data-level="5.3" data-path="section-5.html"><a href="section-5.html#section-5.3"><i class="fa fa-check"></i><b>5.3</b> 缺失值填补</a><ul>
<li class="chapter" data-level="5.3.1" data-path="section-5.html"><a href="section-5.html#section-5.3.1"><i class="fa fa-check"></i><b>5.3.1</b> 中位数或众数填补</a></li>
<li class="chapter" data-level="5.3.2" data-path="section-5.html"><a href="section-5.html#k-"><i class="fa fa-check"></i><b>5.3.2</b> K-近邻填补</a></li>
<li class="chapter" data-level="5.3.3" data-path="section-5.html"><a href="section-5.html#section-5.3.3"><i class="fa fa-check"></i><b>5.3.3</b> 袋状树填补</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="section-5.html"><a href="section-5.html#section-5.4"><i class="fa fa-check"></i><b>5.4</b> 中心化和标量化</a></li>
<li class="chapter" data-level="5.5" data-path="section-5.html"><a href="section-5.html#section-5.5"><i class="fa fa-check"></i><b>5.5</b> 有偏分布</a></li>
<li class="chapter" data-level="5.6" data-path="section-5.html"><a href="section-5.html#section-5.6"><i class="fa fa-check"></i><b>5.6</b> 处理离群点</a></li>
<li class="chapter" data-level="5.7" data-path="section-5.html"><a href="section-5.html#section-5.7"><i class="fa fa-check"></i><b>5.7</b> 共线性</a></li>
<li class="chapter" data-level="5.8" data-path="section-5.html"><a href="section-5.html#section-5.8"><i class="fa fa-check"></i><b>5.8</b> 稀疏变量</a></li>
<li class="chapter" data-level="5.9" data-path="section-5.html"><a href="section-5.html#section-5.9"><i class="fa fa-check"></i><b>5.9</b> 编码名义变量</a></li>
<li class="chapter" data-level="5.10" data-path="section-5.html"><a href="section-5.html#section-5.10"><i class="fa fa-check"></i><b>5.10</b> 本章总结</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="section-6.html"><a href="section-6.html"><i class="fa fa-check"></i><b>6</b> 数据整合和整形</a><ul>
<li class="chapter" data-level="6.1" data-path="section-6.html"><a href="section-6.html#section-6.1"><i class="fa fa-check"></i><b>6.1</b> 数据整合</a><ul>
<li class="chapter" data-level="6.1.1" data-path="section-6.html"><a href="section-6.html#baseapply"><i class="fa fa-check"></i><b>6.1.1</b> base包：apply()</a></li>
<li class="chapter" data-level="6.1.2" data-path="section-6.html"><a href="section-6.html#plyrddply"><i class="fa fa-check"></i><b>6.1.2</b> plyr包：ddply()函数</a></li>
<li class="chapter" data-level="6.1.3" data-path="section-6.html"><a href="section-6.html#dplyr"><i class="fa fa-check"></i><b>6.1.3</b> dplyr包</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="section-6.html"><a href="section-6.html#section-6.2"><i class="fa fa-check"></i><b>6.2</b> 数据整形</a><ul>
<li class="chapter" data-level="6.2.1" data-path="section-6.html"><a href="section-6.html#reshape2"><i class="fa fa-check"></i><b>6.2.1</b> <code>reshape2</code>包</a></li>
<li class="chapter" data-level="6.2.2" data-path="section-6.html"><a href="section-6.html#tidyr"><i class="fa fa-check"></i><b>6.2.2</b> <code>tidyr</code>包</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="section-6.html"><a href="section-6.html#-1"><i class="fa fa-check"></i><b>6.3</b> 本章总结</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="section-7.html"><a href="section-7.html"><i class="fa fa-check"></i><b>7</b> 基础建模技术</a><ul>
<li class="chapter" data-level="7.1" data-path="section-7.html"><a href="section-7.html#section-7.1"><i class="fa fa-check"></i><b>7.1</b> 有监督和无监督</a></li>
<li class="chapter" data-level="7.2" data-path="section-7.html"><a href="section-7.html#section-7.2"><i class="fa fa-check"></i><b>7.2</b> 误差及其来源</a><ul>
<li class="chapter" data-level="7.2.1" data-path="section-7.html"><a href="section-7.html#section-7.2.1"><i class="fa fa-check"></i><b>7.2.1</b> 系统误差和随机误差</a></li>
<li class="chapter" data-level="7.2.2" data-path="section-7.html"><a href="section-7.html#section-7.2.2"><i class="fa fa-check"></i><b>7.2.2</b> 应变量误差</a></li>
<li class="chapter" data-level="7.2.3" data-path="section-7.html"><a href="section-7.html#section-7.2.3"><i class="fa fa-check"></i><b>7.2.3</b> 自变量误差</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="section-7.html"><a href="section-7.html#section-7.3"><i class="fa fa-check"></i><b>7.3</b> 数据划分和再抽样</a><ul>
<li class="chapter" data-level="7.3.1" data-path="section-7.html"><a href="section-7.html#section-7.3.1"><i class="fa fa-check"></i><b>7.3.1</b> 划分训练集和测试集</a></li>
<li class="chapter" data-level="7.3.2" data-path="section-7.html"><a href="section-7.html#section-7.3.2"><i class="fa fa-check"></i><b>7.3.2</b> 重抽样</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="section-7.html"><a href="section-7.html#-2"><i class="fa fa-check"></i><b>7.4</b> 本章总结</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="section-8.html"><a href="section-8.html"><i class="fa fa-check"></i><b>8</b> 模型评估度量</a><ul>
<li class="chapter" data-level="8.1" data-path="section-8.html"><a href="section-8.html#section-8.1"><i class="fa fa-check"></i><b>8.1</b> 回归模型评估度量</a></li>
<li class="chapter" data-level="8.2" data-path="section-8.html"><a href="section-8.html#section-8.2"><i class="fa fa-check"></i><b>8.2</b> 分类模型评估度量</a><ul>
<li class="chapter" data-level="8.2.1" data-path="section-8.html"><a href="section-8.html#kappa"><i class="fa fa-check"></i><b>8.2.1</b> Kappa统计量</a></li>
<li class="chapter" data-level="8.2.2" data-path="section-8.html"><a href="section-8.html#roc"><i class="fa fa-check"></i><b>8.2.2</b> ROC曲线</a></li>
<li class="chapter" data-level="8.2.3" data-path="section-8.html"><a href="section-8.html#section-8.2.3"><i class="fa fa-check"></i><b>8.2.3</b> 提升图</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="section-8.html"><a href="section-8.html#-3"><i class="fa fa-check"></i><b>8.3</b> 本章总结</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="section-9.html"><a href="section-9.html"><i class="fa fa-check"></i><b>9</b> 特征工程</a><ul>
<li class="chapter" data-level="9.1" data-path="section-9.html"><a href="section-9.html#section-9.1"><i class="fa fa-check"></i><b>9.1</b> 特征构建</a></li>
<li class="chapter" data-level="9.2" data-path="section-9.html"><a href="section-9.html#section-9.2"><i class="fa fa-check"></i><b>9.2</b> 特征提取</a><ul>
<li class="chapter" data-level="9.2.1" data-path="section-9.html"><a href="section-9.html#section-9.2.1"><i class="fa fa-check"></i><b>9.2.1</b> 初步探索数据</a></li>
<li class="chapter" data-level="9.2.2" data-path="section-9.html"><a href="section-9.html#section-9.2.2"><i class="fa fa-check"></i><b>9.2.2</b> 主成分分析</a></li>
<li class="chapter" data-level="9.2.3" data-path="section-9.html"><a href="section-9.html#section-9.2.3"><i class="fa fa-check"></i><b>9.2.3</b> 探索性因子分析</a></li>
<li class="chapter" data-level="9.2.4" data-path="section-9.html"><a href="section-9.html#section-9.2.4"><i class="fa fa-check"></i><b>9.2.4</b> 高维标度化</a></li>
<li class="chapter" data-level="9.2.5" data-path="section-9.html"><a href="section-9.html#section-9.2.5"><i class="fa fa-check"></i><b>9.2.5</b> 知识扩展</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="section-9.html"><a href="section-9.html#section-9.3"><i class="fa fa-check"></i><b>9.3</b> 特征选择</a><ul>
<li class="chapter" data-level="9.3.1" data-path="section-9.html"><a href="section-9.html#section-9.3.1"><i class="fa fa-check"></i><b>9.3.1</b> 过滤法</a></li>
<li class="chapter" data-level="9.3.2" data-path="section-9.html"><a href="section-9.html#section-9.3.2"><i class="fa fa-check"></i><b>9.3.2</b> 绕封法</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="section-10.html"><a href="section-10.html"><i class="fa fa-check"></i><b>10</b> 线性回归极其衍生</a><ul>
<li class="chapter" data-level="10.1" data-path="section-10.html"><a href="section-10.html#section-10.1"><i class="fa fa-check"></i><b>10.1</b> 普通线性回归</a><ul>
<li class="chapter" data-level="10.1.1" data-path="section-10.html"><a href="section-10.html#section-10.1.1"><i class="fa fa-check"></i><b>10.1.1</b> 最小二乘线性模型</a></li>
<li class="chapter" data-level="10.1.2" data-path="section-10.html"><a href="section-10.html#section-10.1.2"><i class="fa fa-check"></i><b>10.1.2</b> 回归诊断</a></li>
<li class="chapter" data-level="10.1.3" data-path="section-10.html"><a href="section-10.html#section-10.1.3"><i class="fa fa-check"></i><b>10.1.3</b> 离群点，高杠杆点和强影响点</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="section-10.html"><a href="section-10.html#section-10.2"><i class="fa fa-check"></i><b>10.2</b> 收缩方法</a><ul>
<li class="chapter" data-level="10.2.1" data-path="section-10.html"><a href="section-10.html#section-10.2.1"><i class="fa fa-check"></i><b>10.2.1</b> 岭回归</a></li>
<li class="chapter" data-level="10.2.2" data-path="section-10.html"><a href="section-10.html#lasso"><i class="fa fa-check"></i><b>10.2.2</b> Lasso</a></li>
<li class="chapter" data-level="10.2.3" data-path="section-10.html"><a href="section-10.html#section-10.2.3"><i class="fa fa-check"></i><b>10.2.3</b> 弹性网络</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="section-10.html"><a href="section-10.html#lasso"><i class="fa fa-check"></i><b>10.3</b> 知识扩展：Lasso的变量选择功能</a></li>
<li class="chapter" data-level="10.4" data-path="section-10.html"><a href="section-10.html#section-10.4"><i class="fa fa-check"></i><b>10.4</b> 主成分和偏最小二乘回归</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="glmnet.html"><a href="glmnet.html"><i class="fa fa-check"></i><b>11</b> 广义线性模型压缩方法及<code>glmnet</code>包</a><ul>
<li class="chapter" data-level="11.1" data-path="glmnet.html"><a href="glmnet.html#glmnet"><i class="fa fa-check"></i><b>11.1</b> 初识<code>glmnet</code></a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="section-12.html"><a href="section-12.html"><i class="fa fa-check"></i><b>12</b> 聚类判别分析</a><ul>
<li class="chapter" data-level="12.1" data-path="section-12.html"><a href="section-12.html#section-12.1"><i class="fa fa-check"></i><b>12.1</b> 聚类分析</a></li>
<li class="chapter" data-level="12.2" data-path="section-12.html"><a href="section-12.html#section-12.2"><i class="fa fa-check"></i><b>12.2</b> 判别分析</a><ul>
<li class="chapter" data-level="12.2.1" data-path="section-12.html"><a href="section-12.html#section-12.2.1"><i class="fa fa-check"></i><b>12.2.1</b> 逻辑回归</a></li>
<li class="chapter" data-level="12.2.2" data-path="section-12.html"><a href="section-12.html#section-12.2.2"><i class="fa fa-check"></i><b>12.2.2</b> 线性判别分析</a></li>
<li class="chapter" data-level="12.2.3" data-path="section-12.html"><a href="section-12.html#section-12.2.3"><i class="fa fa-check"></i><b>12.2.3</b> 最小二乘判别分析</a></li>
<li class="chapter" data-level="12.2.4" data-path="section-12.html"><a href="section-12.html#section-12.2.4"><i class="fa fa-check"></i><b>12.2.4</b> 朴素贝叶斯</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="section-12.html"><a href="section-12.html#section-12.3"><i class="fa fa-check"></i><b>12.3</b> 案例：客户分组</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="section-13.html"><a href="section-13.html"><i class="fa fa-check"></i><b>13</b> 树模型</a><ul>
<li class="chapter" data-level="13.1" data-path="section-13.html"><a href="section-13.html#section-13.1"><i class="fa fa-check"></i><b>13.1</b> 分裂准则</a></li>
<li class="chapter" data-level="13.2" data-path="section-13.html"><a href="section-13.html#section-13.2"><i class="fa fa-check"></i><b>13.2</b> 树模型的参数</a></li>
<li class="chapter" data-level="13.3" data-path="section-13.html"><a href="section-13.html#section-13.3"><i class="fa fa-check"></i><b>13.3</b> 装袋树</a></li>
<li class="chapter" data-level="13.4" data-path="section-13.html"><a href="section-13.html#section-13.4"><i class="fa fa-check"></i><b>13.4</b> 随机森林</a></li>
<li class="chapter" data-level="13.5" data-path="section-13.html"><a href="section-13.html#section-13.5"><i class="fa fa-check"></i><b>13.5</b> 其它树话题</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="section-14.html"><a href="section-14.html"><i class="fa fa-check"></i><b>14</b> 深度学习</a><ul>
<li class="chapter" data-level="14.1" data-path="section-6.html"><a href="section-6.html#-1"><i class="fa fa-check"></i><b>14.1</b> 介绍</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>15</b> References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">数据科学家：R语言</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="section-13" class="section level1">
<h1><span class="header-section-number">第13章</span> 树模型</h1>
<p>树模型可以用于回归和判别。这类模型常被称为决策和回归树（CART：Classification And Regression Trees），是经典的机器学习算法，也是最广泛使用的工具。和线性模型相比，树模型能够较好的捕捉到非线性关系。</p>
<p>CART可以用来指代广义的树模型，有时也特别指代Breiman最初提出的建立分类回归树的算法<span class="citation">(al <a href="#ref-Breiman1984">1984</a>)</span>。在Breiman之后又有很多新的算法出现，比如ID3、C4.5和C5.0，这几个都是Ross Quinlan提出的。是C4.5的改进版本，但由于C5.0还没有开源，因此C4.5算法依然非常流行。原始的CART算法只针对二分类的应变量，但是C4.5能处理多分类结果。CART使用Gini系数准则，C4.5使用熵。CART通过复杂性成本模型来对树进行剪枝，通过交互校验来估计参数；C4.5使用的是一个从二项置信区间衍生处的单通算法。在对缺失值的处理上，CART使用的是代理切分。对于每一个切分， 模型会计算一系列的备选方案（称为代理切分）。代理切分是指与树中实际切分结果相类似的备选切分方案，如果一个代理切分对原始切分的近似效果良好，那么当原始切分的预测变量有缺失值时，代理切分就可以发挥作用。在实际应用中，树中的每一个切分都可以事先计算一些代理切分。C4.5算法不是直接填充缺失值或寻找替代，而是用概率知识把信息增益率的求解作些变化。</p>
<p>不太大的决策树简洁明了，容易解释。但是简单的树表现不一定好，复杂的树（比如之后会讲到的集成方法）效果大为提高，但过程如同黑箱，无法解释。所以建模者需要在解释性和精确性之间进行权衡。之所以称其为“树”当然因为结构有类似之处，只是决策树的方向和真实的树相反，根在上，叶在下。一棵决策树从单个根节点开始划分为几个不同的枝桠，从⽽产生更多的节点,在每个节点都可以决定是不是要继续划分，如果停止则该节点就是叶节点，若继续就是枝节点接着分裂产生下一层新节点。每个⾮叶节点都牵扯到决定接下来选择哪根树枝。叶节点包含 最终“决策”，将该样本归于哪个最终类或者取值。树模型中有如下几个重要的定义：</p>
<div class="figure">
<img src="http://scientistcafe.com/book/Figure/tree.png" alt="男神分类器" />
<p class="caption">男神分类器</p>
</div>
<ul>
<li>分类树：用于预测离散型结果的树</li>
<li>回归树：用于预测连续型结果的树</li>
<li>分裂点：每个非叶节点的处都有一个分裂点用来决定样本的走向（如：年龄&lt;=25；25<年龄<=35；年龄>35）</li>
<li>根节点：最开始的包含所有观测的节点</li>
<li>叶节点（或者终节点）：包含样本的最终“决策”，没有分裂点</li>
<li>节点的度：一个节点含有的子树的个数称为该节点的度；</li>
<li>树的度：一棵树中，最大的节点的度称为树的度；</li>
<li>修剪：移除一些不必要的分裂点的过程，这是和分裂相反的过程</li>
<li>树枝（或子树）：非终节点下的一个完整旁支</li>
<li>亲节点和子节点：亲节点分裂后得到子节点</li>
</ul>
<p>比如上面的简易男神分类器（分类器纯属虚构，如有雷同，纯属巧合）：<strong>长相</strong>是<strong>根节点</strong>同时也是<strong>分裂点</strong>；<strong>年龄</strong>和<strong>经济状况</strong>是分裂点; <strong>长相</strong>这个节点的<strong>度</strong>是2；整棵<strong>树的度</strong>是4；最后<strong>小鲜肉</strong>等所在的那些节点是<strong>终节点</strong>；如果我们将<strong>王XX</strong>那个枝桠移除，就是<strong>修剪</strong>；<strong>长相</strong>是<strong>年龄</strong>和<strong>经济状况</strong>的<strong>亲节点</strong>，反之<strong>年龄</strong>和<strong>经济状况</strong>是<strong>长相</strong>的<strong>子节点</strong>。</p>
<p>树模型和规则模型之所以成为非常流行的建模工具有下面几个原因：</p>
<ol style="list-style-type: decimal">
<li>能处理变量个数相对于观测个数很大情况</li>
<li>对冗余变量具有抗性</li>
<li>小型的树非常容易解释（但一旦树结构复杂或者使用集成方法时模型变得无法解释）</li>
<li>根据它们建立模型时采用的逻辑，它们能有效地处理各种类型的预测变量（稀疏的，偏态的，连续的，分类的，等等），而不需要对这些变量事先进行预处理。此外，这些模型没有任何变量分布的假设，而在回归模型中通常有分布假设</li>
<li>这些模型可以有效地处理缺失值，并内嵌有特征选择的功能，这一点在很多实际建模问题中非常有用</li>
</ol>
<p>单棵树或简单规则模型具有局限性，其中最重要的就是模型不稳定以及预测能力比较差。数据中的微小变动可能会引起树或规则结构的巨大变化。单棵树定义的一系列的矩形区域过于简单，一旦因变量与预测变量之间的关系不能充分地通过矩形子空间来进行表达， 那么树模型和规则模型将产生比其他模型更大的预测误差。为了克服这些缺点，研究者提出了集成模型，它们将许多棵树（或规则）进行组合。集成模型通常具有比单一的树模型好得多的预测表现。我们在之后会做介绍。</p>
<div id="section-13.1" class="section level2">
<h2><span class="header-section-number">13.1</span> 分裂准则</h2>
<p>分裂节点的方式能够极大的影响树的精确度。回归树和分类树所使用的分裂准则非常不同。与回归树一样，分类树的目标是把数据划分为更小、同质性更强的组。在这里同质意味着分裂的节点更纯（即在每个节点有一个类的样本比例很大）。最原始的CART算法使用基尼系数作为分裂准则；ID3、C4.5以及C5.0使用交叉熵，也被称为信息或散度作为标准。下面逐一介绍这三个标准。</p>
<p><strong>Gini系数</strong></p>
<p>基尼（Gini）系数<span class="citation">(al <a href="#ref-Breiman1984">1984</a>)</span>用来衡量一个集合样本的杂质（我们希望的是更高的纯度，更少的杂质）。对于二分类问题，给定节点的基尼系数定义为：</p>
<p><span class="math display">\[p_{1}(1-p_{1})+p_{2}(1-p_{2})\]</span></p>
<p>其中<span class="math inline">\(p_{1}\)</span>和<span class="math inline">\(p_{2}\)</span>分别为类1、类2的概率。不难看出，当集合样本纯度很高时，某一类概率趋近于0，基尼系数最小。相反，当<span class="math inline">\(p_{1}=p_{2}=0.5\)</span>时基尼系数最大，在这种情况下，节点的纯度最小。我们来看一个例子，假设我们想要判定哪些学生是计算机专业的，下面是用性别这个变量得到的简单分类树结果。</p>
<div class="figure">
<img src="http://scientistcafe.com/book/Figure/gini.png" alt="用性别判定计算机专业的学生" />
<p class="caption">用性别判定计算机专业的学生</p>
</div>
<p>现在我们来计算用性别变量划分对应的Gini系数：</p>
<ol style="list-style-type: decimal">
<li>计算女生对应的Gini系数=<span class="math inline">\(\frac{1}{6}\times\frac{5}{6}+\frac{5}{6}\times\frac{1}{6}=\frac{5}{18}\)</span></li>
<li>计算男生对应的Gini系数=<span class="math inline">\(0\times1+1\times 0=0\)</span></li>
</ol>
<p>可以用如下加权计算性别这个分裂总体的Gini系数：</p>
<p><span class="math display">\[\frac{3}{5}\times\frac{5}{18}+\frac{2}{5}\times 0=\frac{1}{6}\]</span></p>
<p>亲节点中50个观测的Gini系数是：<span class="math inline">\(\frac{1}{2}\)</span>。很容易得出，经过性别这个分裂点后Gini系数从原来的<span class="math inline">\(\frac{1}{2}\)</span>降至<span class="math inline">\(\frac{1}{6}\)</span>，该分裂点对降低Gini的贡献就是<span class="math inline">\(\frac{1}{3}\)</span>。算法会选择对Gini系数减小最有效的的分裂。</p>
<p><strong>信息增益</strong></p>
<p>看下面二分类问题中三个节点内的样本，哪个最容易描述？显然是C，因为C中所有的样本都是一类的，这样描述起来需要的信息最少。相反B需要更多的信息，A需要的信息最多。换句话说，C的纯度最高，B次之，A的纯度最差。我们可以这么说，纯度更高的节点描述起来需要的信息更少。反之，纯度更低的节点描述起来需要的信息更多。</p>
<div class="figure">
<img src="http://scientistcafe.com/book/Figure/infogain.png" />

</div>
<p>信息理论就是为了衡量系统的无序度的，无序的度量也叫做熵。如果某个节点对应的样本全都是一类（如C）,其熵就是0。如果某个节点中各类样本比例是50%－50%，那么熵就是1。熵自然是越小越好。</p>
<p>熵（Entropy）的计算公式如下：</p>
<p><span class="math display">\[Entropy=-plog_{2}p-(1-p)log_{2}(1-p)\]</span></p>
<p>这里p是其中一类样本的比例。熵也能用来分裂分类树节点。其选择和亲节点以及其它分裂点相比熵最小的那个分裂。计算某个分裂的熵也是对每个分裂后子节点熵的加权平均。比如之前对50个学生的分类树：</p>
<ul>
<li>亲节点中50个学生对应的熵是：<span class="math inline">\(-\frac{25}{50}log_{2}\frac{25}{50}-\frac{25}{50}log_{2}\frac{25}{50}=1\)</span>，这里1表明节点纯度是最低的，也就是各类样本占一半。</li>
<li>对应性别这个分裂的熵计算分为3步：
<ol style="list-style-type: decimal">
<li>女生对应的熵：<span class="math inline">\(-\frac{5}{30}log_{2}\frac{5}{30}-\frac{25}{30}log_{2}\frac{25}{30}=0.65\)</span></li>
<li>男生对应的熵为0，因为<span class="math inline">\(p=1\)</span></li>
<li>性别这个分裂对应的熵为上述两个的加权平均：<span class="math inline">\(\frac{3}{5}\times 0.65+\frac{2}{5}\times 0=0.39\)</span></li>
</ol></li>
</ul>
<p>可以看到，分裂将原先的熵从1降到0.39。</p>
<p><strong>最小化SSE</strong></p>
<p>目前为止我们讨论了对于分类树（离散应变量）情况的分裂准则。下面我们介绍回归树使用的分裂准则。构建回归树有许多不同的准则，其中最古老也最常用的是最小化SSE。对于回归问题，假设要将数据集<span class="math inline">\(S\)</span>分成两组<span class="math inline">\(S_{1}\)</span>和<span class="math inline">\(S_{2}\)</span>，其中<span class="math inline">\(S_{1}\)</span>和<span class="math inline">\(S_{2}\)</span>的选取需要使得整体的误差平方和达到最小：</p>
<p><span class="math display">\[SSE=\Sigma_{i\in S_{1}}(y_{i}-\bar{y}_{1})^{2}+\Sigma_{i\in S_{2}}(y_{i}-\bar{y}_{2})^{2}\]</span></p>
<p>式中<span class="math inline">\(\bar{y}_{1}\)</span>和<span class="math inline">\(\bar{y}_{1}\)</span>是<span class="math inline">\(S_{1}\)</span>和<span class="math inline">\(S_{2}\)</span>组内训练集因变量的的平均值。接下来分别在<span class="math inline">\(S_{1}\)</span>和<span class="math inline">\(S_{2}\)</span>中， 模型继续搜索预测变量和切分点，以使得SSE达到最大的缩减。由于回归树的这一过程本质上是递归的切分，因此这种方法也通常称为递归划分。</p>
<p>看看下面这个简单的回归树，对10个学生以性别为分裂点，对身高做回归：</p>
<div class="figure">
<img src="http://scientistcafe.com/book/Figure/var.png" />

</div>
<ol style="list-style-type: decimal">
<li>女生对应身高测量的SSE为：136</li>
<li>男生对应身高测量的SSE为：32</li>
<li>性别这个分裂对应的SSE为这两个SSE之和：168</li>
</ol>
<p>亲节点中10个观测的SSE是：522.9。经过性别这个分裂点后SSE从原来的522.9降至168。</p>
<p>如果还有另外一种可能的分裂方式，用专业来划分，结果如下：</p>
<div class="figure">
<img src="http://scientistcafe.com/book/Figure/var2.png" />

</div>
<p>这种情况下：</p>
<ol style="list-style-type: decimal">
<li>女生对应身高测量的方差为：184</li>
<li>男生对应身高测量方差为：302.8</li>
<li>专业这个分裂对应的方差为之前两个方差的加权和：486.8</li>
</ol>
<p>比较性别和专业两个不同的分裂，之前性别这个分裂点将SSE从原来的522.9降至168；专业这个分裂点将SSE降至486.8。如果用最小化SSE准则，应该选择使用性别为分裂点。</p>
<p>上面提到的这三种分裂准则是建立树模型的基础。</p>
</div>
<div id="section-13.2" class="section level2">
<h2><span class="header-section-number">13.2</span> 树模型的参数</h2>
<p>树模型面对的主要挑战是过度拟合。假设我们对决策树的参数没有任何限制，那么得到的树模型在训练集上的准确度将会是100%，因为每个终结点将只对应一个样本。因此，防止过度拟合是创建树模型的关键所在。总的说来可以通过下面两种方法来实现这一点：</p>
<ol style="list-style-type: decimal">
<li>对树的大小进行限制</li>
<li>对树进行修剪 现在我们对上面两点逐一展开。</li>
</ol>
<p><strong>限制树的大小</strong></p>
<p>可以通过一些参数来限制树的大小。</p>
<ul>
<li>每个节点处的最小样本量：通过定义节点处的最小样本量可以防止终结点只有一个样本的情况。这里样本量的设置可以作为调优参数。如果设置的样本量太大，那么会导致拟合不足，如果样本量太小，又会过度拟合。在严重类失衡的情况下，最小样本量的设置可能需要小一些，因为某一类样本本身数目就很少。</li>
<li>最大树深度：如果树生长的过深，那么模型就会过度拟合特定的样本。这也是一个需要调优的参数。</li>
<li>最大终结点数目：对终结点数目的限制和对树深度的限制是类似的，可以取代使用。因为这两者是成正比的。 每个分裂考虑的变量个数：在每一层级寻找最优分裂点的时候使用的变量是随机抽取的。通常情况下，取变量个数的平方根效果最好，这也是R种函数的默认设置。但我们还是应该对该参数在变量个数的30%~40%区间内调优。</li>
</ul>
<p><strong>树的修剪</strong></p>
<p>另外一种方法是对完整生长后的树进行剪枝，以回到一个较小的深度。Breiman等的论文中采用的剪枝过程称为代价-复杂度调优<span class="citation">(al <a href="#ref-Breiman1984">1984</a>)</span>。也就是在原来SSE的基础上加上一个关于终节点数目的罚函数，通过一个调优参数控制罚函数的权重：</p>
<p><span class="math display">\[SSE_{c_{p}}=SSE+c_{p}\times 终节点数目\]</span></p>
<p>其中<span class="math inline">\(c_{p}\)</span>被称为复杂度参数。该方法对于一个给定的<span class="math inline">\(c_{p}\)</span>值，我们希望寻找最小的剪枝后的树，以使得惩罚后的误差达到最小。 给定<span class="math inline">\(c_{p}\)</span>的取值时，Breiman等给出了寻找最优树的理论和算法<span class="citation">(al <a href="#ref-Breiman1984">1984</a>)</span>。与之前讨论过的收缩方法类似，较小的罚倾向于产生较大的树。<span class="math inline">\(c_{p}\)</span>取值很大时生成的树可能只有一次分裂，甚至根本没有分裂。后一种情况意味着在当前选择的复杂度参数下， 没有任何一个预测变量能充分地解释因变量的变异。</p>
<p>为了找到最优的剪枝树，需要在一系列的<span class="math inline">\(c_{p}\)</span>取值上对数据进行计算， 这一过程会对每一个<span class="math inline">\(c_{p}\)</span>值计算一个 SSE。但我们知道的是，当选择了一个不同的样本时，SSE的数值也会有所变化。为了体现每一个<span class="math inline">\(c_{p}\)</span>取值下 SSE 的变异，Breiman等建议使用类似于第四章中的交叉验证方法<span class="citation">(al <a href="#ref-Breiman1984">1984</a>)</span>。他们还提出了一倍标准差准则作为优化准则，来给出最简单的树：在一倍的标准差之内，找到最简单的使得绝对误差最小的树。另外一些方法则是选择使得数值上误差达到最小的树尺寸<span class="citation">(Hastie T <a href="#ref-Hastie2008">2008</a>)</span>。</p>
</div>
<div id="section-13.3" class="section level2">
<h2><span class="header-section-number">13.3</span> 装袋树</h2>
<p>Bootstrap 样本是对数据进行有放回随机抽样得到的样本（Efron 和 Tibshirani 1986）。这意味着，当一个样本点被选中时，它有可能会在将来的抽取中继续被选中。Bootstrap 样本和原数据的样本量一样。因此，一些样本可能被抽到过很多次，而另一些则可能没有被选到。没有被选到的样本被称为“袋外样本“（out-of-bag）。在一次 Bootstrap 重抽样迭代中，选中的样本点被用来建立模型，而袋外样本则被用于预测。 在20世纪90年代，集成方法（即将许多模型组合起来进行预测的模型）开始出现。 装袋法（Bagging，bootstrap aggregation 的缩写）最初由 Leo Breiman 提出，它是最早发展起来的集成方法之一（Breiman 1996a）。 装袋法是一种利用 bootstrap的通用方法，可用于任何回归（或分类）模型来构建集成组合。这种方法的构建非常简单，它包含算法 8.1 中所述的步骤。 集成组合中的每一个模型都对新样本进行一次预测，然后这 个预测将进行平均， 来给出装袋法模型的预测值。</p>
<blockquote>
<p><strong>算法：装袋法</strong></p>
<ol style="list-style-type: decimal">
<li>对 i=1 到 m 执行</li>
<li>从原数据中生成bootstrap样本</li>
<li>在生成的bootstrap样本上建立未修剪的树</li>
<li>终止</li>
</ol>
</blockquote>
<p>装袋法模型相对于没有装袋的模型具有若干优势。首先， 装袋法通过模型的聚集过程有效地降低了预测的方差（参见之前关于偏差-方差权衡的讨论）。对于那些预测值不稳定的模型，例如回归树，将不同版本的训练集进行聚集，可以减小预测的方差，从而使得预测值更加稳定。假设我们有10个 bootstrap 样本各自生成了一棵最大深度的树。这些树在结构上有所差异，每棵树对新样本的预测都有所不同。 如果将这10棵树的预测结果进行平均作为新样本的预测， 那么这一平均值将比单棵树的预测方差更小。这意味着， 如果我们产生另一组 bootstrap 样本，在其中每一个样本上建立一个模型， 然后对所有模型的预测值进行平均，那么得到的结果将与前一个装袋模型的结果相类似。这一特性还使得装袋模型比未装袋的模型具有更好的预测效能。 如果建模的目标是得到最优的估计而不是解释树的结构，那么装袋法更有优势。</p>
<p>另一方面，对稳定、方差小的模型（如回归，MARS）进行装袋则只会对其预测效能带来较小的改进。 在预测结果具有内在的不稳定性的情况下，可以用装袋法进行改进。</p>
<p>装袋法模型的另一个优势是它可以提供内在的预测效能估计， 而且这一估计可以与交叉验证估计或测试集估计很好地对应上。原因如下： 构建集成组合中的每个 bootstrap 样本时，会有部分的观测被排除在外。 这部分样本称为袋外样本，它们扮演着测试集的角色，可以用来评估模型的预测效能， 因为这批样本并没有参与建模。因此，集成组合中的每个模型都可以通过袋外样本计算得到一个预测效能的估计， 而对所有袋外效能估计进行平均就能计算出整个集成组合的预测效能。 这一结果通常与交叉验证或测试集验证的结果非常吻合， 该误差估计称为袋外估计。</p>
<p>对于基本的装袋法，用户可以选择bootstrap样本的数目。 通常模型的预测效能与迭代次数之间会出现指数递减的关系； 大部分预测效能的提升是由少数几棵树实现的。 ［加一个例子，］。《应用预测模型（Applied Predictive Modeling）》<span class="citation">(Max Kuhn <a href="#ref-APM">2013</a>)</span>的作者指出，根据他们的经验，一直到50个bootstrap样本，模型都可能还会有微小的改进， 如果迭代了50次代后模型的表现还是不令人满意，那就需要尝试使用其他更强大的集成预测方法，如随机森林和 boosting。</p>
<p>尽管装袋法通常都能改进不稳定模型的预测效能，但它同样有一些缺陷。 首先，是计算量。随着 bootstrap 样本数目的增多，计算成本和内存需求也会相应增加。这一劣势可以通过并行计算来得到大部分的消减，原因是装袋的过程是非常容易并行化的。 回顾之前的介绍可以发现，每一个 boostrap 样本和相应的模型都是独立于其他样本和模型的。 这意味着每个模型都可以单独进行建模， 而只需在最后将所有模型的结果组合起来以生成最终的预测。</p>
<p>装袋法的另一个劣势在于解释性差， 我们无法在装袋法中得到之前单棵回归树给出的简洁的规则。然而，变量重要性依旧可以通过将单个模型的重要性得分组合起来进行构建。 下一节介绍的随机森林将进一步讨论变量重要性。</p>
</div>
<div id="section-13.4" class="section level2">
<h2><span class="header-section-number">13.4</span> 随机森林</h2>
</div>
<div id="section-13.5" class="section level2">
<h2><span class="header-section-number">13.5</span> 其它树话题</h2>

</div>
</div>
<h3> References</h3>
<div id="refs" class="references">
<div id="ref-Breiman1984">
<p>al, Leo Breiman et. 1984. <em>Classification and Regression Trees.</em> ISBN 978-0-412-04841-8. Monterey, CA: Wadsworth &amp; Brooks/Cole Advanced Books &amp; Software.</p>
</div>
<div id="ref-Hastie2008">
<p>Hastie T, Friedman J, Tibshirani R. 2008. <em>The Elements of Statistical Learning: Data Mining, Inference and Prediction</em>. 2nd ed. Springer.</p>
</div>
<div id="ref-APM">
<p>Max Kuhn, Kjell Johnston. 2013. <em>Applied Predictive Modeling</em>. Springer.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="section-12.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="section-14.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/13-shumoxing.Rmd",
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
