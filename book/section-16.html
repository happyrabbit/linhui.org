<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>数据科学家：R语言</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="This is my first book on data science">
  <meta name="generator" content="bookdown 0.1 and GitBook 2.6.7">

  <meta property="og:title" content="数据科学家：R语言" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is my first book on data science" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="数据科学家：R语言" />
  
  <meta name="twitter:description" content="This is my first book on data science" />
  

<meta name="author" content="林荟">

<meta name="date" content="2017-04-23">

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="section-15.html">
<link rel="next" href="section-17.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">数据科学家：R语言</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="section-1.html"><a href="section-1.html"><i class="fa fa-check"></i><b>1</b> 作者自序</a></li>
<li class="chapter" data-level="2" data-path="section-2.html"><a href="section-2.html"><i class="fa fa-check"></i><b>2</b> 序言</a></li>
<li class="chapter" data-level="3" data-path="-1.html"><a href="-1.html"><i class="fa fa-check"></i><b>3</b> 序言</a></li>
<li class="chapter" data-level="4" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>4</b> 介绍</a></li>
<li class="chapter" data-level="5" data-path="section-5.html"><a href="section-5.html"><i class="fa fa-check"></i><b>5</b> 数据科学</a><ul>
<li class="chapter" data-level="5.1" data-path="section-5.html"><a href="section-5.html#section-5.1"><i class="fa fa-check"></i><b>5.1</b> 什么是数据科学？</a></li>
<li class="chapter" data-level="5.2" data-path="section-5.html"><a href="section-5.html#section-5.2"><i class="fa fa-check"></i><b>5.2</b> 什么是数据科学家？</a></li>
<li class="chapter" data-level="5.3" data-path="section-5.html"><a href="section-5.html#section-5.3"><i class="fa fa-check"></i><b>5.3</b> 数据科学家需要的技能</a></li>
<li class="chapter" data-level="5.4" data-path="section-5.html"><a href="section-5.html#section-5.4"><i class="fa fa-check"></i><b>5.4</b> 数据科学算法总结</a></li>
<li class="chapter" data-level="5.5" data-path="section-5.html"><a href="section-5.html#section-5.5"><i class="fa fa-check"></i><b>5.5</b> 数据科学可以解决什么问题？</a><ul>
<li class="chapter" data-level="5.5.1" data-path="section-5.html"><a href="section-5.html#section-5.5.1"><i class="fa fa-check"></i><b>5.5.1</b> 前提要求</a></li>
<li class="chapter" data-level="5.5.2" data-path="section-5.html"><a href="section-5.html#section-5.5.2"><i class="fa fa-check"></i><b>5.5.2</b> 问题种类</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="section-6.html"><a href="section-6.html"><i class="fa fa-check"></i><b>6</b> 数据集模拟和背景介绍</a><ul>
<li class="chapter" data-level="6.1" data-path="section-6.html"><a href="section-6.html#section-6.1"><i class="fa fa-check"></i><b>6.1</b> 服装消费者数据</a></li>
<li class="chapter" data-level="6.2" data-path="section-6.html"><a href="section-6.html#section-6.2"><i class="fa fa-check"></i><b>6.2</b> 航空公司满意度调查</a></li>
<li class="chapter" data-level="6.3" data-path="section-6.html"><a href="section-6.html#section-6.3"><i class="fa fa-check"></i><b>6.3</b> 生猪疫情风险预测数据</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="section-7.html"><a href="section-7.html"><i class="fa fa-check"></i><b>7</b> 数据分析一般流程</a><ul>
<li class="chapter" data-level="7.1" data-path="section-7.html"><a href="section-7.html#section-7.1"><i class="fa fa-check"></i><b>7.1</b> 问题到数据</a></li>
<li class="chapter" data-level="7.2" data-path="section-7.html"><a href="section-7.html#section-7.2"><i class="fa fa-check"></i><b>7.2</b> 数据到信息</a></li>
<li class="chapter" data-level="7.3" data-path="section-7.html"><a href="section-7.html#section-7.3"><i class="fa fa-check"></i><b>7.3</b> 信息到行动</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="section-8.html"><a href="section-8.html"><i class="fa fa-check"></i><b>8</b> 数据预处理</a><ul>
<li class="chapter" data-level="8.1" data-path="section-8.html"><a href="section-8.html#section-8.1"><i class="fa fa-check"></i><b>8.1</b> 介绍</a></li>
<li class="chapter" data-level="8.2" data-path="section-8.html"><a href="section-8.html#section-8.2"><i class="fa fa-check"></i><b>8.2</b> 数据清理</a></li>
<li class="chapter" data-level="8.3" data-path="section-8.html"><a href="section-8.html#section-8.3"><i class="fa fa-check"></i><b>8.3</b> 缺失值填补</a><ul>
<li class="chapter" data-level="8.3.1" data-path="section-8.html"><a href="section-8.html#section-8.3.1"><i class="fa fa-check"></i><b>8.3.1</b> 中位数或众数填补</a></li>
<li class="chapter" data-level="8.3.2" data-path="section-8.html"><a href="section-8.html#k-"><i class="fa fa-check"></i><b>8.3.2</b> K-近邻填补</a></li>
<li class="chapter" data-level="8.3.3" data-path="section-8.html"><a href="section-8.html#section-8.3.3"><i class="fa fa-check"></i><b>8.3.3</b> 袋状树填补</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="section-8.html"><a href="section-8.html#section-8.4"><i class="fa fa-check"></i><b>8.4</b> 中心化和标量化</a></li>
<li class="chapter" data-level="8.5" data-path="section-8.html"><a href="section-8.html#section-8.5"><i class="fa fa-check"></i><b>8.5</b> 有偏分布</a></li>
<li class="chapter" data-level="8.6" data-path="section-8.html"><a href="section-8.html#section-8.6"><i class="fa fa-check"></i><b>8.6</b> 处理离群点</a></li>
<li class="chapter" data-level="8.7" data-path="section-8.html"><a href="section-8.html#section-8.7"><i class="fa fa-check"></i><b>8.7</b> 共线性</a></li>
<li class="chapter" data-level="8.8" data-path="section-8.html"><a href="section-8.html#section-8.8"><i class="fa fa-check"></i><b>8.8</b> 稀疏变量</a></li>
<li class="chapter" data-level="8.9" data-path="section-8.html"><a href="section-8.html#section-8.9"><i class="fa fa-check"></i><b>8.9</b> 编码名义变量</a></li>
<li class="chapter" data-level="8.10" data-path="section-8.html"><a href="section-8.html#section-8.10"><i class="fa fa-check"></i><b>8.10</b> 本章总结</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="section-9.html"><a href="section-9.html"><i class="fa fa-check"></i><b>9</b> 数据操作</a><ul>
<li class="chapter" data-level="9.1" data-path="section-9.html"><a href="section-9.html#section-9.1"><i class="fa fa-check"></i><b>9.1</b> 数据读写</a><ul>
<li class="chapter" data-level="9.1.1" data-path="section-9.html"><a href="section-9.html#tibble"><i class="fa fa-check"></i><b>9.1.1</b> 取代传统数据框的<code>tibble</code>对象</a></li>
<li class="chapter" data-level="9.1.2" data-path="section-9.html"><a href="section-9.html#readr"><i class="fa fa-check"></i><b>9.1.2</b> 高效数据读写：<code>readr</code>包</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="section-9.html"><a href="section-9.html#section-9.2"><i class="fa fa-check"></i><b>9.2</b> 数据整合</a><ul>
<li class="chapter" data-level="9.2.1" data-path="section-9.html"><a href="section-9.html#baseapply"><i class="fa fa-check"></i><b>9.2.1</b> base包：apply()</a></li>
<li class="chapter" data-level="9.2.2" data-path="section-9.html"><a href="section-9.html#plyrddply"><i class="fa fa-check"></i><b>9.2.2</b> plyr包：ddply()函数</a></li>
<li class="chapter" data-level="9.2.3" data-path="section-9.html"><a href="section-9.html#dplyr"><i class="fa fa-check"></i><b>9.2.3</b> dplyr包</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="section-9.html"><a href="section-9.html#section-9.3"><i class="fa fa-check"></i><b>9.3</b> 数据整形</a><ul>
<li class="chapter" data-level="9.3.1" data-path="section-9.html"><a href="section-9.html#reshape2"><i class="fa fa-check"></i><b>9.3.1</b> <code>reshape2</code>包</a></li>
<li class="chapter" data-level="9.3.2" data-path="section-9.html"><a href="section-9.html#tidyr"><i class="fa fa-check"></i><b>9.3.2</b> <code>tidyr</code>包</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="-1.html"><a href="-1.html#-1"><i class="fa fa-check"></i><b>9.4</b> 本章总结</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="section-10.html"><a href="section-10.html"><i class="fa fa-check"></i><b>10</b> 基础建模技术</a><ul>
<li class="chapter" data-level="10.1" data-path="section-10.html"><a href="section-10.html#section-10.1"><i class="fa fa-check"></i><b>10.1</b> 有监督和无监督</a></li>
<li class="chapter" data-level="10.2" data-path="section-10.html"><a href="section-10.html#section-10.2"><i class="fa fa-check"></i><b>10.2</b> 误差及其来源</a><ul>
<li class="chapter" data-level="10.2.1" data-path="section-10.html"><a href="section-10.html#section-10.2.1"><i class="fa fa-check"></i><b>10.2.1</b> 系统误差和随机误差</a></li>
<li class="chapter" data-level="10.2.2" data-path="section-10.html"><a href="section-10.html#section-10.2.2"><i class="fa fa-check"></i><b>10.2.2</b> 应变量误差</a></li>
<li class="chapter" data-level="10.2.3" data-path="section-10.html"><a href="section-10.html#section-10.2.3"><i class="fa fa-check"></i><b>10.2.3</b> 自变量误差</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="section-10.html"><a href="section-10.html#section-10.3"><i class="fa fa-check"></i><b>10.3</b> 数据划分和再抽样</a><ul>
<li class="chapter" data-level="10.3.1" data-path="section-10.html"><a href="section-10.html#section-10.3.1"><i class="fa fa-check"></i><b>10.3.1</b> 划分训练集和测试集</a></li>
<li class="chapter" data-level="10.3.2" data-path="section-10.html"><a href="section-10.html#section-10.3.2"><i class="fa fa-check"></i><b>10.3.2</b> 重抽样</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="section-10.html"><a href="section-10.html#-2"><i class="fa fa-check"></i><b>10.4</b> 本章总结</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="section-11.html"><a href="section-11.html"><i class="fa fa-check"></i><b>11</b> 模型评估度量</a><ul>
<li class="chapter" data-level="11.1" data-path="section-11.html"><a href="section-11.html#section-11.1"><i class="fa fa-check"></i><b>11.1</b> 回归模型评估度量</a></li>
<li class="chapter" data-level="11.2" data-path="section-11.html"><a href="section-11.html#section-11.2"><i class="fa fa-check"></i><b>11.2</b> 分类模型评估度量</a><ul>
<li class="chapter" data-level="11.2.1" data-path="section-11.html"><a href="section-11.html#kappa"><i class="fa fa-check"></i><b>11.2.1</b> Kappa统计量</a></li>
<li class="chapter" data-level="11.2.2" data-path="section-11.html"><a href="section-11.html#roc"><i class="fa fa-check"></i><b>11.2.2</b> ROC曲线</a></li>
<li class="chapter" data-level="11.2.3" data-path="section-11.html"><a href="section-11.html#section-11.2.3"><i class="fa fa-check"></i><b>11.2.3</b> 提升图</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="section-11.html"><a href="section-11.html#-3"><i class="fa fa-check"></i><b>11.3</b> 本章总结</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="section-12.html"><a href="section-12.html"><i class="fa fa-check"></i><b>12</b> 特征工程</a><ul>
<li class="chapter" data-level="12.1" data-path="section-12.html"><a href="section-12.html#section-12.1"><i class="fa fa-check"></i><b>12.1</b> 特征构建</a></li>
<li class="chapter" data-level="12.2" data-path="section-12.html"><a href="section-12.html#section-12.2"><i class="fa fa-check"></i><b>12.2</b> 特征提取</a><ul>
<li class="chapter" data-level="12.2.1" data-path="section-12.html"><a href="section-12.html#section-12.2.1"><i class="fa fa-check"></i><b>12.2.1</b> 初步探索数据</a></li>
<li class="chapter" data-level="12.2.2" data-path="section-12.html"><a href="section-12.html#section-12.2.2"><i class="fa fa-check"></i><b>12.2.2</b> 主成分分析</a></li>
<li class="chapter" data-level="12.2.3" data-path="section-12.html"><a href="section-12.html#section-12.2.3"><i class="fa fa-check"></i><b>12.2.3</b> 探索性因子分析</a></li>
<li class="chapter" data-level="12.2.4" data-path="section-12.html"><a href="section-12.html#section-12.2.4"><i class="fa fa-check"></i><b>12.2.4</b> 高维标度化</a></li>
<li class="chapter" data-level="12.2.5" data-path="section-12.html"><a href="section-12.html#section-12.2.5"><i class="fa fa-check"></i><b>12.2.5</b> 知识扩展</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="section-12.html"><a href="section-12.html#section-12.3"><i class="fa fa-check"></i><b>12.3</b> 特征选择</a><ul>
<li class="chapter" data-level="12.3.1" data-path="section-12.html"><a href="section-12.html#section-12.3.1"><i class="fa fa-check"></i><b>12.3.1</b> 过滤法</a></li>
<li class="chapter" data-level="12.3.2" data-path="section-12.html"><a href="section-12.html#section-12.3.2"><i class="fa fa-check"></i><b>12.3.2</b> 绕封法</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="section-13.html"><a href="section-13.html"><i class="fa fa-check"></i><b>13</b> 线性回归极其衍生</a><ul>
<li class="chapter" data-level="13.1" data-path="section-13.html"><a href="section-13.html#section-13.1"><i class="fa fa-check"></i><b>13.1</b> 普通线性回归</a><ul>
<li class="chapter" data-level="13.1.1" data-path="section-13.html"><a href="section-13.html#section-13.1.1"><i class="fa fa-check"></i><b>13.1.1</b> 最小二乘线性模型</a></li>
<li class="chapter" data-level="13.1.2" data-path="section-13.html"><a href="section-13.html#section-13.1.2"><i class="fa fa-check"></i><b>13.1.2</b> 回归诊断</a></li>
<li class="chapter" data-level="13.1.3" data-path="section-13.html"><a href="section-13.html#section-13.1.3"><i class="fa fa-check"></i><b>13.1.3</b> 离群点，高杠杆点和强影响点</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="section-13.html"><a href="section-13.html#section-13.2"><i class="fa fa-check"></i><b>13.2</b> 收缩方法</a><ul>
<li class="chapter" data-level="13.2.1" data-path="section-13.html"><a href="section-13.html#section-13.2.1"><i class="fa fa-check"></i><b>13.2.1</b> 岭回归</a></li>
<li class="chapter" data-level="13.2.2" data-path="section-13.html"><a href="section-13.html#lasso"><i class="fa fa-check"></i><b>13.2.2</b> Lasso</a></li>
<li class="chapter" data-level="13.2.3" data-path="section-13.html"><a href="section-13.html#section-13.2.3"><i class="fa fa-check"></i><b>13.2.3</b> 弹性网络</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="section-13.html"><a href="section-13.html#lasso"><i class="fa fa-check"></i><b>13.3</b> 知识扩展：Lasso的变量选择功能</a></li>
<li class="chapter" data-level="13.4" data-path="section-13.html"><a href="section-13.html#section-13.4"><i class="fa fa-check"></i><b>13.4</b> 主成分和偏最小二乘回归</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="section-14.html"><a href="section-14.html"><i class="fa fa-check"></i><b>14</b> 广义线性模型压缩方法</a><ul>
<li class="chapter" data-level="14.1" data-path="section-14.html"><a href="section-14.html#glmnet"><i class="fa fa-check"></i><b>14.1</b> 初识<code>glmnet</code></a></li>
<li class="chapter" data-level="14.2" data-path="section-14.html"><a href="section-14.html#section-14.2"><i class="fa fa-check"></i><b>14.2</b> 收缩线性回归</a></li>
<li class="chapter" data-level="14.3" data-path="section-14.html"><a href="section-14.html#section-14.3"><i class="fa fa-check"></i><b>14.3</b> 逻辑回归</a><ul>
<li class="chapter" data-level="14.3.1" data-path="section-14.html"><a href="section-14.html#section-14.3.1"><i class="fa fa-check"></i><b>14.3.1</b> 普通逻辑回归</a></li>
<li class="chapter" data-level="14.3.2" data-path="section-14.html"><a href="section-14.html#section-14.3.2"><i class="fa fa-check"></i><b>14.3.2</b> 收缩逻辑回归</a></li>
<li class="chapter" data-level="14.3.3" data-path="section-13.html"><a href="section-13.html#lasso"><i class="fa fa-check"></i><b>14.3.3</b> 知识扩展：群组lasso逻辑回归</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="section-14.html"><a href="section-14.html#section-14.4"><i class="fa fa-check"></i><b>14.4</b> 收缩多项回归</a></li>
<li class="chapter" data-level="14.5" data-path="section-14.html"><a href="section-14.html#section-14.5"><i class="fa fa-check"></i><b>14.5</b> 泊松收缩回归</a></li>
<li class="chapter" data-level="14.6" data-path="section-14.html"><a href="section-14.html#-4"><i class="fa fa-check"></i><b>14.6</b> 本章总结</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="section-15.html"><a href="section-15.html"><i class="fa fa-check"></i><b>15</b> 树模型</a><ul>
<li class="chapter" data-level="15.1" data-path="section-15.html"><a href="section-15.html#section-15.1"><i class="fa fa-check"></i><b>15.1</b> 分裂准则</a></li>
<li class="chapter" data-level="15.2" data-path="section-15.html"><a href="section-15.html#section-15.2"><i class="fa fa-check"></i><b>15.2</b> 树的修剪</a></li>
<li class="chapter" data-level="15.3" data-path="section-15.html"><a href="section-15.html#section-15.3"><i class="fa fa-check"></i><b>15.3</b> 回归树和决策树</a><ul>
<li class="chapter" data-level="15.3.1" data-path="section-15.html"><a href="section-15.html#section-15.3.1"><i class="fa fa-check"></i><b>15.3.1</b> 回归树</a></li>
<li class="chapter" data-level="15.3.2" data-path="section-15.html"><a href="section-15.html#section-15.3.2"><i class="fa fa-check"></i><b>15.3.2</b> 决策树</a></li>
</ul></li>
<li class="chapter" data-level="15.4" data-path="section-15.html"><a href="section-15.html#section-15.4"><i class="fa fa-check"></i><b>15.4</b> 装袋树</a></li>
<li class="chapter" data-level="15.5" data-path="section-15.html"><a href="section-15.html#section-15.5"><i class="fa fa-check"></i><b>15.5</b> 随机森林</a></li>
<li class="chapter" data-level="15.6" data-path="section-15.html"><a href="section-15.html#section-15.6"><i class="fa fa-check"></i><b>15.6</b> 助推法</a></li>
<li class="chapter" data-level="15.7" data-path="section-15.html"><a href="section-15.html#section-15.7"><i class="fa fa-check"></i><b>15.7</b> 知识扩展：助推法的可加模型框架</a></li>
<li class="chapter" data-level="15.8" data-path="section-15.html"><a href="section-15.html#section-15.8"><i class="fa fa-check"></i><b>15.8</b> 知识扩展：助推树的数学框架</a><ul>
<li class="chapter" data-level="15.8.1" data-path="section-15.html"><a href="section-15.html#section-15.8.1"><i class="fa fa-check"></i><b>15.8.1</b> 数学表达</a></li>
<li class="chapter" data-level="15.8.2" data-path="section-15.html"><a href="section-15.html#section-15.8.2"><i class="fa fa-check"></i><b>15.8.2</b> 梯度助推数值优化</a></li>
</ul></li>
<li class="chapter" data-level="15.9" data-path="section-15.html"><a href="section-15.html#-5"><i class="fa fa-check"></i><b>15.9</b> 本章总结</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="section-16.html"><a href="section-16.html"><i class="fa fa-check"></i><b>16</b> 神经网络</a><ul>
<li class="chapter" data-level="16.1" data-path="section-16.html"><a href="section-16.html#projection-pursuit-regression"><i class="fa fa-check"></i><b>16.1</b> 投影寻踪回归（Projection Pursuit Regression）</a></li>
<li class="chapter" data-level="16.2" data-path="section-16.html"><a href="section-16.html#neural-networks"><i class="fa fa-check"></i><b>16.2</b> 神经网络(Neural Networks)</a></li>
<li class="chapter" data-level="16.3" data-path="section-16.html"><a href="section-16.html#section-16.3"><i class="fa fa-check"></i><b>16.3</b> 神经网络拟合</a></li>
<li class="chapter" data-level="16.4" data-path="section-16.html"><a href="section-16.html#section-16.4"><i class="fa fa-check"></i><b>16.4</b> 训练神经网络</a></li>
<li class="chapter" data-level="16.5" data-path="section-16.html"><a href="section-16.html#caret"><i class="fa fa-check"></i><b>16.5</b> 用<code>caret</code>包训练神经网络</a><ul>
<li class="chapter" data-level="16.5.1" data-path="section-16.html"><a href="section-16.html#section-16.5.1"><i class="fa fa-check"></i><b>16.5.1</b> 普通神经网络</a></li>
<li class="chapter" data-level="16.5.2" data-path="section-16.html"><a href="section-16.html#bootstrap"><i class="fa fa-check"></i><b>16.5.2</b> bootstrap平均神经网络</a></li>
<li class="chapter" data-level="16.5.3" data-path="section-16.html"><a href="section-16.html#section-16.5.3"><i class="fa fa-check"></i><b>16.5.3</b> 模型比较</a></li>
</ul></li>
<li class="chapter" data-level="16.6" data-path="section-16.html"><a href="section-16.html#section-16.6"><i class="fa fa-check"></i><b>16.6</b> 深度学习</a><ul>
<li class="chapter" data-level="16.6.1" data-path="section-16.html"><a href="section-16.html#mxnet"><i class="fa fa-check"></i><b>16.6.1</b> <code>mxnet</code>包</a></li>
<li class="chapter" data-level="16.6.2" data-path="section-16.html"><a href="section-16.html#darch"><i class="fa fa-check"></i><b>16.6.2</b> <code>darch</code>包</a></li>
<li class="chapter" data-level="16.6.3" data-path="section-16.html"><a href="section-16.html#deepnet"><i class="fa fa-check"></i><b>16.6.3</b> <code>deepnet</code>包</a></li>
</ul></li>
<li class="chapter" data-level="16.7" data-path="section-16.html"><a href="section-16.html#-6"><i class="fa fa-check"></i><b>16.7</b> 本章总结</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="section-17.html"><a href="section-17.html"><i class="fa fa-check"></i><b>17</b> 遗失的……干货?</a></li>
<li class="chapter" data-level="18" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>18</b> References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">数据科学家：R语言</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="section-16" class="section level1">
<h1><span class="header-section-number">第16章</span> 神经网络</h1>
<p>神经网络(Neural Networks)模型以及其衍生出的深度挖掘是目前为止炒作最严重的。这是广大不明觉厉的群众对黑箱模型的一贯反应。人们对自己不了解的东西通常有两种反应：神化和魔化。本来不打算写神经网络这个章节。为什么？因为在我的从业生涯中，从来没有发现一个应用场景中神经网络的结果是最优的。但最后为什么决定又写了？因为我对各种炒作的愤慨已然上升到了一种接近厌世感的程度。我常常觉得，人类这种不明觉厉的心理是不是对思想上懒惰的保护性反应。不管怎样，在这一章我会给大家展示神经网络的基本构造，使用注意事项以及如何用R训练调试神经网络模型。希望能够打破神经网络这个谜团，也希望大家日后对自己不了解的东西保有一种空杯的心态，不要主观的去极端化。很多模型就像T台上夸张的衣服，看不懂，也穿不上。</p>
<p>大家对神经网络的生物学解释可能熟透了，这是大部分科普文章都提到什么类似大脑神经处理信息方式的模型。经典的神经网络模型的图大家也应该见过不少，很多节点，一层层向上直到应变量。其实和我们之前讲的偏最小二乘回归（PLS）的结构图非常相似。核心思想是将原自变量的进行线性组合，得到新的特征，也就是一个特征构造的过程。这听起来是不是很熟悉？我们在特征工程那个章节中的特征提取小节讲了很多类似的过程。日光之下并无新事。只是这里更进一步，将应变量视为这些特征的非线性函数。构造这些组合用到的参数就是大家常听到的神经网络的权重。这里组合再组合的，肯定会平添很多参数，参数一多就有过度拟合的问题，这怎么办？压缩模型，也就是加罚函数。也就是我之前碎碎念了一堆讲的收缩模型。思想是一样一样的。神经网络的主要思想其实就这样，剩下的就是细节。我知道大家现在还是云里雾里，下面我们就对这些细节开始展开，用最野蛮也是最直接的方式解释神经网络。是什么方法呢？数学…….前方会有很多公式，但我会尽量用人话给大家解释。</p>
<div id="projection-pursuit-regression" class="section level2">
<h2><span class="header-section-number">16.1</span> 投影寻踪回归（Projection Pursuit Regression）</h2>
<p>在讲神经网络之前，我们先从一个更广的框架开始，也就是PPR(Projection Pursuit Regression)。模型的中文翻译我是用Google找到的，感觉还是怪怪的，所以下面我就直接用PPR。这个模型是Jerome H. Friedman和Werner Stuetzle两位统计学大家对可加模型进行扩展得到的。</p>
<p>讲到可加模型和Friedman老爷子，大家有没有觉得熟悉？在讲到梯度助推（Gradient Boosting）的时候我讲过，就是Friedman等众牛人提供了自适助推算法（Adaptive Boosting）的统计学见解，将其纳入最小化指数损失的向前逐步可加模型的框架。在树模型的知识扩展部分，我唠叨了好多关于可加模型框架的内容。可加模型结构是非常常见的。到此为止你有没有发现，这看上去光怪陆离的世界表面之下其实是由一些基本的结构支撑的。专研一个学科久了，你就会发现知识不仅仅是对相关的事实和公式的罗列，而是围绕核心概念或大观点（big idea）组织的，从更本质的结构上去思考这个领域，就能够做到融会贯通。这就是大道至简。记得本科时候的高等代数教授说过，书先是越读越厚，之后就越读越薄。希望大家通过不断学习也能有这样的感觉。</p>
<p>有点跑题，现在回到PPR。假设<span class="math inline">\(\mathbf{X}\)</span>是p个变量观测组成的自变量矩阵，n行p列。<span class="math inline">\(\mathbf{y}\)</span>是相应的应变量向量。<span class="math inline">\(\mathbf{\omega_{m}},m=1,2,\dots,M\)</span>是含有p个元素的参数向量。PPR模型可以写成：</p>
<p><span class="math display">\[f(\mathbf{X})=\sum_{m=1}^{M}g_{m}(\mathbf{X\omega_{m}^{T}})\]</span></p>
<p>这里通过对原变量<span class="math inline">\(\mathbf{X}\)</span>线性组合得到新的特征<span class="math inline">\(\mathbf{v_{m}}=\mathbf{X\omega_{m}^{T}}\)</span>，然后在这些新特征的基础上建立加性模型。这里的<span class="math inline">\(\mathbf{\omega_{m}}\)</span>是单位向量，这里的新特征<span class="math inline">\(\mathbf{v_m}\)</span>实际上是<span class="math inline">\(\mathbf{X}\)</span>在<span class="math inline">\(\mathbf{\omega_{m}}\)</span>上的投影。其实也就是将之前的p维自变量空间投影到新的M维特征空间上。这个和主成分分析道理一样，主成分是正交投影，这里不一定正交。是不是很抽象，那我们来举个栗子。</p>
<p>假设<span class="math inline">\(p=2\)</span>，也就是说我们有两个变量<span class="math inline">\(x_1\)</span>和<span class="math inline">\(x_2\)</span>。如果<span class="math inline">\(M=1\)</span>，<span class="math inline">\(\mathbf{\omega^{T}}=(\frac{1}{2},\frac{\sqrt{3}}{2})\)</span>，那么对应的<span class="math inline">\(v=\frac{1}{2}x_{1}+\frac{\sqrt{3}}{2}x_{2}\)</span>。我们尝试不同的函数设置，对比得到的结果。</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(\mathbf{\omega^{T}}=(\frac{1}{2},\frac{\sqrt{3}}{2})\)</span>, <span class="math inline">\(v=\frac{1}{2}x_{1}+\frac{\sqrt{3}}{2}x_{2}\)</span> , <span class="math inline">\(g(v)=\frac{1}{1+e^{-v}}\)</span></p></li>
<li><p><span class="math inline">\(\mathbf{\omega^{T}}=(1,0)\)</span>, <span class="math inline">\(v = x_1\)</span>, <span class="math inline">\(g(v)=(v+5)sin(\frac{1}{\frac{v}{3}+0.1})\)</span></p></li>
<li><p><span class="math inline">\(\mathbf{\omega^{T}}=(0,1)\)</span>, <span class="math inline">\(v = x_2\)</span>, <span class="math inline">\(g(v)=e^{\frac{v^2}{5}}\)</span></p></li>
<li><p><span class="math inline">\(\mathbf{\omega^{T}}=(1,0)\)</span>, <span class="math inline">\(v = x_1\)</span>, <span class="math inline">\(g(v)=(v+0.1)sin(\frac{1}{\frac{v}{3}+0.1})\)</span></p></li>
</ol>
<p>我们可以用如下R代码得到最终的映射结果图：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># 我们用plot3D包绘制3维图</span>
<span class="kw">library</span>(plot3D)
<span class="co"># 得到x1和x2</span>
<span class="co"># 注意这里x1和x2要是需要是矩阵的形式</span>
<span class="co"># 你可以分别查看这两个变量会发现：</span>
<span class="co"># x1中每列都是相同的</span>
<span class="co"># x2的每行都是相同的</span>
<span class="co"># 这里的mesh()是plot3D中的函数，用于生成2维或者3维阵列</span>
<span class="co"># 这里不是很好理解，大家可能得稍微想想</span>
M &lt;-<span class="st"> </span><span class="kw">mesh</span>(<span class="kw">seq</span>(-<span class="fl">13.2</span>, <span class="fl">13.2</span>, <span class="dt">length.out =</span> <span class="dv">50</span>),
          <span class="kw">seq</span>(-<span class="fl">37.4</span>, <span class="fl">37.4</span>, <span class="dt">length.out =</span> <span class="dv">50</span>))
x1 &lt;-<span class="st"> </span>M$x
x2 &lt;-<span class="st"> </span>M$y
##----- 第1种函数设置
<span class="co"># 将X映射到w上得到v</span>
v &lt;-<span class="st"> </span>(<span class="dv">1</span>/<span class="dv">2</span>)*x1+(<span class="kw">sqrt</span>(<span class="dv">3</span>)/<span class="dv">2</span>)*x2
<span class="co"># 将函数g()应用于v</span>
g1&lt;-<span class="dv">1</span>/(<span class="dv">1</span>+<span class="kw">exp</span>(-v))
<span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">2</span>), <span class="dt">mar =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>))
<span class="kw">surf3D</span>(x1,x2,g1,<span class="dt">colvar =</span> g1, <span class="dt">border =</span> <span class="st">&quot;black&quot;</span>, <span class="dt">colkey =</span> <span class="ot">FALSE</span>, <span class="dt">box =</span> <span class="ot">FALSE</span>, <span class="dt">main =</span> <span class="st">&quot;1&quot;</span>)
##----- 第2种函数设置
v &lt;-<span class="st"> </span>x1
g2 &lt;-<span class="st"> </span>(v<span class="dv">+5</span>)*<span class="kw">sin</span>(<span class="dv">1</span>/(v/<span class="dv">3</span><span class="fl">+0.1</span>))
<span class="kw">surf3D</span>(x1,x2,g2,<span class="dt">colvar =</span> g2, <span class="dt">border =</span> <span class="st">&quot;black&quot;</span>, <span class="dt">colkey =</span> <span class="ot">FALSE</span>, <span class="dt">box =</span> <span class="ot">FALSE</span>, <span class="dt">main =</span> <span class="st">&quot;2&quot;</span>)
##----- 第3种函数设置
v &lt;-<span class="st"> </span>x2
g3 &lt;-<span class="st"> </span><span class="kw">exp</span>(v^<span class="dv">2</span>/<span class="dv">5</span>)
<span class="kw">surf3D</span>(x1,x2,g3,<span class="dt">colvar =</span> g3, <span class="dt">border =</span> <span class="st">&quot;black&quot;</span>, <span class="dt">colkey =</span> <span class="ot">FALSE</span>, <span class="dt">box =</span> <span class="ot">FALSE</span>, <span class="dt">main =</span> <span class="st">&quot;3&quot;</span>)

##----- 第4种函数设置
v &lt;-<span class="st"> </span>x1
g4 &lt;-<span class="st"> </span>(v<span class="fl">+0.1</span>)*<span class="kw">sin</span>(<span class="dv">1</span>/(v/<span class="dv">3</span><span class="fl">+0.1</span>))
<span class="kw">surf3D</span>(x1,x2,g4,<span class="dt">colvar =</span> g4, <span class="dt">border =</span> <span class="st">&quot;black&quot;</span>, <span class="dt">colkey =</span> <span class="ot">FALSE</span>, <span class="dt">box =</span> <span class="ot">FALSE</span>, <span class="dt">main =</span> <span class="st">&quot;4&quot;</span>)</code></pre></div>
<p><img src="DS_R_files/figure-html/unnamed-chunk-272-1.png" width="672" /></p>
<p>大家可以看到，这个框架可以非常灵活。本质上就是对线性组合进行非线性变换，可以捕捉很多不同的关系。比如<span class="math inline">\(x_{1}x_{2}\)</span>实际上可以写成<span class="math inline">\(\frac{(x_{1}+x_{2})^{2}-(x_{1}-x_{2})^{2}}{4}\)</span>，这里<span class="math inline">\(M=2\)</span>。所有更加高阶的<span class="math inline">\(x_1\)</span>和<span class="math inline">\(x_2\)</span>的因子都可以用类似的方式表达。事实上，如果<span class="math inline">\(M\)</span>足够大，这个框架可以逼近任何<span class="math inline">\(\mathbb{R}^{p}\)</span>上的连续函数。所以说这个模型族覆盖面非常广。但这也是有代价的，这个代价就是模型的可解释性，因为参数随着 <span class="math inline">\(M\)</span>的增加而增加，此外还是一层套一层的。当然，<span class="math inline">\(M=1\)</span>的情况除外。</p>
<p>PPR自1981年问世后并没有在统计学界广泛应用，因为模型对计算的要求超过了当时的计算机水平。但这是一次思想上的进步，这种思想在之后在本章的主角——神经网络模型——身上得以重生并得到应用。擦把汗，感谢大家的耐心，我们终于要正式讲神经网络模型了。</p>
</div>
<div id="neural-networks" class="section level2">
<h2><span class="header-section-number">16.2</span> 神经网络(Neural Networks)</h2>
<p>神经网络是一类模型或者说机器学习器的总称。这里我们介绍其中最广为人知也是用的最多的一类模型——单层级反向传播神经网络（single hidden layer back-propagation network）。这个模型在神经网络模型族中的位置就好比咖啡中的卡布奇诺。这类模型是有如下结构的分层回归或者分类模型：</p>
<center>
<img src="http://scientistcafe.com/book/Figure/nnet.png" />
</center>
<p>这是一个单一潜层级向前神经网络。图来自T. Hastie, R. Tibshirani和J. Friedman这几位大神的书 The Elements of Statistical Learning，这里的讲解也是参考这本书中的理论解释，我觉得这本书是数据科学家的必备读物，英文版的语言平实，建议大家直接购买原版。</p>
<p>这里的K代表应变量的个数。在回归模型中，通常只有一个应变量。但是在多分类模型中（类别数大于3），对应的应变量数目就是类别数。为了简单起见，下面我们假设<span class="math inline">\(K=1\)</span>，应变量只有一个<span class="math inline">\(Y\)</span>。对于分类问题，应变量<span class="math inline">\(Y\)</span>的可能取值就是0/1。我们从下到上的解释模型。假设<span class="math inline">\(\mathbf{X^{T}}=(X_{1},X_{2},\dots,X_{p})\)</span>是p个自变量组成的向量。这里我们用随机变量的表达形式。第一步是从<span class="math inline">\(\mathbf{X}\)</span>到新的特征<span class="math inline">\(\mathbf{Z^{T}}=(Z_1,Z_2,\dots, Z_M)\)</span>。</p>
<p><span class="math display">\[Z_{m}=h(\alpha_{0m}+\mathbf{\alpha_{m}^{T}}X),\ m=1,\dots,M\]</span></p>
<p>这里的<span class="math inline">\(h(\cdot)\)</span>也称为<strong>启动函数(activation function)</strong>，通常情况下<span class="math inline">\(h(v)=\frac{1}{1+e^{-v}}\)</span>，也就是大家在逻辑回归中常看到的那个S形函数。这里稍微的扩展一点，假设我们在上面公式中的<span class="math inline">\(v\)</span>前面加一个因子<span class="math inline">\(s\)</span>，也就是：<span class="math inline">\(h(v)=\frac{1}{1+e^{-sv}}\)</span>，看看s的不同取值对曲线形状的影响：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">v &lt;-<span class="st"> </span><span class="kw">seq</span>(-<span class="dv">10</span>,<span class="dv">10</span>,<span class="dt">length=</span><span class="dv">200</span>)
s1 &lt;-<span class="st"> </span><span class="dv">1</span>
s2 &lt;-<span class="st"> </span><span class="fl">0.3</span>
s3 &lt;-<span class="st"> </span><span class="dv">20</span>
h1 &lt;-<span class="st"> </span><span class="dv">1</span>/(<span class="dv">1</span>+<span class="kw">exp</span>(-s1*v))
h2 &lt;-<span class="st"> </span><span class="dv">1</span>/(<span class="dv">1</span>+<span class="kw">exp</span>(-s2*v))
h3 &lt;-<span class="st"> </span><span class="dv">1</span>/(<span class="dv">1</span>+<span class="kw">exp</span>(-s3*v))
<span class="kw">plot</span>(v,h1,<span class="dt">type=</span><span class="st">&quot;l&quot;</span>,<span class="dt">col=</span><span class="dv">1</span>, <span class="dt">ylab=</span><span class="st">&quot;h&quot;</span>, <span class="dt">lty =</span> <span class="dv">1</span>)
<span class="kw">lines</span>(v,h2,<span class="dt">col=</span><span class="dv">2</span>, <span class="dt">lty =</span> <span class="dv">2</span>)
<span class="kw">lines</span>(v,h3,<span class="dt">col=</span><span class="dv">3</span>, <span class="dt">lty =</span> <span class="dv">3</span>)
<span class="kw">legend</span>(<span class="st">&quot;topleft&quot;</span>, <span class="kw">c</span>(<span class="st">&quot;s=1&quot;</span>,<span class="st">&quot;s=0.3&quot;</span>,<span class="st">&quot;s=20&quot;</span>), <span class="dt">lty =</span> <span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>), <span class="dt">col =</span> <span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>))</code></pre></div>
<p><img src="DS_R_files/figure-html/unnamed-chunk-273-1.png" width="672" /></p>
<p>大家可以看到，s越小，曲线越接近线性，s越大曲线越接近分段函数，理解这一点有助于大家之后理解初始值的选择对模型结果的影响。接下来就是如何从<span class="math inline">\(\mathbf{Z}\)</span>到<span class="math inline">\(Y\)</span>：</p>
<p><span class="math display">\[Y=g(\beta_{0}+\mathbf{\beta^{T}Z})=f(\mathbf{X})\]</span></p>
<p>对于回归模型，通常直接定义：</p>
<p><span class="math display">\[Y=\beta_{0}+\mathbf{\beta^{T}Z}\]</span> 对于有K个类别的判别模型，每个类都对应一个<span class="math inline">\(\mathbf{Z}\)</span>的线性组合，通常用的是softmax函数：</p>
<p><span class="math display">\[Y_{k}=\frac{e^{\beta_{0k}+\mathbf{\beta_{k}^{T}Z}}}{\Sigma_{l=1}^{K}e^{\beta_{0l}+\mathbf{\beta_{l}^{T}Z}}}\]</span></p>
<p>其原因很好理解，这就和标准化一样，经过softmax函数变换后得到的估计类似于一个概率值，在<span class="math inline">\([0,1]\)</span>之间。如果<span class="math inline">\(g(\cdot)\)</span>是一个恒等函数，那么整个模型就是一个线性模型的扩展。通过非线性变换<span class="math inline">\(h(\cdot)\)</span>对线性模型进行了极大的扩展。之前我们讲到过，<span class="math inline">\(h(\cdot)\)</span>称为激活函数，函数名中的英文activation就是激活某种状态的意思。这里激活程度其实和<span class="math inline">\(\Vert\mathbf{\alpha_{m}}\Vert\)</span>的大小有关。从前面3条不同S形曲线图可以看到，当<span class="math inline">\(\Vert\mathbf{\alpha_{m}}\Vert\)</span>很小时，激活函数几乎是线性的。反之当<span class="math inline">\(\Vert\mathbf{\alpha_{m}}\Vert\)</span>很大时，就趋近于分段函数。如果真正使用分段函数会给后面的拟合带来困难，因为在断点处函数不可导。由此可以看出，通过选择不同的参数可以调节模型由线性到非线性的过程。现在大家是不是更能理解为什么要用这么一个函数了呢？</p>
<p>不知道你们注意到没有，这里单层级的神经网络模型，其实等价于之前讲的PPR。不同之处在于，PPR中的<span class="math inline">\(g_m(v)\)</span>是一个非参函数，也就是用局部平滑的方法估计，你可以想象下局部滑动平均回归。而神经网络中的<span class="math inline">\(h(\cdot)\)</span>是一个有着固定形式的函数。我们可以将神经网络模型转化成PPR模型的格式：</p>
<p><span class="math display">\[\beta_{m}h(\alpha_{0m}+\mathbf{\alpha_{m}^{T}X})=\beta_{m}h(\alpha_{0m}+\Vert\mathbf{\alpha_{m}}\Vert(\mathbf{\omega_{m}^{T}X}))=g_{m}(\mathbf{\omega_{m}^{T}X})\]</span></p>
<p>其中<span class="math inline">\(\mathbf{\omega_{m}=\frac{\mathbf{\alpha_{m}}}{\Vert\mathbf{\alpha_{m}}\Vert}}\)</span>，是一个单位向量。PPR中广义的非参函数<span class="math inline">\(g(\cdot)\)</span>比这里有着明确参数的<span class="math inline">\(h_{\beta,\alpha_{0},s}(v)=\beta h(\alpha_{0}+sv)\)</span>更加复杂。虽然看上去有参数的式子写出来要吓人多了。因此通常神经网络中隐变量个数要比PPR中的多，也就是<span class="math inline">\(M\)</span>的取值。</p>
<p>到此为止终于可以真正看出一些神经网络名字的由来了。这里每一个最初的变量<span class="math inline">\(X_1,\dots, X_p\)</span>就好比神经元。这里的函数<span class="math inline">\(h(\cdot)\)</span>就好比突触。为嘛？想想突触是干嘛用的，突触是神经元之间，或神经元与肌细胞、腺体之间通信的特异性接头。一个神经元释放的电或化学信号要经过突触传递给另外一个神经元。在这里的神经网络模型中，如果<span class="math inline">\(h(\cdot)\)</span>是一个阶梯函数，也就是当输入超过某个值时函数才非零，就类似于神经元<span class="math inline">\(\mathbf{X}\)</span>发送的信号经过突触<span class="math inline">\(h\)</span>，当信号强度在某一范围时，就可以传递给下面的神经元<span class="math inline">\(\mathbf{Z}\)</span>。之前讲过，分段函数有求导的问题，因此后来人将其稍微改了改，将断点用很陡峭的线这么一连，就成了这里多次提到的S曲线啦。当然S曲线可以通过参数进行变形，能够逼近线性函数，于是对于神经网络其实可以从线性向非线性逐步调优，好像一个电阻开关一样。</p>
</div>
<div id="section-16.3" class="section level2">
<h2><span class="header-section-number">16.3</span> 神经网络拟合</h2>
<p>神经网络模型中的参数就是我们常说的权重。也就是这里的：</p>
<p><span class="math display">\[\{\alpha_{0m},\mathbf{\alpha_{m}};m=1,2,\dots,M\},\ M(p+1)个权重\]</span> <span class="math display">\[\{\beta_{0},\mathbf{\beta}\},\ M+1个权重\]</span></p>
<p>也就是说，一共有<span class="math inline">\(M(p+1)+M+1\)</span>个需要估计的参数。我们将所有参数用老朋友<span class="math inline">\(\Theta\)</span>表示。对于回归模型，拟合模型就是优化下面的误差平方和：</p>
<p><span class="math display">\[R(\Theta)=\sum_{i=1}^{N}(y_{i}-f(x_{i}))^{2}\]</span></p>
<p>对于分类模型，优化的是熵（在树模型章节中，模型分裂准则的小节有详细介绍）：</p>
<p><span class="math display">\[R(\Theta)=-\sum_{i=1}^{N}\Sigma_{k=1}^{K}y_{ik}log(f_{k}(x_{i}))\]</span></p>
<p>这里的<span class="math inline">\(K\)</span>表示类别的数目。</p>
<p>对于100个自变量的模型，如果我们设置<span class="math inline">\(M=30\)</span>的话，模型全部参数个数将为3061。随着变量和M的增长，模型参数的个数也会很快增加。不难想象，非常容易发生过度拟合。因此这里我们并非想要<span class="math inline">\(R(\Theta)\)</span>的全局最优解，而是在此基础上对参数进行压缩。本章后面部分还会详细介绍。</p>
<p>最小化<span class="math inline">\(R(\Theta)\)</span> 的一般方法是梯度法。优化过程也就是名字中的“反向传播”的由来。由于模型的形式，一层套一层，所以梯度很容易通过链式求导得到。什么是链式求导？就是：</p>
<p><span class="math display">\[\frac{df(g(x))}{dx}=\frac{df}{dg}\frac{dg}{dx}\]</span></p>
<p>逼近的过程仿佛是从网络的底端到顶端，然后从顶端回来这样迭代，因此称为反向传播。传播是向前，反向是退回。这样扫来扫去的不断降低<span class="math inline">\(R(\Theta)\)</span>。</p>
<p>下面我们对回归的情况进行展开。假设<span class="math inline">\(z_{mi}=h(\alpha_{0m}+\mathbf{\alpha_{m}^{T}}x_{i})\)</span>，且<span class="math inline">\(\mathbf{z_{i}^{T}}=(z_{1i},z_{2i},\dots,z_{Mi})\)</span>，那么我们有：</p>
<p><span class="math display">\[R(\Theta)=\Sigma_{i=1}^{N}R_{i}=\Sigma_{i=1}^{N}(y_{i}-\beta_{0}-\mathbf{\beta^{T}z_{i}})^{2}\]</span></p>
<p>用链式求导可得：</p>
<p><span class="math display">\[\frac{\partial R_{i}}{\partial\beta_{m}}=-2(y_{ik}-\beta_{0}-\mathbf{\mathbf{\beta}^{T}z_{i}})z_{mi}\]</span></p>
<p><span class="math display">\[\frac{\partial R_{i}}{\partial\alpha_{ml}}=-2(y_{ik}-\beta_{0}-\mathbf{\mathbf{\beta}^{T}z_{i}})\beta_{m}h&#39;(\alpha_{0m}+\mathbf{\alpha_{m}^{T}}x_{i})x_{il}\]</span></p>
<p>有了这些梯度，第(r+1)步迭代为：</p>
<p><span class="math display">\[\beta_{m}^{(r+1)}=\beta_{m}^{(r)}-\gamma_{r}\Sigma_{i=1}^{N}\frac{\partial R_{i}}{\partial\beta_{m}^{(r)}}\]</span></p>
<p><span class="math display">\[\alpha_{ml}^{(r+1)}=\alpha_{ml}^{(r)}-\gamma_{r}\Sigma_{i=1}^{N}\frac{\partial R_{i}}{\partial\alpha_{ml}^{(r)}}\]</span></p>
<p>其中又引入了一个参数<span class="math inline">\(\gamma_{r}\)</span>，称为学习率。我们可以将之前梯度的式子简写为：</p>
<p><span class="math display">\[\frac{\partial R_{i}}{\partial\beta_{m}}=\delta_{i}z_{mi}\]</span> <span class="math display">\[\frac{\partial R_{i}}{\partial\alpha_{ml}}=s_{mi}x_{il}\]</span></p>
<p>其中<span class="math inline">\(\delta_{i}\)</span>和<span class="math inline">\(s_{mi}\)</span>分别称为当前模型在输出层级和隐层级的“误差”。从定义就可以看出：</p>
<p><span class="math display">\[s_{mi}=-2(y_{ik}-\beta_{0}-\mathbf{\mathbf{\beta}^{T}z_{i}})\beta_{m}h&#39;(\alpha_{0m}+\mathbf{\alpha_{m}^{T}}x_{i})\]</span></p>
<p>这是反向传播函数，不同层级就是通过这个函数来回穿梭。对于学习率<span class="math inline">\(\gamma_{r}\)</span>，通常是固定一个常数。也可以当做参数调优。</p>
</div>
<div id="section-16.4" class="section level2">
<h2><span class="header-section-number">16.4</span> 训练神经网络</h2>
<p>训练神经网络的过程中需要注意几个方面。</p>
<ul>
<li>初始值</li>
</ul>
<p>如果权重接近0，模型倾向于线性。还记得之前的那几条S形曲线么，可以用这些曲线来帮助直观的理解。由于模型参数通常都比较多，所以训练神经网络的时候都是由算法自动选取一些接近0的值作为初始权重。这样随着迭代进行，权重不断增加，模型从线性逐渐调整成非线性。</p>
<ul>
<li>过度拟合</li>
</ul>
<p>之前讲过，神经网络的参数增加很快，非常容易过度拟合。解决过度拟合的方法有两种：（1）提前结束迭代；（2）加罚函数。提前结束就是指训练一小会，在达到最优参数估计前停止迭代，防止过度拟合。加罚函数的方法现在应该不陌生了。也就是对参数进行惩罚，当惩罚很大时，所有参数估计都将是0。我们还是用之前介绍罚函数时引入的<span class="math inline">\(\lambda\)</span>来表示权重衰减参数。之前优化的是<span class="math inline">\(R(\Theta)\)</span>，现在优化的是<span class="math inline">\(R(\Theta)+\lambda J(\Theta)\)</span>，其中函数<span class="math inline">\(J(\Theta)\)</span>可以有如下两种:</p>
<p><span class="math display">\[J(\Theta)=\Sigma_{m}\beta_{m}^{2}+\Sigma_{ml}\alpha_{ml}^{2}\]</span></p>
<p>或者：</p>
<p><span class="math display">\[J(\Theta)=\Sigma_{m}\frac{\beta_{m}^{2}}{1+\beta_{m}^{2}}+\Sigma_{ml}\frac{\alpha_{ml}^{2}}{1+\alpha_{ml}^{2}}\]</span></p>
<p>前者对权重收缩的力度比后者大。</p>
<ul>
<li>标准化</li>
</ul>
<p>一旦牵扯到对参数进行惩罚大家就该条件反射的想到参数的标度必须统一。因为这会极大的影响每个参数在罚函数中的地位。所以我们通常会在训练神经网络模型前对数据进行标准化。之后展示代码的时候会再次强调。</p>
<ul>
<li>层数和隐变量数目</li>
</ul>
<p>理论上说隐层级的数目越多越好，层级数目越多，捕捉数据中非线性关系的能力越灵活。但显然是参数个数也随之增加。但我们可以通过添加罚函数的方法来对参数进行控制。传说中的深度挖掘，就是多层的神经网络。</p>
<p>另外一个需要决定的就是隐变量的数目。在训练神经网络模型过程中这也是一个需要调优的参数。</p>
<ul>
<li>多个局部极值</li>
</ul>
<p>拟合神经网络模型还有一个问题就是<span class="math inline">\(R(\Theta)\)</span>不是纯正的凸函数，也就是说会有许多局部极小值。你找到哪个局部极小值取决于你的初始值。这给模型预测带来不稳定性。解决的方法通常有两种：（1）选择一系列不同的初始值，然后最终得到的模型权重平均。 （2）用装袋法，采用bootstrap抽样，然后将这些基于bootstrap样本得到的结果平均。这两种方法都是采用了在拟合过程中的某一阶段加入一些随机性，然后将结果平均来得到稳定估计的思想。</p>
</div>
<div id="caret" class="section level2">
<h2><span class="header-section-number">16.5</span> 用<code>caret</code>包训练神经网络</h2>
<div id="section-16.5.1" class="section level3">
<h3><span class="header-section-number">16.5.1</span> 普通神经网络</h3>
<p><code>caret</code>包中涵盖了很多其它模型包，比如神经网络常用的<code>nnet</code>包。<code>nnet</code>包中有两个函数<code>nnet()</code>可以用来你和单个隐层级神经网络，<code>avNNet()</code>是在此基础上加入了bootstrap过程，然后对结果取平均。我们可以以<code>caret</code>包中的<code>train()</code>函数为接口，方便的训练这些模型。</p>
<p>现在我们以生猪疫情的数据为例，展示如何对神经网络进行调优。先载入数据，将自变量存在<code>trainx</code>中，应变量存在<code>trainy</code>中：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># 这是我自己写的一个包，没有安装的可以通过下面这行代码安装</span>
<span class="co"># devtools::install_github(&quot;happyrabbit/DataScienceR&quot;)</span>
<span class="kw">library</span>(DataScienceR)
<span class="kw">data</span>(<span class="st">&quot;sim1_da1&quot;</span>)
trainx =<span class="st"> </span>dplyr::<span class="kw">select</span>(sim1_da1, -y)
trainy =<span class="st"> </span><span class="kw">paste0</span>(<span class="st">&quot;BREAK&quot;</span>,sim1_da1$y)</code></pre></div>
<p>接下来设置需要调试的参数取值。之前讲过了，这里需要调试的有权重和隐变量的个数。先设置调优参数集。<code>decay</code>是权重衰减参数，这里设置了3个参数值。<code>size</code>是隐变量的个数。</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(caret)
nnetGrid &lt;-<span class="st"> </span><span class="kw">expand.grid</span>(<span class="dt">decay =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="fl">0.01</span>, .<span class="dv">1</span>),
                        <span class="dt">size =</span> <span class="kw">c</span>(<span class="dv">1</span>:<span class="dv">10</span>))
<span class="co"># 得到最大的隐变量个数</span>
maxSize &lt;-<span class="st"> </span><span class="kw">max</span>(nnetGrid$size)
<span class="co"># 计算最大隐变量个数对应的参数个数</span>
<span class="co"># 之前讲过，参数一共M(p+1)+M+1个</span>
numWts &lt;-<span class="st"> </span><span class="dv">1</span>*(maxSize *<span class="st"> </span>(<span class="kw">length</span>(trainx) +<span class="st"> </span><span class="dv">1</span>) +<span class="st"> </span>maxSize +<span class="st"> </span><span class="dv">1</span>)
<span class="co"># 为了保证结果的可重复性，这里设置一个随机种子</span>
<span class="kw">set.seed</span>(<span class="dv">2017</span>)</code></pre></div>
<p>然后设置交互校验的形式，这里我们按老规矩，设置10层交互校验。<code>summaryFunction = twoClassSummary</code>告诉函数返回ROC曲线下面积，敏感度和特异度统计量。</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ctrl &lt;-<span class="st"> </span><span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>, 
                     <span class="dt">number =</span> <span class="dv">10</span>, 
                     <span class="dt">classProbs =</span> <span class="ot">TRUE</span>, 
                     <span class="dt">summaryFunction =</span> twoClassSummary)</code></pre></div>
<p>接下来我们就可以将这些参数传递给相应的<code>train()</code>函数进行调优。</p>
<ul>
<li><code>method = &quot;nnet&quot;</code>： 使用<code>nnet</code>函数</li>
<li><code>tuneGrid</code>是调优参数，<code>trControl</code>是调优过程定义参数，这两个参数之前已经设置好了，直接赋值就好。</li>
<li><code>preProc = c(&quot;center&quot;, &quot;scale&quot;)</code>：对数据进行标准化</li>
<li><code>trace = FALSE</code>：不需要输出拟合的轨迹</li>
<li><code>MaxNWts</code>：最大权重参数个数</li>
<li><code>maxit</code>：最高迭代次数</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">nnetTune &lt;-<span class="st"> </span><span class="kw">train</span>(trainx, trainy,
                   <span class="dt">method =</span> <span class="st">&quot;nnet&quot;</span>,
                   <span class="dt">tuneGrid =</span> nnetGrid,
                   <span class="dt">trControl =</span> ctrl,
                   <span class="dt">preProc =</span> <span class="kw">c</span>(<span class="st">&quot;center&quot;</span>, <span class="st">&quot;scale&quot;</span>),
                   <span class="dt">trace =</span> <span class="ot">FALSE</span>,
                   <span class="dt">MaxNWts =</span> numWts, 
                   <span class="dt">maxit =</span> <span class="dv">500</span>)</code></pre></div>
<p>结果显示最优的隐变量个数是1，权重衰减参数为0.1：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">nnetTune</code></pre></div>
<pre><code>## Neural Network 
## 
## 800 samples
## 240 predictors
##   2 classes: &#39;BREAK0&#39;, &#39;BREAK1&#39; 
## 
## Pre-processing: centered (240), scaled (240) 
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 720, 720, 720, 720, 720, 720, ... 
## Resampling results across tuning parameters:
## 
##   decay  size  ROC        Sens       Spec     
##   0.00    1    0.8795739  0.8071429  0.8000000
##   0.00    2    0.8847744  0.8190476  0.8263158
##   0.00    3    0.8710526  0.8142857  0.7894737
##   0.00    4    0.8666667  0.8166667  0.8078947
##   0.00    5    0.8128759  0.7547619  0.7552632
##   0.00    6    0.8096491  0.7785714  0.7473684
##   0.00    7    0.8179825  0.7642857  0.7894737
##   0.00    8    0.8029449  0.8023810  0.7526316
##   0.00    9    0.8017544  0.7738095  0.7447368
##   0.00   10    0.8314536  0.7880952  0.7578947
##   0.01    1    0.9323308  0.8738095  0.8236842
##   0.01    2    0.9334586  0.8690476  0.8263158
##   0.01    3    0.9329574  0.8690476  0.8210526
##   0.01    4    0.9121554  0.8428571  0.8000000
##   0.01    5    0.9318922  0.8642857  0.8105263
##   0.01    6    0.9289474  0.8642857  0.8105263
##   0.01    7    0.9305764  0.8690476  0.8131579
##   0.01    8    0.9281328  0.8690476  0.7973684
##   0.01    9    0.9303885  0.8785714  0.7921053
##   0.01   10    0.9320175  0.8666667  0.8184211
##   0.10    1    0.9344612  0.8642857  0.8263158
##   0.10    2    0.9332080  0.8666667  0.8236842
##   0.10    3    0.9325188  0.8690476  0.8184211
##   0.10    4    0.9322055  0.8785714  0.8131579
##   0.10    5    0.9285088  0.8619048  0.8078947
##   0.10    6    0.9311404  0.8714286  0.7921053
##   0.10    7    0.9308271  0.8714286  0.8052632
##   0.10    8    0.9317043  0.8785714  0.8078947
##   0.10    9    0.9327694  0.8690476  0.8052632
##   0.10   10    0.9296366  0.8738095  0.8052632
## 
## ROC was used to select the optimal model using  the largest value.
## The final values used for the model were size = 1 and decay = 0.1.</code></pre>
<p>可以直接用<code>plot()</code>函数得到调优过程图：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(nnetTune)</code></pre></div>
<p><img src="DS_R_files/figure-html/unnamed-chunk-279-1.png" width="672" /></p>
</div>
<div id="bootstrap" class="section level3">
<h3><span class="header-section-number">16.5.2</span> bootstrap平均神经网络</h3>
<p>之前讲过，拟合神经网络模型会遇到多个局部最小值得问题，导致结果不稳定。可以用装袋法稳定权重估计，也就是采用bootstrap抽样，然后将这些基于bootstrap样本得到的结果平均。下面我们还是用 <code>train()</code>函数，但是这次调用的是<code>avNNet()</code>来训练模型。这里我们需要稍微更改下模型调优的参数设置，多了一个<code>bag</code>是一个逻辑参数，表示是否需要进行bootstrap抽样平均结果。这里设置是<code>TRUE</code>。</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># 这里运行速度比较慢</span>
trainx =<span class="st"> </span>dplyr::<span class="kw">select</span>(sim1_da1, -y)
trainy =<span class="st"> </span><span class="kw">paste0</span>(<span class="st">&quot;BREAK&quot;</span>,sim1_da1$y)

avnnetGrid &lt;-<span class="st"> </span><span class="kw">expand.grid</span>(<span class="dt">decay =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="fl">0.01</span>, .<span class="dv">1</span>),
                        <span class="dt">size =</span> <span class="kw">c</span>(<span class="dv">1</span>:<span class="dv">10</span>),
                        <span class="dt">bag =</span> <span class="ot">TRUE</span>)
                        
avnnetTune &lt;-<span class="st"> </span><span class="kw">train</span>(trainx, trainy,
                   <span class="dt">method =</span> <span class="st">&quot;avNNet&quot;</span>,
                   <span class="dt">tuneGrid =</span> avnnetGrid,
                   <span class="dt">trControl =</span> ctrl,
                   <span class="dt">preProc =</span> <span class="kw">c</span>(<span class="st">&quot;center&quot;</span>, <span class="st">&quot;scale&quot;</span>),
                   <span class="dt">trace =</span> <span class="ot">FALSE</span>,
                   <span class="dt">MaxNWts =</span> numWts, 
                   <span class="dt">maxit =</span> <span class="dv">500</span>)</code></pre></div>
<p>这里隐变量个数为10时对应的权重过多，因此出现下面错误提示：</p>
<pre>
In eval(expr, envir, enclos) :
  model fit failed for Fold10: decay=0.00, size=10, bag=TRUE Error in { : task 1 failed - "too many (2432) weights"
</pre>
<p>但这并不妨碍size从1到9的调优，我们可以查看调优结果，最优的 <code>size = 3</code>, <code>decay = 0.01</code>：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">avnnetTune</code></pre></div>
<pre><code>## Model Averaged Neural Network 
## 
## 800 samples
## 240 predictors
##   2 classes: &#39;BREAK0&#39;, &#39;BREAK1&#39; 
## 
## Pre-processing: centered (240), scaled (240) 
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 720, 720, 720, 720, 720, 720, ... 
## Resampling results across tuning parameters:
## 
##   decay  size  ROC        Sens       Spec     
##   0.00    1    0.7805138  0.7333333  0.7000000
##   0.00    2    0.7957393  0.7238095  0.7263158
##   0.00    3    0.7912281  0.7380952  0.7000000
##   0.00    4    0.8109962  0.7738095  0.6947368
##   0.00    5    0.8039474  0.7547619  0.6815789
##   0.00    6    0.8094925  0.7452381  0.7263158
##   0.00    7    0.8208647  0.7523810  0.7289474
##   0.00    8    0.8104323  0.7928571  0.6973684
##   0.00    9    0.7916040  0.7380952  0.6947368
##   0.00   10          NaN        NaN        NaN
##   0.01    1    0.9124687  0.8476190  0.8263158
##   0.01    2    0.9010652  0.8357143  0.7973684
##   0.01    3    0.9213659  0.8523810  0.8157895
##   0.01    4    0.9173559  0.8357143  0.8342105
##   0.01    5    0.9153509  0.8309524  0.8210526
##   0.01    6    0.9058271  0.8380952  0.7894737
##   0.01    7    0.9166667  0.8476190  0.8157895
##   0.01    8    0.9065789  0.8309524  0.8157895
##   0.01    9    0.9167293  0.8309524  0.8289474
##   0.01   10          NaN        NaN        NaN
##   0.10    1    0.9125313  0.8380952  0.8078947
##   0.10    2    0.9124687  0.8571429  0.8078947
##   0.10    3    0.9136591  0.8404762  0.8105263
##   0.10    4    0.9124687  0.8285714  0.8157895
##   0.10    5    0.9170426  0.8428571  0.8184211
##   0.10    6    0.9103383  0.8285714  0.8052632
##   0.10    7    0.9119674  0.8547619  0.7921053
##   0.10    8    0.9036341  0.8404762  0.8052632
##   0.10    9    0.9117794  0.8380952  0.8184211
##   0.10   10          NaN        NaN        NaN
## 
## Tuning parameter &#39;bag&#39; was held constant at a value of TRUE
## ROC was used to select the optimal model using  the largest value.
## The final values used for the model were size = 3, decay = 0.01 and bag
##  = TRUE.</code></pre>
</div>
<div id="section-16.5.3" class="section level3">
<h3><span class="header-section-number">16.5.3</span> 模型比较</h3>
<p>我们现在对同样的数据集，应用不同的模型，以此来和神经网络模型的结果进行比较。</p>
<ul>
<li>随机梯度助推</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">trainx =<span class="st"> </span>dplyr::<span class="kw">select</span>(sim1_da1, -y)
trainy =<span class="st"> </span>sim1_da1$y

gbmGrid &lt;-<span class="st"> </span><span class="kw">expand.grid</span>(<span class="dt">interaction.depth =</span> <span class="kw">seq</span>(<span class="dv">1</span>, <span class="dv">7</span>, <span class="dt">by =</span> <span class="dv">2</span>), 
                       <span class="dt">n.trees =</span> <span class="kw">seq</span>(<span class="dv">100</span>, <span class="dv">1000</span>, <span class="dt">by =</span> <span class="dv">50</span>),
                       <span class="dt">shrinkage =</span> <span class="kw">c</span>(<span class="fl">0.01</span>, <span class="fl">0.1</span>),
                       <span class="dt">n.minobsinnode =</span> <span class="dv">5</span>)
<span class="kw">set.seed</span>(<span class="dv">2017</span>)
gbmTune &lt;-<span class="st"> </span><span class="kw">train</span>(trainx, trainy,
                 <span class="dt">method=</span><span class="st">&quot;gbm&quot;</span>,
                 <span class="dt">tuneGrid =</span> gbmGrid,
                 <span class="dt">verbose =</span> <span class="ot">FALSE</span>)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(pROC)
pregbm &lt;-<span class="st"> </span><span class="kw">predict</span>(gbmTune, trainx)
<span class="kw">roc</span>(pregbm, trainy)</code></pre></div>
<pre><code>## 
## Call:
## roc.default(response = pregbm, predictor = trainy)
## 
## Data: trainy in 426 controls (pregbm BREAK0) &lt; 374 cases (pregbm BREAK1).
## Area under the curve: 0.9503</code></pre>
<p>这里随机梯度助推得到的<code>AUC=0.9503</code>。</p>
<ul>
<li>群组lasso逻辑回归</li>
</ul>
<p>我们在之前章节的一个知识扩展小节中介绍过群组lasso逻辑回归，其中对相同的数据使用了群组lasso，结果和神经网络差不多。为了更加清晰的比较，这里再次重复一下之前用过的方法。关于更多的模型背景介绍，大家可以返回“知识扩展：群组lasso逻辑回归”。</p>
<p>我自己写的包<code>DataScienceR</code>中的函数 <code>cv_glasso()</code>可以用来对不同的参数进行调优：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># trainx是自变量矩阵，去除应变量列</span>
trainx =<span class="st"> </span>dplyr::<span class="kw">select</span>(sim1_da1, -y)
<span class="co"># 将应变量存在trainy中</span>
trainy =<span class="st"> </span>sim1_da1$y
<span class="co"># 得到关于群组的指针</span>
index &lt;-<span class="st"> </span><span class="kw">gsub</span>(<span class="st">&quot;</span><span class="ch">\\</span><span class="st">..*&quot;</span>, <span class="st">&quot;&quot;</span>, <span class="kw">names</span>(trainx))
<span class="co"># 对100个调优参数值进行调优</span>
nlam &lt;-<span class="st"> </span><span class="dv">100</span>
<span class="co"># 设置调优过程中模型的预测类型</span>
<span class="co"># - `link`：返回链结函数的拟合值</span>
<span class="co"># - `response`：返回拟合的概率值</span>
<span class="co"># number of cross-validation folds</span>
kfold &lt;-<span class="st"> </span><span class="dv">10</span>
cv_fit &lt;-<span class="st"> </span><span class="kw">cv_glasso</span>(trainx, trainy, <span class="dt">nlam =</span> nlam, <span class="dt">kfold =</span> kfold)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">cv_fit$lambda.max.auc </code></pre></div>
<div class="sourceCode"><pre class="sourceCode html"><code class="sourceCode html">   lambda       auc 
1.0009576 0.9453094 </code></pre></div>
<p>对应最优的调优参数值<code>lambda=1</code>，最优的<code>AUC=0.945</code>。略优于神经网络，但是可解释性和速度都高很多。总结最后得到的3个模型表现结果：</p>
<pre><code>##  nnet   gbm lasso 
##  0.93  0.95  0.95</code></pre>
<p>大家可以看到，3个模型表现非常类似，随机助推和lasso逻辑回归的结果略优于神经网络。但从计算时间和模型的可解释性上考虑，我会选择使用lasso。</p>
</div>
</div>
<div id="section-16.6" class="section level2">
<h2><span class="header-section-number">16.6</span> 深度学习</h2>
<p>最后稍微提一下深度学习，这个词大家应该都该听过。不一定吃过猪肉但全都见过猪跑。深度学习是近几年发展出的<strong>多层神经网络</strong>，其实说是<strong>深度神经网络</strong>更加恰当。可以应用于高度非线性的数据。这个模型的应用主要集中在语言识别，无人车，自然语言处理这类问题上。其背后的数学原理并不是新的，只是由于计算机的发展有了新的应用。过去的一年里，深度学习炒作的很热。这里再次强调，不同的模型适用的领域不同。这类模型在社会学，心理学，市场营销方面的应用极少。我的感觉是，神经网络，或者这里的多层神经网络（也就是深度学习）适用的是那些人类可以在短时间内做出反应的事情，比如开车，识别语言，面孔，或者规则明确的事情，比如棋牌。而不适用于更加开放的问题，比如客户感知分析，客户购买行为分析，因果分析，风险控制等等。这里不打算对深度学习进行过多展开。只想提一下深度学习的几个R包，并且用本章一直使用的数据提供一些案例代码。这些代码都可以重复，但这里不会进行过多的说明。</p>
<p>下面的表格中总结了R中主要深度学习包和功能：</p>
<table>
<colgroup>
<col width="13%" />
<col width="86%" />
</colgroup>
<thead>
<tr class="header">
<th>深度学习R包</th>
<th>功能</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>mxnet</td>
<td>前馈神经网络（Feed-forward neural network），卷积神经网络（convolutional neural network）</td>
</tr>
<tr class="even">
<td>darch</td>
<td>受限玻尔兹曼机（Restricted Boltzmann machine）, 深度信念网络（deep belief network）</td>
</tr>
<tr class="odd">
<td>deepnet</td>
<td>前馈神经网络，受限玻尔兹曼机，深度信念网络 ，堆栈式自编码器（stacked autoencoders）</td>
</tr>
<tr class="even">
<td>h2o</td>
<td>前馈神经网络，深度自编码器（deep autoencoders）</td>
</tr>
</tbody>
</table>
<div id="mxnet" class="section level3">
<h3><span class="header-section-number">16.6.1</span> <code>mxnet</code>包</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># 可以从github上下载该包</span>
<span class="kw">install.packages</span>(<span class="st">&quot;drat&quot;</span>, <span class="dt">repos =</span> <span class="st">&quot;https://cran.rstudio.com&quot;</span>)
drat:::<span class="kw">addRepo</span>(<span class="st">&quot;dmlc&quot;</span>)
<span class="kw">install.packages</span>(<span class="st">&quot;mxnet&quot;</span>)

<span class="co"># 载入程序包</span>
<span class="kw">library</span>(mxnet)

<span class="co"># 这里用10层交互校验</span>
idx &lt;-<span class="st"> </span><span class="kw">createFolds</span>(sim1_da1$y, <span class="dt">k =</span> <span class="dv">10</span>, <span class="dt">list =</span> <span class="ot">TRUE</span>, <span class="dt">returnTrain =</span> <span class="ot">FALSE</span>)

res &lt;-<span class="st"> </span><span class="ot">NULL</span>
test &lt;-<span class="st"> </span><span class="ot">NULL</span>

for (i in <span class="dv">1</span>:<span class="dv">10</span>) {
    testx =<span class="st"> </span>dplyr::<span class="kw">select</span>(sim1_da1[idx[[i]], ], -y)
    testy =<span class="st"> </span>sim1_da1[idx[[i]], ]$y
    
    trainx =<span class="st"> </span>dplyr::<span class="kw">select</span>(sim1_da1[-idx[[i]], ], -y)
    trainy =<span class="st"> </span>sim1_da1[-idx[[i]], ]$y
    
    model &lt;-<span class="st"> </span><span class="kw">mx.mlp</span>(<span class="kw">as.matrix</span>(trainx), 
                    trainy,
                    <span class="dt">hidden_node =</span> <span class="kw">c</span>(<span class="dv">128</span>, <span class="dv">64</span>), 
                    <span class="dt">out_node =</span> <span class="dv">2</span>, 
                    <span class="dt">activation =</span> <span class="st">&quot;relu&quot;</span>, 
                    <span class="dt">out_activation =</span> <span class="st">&quot;softmax&quot;</span>, 
                    <span class="dt">num.round =</span> <span class="dv">100</span>, 
                    <span class="dt">array.batch.size =</span> <span class="dv">15</span>, 
                    <span class="dt">learning.rate =</span> <span class="fl">0.07</span>, 
                    <span class="dt">momentum =</span> <span class="fl">0.9</span>,
                    <span class="dt">device =</span> <span class="kw">mx.cpu</span>())
    
    <span class="co"># 将预测的结果存在res对象内</span>
    res &lt;-<span class="st"> </span><span class="kw">rbind</span>(res, <span class="kw">t</span>(<span class="kw">predict</span>(model, <span class="kw">as.matrix</span>(testx))))
    
    <span class="co"># 将测试集上的真实结果存在test对象内</span>
    test &lt;-<span class="st"> </span><span class="kw">c</span>(test, testy)
}

<span class="co"># 检查模型结果的auc</span>
pROC::<span class="kw">roc</span>(res[, <span class="dv">1</span>], test)</code></pre></div>
</div>
<div id="darch" class="section level3">
<h3><span class="header-section-number">16.6.2</span> <code>darch</code>包</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># 该包可以直接从CRAN下载 载入程序包</span>
<span class="kw">library</span>(darch)

<span class="co"># 这里用10层交互校验</span>
idx &lt;-<span class="st"> </span><span class="kw">createFolds</span>(sim1_da1$y, <span class="dt">k =</span> <span class="dv">10</span>, <span class="dt">list =</span> <span class="ot">TRUE</span>, <span class="dt">returnTrain =</span> <span class="ot">FALSE</span>)

res &lt;-<span class="st"> </span><span class="ot">NULL</span>
test &lt;-<span class="st"> </span><span class="ot">NULL</span>

for (i in <span class="dv">1</span>:<span class="dv">10</span>) {
    testx =<span class="st"> </span>dplyr::<span class="kw">select</span>(sim1_da1[idx[[i]], ], -y)
    testy =<span class="st"> </span>sim1_da1[idx[[i]], ]$y
    
    trainx =<span class="st"> </span>dplyr::<span class="kw">select</span>(sim1_da1[-idx[[i]], ], -y)
    trainy =<span class="st"> </span>sim1_da1[-idx[[i]], ]$y
    
    model &lt;-<span class="st"> </span><span class="kw">darch</span>(trainx,
                   trainy, 
                   <span class="dt">rbm.numEpochs =</span> <span class="dv">0</span>, 
                   <span class="dt">rbm.batchSize =</span> <span class="dv">100</span>, 
                   <span class="dt">rbm.trainOutputLayer =</span> F, 
                   <span class="dt">layers =</span> <span class="kw">c</span>(<span class="dv">784</span>, <span class="dv">100</span>, <span class="dv">10</span>),
                   <span class="dt">darch.batchSize =</span> <span class="dv">100</span>, 
                   <span class="dt">darch.learnRate =</span> <span class="dv">2</span>, 
                   <span class="dt">darch.retainData =</span> F, 
                   <span class="dt">darch.numEpochs =</span> <span class="dv">20</span>)
    
    <span class="co"># 将预测的结果存在res对象内</span>
    res &lt;-<span class="st"> </span><span class="kw">c</span>(res, <span class="kw">predict</span>(model, testx))
    <span class="co"># 将测试集上的真实结果存在test对象内</span>
    test &lt;-<span class="st"> </span><span class="kw">c</span>(test, testy)
}

<span class="co"># 检查模型结果的auc</span>
pROC::<span class="kw">auc</span>(res, test)</code></pre></div>
</div>
<div id="deepnet" class="section level3">
<h3><span class="header-section-number">16.6.3</span> <code>deepnet</code>包</h3>
<p>恭喜你，<code>caret</code>包有关于<code>deepnet</code>的接口，这意味着我们可以直接用<code>train()</code>函数进行调优啦：）。</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">trainx =<span class="st"> </span>dplyr::<span class="kw">select</span>(sim1_da1, -y)
trainy =<span class="st"> </span><span class="kw">paste0</span>(<span class="st">&quot;BREAK&quot;</span>, sim1_da1$y)

dnnGrid &lt;-<span class="st"> </span><span class="kw">expand.grid</span>(<span class="dt">layer1 =</span> <span class="dv">1</span>:<span class="dv">5</span>, <span class="dt">layer2 =</span> <span class="dv">0</span>:<span class="dv">3</span>, <span class="dt">layer3 =</span> <span class="dv">0</span>:<span class="dv">3</span>, <span class="dt">hidden_dropout =</span> <span class="kw">c</span>(<span class="dv">0</span>, 
    <span class="fl">0.1</span>, <span class="fl">0.2</span>, <span class="fl">0.3</span>), <span class="dt">visible_dropout =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="fl">0.1</span>, <span class="fl">0.2</span>, <span class="fl">0.3</span>))

dnnTune &lt;-<span class="st"> </span><span class="kw">train</span>(trainx,
                 trainy, 
                 <span class="dt">method =</span> <span class="st">&quot;dnn&quot;</span>, 
                 <span class="dt">tuneGrid =</span> dnnGrid, 
                 <span class="dt">trControl =</span> <span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>), 
                 <span class="dt">preProc =</span> <span class="kw">c</span>(<span class="st">&quot;center&quot;</span>, <span class="st">&quot;scale&quot;</span>))
<span class="co"># 检查模型结果的auc</span>
<span class="kw">auc</span>(<span class="kw">predict</span>(dnnTune, trainx), trainy)</code></pre></div>
</div>
</div>
<div id="-6" class="section level2">
<h2><span class="header-section-number">16.7</span> 本章总结</h2>
<p>本章介绍了神经网络的相关背景知识。希望通过阅读这个章节，大家能够对神经网络这个黑箱，且过度神化的模型有更加理性的认识。 书中有的代码运行需要一些时间，大家可以通过并行计算提高速度。</p>
<p>关于神经网络需要注意的几个点：</p>
<ol style="list-style-type: decimal">
<li>数据标准化</li>
<li>对于标准化后的数据，R中的函数通常都会有权重初始值的默认设置，这个初始值一般接近于0，这样神经网络能从线性开始向着非线性迭代</li>
<li>激活函数的形式能够对神经网络产生很大的影响</li>
</ol>
<p>因为神经网络没有可解释性，所以彻底靠结果定成败，如果精确度不达标，果断舍弃。精确度相当的情况下，选择最简单最容易解释的模型。还是那句话，所有模型本质上都是错的，但有一些是有用的。</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="section-15.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="section-17.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/14-shenduxuexi.Rmd",
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
