<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>数据科学家：R语言</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="This is my first book on data science">
  <meta name="generator" content="bookdown 0.1 and GitBook 2.6.7">

  <meta property="og:title" content="数据科学家：R语言" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is my first book on data science" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="数据科学家：R语言" />
  
  <meta name="twitter:description" content="This is my first book on data science" />
  

<meta name="author" content="林荟">

<meta name="date" content="2016-12-11">

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="section-6.html">
<link rel="next" href="section-8.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">数据科学家：R语言</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> 介绍</a></li>
<li class="chapter" data-level="2" data-path="section-2.html"><a href="section-2.html"><i class="fa fa-check"></i><b>2</b> 数据科学</a><ul>
<li class="chapter" data-level="2.1" data-path="section-2.html"><a href="section-2.html#section-2.1"><i class="fa fa-check"></i><b>2.1</b> 什么是数据科学？</a></li>
<li class="chapter" data-level="2.2" data-path="section-2.html"><a href="section-2.html#section-2.2"><i class="fa fa-check"></i><b>2.2</b> 什么是数据科学家？</a></li>
<li class="chapter" data-level="2.3" data-path="section-2.html"><a href="section-2.html#section-2.3"><i class="fa fa-check"></i><b>2.3</b> 数据科学家需要的技能</a></li>
<li class="chapter" data-level="2.4" data-path="section-2.html"><a href="section-2.html#section-2.4"><i class="fa fa-check"></i><b>2.4</b> 数据科学可以解决什么问题？</a><ul>
<li class="chapter" data-level="2.4.1" data-path="section-2.html"><a href="section-2.html#section-2.4.1"><i class="fa fa-check"></i><b>2.4.1</b> 前提要求</a></li>
<li class="chapter" data-level="2.4.2" data-path="section-2.html"><a href="section-2.html#section-2.4.2"><i class="fa fa-check"></i><b>2.4.2</b> 问题种类</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="section-3.html"><a href="section-3.html"><i class="fa fa-check"></i><b>3</b> 数据集模拟和背景介绍</a><ul>
<li class="chapter" data-level="3.1" data-path="section-3.html"><a href="section-3.html#section-3.1"><i class="fa fa-check"></i><b>3.1</b> 服装消费者数据</a></li>
<li class="chapter" data-level="3.2" data-path="section-3.html"><a href="section-3.html#section-3.2"><i class="fa fa-check"></i><b>3.2</b> 航空公司满意度调查</a></li>
<li class="chapter" data-level="3.3" data-path="section-3.html"><a href="section-3.html#section-3.3"><i class="fa fa-check"></i><b>3.3</b> 生猪疫情风险预测数据</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="section-4.html"><a href="section-4.html"><i class="fa fa-check"></i><b>4</b> 数据分析一般流程</a><ul>
<li class="chapter" data-level="4.1" data-path="section-4.html"><a href="section-4.html#section-4.1"><i class="fa fa-check"></i><b>4.1</b> 问题到数据</a></li>
<li class="chapter" data-level="4.2" data-path="section-4.html"><a href="section-4.html#section-4.2"><i class="fa fa-check"></i><b>4.2</b> 数据到信息</a></li>
<li class="chapter" data-level="4.3" data-path="section-4.html"><a href="section-4.html#section-4.3"><i class="fa fa-check"></i><b>4.3</b> 信息到行动</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="section-5.html"><a href="section-5.html"><i class="fa fa-check"></i><b>5</b> 数据预处理</a><ul>
<li class="chapter" data-level="5.1" data-path="section-5.html"><a href="section-5.html#section-5.1"><i class="fa fa-check"></i><b>5.1</b> 介绍</a></li>
<li class="chapter" data-level="5.2" data-path="section-5.html"><a href="section-5.html#section-5.2"><i class="fa fa-check"></i><b>5.2</b> 数据清理</a></li>
<li class="chapter" data-level="5.3" data-path="section-5.html"><a href="section-5.html#section-5.3"><i class="fa fa-check"></i><b>5.3</b> 缺失值填补</a><ul>
<li class="chapter" data-level="5.3.1" data-path="section-5.html"><a href="section-5.html#section-5.3.1"><i class="fa fa-check"></i><b>5.3.1</b> 中位数或众数填补</a></li>
<li class="chapter" data-level="5.3.2" data-path="section-5.html"><a href="section-5.html#k-"><i class="fa fa-check"></i><b>5.3.2</b> K-近邻填补</a></li>
<li class="chapter" data-level="5.3.3" data-path="section-5.html"><a href="section-5.html#section-5.3.3"><i class="fa fa-check"></i><b>5.3.3</b> 袋状树填补</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="section-5.html"><a href="section-5.html#section-5.4"><i class="fa fa-check"></i><b>5.4</b> 中心化和标量化</a></li>
<li class="chapter" data-level="5.5" data-path="section-5.html"><a href="section-5.html#section-5.5"><i class="fa fa-check"></i><b>5.5</b> 有偏分布</a></li>
<li class="chapter" data-level="5.6" data-path="section-5.html"><a href="section-5.html#section-5.6"><i class="fa fa-check"></i><b>5.6</b> 处理离群点</a></li>
<li class="chapter" data-level="5.7" data-path="section-5.html"><a href="section-5.html#section-5.7"><i class="fa fa-check"></i><b>5.7</b> 共线性</a></li>
<li class="chapter" data-level="5.8" data-path="section-5.html"><a href="section-5.html#section-5.8"><i class="fa fa-check"></i><b>5.8</b> 稀疏变量</a></li>
<li class="chapter" data-level="5.9" data-path="section-5.html"><a href="section-5.html#section-5.9"><i class="fa fa-check"></i><b>5.9</b> 编码名义变量</a></li>
<li class="chapter" data-level="5.10" data-path="section-5.html"><a href="section-5.html#section-5.10"><i class="fa fa-check"></i><b>5.10</b> 本章总结</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="section-6.html"><a href="section-6.html"><i class="fa fa-check"></i><b>6</b> 数据操作</a><ul>
<li class="chapter" data-level="6.1" data-path="section-6.html"><a href="section-6.html#section-6.1"><i class="fa fa-check"></i><b>6.1</b> 数据读写</a><ul>
<li class="chapter" data-level="6.1.1" data-path="section-6.html"><a href="section-6.html#tibble"><i class="fa fa-check"></i><b>6.1.1</b> 取代传统数据框的<code>tibble</code>对象</a></li>
<li class="chapter" data-level="6.1.2" data-path="section-6.html"><a href="section-6.html#readr"><i class="fa fa-check"></i><b>6.1.2</b> 高效数据读写：<code>readr</code>包</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="section-6.html"><a href="section-6.html#section-6.2"><i class="fa fa-check"></i><b>6.2</b> 数据整合</a><ul>
<li class="chapter" data-level="6.2.1" data-path="section-6.html"><a href="section-6.html#baseapply"><i class="fa fa-check"></i><b>6.2.1</b> base包：apply()</a></li>
<li class="chapter" data-level="6.2.2" data-path="section-6.html"><a href="section-6.html#plyrddply"><i class="fa fa-check"></i><b>6.2.2</b> plyr包：ddply()函数</a></li>
<li class="chapter" data-level="6.2.3" data-path="section-6.html"><a href="section-6.html#dplyr"><i class="fa fa-check"></i><b>6.2.3</b> dplyr包</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="section-6.html"><a href="section-6.html#section-6.3"><i class="fa fa-check"></i><b>6.3</b> 数据整形</a><ul>
<li class="chapter" data-level="6.3.1" data-path="section-6.html"><a href="section-6.html#reshape2"><i class="fa fa-check"></i><b>6.3.1</b> <code>reshape2</code>包</a></li>
<li class="chapter" data-level="6.3.2" data-path="section-6.html"><a href="section-6.html#tidyr"><i class="fa fa-check"></i><b>6.3.2</b> <code>tidyr</code>包</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="section-6.html"><a href="section-6.html#-1"><i class="fa fa-check"></i><b>6.4</b> 本章总结</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="section-7.html"><a href="section-7.html"><i class="fa fa-check"></i><b>7</b> 基础建模技术</a><ul>
<li class="chapter" data-level="7.1" data-path="section-7.html"><a href="section-7.html#section-7.1"><i class="fa fa-check"></i><b>7.1</b> 有监督和无监督</a></li>
<li class="chapter" data-level="7.2" data-path="section-7.html"><a href="section-7.html#section-7.2"><i class="fa fa-check"></i><b>7.2</b> 误差及其来源</a><ul>
<li class="chapter" data-level="7.2.1" data-path="section-7.html"><a href="section-7.html#section-7.2.1"><i class="fa fa-check"></i><b>7.2.1</b> 系统误差和随机误差</a></li>
<li class="chapter" data-level="7.2.2" data-path="section-7.html"><a href="section-7.html#section-7.2.2"><i class="fa fa-check"></i><b>7.2.2</b> 应变量误差</a></li>
<li class="chapter" data-level="7.2.3" data-path="section-7.html"><a href="section-7.html#section-7.2.3"><i class="fa fa-check"></i><b>7.2.3</b> 自变量误差</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="section-7.html"><a href="section-7.html#section-7.3"><i class="fa fa-check"></i><b>7.3</b> 数据划分和再抽样</a><ul>
<li class="chapter" data-level="7.3.1" data-path="section-7.html"><a href="section-7.html#section-7.3.1"><i class="fa fa-check"></i><b>7.3.1</b> 划分训练集和测试集</a></li>
<li class="chapter" data-level="7.3.2" data-path="section-7.html"><a href="section-7.html#section-7.3.2"><i class="fa fa-check"></i><b>7.3.2</b> 重抽样</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="section-7.html"><a href="section-7.html#-2"><i class="fa fa-check"></i><b>7.4</b> 本章总结</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="section-8.html"><a href="section-8.html"><i class="fa fa-check"></i><b>8</b> 模型评估度量</a><ul>
<li class="chapter" data-level="8.1" data-path="section-8.html"><a href="section-8.html#section-8.1"><i class="fa fa-check"></i><b>8.1</b> 回归模型评估度量</a></li>
<li class="chapter" data-level="8.2" data-path="section-8.html"><a href="section-8.html#section-8.2"><i class="fa fa-check"></i><b>8.2</b> 分类模型评估度量</a><ul>
<li class="chapter" data-level="8.2.1" data-path="section-8.html"><a href="section-8.html#kappa"><i class="fa fa-check"></i><b>8.2.1</b> Kappa统计量</a></li>
<li class="chapter" data-level="8.2.2" data-path="section-8.html"><a href="section-8.html#roc"><i class="fa fa-check"></i><b>8.2.2</b> ROC曲线</a></li>
<li class="chapter" data-level="8.2.3" data-path="section-8.html"><a href="section-8.html#section-8.2.3"><i class="fa fa-check"></i><b>8.2.3</b> 提升图</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="section-8.html"><a href="section-8.html#-3"><i class="fa fa-check"></i><b>8.3</b> 本章总结</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="section-9.html"><a href="section-9.html"><i class="fa fa-check"></i><b>9</b> 特征工程</a><ul>
<li class="chapter" data-level="9.1" data-path="section-9.html"><a href="section-9.html#section-9.1"><i class="fa fa-check"></i><b>9.1</b> 特征构建</a></li>
<li class="chapter" data-level="9.2" data-path="section-9.html"><a href="section-9.html#section-9.2"><i class="fa fa-check"></i><b>9.2</b> 特征提取</a><ul>
<li class="chapter" data-level="9.2.1" data-path="section-9.html"><a href="section-9.html#section-9.2.1"><i class="fa fa-check"></i><b>9.2.1</b> 初步探索数据</a></li>
<li class="chapter" data-level="9.2.2" data-path="section-9.html"><a href="section-9.html#section-9.2.2"><i class="fa fa-check"></i><b>9.2.2</b> 主成分分析</a></li>
<li class="chapter" data-level="9.2.3" data-path="section-9.html"><a href="section-9.html#section-9.2.3"><i class="fa fa-check"></i><b>9.2.3</b> 探索性因子分析</a></li>
<li class="chapter" data-level="9.2.4" data-path="section-9.html"><a href="section-9.html#section-9.2.4"><i class="fa fa-check"></i><b>9.2.4</b> 高维标度化</a></li>
<li class="chapter" data-level="9.2.5" data-path="section-9.html"><a href="section-9.html#section-9.2.5"><i class="fa fa-check"></i><b>9.2.5</b> 知识扩展</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="section-9.html"><a href="section-9.html#section-9.3"><i class="fa fa-check"></i><b>9.3</b> 特征选择</a><ul>
<li class="chapter" data-level="9.3.1" data-path="section-9.html"><a href="section-9.html#section-9.3.1"><i class="fa fa-check"></i><b>9.3.1</b> 过滤法</a></li>
<li class="chapter" data-level="9.3.2" data-path="section-9.html"><a href="section-9.html#section-9.3.2"><i class="fa fa-check"></i><b>9.3.2</b> 绕封法</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="section-10.html"><a href="section-10.html"><i class="fa fa-check"></i><b>10</b> 线性回归极其衍生</a><ul>
<li class="chapter" data-level="10.1" data-path="section-10.html"><a href="section-10.html#section-10.1"><i class="fa fa-check"></i><b>10.1</b> 普通线性回归</a><ul>
<li class="chapter" data-level="10.1.1" data-path="section-10.html"><a href="section-10.html#section-10.1.1"><i class="fa fa-check"></i><b>10.1.1</b> 最小二乘线性模型</a></li>
<li class="chapter" data-level="10.1.2" data-path="section-10.html"><a href="section-10.html#section-10.1.2"><i class="fa fa-check"></i><b>10.1.2</b> 回归诊断</a></li>
<li class="chapter" data-level="10.1.3" data-path="section-10.html"><a href="section-10.html#section-10.1.3"><i class="fa fa-check"></i><b>10.1.3</b> 离群点，高杠杆点和强影响点</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="section-10.html"><a href="section-10.html#section-10.2"><i class="fa fa-check"></i><b>10.2</b> 收缩方法</a><ul>
<li class="chapter" data-level="10.2.1" data-path="section-10.html"><a href="section-10.html#section-10.2.1"><i class="fa fa-check"></i><b>10.2.1</b> 岭回归</a></li>
<li class="chapter" data-level="10.2.2" data-path="section-10.html"><a href="section-10.html#lasso"><i class="fa fa-check"></i><b>10.2.2</b> Lasso</a></li>
<li class="chapter" data-level="10.2.3" data-path="section-10.html"><a href="section-10.html#section-10.2.3"><i class="fa fa-check"></i><b>10.2.3</b> 弹性网络</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="section-10.html"><a href="section-10.html#lasso"><i class="fa fa-check"></i><b>10.3</b> 知识扩展：Lasso的变量选择功能</a></li>
<li class="chapter" data-level="10.4" data-path="section-10.html"><a href="section-10.html#section-10.4"><i class="fa fa-check"></i><b>10.4</b> 主成分和偏最小二乘回归</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="section-11.html"><a href="section-11.html"><i class="fa fa-check"></i><b>11</b> 广义线性模型压缩方法</a><ul>
<li class="chapter" data-level="11.1" data-path="section-11.html"><a href="section-11.html#glmnet"><i class="fa fa-check"></i><b>11.1</b> 初识<code>glmnet</code></a></li>
<li class="chapter" data-level="11.2" data-path="section-11.html"><a href="section-11.html#section-11.2"><i class="fa fa-check"></i><b>11.2</b> 收缩线性回归</a></li>
<li class="chapter" data-level="11.3" data-path="section-11.html"><a href="section-11.html#section-11.3"><i class="fa fa-check"></i><b>11.3</b> 逻辑回归</a><ul>
<li class="chapter" data-level="11.3.1" data-path="section-11.html"><a href="section-11.html#section-11.3.1"><i class="fa fa-check"></i><b>11.3.1</b> 普通逻辑回归</a></li>
<li class="chapter" data-level="11.3.2" data-path="section-11.html"><a href="section-11.html#section-11.3.2"><i class="fa fa-check"></i><b>11.3.2</b> 收缩逻辑回归</a></li>
<li class="chapter" data-level="11.3.3" data-path="section-10.html"><a href="section-10.html#lasso"><i class="fa fa-check"></i><b>11.3.3</b> 知识扩展：群组lasso逻辑回归</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="section-11.html"><a href="section-11.html#section-11.4"><i class="fa fa-check"></i><b>11.4</b> 收缩多项回归</a></li>
<li class="chapter" data-level="11.5" data-path="section-11.html"><a href="section-11.html#section-11.5"><i class="fa fa-check"></i><b>11.5</b> 泊松收缩回归</a></li>
<li class="chapter" data-level="11.6" data-path="section-11.html"><a href="section-11.html#-4"><i class="fa fa-check"></i><b>11.6</b> 本章总结</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="section-12.html"><a href="section-12.html"><i class="fa fa-check"></i><b>12</b> 树模型</a><ul>
<li class="chapter" data-level="12.1" data-path="section-12.html"><a href="section-12.html#section-12.1"><i class="fa fa-check"></i><b>12.1</b> 分裂准则</a></li>
<li class="chapter" data-level="12.2" data-path="section-12.html"><a href="section-12.html#section-12.2"><i class="fa fa-check"></i><b>12.2</b> 树的修剪</a></li>
<li class="chapter" data-level="12.3" data-path="section-12.html"><a href="section-12.html#section-12.3"><i class="fa fa-check"></i><b>12.3</b> 回归树和决策树</a><ul>
<li class="chapter" data-level="12.3.1" data-path="section-12.html"><a href="section-12.html#section-12.3.1"><i class="fa fa-check"></i><b>12.3.1</b> 回归树</a></li>
<li class="chapter" data-level="12.3.2" data-path="section-12.html"><a href="section-12.html#section-12.3.2"><i class="fa fa-check"></i><b>12.3.2</b> 决策树</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="section-12.html"><a href="section-12.html#section-12.4"><i class="fa fa-check"></i><b>12.4</b> 装袋树</a></li>
<li class="chapter" data-level="12.5" data-path="section-12.html"><a href="section-12.html#section-12.5"><i class="fa fa-check"></i><b>12.5</b> 随机森林</a></li>
<li class="chapter" data-level="12.6" data-path="section-12.html"><a href="section-12.html#section-12.6"><i class="fa fa-check"></i><b>12.6</b> 助推法</a></li>
<li class="chapter" data-level="12.7" data-path="section-12.html"><a href="section-12.html#section-12.7"><i class="fa fa-check"></i><b>12.7</b> 知识扩展：助推法的可加模型框架</a></li>
<li class="chapter" data-level="12.8" data-path="section-12.html"><a href="section-12.html#section-12.8"><i class="fa fa-check"></i><b>12.8</b> 知识扩展：助推树的数学框架</a><ul>
<li class="chapter" data-level="12.8.1" data-path="section-12.html"><a href="section-12.html#section-12.8.1"><i class="fa fa-check"></i><b>12.8.1</b> 数学表达</a></li>
<li class="chapter" data-level="12.8.2" data-path="section-12.html"><a href="section-12.html#section-12.8.2"><i class="fa fa-check"></i><b>12.8.2</b> 梯度助推数值优化</a></li>
</ul></li>
<li class="chapter" data-level="12.9" data-path="section-12.html"><a href="section-12.html#-5"><i class="fa fa-check"></i><b>12.9</b> 本章总结</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="section-13.html"><a href="section-13.html"><i class="fa fa-check"></i><b>13</b> 深度学习</a><ul>
<li class="chapter" data-level="13.1" data-path="section-6.html"><a href="section-6.html#-1"><i class="fa fa-check"></i><b>13.1</b> 介绍</a></li>
<li class="chapter" data-level="13.2" data-path="section-13.html"><a href="section-13.html#r"><i class="fa fa-check"></i><b>13.2</b> R中深度学习包</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>14</b> References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">数据科学家：R语言</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="section-7" class="section level1">
<h1><span class="header-section-number">第7章</span> 基础建模技术</h1>
<p>建模技术指代一系列用于理解数据的工具。本章介绍基本的统计学习术语，概念，以及一些辅助性的技能。后面章节会分别对一些特定模型进行展开。</p>
<div id="section-7.1" class="section level2">
<h2><span class="header-section-number">7.1</span> 有监督和无监督</h2>
<p>建模技术可以粗略的分为有监督和无监督这两类。大部分统计学习方法都可以归于其中一种。广义上说<strong>有监督方法</strong>涉及根据一个或者多个输入变量（也称为自变量，解释变量，预测变量），估计或者预测一个<strong>结果变量</strong>（也称为因变量，响应变量）。而<strong>无监督方法</strong>只考虑自变量，没有应变量作为“监督”，我们通过这类方法探索观测数据中内在变量结构。我们在之前提到的方法中，袋状树，广义线性回归是有监督方法；主成分分析，探索性因子分析，对近0方差和高相关变量的筛选都是无监督方法。我们先介绍这里的数学公式表达。</p>
<p>我们用<span class="math inline">\(n\)</span>表示样本量（或者观测数目）。<span class="math inline">\(p\)</span>代表自变量数目。我们用<span class="math inline">\(\mathbf{X}\)</span>表示<span class="math inline">\(n\times p\)</span>观测矩阵：</p>
<p><span class="math display">\[
\mathbf{X}=\left[\begin{array}{cccc}
x_{11} &amp; x_{12} &amp; \cdots &amp; x_{1p}\\
x_{21} &amp; x_{22} &amp; \cdots &amp; x_{2p}\\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
x_{n1} &amp; x_{n2} &amp; \cdots &amp; x_{np}
\end{array}\right]
\]</span></p>
<p>其中<span class="math inline">\(x_{ij}\)</span>代表第i个样本第j个变量的观测，<span class="math inline">\(i=1, \ldots, n\)</span>，<span class="math inline">\(j=1, \ldots, p\)</span>。<span class="math inline">\(\mathbf{x_{i.}}\)</span>代表第i个样本的所有变量观测组成的向量，向量统一按列排：</p>
<p><span class="math display">\[
\mathbf{x_{i.}}=\left[\begin{array}{c}
x_{i1}\\
x_{i2}\\
\vdots\\
x_{ip}
\end{array}\right]
\]</span></p>
<p>类似的，<span class="math inline">\(\mathbf{x_{.j}}\)</span>代表第j个变量的所有样本观测组成的向量：</p>
<p><span class="math display">\[
\mathbf{x_{.j}}=\left[\begin{array}{c}
x_{1j}\\
x_{2j}\\
\vdots\\
x_{nj}
\end{array}\right]
\]</span></p>
<p>于是我们有：</p>
<p><span class="math display">\[
\mathbf{X}=\left[\begin{array}{cccc}
x_{11} &amp; x_{12} &amp; \cdots &amp; x_{1p}\\
x_{21} &amp; x_{22} &amp; \cdots &amp; x_{2p}\\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
x_{n1} &amp; x_{n2} &amp; \cdots &amp; x_{np}
\end{array}\right]=\left[\begin{array}{c}
\mathbf{x_{1.}^{T}}\\
\mathbf{x_{2.}^{T}}\\
\vdots\\
\mathbf{x_{n.}^{T}}
\end{array}\right]=\left[\begin{array}{cccc}
\mathbf{x_{.1}} &amp; \mathbf{x_{.2}} &amp; \ldots &amp; \mathbf{x_{.p}}\end{array}\right]
\]</span></p>
<p>其中<span class="math inline">\(^{T}\)</span>代表矩阵转秩。我们用<span class="math inline">\(y_{i}\)</span>代表第i个样本对应的响应变量。所有<span class="math inline">\(n\)</span>个响应变量组成的向量为：</p>
<p><span class="math display">\[
\mathbf{y}=\left[\begin{array}{c}
y_{1}\\
y_{2}\\
\vdots\\
y_{n}
\end{array}\right]
\]</span></p>
<p>自变量和应变量的关系为：</p>
<p><span class="math display">\[\mathbf{y}=f(\mathbf{X})+\mathbf{\epsilon}\]</span></p>
<p>有监督和无监督建模技术用上面的符号语言表达就是：</p>
<ol style="list-style-type: decimal">
<li>无监督建模：探索<span class="math inline">\(\mathbf{X}\)</span>中的自变量之间的关系</li>
<li>有监督建模：估计<span class="math inline">\(\mathbf{y}\)</span>和<span class="math inline">\(\mathbf{X}\)</span>之间的关系 <span class="math inline">\(f(\cdot)\)</span></li>
</ol>
<p>其中<span class="math inline">\(\mathbf{\epsilon}\)</span> 是随机误差，均值为<span class="math inline">\(\mathbf{0}\)</span>。函数<span class="math inline">\(f(\cdot)\)</span>是我们的建模目标，代表X能够提供的关于Y的系统信息（和随机性相对应）。估计<span class="math inline">\(f(\cdot)\)</span>目的主要是推断或者预测，有时兼有两者。通常情况下，模型的灵活性和可解释性之间是一种此消彼长的关系——灵活性越高的模型可解释性越弱。因此数据科学家需要把握这两者间微妙的平衡。不同的建模目的对模型解释性的要求不同，因而极大影响了模型选择。如果预测是唯一目的，那么模型的解释性就不在考虑范围内，这种情况下可以使用一些复杂的灵活度高的“黑箱”模型，装袋，助推，非线性核函数支持向量机，神经网络和随机森林等。这些模型都非常灵活，但是很难解释自变量和应变量之间的关系。人们可能会觉得这些模型的预测精度通常更高，但就个人经验来说，那些灵活性不那么高的模型预测精度更高的情况时常发生。咋一看来好像不符合逻辑，但是认真想想也并不奇怪，这些模型之所以复杂，就在于它们极力拟合当前观测数据，因此它们更有可能过度拟合（把噪声也拟合进去了），这些模型在训练集上的表现可能更好，但预测未必更准确。</p>
</div>
<div id="section-7.2" class="section level2">
<h2><span class="header-section-number">7.2</span> 误差及其来源</h2>
<div id="section-7.2.1" class="section level3">
<h3><span class="header-section-number">7.2.1</span> 系统误差和随机误差</h3>
<p>假设我们对于<span class="math inline">\(\mathbf{X}\)</span>得到<span class="math inline">\(f\)</span>的估计<span class="math inline">\(\hat{f}\)</span>，进而得到<span class="math inline">\(\mathbf{y}\)</span>的预测 <span class="math inline">\(\hat{\mathbf{y}}=\hat{f}(\mathbf{X})\)</span>。预测的误差分成两部分，系统误差和随机误差：</p>
<p><span class="math display">\[
E(\mathbf{y}-\hat{\mathbf{y}})^{2}=E[f(\mathbf{X})+\mathbf{\epsilon}-\hat{f}(\mathbf{X})]^{2}=\underset{\text{(1)}}{\underbrace{E[f(\mathbf{X})-\hat{f}(\mathbf{X})]^{2}}}+\underset{\text{(2)}}{\underbrace{Var(\mathbf{\epsilon})}}
  \label{eq:error}\]</span></p>
<p>其中（1）是系统误差， <span class="math inline">\(\hat{f}\)</span>通常不能彻底对<span class="math inline">\(\mathbf{X}\)</span>和<span class="math inline">\(\mathbf{y}\)</span>之间的“系统关系”建模，这里系统关系指的是在不同样本上存在的稳定关系。这一部分误差能通过改进模型得到提高；（2）是随机误差，这部分误差代表当前数据无法解释的部分，因此无法通过建立更复杂的模型来改进。那些拥有众多参数的复杂黑箱模型最大的问题就是试图通过自变量解释这部分误差，也就是过度拟合。随机误差的显著特点就是在不同的样本上是无法重复的，于是判断是否存在过度拟合的一个准则就是预留一部分样本作为测试集，然后检验训练出来的模型在测试集上的表现。这个我们随后会讲到。这里要澄清一点，过度拟合不只发生在这些黑箱模型上，其发生的根源在于参数个数太多（常超过观测个数），理论上说任何模型都可能过度拟合，只是因为黑箱模型的参数尤其多，其高灵活性和复杂度放大了过度拟合的问题。有些黑箱模型在训练的过程中会使用“袋外数据”（又称为Out of Bag [OOB]）来尽量避免过度拟合的影响。</p>
<p>如果建模的目的也包含推断，那么这些“黑箱”模型就不合适，这就需要在模型可以解释的范围内使用尽量灵活的模型，比如Lasso回归，多元自适应回归样条等。有人可能不同意Lasso回归是灵活的。从其本质还是传统回归的角度看，它确实没有那么灵活，受到很多模型假设的限制。但由于Lasso的罚函数能同时起到变量选择的作用，这个变量的选择的过程可以不依赖于p值之类的参数（这些参数基于数据分布假设因此具有局限性），而可以通过优化模型预测值和真实值的差距来进行变量选择，从这个角度上看，该模型是灵活的。根据笔者的应用经验，Lasso作为收缩（或变量选择）方法在实际应用中的效果非常好。对于一些市场营销或者社会心理学相关的抽样调查数据分析，分层贝叶斯可能是一种灵活有效的方法，但拟合所需的计算时间更长。</p>
<p>其中系统误差可以进一步分解：</p>
<p><span class="math display">\[
\begin{array}{ccc}
E[f(\mathbf{X})-\hat{f}(\mathbf{X})]^{2} &amp; = &amp; E\left(f(\mathbf{X})-E[\hat{f}(\mathbf{X})]+E[\hat{f}(\mathbf{X})]-\hat{f}(\mathbf{X})\right)^{2}\\
 &amp; = &amp; E\left(E[\hat{f}(\mathbf{X})]-f(\mathbf{X})\right)^{2}+E\left(\hat{f}(\mathbf{X})-E[\hat{f}(\mathbf{X})]\right)^{2}\\
 &amp; = &amp; [Bias(\hat{f}(\mathbf{X}))]^{2}+Var(\hat{f}(\mathbf{X}))
\end{array}
\]</span></p>
<p>系统误差由两部分组成，估计的偏度<span class="math inline">\(Bias(\hat{f}(\mathbf{X}))\)</span>和估计的方差<span class="math inline">\(Var(\hat{f}(\mathbf{X}))\)</span>。上面公式告诉我们，如果要最小化系统误差，需要同时最小化估计偏度和估计方差。偏度代表用模型逼近现实情况导致的误差，这部分误差可能非常复杂。比如线性回归假设自变量和应变量之间是线性关系，但现实生活中完全的线性关系并不常见。下图中x和fx的关系就是非线性的。因此，观测样本量再大，也无法用线性回归给出准确的预测。换句话说，在这种情况下，线性回归模型的预测具有很高的偏度。</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(grid)
<span class="kw">library</span>(lattice)
<span class="kw">library</span>(ggplot2)
<span class="co"># 可以从网站下载multiplot()函数代码</span>
<span class="co"># 用该函数在同一张画布上放置多张ggplot图</span>
<span class="co"># 需要用到grid包</span>
<span class="kw">source</span>(<span class="st">&quot;/Users/happyrabbit/Documents/GitHub/DataScientistR/R_Code/multiplot.r&quot;</span>)
<span class="co"># 随机抽取一些非线性样本</span>
x=<span class="kw">seq</span>(<span class="dv">1</span>,<span class="dv">10</span>,<span class="fl">0.01</span>)*pi
e=<span class="kw">rnorm</span>(<span class="kw">length</span>(x),<span class="dt">mean=</span><span class="dv">0</span>,<span class="dt">sd=</span><span class="fl">0.2</span>)
fx&lt;-<span class="kw">sin</span>(x)+e+<span class="kw">sqrt</span>(x)
dat=<span class="kw">data.frame</span>(x,fx)
<span class="co"># 绘制线性拟合图</span>
<span class="kw">ggplot</span>(dat,<span class="kw">aes</span>(x,fx))+<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>() +
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="dt">se =</span> <span class="ot">FALSE</span>)</code></pre></div>
<p><img src="DS_R_files/figure-html/unnamed-chunk-84-1.png" width="672" /></p>
<p>由于我们通常使用训练集进行参数估计，如果训练集不同，得到的参数估计也会不同。直观的讲，估计方差表示如果我们用不同的数据集拟合相同的模型，得到估计值的变化，理想的情况是估计值的变化不会太大。对于高方差的模型，训练集的微小变化会导致很不相同的估计值。通常情况下，灵活度高的模型方差也更高，比如树模型，以及最初的助推法，后来在此基础上的随机森林模型和梯度助推法这样的集成方法的重要目标之一，就是通过汇总不同样本上得到的结果来降低估计方差。下图中的蓝色曲线是用平滑方法对上面的非线性观测进行拟合得到的，该曲线的灵活性很高，能够高度拟合当前数据：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(dat,<span class="kw">aes</span>(x,fx))+<span class="kw">geom_smooth</span>(<span class="dt">span =</span> <span class="fl">0.03</span>)</code></pre></div>
<pre><code>## `geom_smooth()` using method = &#39;loess&#39;</code></pre>
<p><img src="DS_R_files/figure-html/unnamed-chunk-85-1.png" width="672" /></p>
<p>但是该方法有很高的方差，如果我们随机抽取不同的样本子集，得到的拟合曲线会有明显变化：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># 设置随机种子</span>
<span class="kw">set.seed</span>(<span class="dv">2016</span>)
<span class="co"># 抽取其中部分样本拟合模型</span>
<span class="co"># 样本1</span>
idx1=<span class="kw">sample</span>(<span class="dv">1</span>:<span class="kw">length</span>(x),<span class="dv">100</span>)
dat1=<span class="kw">data.frame</span>(<span class="dt">x1=</span>x[idx1],<span class="dt">fx1=</span>fx[idx1])
p1=<span class="kw">ggplot</span>(dat1,<span class="kw">aes</span>(x1,fx1))+<span class="kw">geom_smooth</span>(<span class="dt">span =</span> <span class="fl">0.03</span>)
<span class="co"># 样本2</span>
idx2=<span class="kw">sample</span>(<span class="dv">1</span>:<span class="kw">length</span>(x),<span class="dv">100</span>)
dat2=<span class="kw">data.frame</span>(<span class="dt">x2=</span>x[idx2],<span class="dt">fx2=</span>fx[idx2])
p2=<span class="kw">ggplot</span>(dat2,<span class="kw">aes</span>(x2,fx2))+<span class="kw">geom_smooth</span>(<span class="dt">span =</span> <span class="fl">0.03</span>)
<span class="co"># 样本3</span>
idx3=<span class="kw">sample</span>(<span class="dv">1</span>:<span class="kw">length</span>(x),<span class="dv">100</span>)
dat3=<span class="kw">data.frame</span>(<span class="dt">x3=</span>x[idx3],<span class="dt">fx3=</span>fx[idx3])
p3=<span class="kw">ggplot</span>(dat3,<span class="kw">aes</span>(x3,fx3))+<span class="kw">geom_smooth</span>(<span class="dt">span =</span> <span class="fl">0.03</span>)
<span class="co"># 样本4</span>
idx4=<span class="kw">sample</span>(<span class="dv">1</span>:<span class="kw">length</span>(x),<span class="dv">100</span>)
dat4=<span class="kw">data.frame</span>(<span class="dt">x4=</span>x[idx4],<span class="dt">fx4=</span>fx[idx4])
p4=<span class="kw">ggplot</span>(dat4,<span class="kw">aes</span>(x4,fx4))+<span class="kw">geom_smooth</span>(<span class="dt">span =</span> <span class="fl">0.03</span>)
<span class="kw">multiplot</span>(p1,p2,p3,p4,<span class="dt">cols=</span><span class="dv">2</span>)</code></pre></div>
<pre><code>## `geom_smooth()` using method = &#39;loess&#39;
## `geom_smooth()` using method = &#39;loess&#39;
## `geom_smooth()` using method = &#39;loess&#39;
## `geom_smooth()` using method = &#39;loess&#39;</code></pre>
<p><img src="DS_R_files/figure-html/unnamed-chunk-86-1.png" width="672" /></p>
<p>对相同的4个子集拟合线性模型，变化非常小：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">p1=<span class="kw">ggplot</span>(dat1,<span class="kw">aes</span>(x1,fx1))+<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>() +
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="dt">se =</span> <span class="ot">FALSE</span>)
p2=<span class="kw">ggplot</span>(dat2,<span class="kw">aes</span>(x2,fx2))+<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>() +
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="dt">se =</span> <span class="ot">FALSE</span>)
p3=<span class="kw">ggplot</span>(dat3,<span class="kw">aes</span>(x3,fx3))+<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>() +
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="dt">se =</span> <span class="ot">FALSE</span>)
p4=<span class="kw">ggplot</span>(dat4,<span class="kw">aes</span>(x4,fx4))+<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>() +
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="dt">se =</span> <span class="ot">FALSE</span>)
<span class="kw">multiplot</span>(p1,p2,p3,p4,<span class="dt">cols=</span><span class="dv">2</span>)</code></pre></div>
<p><img src="DS_R_files/figure-html/unnamed-chunk-87-1.png" width="672" /></p>
<p>总体说来，方差随着模型灵活度的增加而增加，偏差随着模型的灵活度增加而降低。在方差和偏差的变化共同决定了系统误差（或者均方误差，MSE）的变化。当我们提高模型的灵活性，偏差减小的速度开始时会超过方差增大的速度，MSE随之减小。但到了一定程度以后，提高模型的灵活性对偏差的影响不大但是方差大幅度增加，MSE随之增加。在之后的章节我们会看到，对于那些有一个控制模型灵活性的调优参数的模型，随着调优参数的减小（参数越大灵活度越低），模型误差先升高，后降低。</p>
<p>模型选择向来是非常困难的，这种困难不是数据分析行业特有的，很多专业领域都有类似的情况，比如医生判断病人所患的疾病，并在众多治疗方案中选择最合适的，这不是答案一目了然的选择题，决策的过程需要很多权衡和妥协。模型选择也类似，在选择过程中需要考虑具体的情况：项目目的，客户要求的精确度（这点很重要），计算量等等。这个选择的过程很难白纸黑字的像食谱一样写下来，这里我们只是尽己所能的介绍模型选择过程中需要考虑的点，以及评估不同模型的辅助性技术。具体的应用和“数据科学思维”还需要大家在从业过程中通过实践思考不断学习打磨。</p>
</div>
<div id="section-7.2.2" class="section level3">
<h3><span class="header-section-number">7.2.2</span> 应变量误差</h3>
<p>若应变量包含可观的测量误差，那么这部分误差将反映在随机误差（<span class="math inline">\(\mathbf{\epsilon}\)</span>）中。这部分误差使得均方根误差（RMSE）和<span class="math inline">\(R^2\)</span>有相应的上下限。RMSE和<span class="math inline">\(R^2\)</span>是回归模型常用的表现度量方法，我们在本章后面部分会进行介绍。因此，随机误差项不仅仅代表模型无法解释的波动，还含有测量误差。《应用预测建模（Applied Predictive Modeling）》<span class="citation">[<a href="#ref-APM">13</a>]</span>的第20.2小节有一个例子展示了因变量的测量误差对模型表现（RMSE和<span class="math inline">\(R^2\)</span>）的影响。作者在因变量上加入了不同强度的随机正态噪声，重复拟合不同的模型，研究模型均方根误差（RMSE）和<span class="math inline">\(R^2\)</span>的变化。这里我们用服装消费者数据进行类似的展示。假设我们面对这样一个问题，实际中消费者的收入并不是那么容易收集，很多人不愿透露这样的私人信息。于是我们希望利用消费记录变量建立关于消费者收入的预测模型，模型可以对那些数据库中缺失收入信息的记录进行填补。我们建立下面模型：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># 载入数据</span>
sim.dat&lt;-<span class="kw">read.csv</span>(<span class="st">&quot;/Users/happyrabbit/Documents/GitHub/DataScientistR/Data/SegData.csv&quot;</span>)
ymad&lt;-<span class="kw">mad</span>(<span class="kw">na.omit</span>(sim.dat$income))
<span class="co"># 计算Z分值</span>
zs&lt;-(sim.dat$income-<span class="kw">mean</span>(<span class="kw">na.omit</span>(sim.dat$income)))/ymad
<span class="co"># which(na.omit(zs&gt;3.5)) 找到利群点</span>
<span class="co"># which(is.na(zs)) 找到缺失值</span>
idex&lt;-<span class="kw">c</span>(<span class="kw">which</span>(<span class="kw">na.omit</span>(zs&gt;<span class="fl">3.5</span>)),<span class="kw">which</span>(<span class="kw">is.na</span>(zs)))
<span class="co"># 删除含有离群点和缺失值的行</span>
sim.dat&lt;-sim.dat[-idex,]
fit&lt;-<span class="kw">lm</span>(income~store_exp+online_exp+store_trans+online_trans,<span class="dt">data=</span>sim.dat)</code></pre></div>
<p>由输出可见，在没有额外添加噪音时模型的均方根误差（RMSE）是 29567，<span class="math inline">\(R^2\)</span>是 0.6。下面我们在应变量年收入（<code>income</code>）上添加不同程度的噪音（均方根误差的0到3倍）：</p>
<p><span class="math display">\[ RMSE \times (0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0) \]</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">noise&lt;-<span class="kw">matrix</span>(<span class="kw">rep</span>(<span class="ot">NA</span>,<span class="dv">7</span>*<span class="kw">nrow</span>(sim.dat)),<span class="dt">nrow=</span><span class="kw">nrow</span>(sim.dat),<span class="dt">ncol=</span><span class="dv">7</span>)
for (i in <span class="dv">1</span>:<span class="kw">nrow</span>(sim.dat)){
noise[i,]&lt;-<span class="kw">rnorm</span>(<span class="dv">7</span>,<span class="kw">rep</span>(<span class="dv">0</span>,<span class="dv">7</span>),<span class="kw">summary</span>(fit)$sigma*<span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">3</span>,<span class="dt">by=</span><span class="fl">0.5</span>))
}</code></pre></div>
<p>我们接下来检查噪音强度对复杂度不同的模型拟合<span class="math inline">\(R^2\)</span>的影响。 拟合的模型复杂度从低到高依次为：一般线性回归，偏最小二乘回归，多元自适应回归样条，支持向量机（核函数是径向基函数），随机森林。</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># 拟合一般线性回归模型</span>
rsq_linear&lt;-<span class="kw">rep</span>(<span class="dv">0</span>,<span class="kw">ncol</span>(noise))
for (i in <span class="dv">1</span>:<span class="dv">7</span>){
withnoise&lt;-sim.dat$income+noise[,i]
fit0&lt;-<span class="kw">lm</span>(withnoise~store_exp+online_exp+store_trans+online_trans,<span class="dt">data=</span>sim.dat)
rsq_linear[i]&lt;-<span class="kw">summary</span>(fit0)$adj.r.squared
}</code></pre></div>
<p>下面我们接着拟合偏最小二乘回归（PLS）。偏最小二乘源自于Herman Wold的非线性迭代偏最小二乘（NIPALS）算法 <span class="citation">[<a href="#ref-woldh1">15</a>, <a href="#ref-woldh2">16</a>]</span>，是一种通过隐层级将非线性关系线性化的方法。该方法和主成分回归类似，不同在于主成分回归在选择成分的时候没有考虑因变量的信息，其目的是找到最大程度概括自变量空间变异性的线性组合（即，是无监督方法）。当自变量和因变量相关时，主成分回归能够很好的识别出它们之间的系统关系。然而，当存在和因变量不相关的自变量时，该方法的效果就会受到影响。而PLS最大程度概括与因变量相关性的线性组合。推荐大家用PLS解决那些自变量之间存在相关性，但不确定所有自变量都和因变量有关，同时希望用线性回归来解决的问题。在当前情况下，更加复杂的PLS表现效果并不比简单线性好，因为这里几个自变量都和因变量有关的不同信息（从前面的拟合结果看到所有变量都是显著的）。</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># pls: 进行偏最小二乘回归和主成分回归</span>
<span class="kw">library</span>(pls)
rsq_pls&lt;-<span class="kw">rep</span>(<span class="dv">0</span>,<span class="kw">ncol</span>(noise))
<span class="co"># 拟合PLS模型</span>
for (i in <span class="dv">1</span>:<span class="dv">7</span>){
withnoise&lt;-sim.dat$income+noise[,i]
fit0&lt;-<span class="kw">plsr</span>(withnoise~store_exp+online_exp+store_trans+online_trans,<span class="dt">data=</span>sim.dat)
<span class="co"># plsr函数结果是mvr对象，需要用特定函数提取模型解释的应变量方差</span>
rsq_pls[i]&lt;-<span class="kw">max</span>(<span class="kw">drop</span>(<span class="kw">R2</span>(fit0, <span class="dt">estimate =</span> <span class="st">&quot;train&quot;</span>,<span class="dt">intercept =</span> <span class="ot">FALSE</span>)$val))
}</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># earth: 拟合多元自适应回归样条</span>
<span class="kw">library</span>(earth)
rsq_mars&lt;-<span class="kw">rep</span>(<span class="dv">0</span>,<span class="kw">ncol</span>(noise))
<span class="co"># 拟合多元自适应回归样条</span>
for (i in <span class="dv">1</span>:<span class="dv">7</span>){
withnoise&lt;-sim.dat$income+noise[,i]
fit0&lt;-<span class="kw">earth</span>(withnoise~store_exp+online_exp+store_trans+online_trans,<span class="dt">data=</span>sim.dat)
<span class="co"># 提取模型解释的应变量方差</span>
rsq_mars[i]&lt;-fit0$rsq
}</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># caret: 用于建立预测模型的包，可以拟合多种模型</span>
<span class="kw">library</span>(caret)
rsq_svm&lt;-<span class="kw">rep</span>(<span class="dv">0</span>,<span class="kw">ncol</span>(noise))
<span class="co"># 拟合支持向量机</span>
<span class="co"># 注意：运行需要一些时间</span>
for (i in <span class="dv">1</span>:<span class="dv">7</span>){
idex&lt;-<span class="kw">which</span>(<span class="kw">is.na</span>(sim.dat$income))
withnoise&lt;-sim.dat$income+noise[,i]
trainX&lt;-sim.dat[,<span class="kw">c</span>(<span class="st">&quot;store_exp&quot;</span>,<span class="st">&quot;online_exp&quot;</span>,<span class="st">&quot;store_trans&quot;</span>,<span class="st">&quot;online_trans&quot;</span>)]
trainY&lt;-withnoise
fit0&lt;-<span class="kw">train</span>(trainX,trainY,<span class="dt">method=</span><span class="st">&quot;svmRadial&quot;</span>,
            <span class="dt">tuneLength=</span><span class="dv">15</span>,
            <span class="dt">trControl=</span><span class="kw">trainControl</span>(<span class="dt">method=</span><span class="st">&quot;cv&quot;</span>))
<span class="co"># 提取模型解释的应变量方差</span>
rsq_svm[i]&lt;-<span class="kw">max</span>(fit0$results$Rsquared)
}</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># randomForest: 拟合随机森林模型</span>
<span class="kw">library</span>(randomForest)
rsq_rf&lt;-<span class="kw">rep</span>(<span class="dv">0</span>,<span class="kw">ncol</span>(noise))
<span class="co"># 拟合随机森林模型</span>
<span class="co"># ntree=500 用500棵树</span>
<span class="co"># na.action = na.omit 忽略缺失值</span>
for (i in <span class="dv">1</span>:<span class="dv">7</span>){
withnoise&lt;-sim.dat$income+noise[,i]
fit0&lt;-<span class="kw">randomForest</span>(withnoise~store_exp+online_exp+store_trans+online_trans,<span class="dt">data=</span>sim.dat,<span class="dt">ntree=</span><span class="dv">500</span>,<span class="dt">na.action =</span> na.omit)
<span class="co"># 提取模型解释的应变量方差</span>
rsq_rf[i]&lt;-<span class="kw">tail</span>(fit0$rsq,<span class="dv">1</span>)
}
<span class="co"># reshape2在之前介绍过，用于数据整形</span>
<span class="kw">library</span>(reshape2)
rsq&lt;-<span class="kw">data.frame</span>(<span class="kw">cbind</span>(<span class="dt">Noise=</span><span class="kw">c</span>(<span class="fl">0.0</span>, <span class="fl">0.5</span>, <span class="fl">1.0</span>, <span class="fl">1.5</span>, <span class="fl">2.0</span>, <span class="fl">2.5</span>, <span class="fl">3.0</span>),rsq_linear,rsq_pls,rsq_mars,rsq_svm,rsq_rf))
<span class="co"># 将数据转化成长型</span>
rsq&lt;-<span class="kw">melt</span>(rsq,<span class="dt">id.vars=</span><span class="st">&quot;Noise&quot;</span>,<span class="dt">measure.vars=</span><span class="kw">c</span>(<span class="st">&quot;rsq_linear&quot;</span>,<span class="st">&quot;rsq_pls&quot;</span>,<span class="st">&quot;rsq_mars&quot;</span>,<span class="st">&quot;rsq_svm&quot;</span>,<span class="st">&quot;rsq_rf&quot;</span>))</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># 功能强大的绘图包</span>
<span class="kw">library</span>(ggplot2)
<span class="co"># 用ggplot2包进行可视化</span>
<span class="kw">ggplot</span>(<span class="dt">data=</span>rsq, <span class="kw">aes</span>(<span class="dt">x=</span>Noise, <span class="dt">y=</span>value, <span class="dt">group=</span>variable, <span class="dt">colour=</span>variable)) +
<span class="st">    </span><span class="kw">geom_line</span>() +
<span class="st">    </span><span class="kw">geom_point</span>()+
<span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;R2&quot;</span>) </code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:error"></span>
<img src="DS_R_files/figure-html/error-1.png" alt="模型$R^2$随应变量噪音强度变化，rsq_linear是简单线性回归，rsq_pls是偏最小二乘回归，rsq_mars是多元自适应回归样条回归，rsq_svm是支持向量机，rsq_rf是随机森林。" width="80%" />
<p class="caption">
Figure 7.1: 模型<span class="math inline">\(R^2\)</span>随应变量噪音强度变化，rsq_linear是简单线性回归，rsq_pls是偏最小二乘回归，rsq_mars是多元自适应回归样条回归，rsq_svm是支持向量机，rsq_rf是随机森林。
</p>
</div>
<p>由图<a href="section-7.html#fig:error">7.1</a>中可以看到：</p>
<ol style="list-style-type: decimal">
<li>所有模型拟合效果随着噪音强度的增加急剧下降。对变量测量系统的理解能够帮助我们更好的预期模型的表现。这是在之前“数据分析一般流程”中说过的从问题到数据这个环节需要弄清的问题。你应该清楚当前数据库中已有的数据的质量。如果客户提供给你额外的数据，或者需要你从其它地方获得数据，数据质量是必须交流清楚的问题，笔者就曾在这里栽过跟头，希望大家可以避免类似的错误。</li>
<li>使用更加复杂的模型的效果不一定更好，如复杂的随机森林和支持向量机表现居中，简单线性回归和偏最小二乘回归在噪音低的时候拟合效果最差。效果最好的是多元自适应回归样条回归，该模型比简单线性回归复杂，但比剩下其它的模型的解释性都更强。</li>
<li>噪音增加到一定程度，复杂的随机森林模型能够发现的潜在结构变得更加模糊，模型表现不如其它更简单的模型。因此系统测量误差较大时，使用更简单的易于解释的模型可能是更好的选择，大家建模的时候要尽量多尝试几种模型，在表现相当的情况下选择最简单的模型，模型的评估和选择很好的反应了一个数据科学家的职业“成熟度”。</li>
</ol>
</div>
<div id="section-7.2.3" class="section level3">
<h3><span class="header-section-number">7.2.3</span> 自变量误差</h3>
<p>传统的统计模型通常假设自变量的测量无误差（或者随机性），这在实际中是不可能的，所以我们需要考虑自变量观测的随机性。自变量观测中的随机性产生的影响取决于如下几个因素：随机性的强度，相应因变量在模型中的重要性，使用模型的类别。我们选取自变量“在线消费”（<code>online_exp</code>）为例，用和上面相似的方法在该自变量上添加不同程度的噪音看其对模型拟合情况的影响。我们在自变量<code>online_exp</code>和上添加如下不同程度的噪音（标准差的0到3倍）：</p>
<p><span class="math display">\[ \sigma_{0} \times (0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0) \]</span></p>
<p>其中<span class="math inline">\(\sigma_{0}\)</span>是在线消费观测的标准差。</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">noise&lt;-<span class="kw">matrix</span>(<span class="kw">rep</span>(<span class="ot">NA</span>,<span class="dv">7</span>*<span class="kw">nrow</span>(sim.dat)),<span class="dt">nrow=</span><span class="kw">nrow</span>(sim.dat),<span class="dt">ncol=</span><span class="dv">7</span>)
for (i in <span class="dv">1</span>:<span class="kw">nrow</span>(sim.dat)){
noise[i,]&lt;-<span class="kw">rnorm</span>(<span class="dv">7</span>,<span class="kw">rep</span>(<span class="dv">0</span>,<span class="dv">7</span>),<span class="kw">sd</span>(sim.dat$online_exp)*<span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">3</span>,<span class="dt">by=</span><span class="fl">0.5</span>))
}</code></pre></div>
<p>同样的，我们检查噪音强度对复杂度不同的模型拟合<span class="math inline">\(R^2\)</span>的影响。拟合的模型复杂度从低到高依次为：一般线性回归，偏最小二乘回归，多元自适应回归样条，支持向量机（核函数是径向基函数），随机森林。代码和之前类似，这里就不重复展示。</p>
<!--
```r
# 拟合一般线性回归模型
rsq_linear<-rep(0,ncol(noise))
for (i in 1:7){
withnoise_online<-sim.dat$online_exp+noise[,i]
fit0<-lm(income ~ store_exp + withnoise_online + store_trans + online_trans,data=sim.dat)
#fit0<-lm(income ~ store_exp + store_trans + online_trans,data=sim.dat)

rsq_linear[i]<-summary(fit0)$adj.r.squared
}
```

```r
# pls: 进行偏最小二乘回归和主成分回归
library(pls)
rsq_pls<-rep(0,ncol(noise))
# 拟合PLS模型
for (i in 1:7){
withnoise_online<-sim.dat$online_exp+noise[,i]
fit0<-plsr(income~store_exp+withnoise_online+store_trans+online_trans,data=sim.dat)
#fit0<-plsr(income~store_exp+store_trans+online_trans,data=sim.dat)
# plsr函数结果是mvr对象，需要用特定函数提取模型解释的应变量方差
rsq_pls[i]<-max(drop(R2(fit0, estimate = "train",intercept = FALSE)$val))
}
```

```r
# earth: 拟合多元自适应回归样条
library(earth)
rsq_mars<-rep(0,ncol(noise))
# 拟合多元自适应回归样条
for (i in 1:7){
withnoise_online<-sim.dat$online_exp+noise[,i]
fit0<-earth(income~store_exp+withnoise_online+store_trans+online_trans,data=sim.dat)
#fit0<-earth(income~store_exp+store_trans+online_trans,data=sim.dat)
# 提取模型解释的应变量方差
rsq_mars[i]<-fit0$rsq
}
```

```r
# caret: 用于建立预测模型的包，可以拟合多种模型
library(caret)
rsq_svm<-rep(0,ncol(noise))
# 拟合支持向量机
# 注意：运行需要一些时间
for (i in 1:7){
idex<-which(is.na(sim.dat$income))
withnoise_online<-sim.dat$online_exp+noise[,i]
trainX<-cbind(sim.dat[,c("store_exp","store_trans","online_trans")],withnoise_online)
trainY<-sim.dat$income
fit0<-train(trainX,trainY,method="svmRadial",
            tuneLength=15,
            trControl=trainControl(method="cv"))
# 提取模型解释的应变量方差
rsq_svm[i]<-max(fit0$results$Rsquared)
}
```

```r
# randomForest: 拟合随机森林模型
library(randomForest)
rsq_rf<-rep(0,ncol(noise))
# 拟合随机森林模型
# ntree=500 用500棵树
# na.action = na.omit 忽略缺失值
for (i in 1:7){
withnoise_online<-sim.dat$online_exp+noise[,i]
fit0<-randomForest(income~store_exp+withnoise_online+store_trans+online_trans,data=sim.dat,ntree=500,na.action = na.omit)
#fit0<-randomForest(income~store_exp+store_trans+online_trans,data=sim.dat,ntree=500,na.action = na.omit)
# 提取模型解释的应变量方差
rsq_rf[i]<-tail(fit0$rsq,1)
}
# reshape2在之前介绍过，用于数据整形
library(reshape2)
rsq<-data.frame(cbind(Noise=c(0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0),rsq_linear,rsq_pls,rsq_mars,rsq_svm,rsq_rf))
# 将数据转化成长型
rsq<-melt(rsq,id.vars="Noise",measure.vars=c("rsq_linear","rsq_pls","rsq_mars","rsq_svm","rsq_rf"))
```
-->
<div class="figure" style="text-align: center"><span id="fig:errorvariable"></span>
<img src="DS_R_files/figure-html/errorvariable-1.png" alt="模型$R^2$随自变量(在线消费)噪音强度变化，rsq_linear是简单线性回归，rsq_pls是偏最小二乘回归，rsq_mars是多元自适应回归样条回归，rsq_svm是支持向量机，rsq_rf是随机森林。" width="80%" />
<p class="caption">
Figure 7.2: 模型<span class="math inline">\(R^2\)</span>随自变量(在线消费)噪音强度变化，rsq_linear是简单线性回归，rsq_pls是偏最小二乘回归，rsq_mars是多元自适应回归样条回归，rsq_svm是支持向量机，rsq_rf是随机森林。
</p>
</div>
<p>比较图<a href="section-7.html#fig:errorvariable">7.2</a>和图<a href="section-7.html#fig:error">7.1</a>，可以看到自变量的误差和应变量误差对模型拟合结果的影响很不相同。应变量误差是无法克服的，对任何模型来说都是个硬伤。而自变量误差确不一定。试想极端的情况，在线消费这个变量完全是随机噪音，也就是所说的无信息变量，随机森林和支持向量机受的影响并不太大。线性模型和偏最小二乘回归的结果依旧基本重合，而且随着噪音的增加拟合效果开始下降较快，到一定程度后趋于平稳，如果噪音不断增加，最后拟合的情况实际上会趋近于移除“在线消费”这个变量的结果。总体说来，如果某个自变量含有误差，其它与之相关的变量在某种程度上可以进行弥补。线性模型对于自变量的观测误差的抗性普遍较差。</p>
</div>
</div>
<div id="section-7.3" class="section level2">
<h2><span class="header-section-number">7.3</span> 数据划分和再抽样</h2>
<p>模型训练和选择过程都离不开数据的划分和再抽样。数据划分是将一部分数据预留出来用于模型测试，只用另外的部分数据用于模型的训练。再抽样过程牵扯到重复的从训练集中抽取样本并且在不同的样本上拟合模型，以此来得到关于拟合模型的信息。假设我们想知道某线性模型拟合度<span class="math inline">\(R^2\)</span>的稳定性（也可以用其它模型拟合度量），可以重复的抽取不同的样本，然后拟合相同的线性模型，检查这些模型对应<span class="math inline">\(R^2\)</span>的变化。由于牵扯到使用随机样本重复拟合模型，这个过程有一定的计算量，最近五年里，数据处理工具和技术获得了飞速的发展。除非你需要处理PB（<span class="math inline">\(2^{50}\)</span>比特）级别的数据，或者每天要处理千亿级的事件，现阶段大多数技术已经能轻松满足你的需求了。</p>
<p>你可能会问：为什么要对数据划分和再抽样？简单的回答是避免过度拟合。在预测问题中，有时拟合的模型能很好的描述现有数据中的变量关系，但是对新样本的预测有很大的偏差，这时就发生了过度拟合。很多领域都会讨论过度拟合，如医学研究，化学计量，气象，金融和社会学研究等等。现代很多含有调优参数的分类 和回归模型有高度的灵活性，如之前提到的随机森林、支持向量机等。它们能够对复杂的关系进行建模，但是很容易过度强调不可再现的数据关系。要注意，虽然过度拟合的问题在灵活度高的模型中更加突出，所有模型（包括简单线性回归）在应用中都可能出现该问题。建模的目的是找到<strong>可重复的数据关系</strong>， 这就需要将现有数据划分成不同的数据集来调试模型参数和评估模型表现。 划分和再抽样的一般过程如下：</p>
<ol style="list-style-type: decimal">
<li>将样本划分成训练集和测试集</li>
<li>使用训练集拟合模型</li>
<li>将拟合的模型应用于测试集评估模型表现</li>
</ol>
<p>关于数据划分，我们会介绍3种划分数据的方法：（1）按照结果变量划分数据；（2）按照预测变量划分数据；（3）按照时间序列划分数据。之后我们会介绍两种主要的再抽样方法： bootstrap和交互校验。</p>
<div id="section-7.3.1" class="section level3">
<h3><span class="header-section-number">7.3.1</span> 划分训练集和测试集</h3>
<p>关于数据划分大家可能主要会问这三个问题：（1）为什么要划分训练集和测试集？（2）多少比例的数据用于训练集？（3）具体如何划分？我们现在就对此逐一回答。</p>
<div class="figure">
<img src="http://scientistcafe.com/book/Figure/traintest.png" alt="70%用于训练集，30%用于测试集数据划分示意图" />
<p class="caption">70%用于训练集，30%用于测试集数据划分示意图</p>
</div>
<p>刚接触数据科学的人常常会问<strong>为什么我们要预留一部分数据作为测试集而不是使用全部的数据用于训练</strong>。印象中传统商业智能声称的数据分析通常只是数据描述。通过从数据库中查询相关测量来回答简单的问题，如：2015年某产品每月销售量是多少？我们网站在过去一个月每天的访问量是多少？两种包装设计的同类产品在某大零售店上个月的销量差距多大？像这样的问题确实不用对数据进行划分，相反我们需要用尽可能完整的数据，然后对感兴趣的部分求和或者平均。假设数据观测准确，我们不需要怀疑问题的答案，因为这些问题本质上就是对数据进行某种描述总结，没有牵扯到任何分析推断。</p>
<p>数据科学家需要解决的不会是这样的问题，常是预测问题，或者同时还需要从预测模型中得到相应能够指导决策的推断。在这些情况下，分析的重心在于找到自变量<span class="math inline">\(\mathbf{X}\)</span>和应变量<span class="math inline">\(\mathbf{y}\)</span>之间的系统关系。这时我们就必须非常小心，因为我们在用一个样本得到一般化的结论，进而对将来可能出现的观测进行预测，这远远超越了描述统计的界限。根据彭加莱的理论，在预测未来的过程中，预测的越远的未来要求模型越精确，因为你的错误率会迅速上升。每向前预测一步，噪声会随着以一种非线性的方式迅速增加，因此我很难相信对5年以后某事件的定量预测。我们能够处理定性的事物，能够讨论系统的某些特点，但能够计算的东西是很局限的。在《黑天鹅》那本书中，作者以数学家Michael Berry的弹子球计算为例说明了这种放大效应。该实验是预测弹子球在球桌上的运动轨迹。如果弹子球的基本参数已知，你能够计算出桌面阻力，测量撞击量，那么就可以预测第1次撞击的结果。要预测第2次撞击就更为复杂一些，你需要小心确定球的初始状态，但不是不可能。如果要计算第9次撞击的结果你需要考虑某个站在桌子旁边的人的体重和产生的引力。要计算第56次撞击结果你需要考虑宇宙中的每一个基本粒子。注意这还只是单独的弹子球而没有牵扯到有着自由意志的人，以及不同人之间相互的影响。对现实世界的复杂局面，人的预测能力有着本质上的局限性。因此在实际预测分析当中，你需要很小心的界定这个可预测的边界，好比在弹子球实验中，你能预测第1次撞击的结果或者咬咬牙，再多杀一大片脑细胞做第2次撞击预测，但不要试图再进一步，承认自己的局限需要知识和勇气。回到实际分析中，如何找到预测的边界？（注：随着你经验的增长，你会遇到很多你无法预测（有时是分析）的情况。）目前我知道的方法就是在仔细确保当前情况基本符合假设的情况下，严格划分训练集和测试集，尽可能对模型的预测情况进行评估，检测预测模型的精确度和稳定性。划分背后隐含的假设是：</p>
<ol style="list-style-type: decimal">
<li>我们用于分析的数据展现的过程能够反应真实世界中事情的发展过程</li>
<li>我们想要对其建模的真实世界中事情的发展过程随着时间变化是相对稳定的。如，用上个月的数据建立的表现良好的模型，在接下来的一个月的观测上依旧能够有类似的良好表现</li>
</ol>
<p>换句话说，我们想要知道如果我们用模型来对新样本进行预测时会发生什么。我们的预测和真实将观测到的值有多接近？预测值偏离真实值的误差大致是多少？模型的误差是不是单向的，即预测是不是总大于真实值？这些都是很自然的问题，但它们的答案并非那么容易获得。最简单的理解模型在将来数据集上表现的方法就是试图模拟这件事。虽然严格说来，在将来事件发生之前，我们不可能得到相应的数据，但是我们能够预留一部分当前的数据并将它们视为将来的观测。例如，如果我们要预测2016年哪些农民还会某品牌的种子，可以用之前到2015年的历史数据建立预测模型，然后预测2016年的购买情况。这是一个相当好的模拟，由于我们其实已经知道2016年实际购买情况，可以将预测和真实情况进行对比。</p>
<p>在商业促销活动和信用风险的案例中，我们得到的数据通常和某个时间点相连（或者时间区间：一周，一个月，一个促销活动期间）。通常称这样的数据有代表性（用某时间点或者时间段的数据代表普遍情况）。在这样的情况下我们通常将数据集随机分成不同部分，然后用一部分（训练集）建立模型，另外一部分（测试集）来评估模型表现，可能的话对模型做出调整。</p>
<p>如果这两条假设大致正确，那么当前数据就能够合理反映未来的情况。因此在这种情况下，预留一部分当前数据来估计模型在将来的表现是合理的。明确了预测模型的一些假设前提，以及划分训练集和测试集的必要性之后，下一个问题是<strong>我们该将多少比例的数据用于训练集</strong>。</p>
<p>一般这需要视具体情况而定。通常需要考虑的两个因素是：（1）样本量；（2）计算速度。当样本量较大时，在考虑计算速度的条件下，我一般会尝试60%，70％和80%这三个比例，看哪个效果更好。如果样本量很小，那么测试集其评估模型效果的能力将非常有限，并且在原本样本量就不大的情况下再分出一部分数据会极大影响模型拟合。这种情况下，使用再抽样技术更加有效。常用的再抽样方法有交互校验和Bootstrap。</p>
<p>我们可以用<code>createResample()</code>函数生成简单bootstrap样本，createFolds函数可以生成平衡的交互校验样本集。</p>
<p><strong>具体如何划分?</strong></p>
<p>划分训练集和测试集时需要小心避免两个数据集有系统差别。例如，我们不能简单的把前半部分数据当作训练集，后半部分当作测试集。因为数据有可能是以某种方式排列的，如，按收入从大到小，按访问次数多少排列等等。有一种避免数据集间随机差别的方法是用简单的随机抽样，如对每个样本我们都抛下硬币，人头面就归于训练集，菊花面就归于测试集。有时还有一些其它因素需要考虑，但本质都是随机抽样。要想真正理解划分数据背后的逻辑需要实践。下面我们介绍经常使用的几种划分方法。</p>
<ul>
<li>按照结果变量划分数据</li>
</ul>
<p>若结果变量<span class="math inline">\(\mathbf{y}\)</span>为分类变量，那么我们的到的测试集和训练集中结果变量各类的分布比例应该类似。可以使用<code>caret</code>包中的<code>createDataPartition()</code>函数平衡划分样本集。回到我们之前使用的服装消费者数据集，假设我们想要建立关于消费者类别（<code>segment</code>）的判别模型，这时结果变量为<code>segment</code>，我们用80%的样本训练模型，20%的样本做为测试集，且训练集和测试集中各类别的比例要尽可能相近。我们可以用如下R代码实现：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># 载入数据</span>
sim.dat&lt;-<span class="kw">read.csv</span>(<span class="st">&quot;/Users/happyrabbit/Documents/GitHub/DataScientistR/Data/SegData.csv&quot;</span>)
<span class="co"># 需要caret包</span>
<span class="kw">library</span>(caret)
<span class="co"># 设置随机种子这样能得到相同的抽样结果</span>
<span class="kw">set.seed</span>(<span class="dv">3456</span>)
trainIndex &lt;-<span class="st"> </span><span class="kw">createDataPartition</span>(sim.dat$segment, <span class="dt">p =</span> .<span class="dv">8</span>, <span class="dt">list =</span> <span class="ot">FALSE</span>, <span class="dt">times =</span> <span class="dv">1</span>)
<span class="kw">head</span>(trainIndex)</code></pre></div>
<pre><code>##      Resample1
## [1,]         1
## [2,]         2
## [3,]         3
## [4,]         4
## [5,]         6
## [6,]         7</code></pre>
<p><code>list = FALSE</code>选项使得返回的值是数据框。该函数还有一个选项<code>times</code>，用于设置划分的次数，你可以一次返回多次划分的结果，函数会返回一个（或多个）整数向量（指针向量），指明归于训练集的行（你可以设置<code>times＝2</code>再运行一下上面的代码看看输出有什么不同）。下面我们通过返回的指针向量（<code>trainIndex</code>）得到训练集和测试集：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># 得到训练集</span>
datTrain &lt;-<span class="st"> </span>sim.dat[ trainIndex,]
<span class="co"># 得到测试集</span>
datTest &lt;-<span class="st"> </span>sim.dat[-trainIndex,]</code></pre></div>
<p>按照设置，训练集中该有800个样本，测试集中有200个样本。我来看看两个集合中消费者类别的比例分布是否相似：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(plyr)
<span class="kw">ddply</span>(datTrain,<span class="st">&quot;segment&quot;</span>,summarise,<span class="dt">count=</span><span class="kw">length</span>(segment),
     <span class="dt">percentage=</span><span class="kw">round</span>( <span class="kw">length</span>(segment)/<span class="kw">nrow</span>(datTrain),<span class="dv">2</span>))</code></pre></div>
<pre><code>##       segment count percentage
## 1 Conspicuous   160       0.20
## 2       Price   200       0.25
## 3     Quality   160       0.20
## 4       Style   280       0.35</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ddply</span>(datTest,<span class="st">&quot;segment&quot;</span>,summarise,<span class="dt">count=</span><span class="kw">length</span>(segment),
      <span class="dt">percentage=</span><span class="kw">round</span>(<span class="kw">length</span>(segment)/<span class="kw">nrow</span>(datTest),<span class="dv">2</span>))</code></pre></div>
<pre><code>##       segment count percentage
## 1 Conspicuous    40       0.20
## 2       Price    50       0.25
## 3     Quality    40       0.20
## 4       Style    70       0.35</code></pre>
<p>很明显两个集合中消费者类别比例分布是一样的（实际应用中两个集合分布不一定严格相似，但应该非常接近）。</p>
<ul>
<li>按照自变量划分</li>
</ul>
<p>还可以使用最大差异度法<span class="citation">[<a href="#ref-willett">17</a>]</span>划分数据（<code>maxDissim()</code>函数）。假设样本集A中含有m个样本，样本集B含有n个样本，n&gt;m，且我们要从B中选出一些样本加到A中，该子集中的样本要尽量和A中的不同。要实现这一点，对B中的一个样本，计算A中样本和该样本的差异度（距离，这里会算出m个值，因为A中有m个样本）。然后将和A中样本最不相同的B的样本抽取出来加入A，重复这个过程，直到A的样本量达到要求。关于这么权衡这m个差异度找到和A“最不相似”的样本，有不同的方法，比如以最小的值为准，将所有距离求和等等。这里没有什么黄金法则，建议大家尝试几种方法，查看比较得到的训练/测试样本自变量分布，选取其中一种。用这种方式可以得到自变量分布相似的不同样本集。R中有不同的计算样本间差异度（基于自变量观测）的函数。<code>caret</code>包中调用的是<code>proxy</code>包中的函数。关于不同的差异度测量，见相关包的帮助文档。我们可以通过选项 <code>obj</code>设置和样本集A“最不相似”的样本的方式，其中<code>minDiss</code>表示以最小差异度为准，<code>sumDiss</code>表示使用差异度之和。</p>
<p>我们用服装数据的一个子集为例展示按照自变量抽样。这里选取年龄和收入这两个变量。</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># 最大差异度抽样用到proxy包</span>
<span class="kw">library</span>(proxy)
<span class="co"># 用lattice包绘制散点图</span>
<span class="kw">library</span>(lattice)
<span class="co"># 选取年龄和收入这两个变量</span>
testing&lt;-<span class="kw">subset</span>(sim.dat,<span class="dt">select=</span><span class="kw">c</span>(<span class="st">&quot;age&quot;</span>,<span class="st">&quot;income&quot;</span> ))</code></pre></div>
<p>我们先随机选取5个样本做为初始集（<code>start</code>），剩下的样本组成集合<code>samplePool</code>：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">5</span>)
<span class="co"># 随机选取5个样本</span>
startSet &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="dv">1</span>:<span class="kw">dim</span>(testing)[<span class="dv">1</span>], <span class="dv">5</span>)
start &lt;-<span class="st"> </span>testing[startSet,]
<span class="co"># 剩下的样本存在对象samplePool中</span>
samplePool &lt;-<span class="st"> </span>testing[-startSet,]</code></pre></div>
<p>通过<code>maxDissim()</code>函数从<code>samplePool</code>中抽取5个样本，这5个样本尽量和<code>start</code>中已有的样本不同。：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># 通过最大化差异得到的样本存在数据框new内</span>
<span class="co"># obj = minDiss 表示总体差异度以最小差异度为准</span>
newSamp &lt;-<span class="st"> </span><span class="kw">maxDissim</span>(start, samplePool,<span class="dt">obj =</span> minDiss, <span class="dt">n =</span> <span class="dv">5</span>)
new&lt;-samplePool[newSamp,]</code></pre></div>
<p>我们再从<code>samplePool</code>中不用最大差异法，而是随机抽取5个样本，将这5个样本存在数据框<code>new2</code>中：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">newSet &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="dv">1</span>:<span class="kw">dim</span>(samplePool)[<span class="dv">1</span>], <span class="dv">5</span>)
new2&lt;-testing[newSet,]</code></pre></div>
<p>绘制散点图比较两种不同方法（<code>new</code>：用最大化差异法抽取的样本；<code>new2</code>：随机抽取的样本）抽取的样本和初始样本（<code>start</code>）有什么不同：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">start$group&lt;-<span class="kw">rep</span>(<span class="st">&quot;start&quot;</span>,<span class="kw">nrow</span>(start))
new$group&lt;-<span class="kw">rep</span>(<span class="st">&quot;new&quot;</span>,<span class="kw">nrow</span>(new))
new2$group&lt;-<span class="kw">rep</span>(<span class="st">&quot;new2&quot;</span>,<span class="kw">nrow</span>(new2))
<span class="kw">xyplot</span>(age~income,<span class="dt">data=</span><span class="kw">rbind</span>(start,new,new2),<span class="dt">grid =</span> <span class="ot">TRUE</span>,
       <span class="dt">group =</span> group, <span class="dt">auto.key =</span> <span class="ot">TRUE</span>
)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:maxdis"></span>
<img src="DS_R_files/figure-html/maxdis-1.png" alt="按自变量最大化差异抽样" width="80%" />
<p class="caption">
Figure 7.3: 按自变量最大化差异抽样
</p>
</div>
<p>由图<a href="section-7.html#fig:maxdis">7.3</a>可见，通过最大化差异抽取的样本（<code>new</code>）和初始样本点（<code>start</code>）分布在图的不同位置。而随机抽取的新样本（<code>new2</code>）和原始样本更加接近。我们为什么希望每次抽取的样本和之前的不一样呢？因为我们希望最后得到的训练集和测试集覆盖的自变量观测区间相似。如果抽取的样本点都来自一个区域的话（比如全部都是年龄30以下，收入10万以下），如果讲这个样本用于训练的模型很可能不具有预测这个区域外样本的能力。反之要是用这个样本做为测试集，则无法检测模型在这个区域外样本上的表现。</p>
<ul>
<li>按时间序列划分</li>
</ul>
<p>对于时间序列数据，用简单随机抽样通常不是最好的方式。有一种按时间序列划分训练集和测试集的方法，关于该方法的讨论见<span class="citation">[<a href="#ref-Hyndman">18</a>]</span>。我们临时抽取一个长度为100的来自1阶自回归模型［AR(1)］的时间序列样本，用来展示<code>caret</code>包中对时间序列样本划分测试集和训练集的函数<code>createTimeSlices()</code>。由于时间序列话题不在本书范围之内，这里不会进行过多讨论。</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># 抽取符合AR(1)的时间序列向量</span>
timedata =<span class="st"> </span><span class="kw">arima.sim</span>(<span class="kw">list</span>(<span class="dt">order=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>), <span class="dt">ar=</span>-.<span class="dv">9</span>), <span class="dt">n=</span><span class="dv">100</span>)
<span class="co"># 对时间序列作图</span>
<span class="kw">plot</span>(timedata, <span class="dt">main=</span>(<span class="kw">expression</span>(<span class="kw">AR</span>(<span class="dv">1</span>)~<span class="er">~~</span>phi==-.<span class="dv">9</span>)))     </code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:times"></span>
<img src="DS_R_files/figure-html/times-1.png" alt="时间序列样本图" width="80%" />
<p class="caption">
Figure 7.4: 时间序列样本图
</p>
</div>
<p>图<a href="section-7.html#fig:maxdis">7.3</a>展示了100个模拟的时间序列观测。对这样的数据，我们希望训练集和测试集都能覆盖到不同时段的观测。下面用<code>createTimeSlices()</code>函数对数据进行划分。该函数中有3个需要设置的参数：</p>
<ul>
<li>initialWindow: 初始训练集样本中的连续观测数目</li>
<li>horizon: 测试集中的观测数目</li>
<li>fixedWindow: 逻辑值，取值为FALSE时，训练集从第一个样本开始划分区间长度不固定。</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">timeSlices &lt;-<span class="st"> </span><span class="kw">createTimeSlices</span>(<span class="dv">1</span>:<span class="kw">length</span>(timedata), 
                   <span class="dt">initialWindow =</span> <span class="dv">36</span>, <span class="dt">horizon =</span> <span class="dv">12</span>, <span class="dt">fixedWindow =</span> T)
<span class="kw">str</span>(timeSlices,<span class="dt">max.level =</span> <span class="dv">1</span>)</code></pre></div>
<pre><code>## List of 2
##  $ train:List of 53
##  $ test :List of 53</code></pre>
<p>可以看到函数结果返回2个列表，分别含有训练集和测试集的样本索引。我们查看第一个训练集和测试集样本。</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># 将训练集索引信息存在trainSlices对象内</span>
trainSlices &lt;-<span class="st"> </span>timeSlices[[<span class="dv">1</span>]]
<span class="co"># 将测试集索引信息存在testSlices对象内</span>
testSlices &lt;-<span class="st"> </span>timeSlices[[<span class="dv">2</span>]]
<span class="co"># 分别查看第一个训练集样本和测试集样本</span>
trainSlices[[<span class="dv">1</span>]]</code></pre></div>
<pre><code>##  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
## [24] 24 25 26 27 28 29 30 31 32 33 34 35 36</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">testSlices[[<span class="dv">1</span>]]</code></pre></div>
<pre><code>##  [1] 37 38 39 40 41 42 43 44 45 46 47 48</code></pre>
<p>第一个训练集样本是原数据中第1个观测到第36个观测（因为<code>initialWindow = 36</code>），接下来从第37到48这12个观测被划分到第一个测试集（因为<code>horizon = 12</code>）。你可以通过<code>head(trainSlices)</code>查看后续的样本索引。尝试着改变<code>fixedWindow =</code>的设置，然后重复上面的代码得到新的<code>trainSlices</code>和<code>testSlices</code>，然后键入：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">head</span>(trainSlices)
<span class="kw">head</span>(testSlices)</code></pre></div>
<p>比较两种结果的不同就能够很容易理解该选项的作用了。</p>
<p>训练集和测试集的划分很容易理解和实现。但注意其中两个潜在的缺陷：</p>
<ol style="list-style-type: decimal">
<li>由于训练集和测试集的划分是随机的，所以重复这一过程在测试集上得到的误差会有波动。</li>
<li>由于训练集中只包含原始观测的一个子集，拟合模型使用的是部分数据。通常当数据量不是非常大的时候，使用更少的观测多少会对模型拟合造成负面影响。这就意味着该过程可能过度估计模型误差（即，使用所有观测拟合的模型的误差应该比当前估计的要小）。</li>
</ol>
</div>
<div id="section-7.3.2" class="section level3">
<h3><span class="header-section-number">7.3.2</span> 重抽样</h3>
<p>重抽样即对样本进行重复划分，所以是建立在数据划分的基础上。其基本原理是：用部分样本拟合模型，用剩下的样本评估模型。多次重复这一过程，然后对结果进行汇总。进行重抽样的目的可能有：</p>
<ol style="list-style-type: decimal">
<li><p>对于有调优参数的模型，如支持向量机，罚函数模型等，必须通过重抽样估计调优参数。这时的目的是针对一个模型表现的度量（如RMSE），找到能够优化该度量的调优参数值。</p></li>
<li><p>对于不含有调优参数的模型，如普通线性回归，最小二乘回归等，就模型拟合本身不需要重抽样，但可以通过重抽样考察模型拟合结果的稳定性，也可以用于检验模型在和训练集无关的样本上的表现。</p></li>
</ol>
<p>这一小节将介绍几种主要的重抽样方法。</p>
<div id="k" class="section level4">
<h4><span class="header-section-number">7.3.2.1</span> k折交叉验证</h4>
<p>k折交叉验证的主要过程如下：</p>
<blockquote>
<ol style="list-style-type: decimal">
<li>将样本随机划分为<span class="math inline">\(k\)</span>个大小相当的子集</li>
<li>对<span class="math inline">\(i=1…k\)</span></li>
</ol>
<ol style="list-style-type: lower-alpha">
<li>用除了第<span class="math inline">\(i\)</span>个样本集之外的样本拟合模型<span class="math inline">\(M_{i}\)</span></li>
<li>将<span class="math inline">\(M_{i}\)</span>用在第i个样本集上，对结果进行评估</li>
</ol>
</blockquote>
<div class="figure">
<img src="http://scientistcafe.com/book/Figure/DivideData.png" alt="5折交叉验证" />
<p class="caption">5折交叉验证</p>
</div>
<p>这样会得到k个模型评估结果，将这些结果进行汇总（通常是计算均值和标准差），然后基于此了解调优参数和模型表现之间的关系。联系之前介绍的不同划分方法，可以将这些划分方法应用到k折交叉验证中，使k个子集中的因变量组成尽可能平衡。k折交叉验证的一个特定是k等于样本量，这时每次只有一个预留样本，该情况也称为留一交叉验证（LOOCV），注意在这种情况下模型最终的评估结果将根据所有的预测值进行计算。通常在样本量较小的时候使用LOOCV，道理很简单，样本量小的时候我们应该用尽可能多的样本拟合模型。关于交互校验的折数，很多R函数默认设置k=10，但没有黄金标准。折数越多，每次预留在外的样本就越少，模型表现估计值和真实值之间的差距就越小。但LOOCV的计算量最大，因为其模型拟合的次数等于样本量，且每次模型拟合使用的子集样本量几乎和训练集相同。另一方面，当k值很小时（2或者3），计算效率高但是结果的方差和偏差都会增加。这意味着如果重复抽样的过程得到的结果可能很不一样。当样本量足够大时，方差和偏差的潜在影响就可以忽略不计，在这种情况下可以使用折数较低的交叉验证。这里讲到的关于计算效率和偏差之间的权衡，需要读者自己反复实践才能真正理解。</p>
<p>caret包中有几个关于重抽样的函数。<code>createFolds()</code>函数用于k折交叉验证。我们按照 消费者类别变量对服装消费者数据抽取k折交叉验证样本。</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(caret)
class&lt;-sim.dat$segment
<span class="co">#k折校验重抽样</span>
<span class="kw">set.seed</span>(<span class="dv">1</span>)
cv&lt;-<span class="kw">createFolds</span>(class,<span class="dt">k=</span><span class="dv">10</span>,<span class="dt">returnTrain=</span>T)
<span class="kw">str</span>(cv)</code></pre></div>
<pre><code>## List of 10
##  $ Fold01: int [1:900] 1 2 3 4 5 6 7 8 9 10 ...
##  $ Fold02: int [1:900] 1 2 3 4 5 6 7 9 10 11 ...
##  $ Fold03: int [1:900] 1 2 3 4 5 6 7 8 10 11 ...
##  $ Fold04: int [1:900] 1 2 3 4 5 6 7 8 9 11 ...
##  $ Fold05: int [1:900] 1 3 4 6 7 8 9 10 11 12 ...
##  $ Fold06: int [1:900] 1 2 3 4 5 6 7 8 9 10 ...
##  $ Fold07: int [1:900] 2 3 4 5 6 7 8 9 10 11 ...
##  $ Fold08: int [1:900] 1 2 3 4 5 8 9 10 11 12 ...
##  $ Fold09: int [1:900] 1 2 4 5 6 7 8 9 10 11 ...
##  $ Fold10: int [1:900] 1 2 3 5 6 7 8 9 10 11 ...</code></pre>
<p>结果返回10个子样本集中样本对应的行数。我们可以通过交叉验证来估计调优参数。回忆之前应变量误差的小节中拟合支持向量机模型的代码：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#这里只是截取了之前的代码用于展示，并不能独立运行</span>
fit0&lt;-<span class="kw">train</span>(trainX,trainY,<span class="dt">method=</span><span class="st">&quot;svmRadial&quot;</span>,
            <span class="dt">tuneLength=</span><span class="dv">15</span>,
            <span class="dt">trControl=</span><span class="kw">trainControl</span>(<span class="dt">method=</span><span class="st">&quot;cv&quot;</span>))</code></pre></div>
<p>上面代码中“<code>method=&quot;cv&quot;</code>”告诉R进行交叉验证，这里默认<span class="math inline">\(k=10\)</span>。</p>
</div>
<div id="section-7.3.2.2" class="section level4">
<h4><span class="header-section-number">7.3.2.2</span> 重复训练/测试集划分</h4>
<p>该方法其实就是对数据集重复多次训练集／测试集划分，用训练集建立模型，用测试集评估模型。和k折交叉验证不同，该过程生成的测试集可能有重复的样本，其通常重复更多次。对于划分比例和重复次数没有固定法则，通常将总样本的75%到80%用于训练，剩下的用于测试，用于训练的样本越接近，得到模型估计的偏差就越小。该方法中重复的次数的增加可以减少模型评估结果的不确定性，当然代价就是在模型复杂时的计算时间。当然，重复的次数也和测试集的样本占总体比例有关，如果比例小，那么得到的预测评估结果的波动性就更大，这时就需要增加重复次数来见效评估结果的不确定性。</p>
<p>假设我们还是按照消费者类别（<code>segment</code>）划分数据，这依旧可以使用之前用于划分训练集和测试集的函数<code>createDataPartition()</code>。记得之前该函数中的选项设置<code>times=1</code>么？这里只要将其设置成你想要重复的次数即可。</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">trainIndex &lt;-<span class="st"> </span><span class="kw">createDataPartition</span>(sim.dat$segment, <span class="dt">p =</span> .<span class="dv">8</span>, <span class="dt">list =</span> <span class="ot">FALSE</span>, <span class="dt">times =</span> <span class="dv">5</span>)
dplyr::<span class="kw">glimpse</span>(trainIndex)</code></pre></div>
<pre><code>##  int [1:800, 1:5] 1 3 4 5 6 7 8 9 10 11 ...
##  - attr(*, &quot;dimnames&quot;)=List of 2
##   ..$ : NULL
##   ..$ : chr [1:5] &quot;Resample1&quot; &quot;Resample2&quot; &quot;Resample3&quot; &quot;Resample4&quot; ...</code></pre>
<p>类似的，对于其它划分方式，只要大家知道如何划分，重复划分应该很容易。</p>
</div>
<div id="bootstrap-" class="section level4">
<h4><span class="header-section-number">7.3.2.3</span> Bootstrap 方法</h4>
<p>Bootstrap是一种应用及其广泛而且强大的统计工具。它可以用来定量分析参数估计或统计模型的不确定性<span class="citation">[<a href="#ref-bootstrap1986">19</a>]</span>。如，可以通过Bootstrap估计线性回归模型参数拟合的标准差，这是另外一种取代p值的方法。该方法的强大在于其可以很容易应用于每个模型（说白了就是是对数据重复进行有放回随机抽样，拟合的过程），有的模型要是用传统的统计推断方法很难得到标准差和置信区间。这是一个典型的听起来很高端，其实没太多技术含量的方法。但很多天才的想法不都是这样么？之前没有人想到，之后大家都觉得这么简单粗暴有效怎么会想不到。由于是有放回抽样，一个样本可能多次被选中，且Bootstrap样本量和原数据样本量一样。这些没有被选中的样本称为“袋外（out-of-bag）样本”。选中的样本用来建立模型，带外样本用来评估模型。Efron指出，一般情况下<span class="citation">[<a href="#ref-efron1983">20</a>]</span>，Bootstrap估计的模型错误率的不确定性更小。平均而言，63.2%的样本点在 Bootstrap 中出现过至少一次，因此其估计的偏差与2折交叉验证相似。如之前所述，折数越小，用于训练的样本数目越少，这意味着估计的偏差越大。增加样本量可以缓解该问题。总的来说，和交叉验证相比，Bootstrap偏差更大，不确定性更小。针对估计偏差问题，Efron对原始Bootstrap过程进行了改进，得到下面的632方法：</p>
<p><span class="math display">\[(0.632 × 原始 Bootstrap 估计错误率) + (0.368 ×显性错误率)\]</span></p>
<p>其中显性错误率就是用所有样本进行建模，然后再作用于相同的样本集得到的模型错误率，该估计显然过度乐观。这一改进虽然在某种程度上降低了偏差，但在样本量小的时候依旧表现不佳。试想严重过度拟合的情况下显性错误率几乎是0，那么上面公式中的第二项也就不存在了，这个时候，632方法给出的错误率估计可能过度乐观。Efron 和 Tibshirani之后进一步改进了632方法，得到“632+ 方法”，进一步调整了Bootstrap 估计<span class="citation">[<a href="#ref-b632plus">21</a>]</span>。</p>
</div>
</div>
</div>
<div id="-2" class="section level2">
<h2><span class="header-section-number">7.4</span> 本章总结</h2>
<p>本章介绍了一些基础建模技术，包括有监督和无监督的概念。模型误差分类：</p>
<ol style="list-style-type: decimal">
<li>系统误差：能够通过改进模型减小这部分误差</li>
<li>随机误差：当前数据无法解释的部分，无法通过建立更加复杂的模型来改进</li>
</ol>
<p>此外我们讲了误差的两个来源，应变量误差和自变量误差。其中应变量误差会反映在随机误差中，这是个硬伤，无法克服。而某些自变量的误差可能通过其它相关自变量得到弥补，其影响取决于随机性强度，相应变量在模型中的重要性，以及自变量之间的相关性。</p>
<p>最后，也是最重要的一个话题是数据的划分和再抽样。其主要目的是为了判断模型真实的表现。我们介绍了3种数据划分的方法：</p>
<ol style="list-style-type: decimal">
<li>按照结果变量划分数据；</li>
<li>按照预测变量划分数据；</li>
<li>按照时间序列划分数据。</li>
</ol>
<p>我们还介绍了两种主要的再抽样方法：bootstrap和交互校验。重抽样是建立在数据划分的基础上，其主要目的有两个：</p>
<ol style="list-style-type: decimal">
<li>对于有调优参数的模型估计调优参数；</li>
<li>考察模型拟合结果的稳定性。</li>
</ol>
<p>其中我们讨论了不同重抽样的影响，以及在方差，偏差和计算效率之间的权衡。这里讲的所有方法都需要大家在实践中总结，才能真正成为自己的技能。</p>

</div>
</div>
<h3> References</h3>
<div id="refs" class="references">
<div id="ref-APM">
<p>13. Max Kuhn, K.J.: Applied predictive modeling. Springer (2013).</p>
</div>
<div id="ref-woldh1">
<p>15. H, W.: Estimation of principal components and related models by iterative least squares. In P Krishnaiah (ed.), Journal of Multivariate Analysis , Academic Press, New York. (1966).</p>
</div>
<div id="ref-woldh2">
<p>16. H, W.: Soft modeling: The basic design and some extensions. In K Joreskog, H Wold (eds.), “Systems Under Indirect Observation: Causality, Structure, Prediction,” North–Holland, Amsterdam. (1982).</p>
</div>
<div id="ref-willett">
<p>17. Willett, P.: Dissimilarity-based algorithms for selecting structurally diverse sets of compounds. Journal of Computational Biology. 6(3-4), 447–457 (2004).</p>
</div>
<div id="ref-Hyndman">
<p>18. Hyndman, R., Athanasopoulos, G.: Forecasting: Principles and practice. OTect: Melbourne, Australia (2013).</p>
</div>
<div id="ref-bootstrap1986">
<p>19. Efron, B., Tibshirani, R.: Bootstrap methods for standard errors, confidence intervals, and other measures of statistical accuracy. Statistical Science. 54–75 (1986).</p>
</div>
<div id="ref-efron1983">
<p>20. Efron, B.: Estimating the error rate of a prediction rule: Improvement on cross-validation. Journal of the American Statistical Association. 316–331 (1983).</p>
</div>
<div id="ref-b632plus">
<p>21. Efron, B., Tibshirani, R.: Improvements on cross-validation: The 632+ bootstrap method. Journal of the American Statistical Association. 92, 548–560 (1997).</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="section-6.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="section-8.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/07-jianmojishu.Rmd",
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
