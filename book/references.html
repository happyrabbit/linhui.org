<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>数据科学家：R语言</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="This is my first book on data science">
  <meta name="generator" content="bookdown 0.1 and GitBook 2.6.7">

  <meta property="og:title" content="数据科学家：R语言" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is my first book on data science" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="数据科学家：R语言" />
  
  <meta name="twitter:description" content="This is my first book on data science" />
  

<meta name="author" content="林荟">

<meta name="date" content="2016-12-11">

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="section-13.html">


<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">数据科学家：R语言</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> 介绍</a></li>
<li class="chapter" data-level="2" data-path="section-2.html"><a href="section-2.html"><i class="fa fa-check"></i><b>2</b> 数据科学</a><ul>
<li class="chapter" data-level="2.1" data-path="section-2.html"><a href="section-2.html#section-2.1"><i class="fa fa-check"></i><b>2.1</b> 什么是数据科学？</a></li>
<li class="chapter" data-level="2.2" data-path="section-2.html"><a href="section-2.html#section-2.2"><i class="fa fa-check"></i><b>2.2</b> 什么是数据科学家？</a></li>
<li class="chapter" data-level="2.3" data-path="section-2.html"><a href="section-2.html#section-2.3"><i class="fa fa-check"></i><b>2.3</b> 数据科学家需要的技能</a></li>
<li class="chapter" data-level="2.4" data-path="section-2.html"><a href="section-2.html#section-2.4"><i class="fa fa-check"></i><b>2.4</b> 数据科学可以解决什么问题？</a><ul>
<li class="chapter" data-level="2.4.1" data-path="section-2.html"><a href="section-2.html#section-2.4.1"><i class="fa fa-check"></i><b>2.4.1</b> 前提要求</a></li>
<li class="chapter" data-level="2.4.2" data-path="section-2.html"><a href="section-2.html#section-2.4.2"><i class="fa fa-check"></i><b>2.4.2</b> 问题种类</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="section-3.html"><a href="section-3.html"><i class="fa fa-check"></i><b>3</b> 数据集模拟和背景介绍</a><ul>
<li class="chapter" data-level="3.1" data-path="section-3.html"><a href="section-3.html#section-3.1"><i class="fa fa-check"></i><b>3.1</b> 服装消费者数据</a></li>
<li class="chapter" data-level="3.2" data-path="section-3.html"><a href="section-3.html#section-3.2"><i class="fa fa-check"></i><b>3.2</b> 航空公司满意度调查</a></li>
<li class="chapter" data-level="3.3" data-path="section-3.html"><a href="section-3.html#section-3.3"><i class="fa fa-check"></i><b>3.3</b> 生猪疫情风险预测数据</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="section-4.html"><a href="section-4.html"><i class="fa fa-check"></i><b>4</b> 数据分析一般流程</a><ul>
<li class="chapter" data-level="4.1" data-path="section-4.html"><a href="section-4.html#section-4.1"><i class="fa fa-check"></i><b>4.1</b> 问题到数据</a></li>
<li class="chapter" data-level="4.2" data-path="section-4.html"><a href="section-4.html#section-4.2"><i class="fa fa-check"></i><b>4.2</b> 数据到信息</a></li>
<li class="chapter" data-level="4.3" data-path="section-4.html"><a href="section-4.html#section-4.3"><i class="fa fa-check"></i><b>4.3</b> 信息到行动</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="section-5.html"><a href="section-5.html"><i class="fa fa-check"></i><b>5</b> 数据预处理</a><ul>
<li class="chapter" data-level="5.1" data-path="section-5.html"><a href="section-5.html#section-5.1"><i class="fa fa-check"></i><b>5.1</b> 介绍</a></li>
<li class="chapter" data-level="5.2" data-path="section-5.html"><a href="section-5.html#section-5.2"><i class="fa fa-check"></i><b>5.2</b> 数据清理</a></li>
<li class="chapter" data-level="5.3" data-path="section-5.html"><a href="section-5.html#section-5.3"><i class="fa fa-check"></i><b>5.3</b> 缺失值填补</a><ul>
<li class="chapter" data-level="5.3.1" data-path="section-5.html"><a href="section-5.html#section-5.3.1"><i class="fa fa-check"></i><b>5.3.1</b> 中位数或众数填补</a></li>
<li class="chapter" data-level="5.3.2" data-path="section-5.html"><a href="section-5.html#k-"><i class="fa fa-check"></i><b>5.3.2</b> K-近邻填补</a></li>
<li class="chapter" data-level="5.3.3" data-path="section-5.html"><a href="section-5.html#section-5.3.3"><i class="fa fa-check"></i><b>5.3.3</b> 袋状树填补</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="section-5.html"><a href="section-5.html#section-5.4"><i class="fa fa-check"></i><b>5.4</b> 中心化和标量化</a></li>
<li class="chapter" data-level="5.5" data-path="section-5.html"><a href="section-5.html#section-5.5"><i class="fa fa-check"></i><b>5.5</b> 有偏分布</a></li>
<li class="chapter" data-level="5.6" data-path="section-5.html"><a href="section-5.html#section-5.6"><i class="fa fa-check"></i><b>5.6</b> 处理离群点</a></li>
<li class="chapter" data-level="5.7" data-path="section-5.html"><a href="section-5.html#section-5.7"><i class="fa fa-check"></i><b>5.7</b> 共线性</a></li>
<li class="chapter" data-level="5.8" data-path="section-5.html"><a href="section-5.html#section-5.8"><i class="fa fa-check"></i><b>5.8</b> 稀疏变量</a></li>
<li class="chapter" data-level="5.9" data-path="section-5.html"><a href="section-5.html#section-5.9"><i class="fa fa-check"></i><b>5.9</b> 编码名义变量</a></li>
<li class="chapter" data-level="5.10" data-path="section-5.html"><a href="section-5.html#section-5.10"><i class="fa fa-check"></i><b>5.10</b> 本章总结</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="section-6.html"><a href="section-6.html"><i class="fa fa-check"></i><b>6</b> 数据操作</a><ul>
<li class="chapter" data-level="6.1" data-path="section-6.html"><a href="section-6.html#section-6.1"><i class="fa fa-check"></i><b>6.1</b> 数据读写</a><ul>
<li class="chapter" data-level="6.1.1" data-path="section-6.html"><a href="section-6.html#tibble"><i class="fa fa-check"></i><b>6.1.1</b> 取代传统数据框的<code>tibble</code>对象</a></li>
<li class="chapter" data-level="6.1.2" data-path="section-6.html"><a href="section-6.html#readr"><i class="fa fa-check"></i><b>6.1.2</b> 高效数据读写：<code>readr</code>包</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="section-6.html"><a href="section-6.html#section-6.2"><i class="fa fa-check"></i><b>6.2</b> 数据整合</a><ul>
<li class="chapter" data-level="6.2.1" data-path="section-6.html"><a href="section-6.html#baseapply"><i class="fa fa-check"></i><b>6.2.1</b> base包：apply()</a></li>
<li class="chapter" data-level="6.2.2" data-path="section-6.html"><a href="section-6.html#plyrddply"><i class="fa fa-check"></i><b>6.2.2</b> plyr包：ddply()函数</a></li>
<li class="chapter" data-level="6.2.3" data-path="section-6.html"><a href="section-6.html#dplyr"><i class="fa fa-check"></i><b>6.2.3</b> dplyr包</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="section-6.html"><a href="section-6.html#section-6.3"><i class="fa fa-check"></i><b>6.3</b> 数据整形</a><ul>
<li class="chapter" data-level="6.3.1" data-path="section-6.html"><a href="section-6.html#reshape2"><i class="fa fa-check"></i><b>6.3.1</b> <code>reshape2</code>包</a></li>
<li class="chapter" data-level="6.3.2" data-path="section-6.html"><a href="section-6.html#tidyr"><i class="fa fa-check"></i><b>6.3.2</b> <code>tidyr</code>包</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="section-6.html"><a href="section-6.html#-1"><i class="fa fa-check"></i><b>6.4</b> 本章总结</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="section-7.html"><a href="section-7.html"><i class="fa fa-check"></i><b>7</b> 基础建模技术</a><ul>
<li class="chapter" data-level="7.1" data-path="section-7.html"><a href="section-7.html#section-7.1"><i class="fa fa-check"></i><b>7.1</b> 有监督和无监督</a></li>
<li class="chapter" data-level="7.2" data-path="section-7.html"><a href="section-7.html#section-7.2"><i class="fa fa-check"></i><b>7.2</b> 误差及其来源</a><ul>
<li class="chapter" data-level="7.2.1" data-path="section-7.html"><a href="section-7.html#section-7.2.1"><i class="fa fa-check"></i><b>7.2.1</b> 系统误差和随机误差</a></li>
<li class="chapter" data-level="7.2.2" data-path="section-7.html"><a href="section-7.html#section-7.2.2"><i class="fa fa-check"></i><b>7.2.2</b> 应变量误差</a></li>
<li class="chapter" data-level="7.2.3" data-path="section-7.html"><a href="section-7.html#section-7.2.3"><i class="fa fa-check"></i><b>7.2.3</b> 自变量误差</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="section-7.html"><a href="section-7.html#section-7.3"><i class="fa fa-check"></i><b>7.3</b> 数据划分和再抽样</a><ul>
<li class="chapter" data-level="7.3.1" data-path="section-7.html"><a href="section-7.html#section-7.3.1"><i class="fa fa-check"></i><b>7.3.1</b> 划分训练集和测试集</a></li>
<li class="chapter" data-level="7.3.2" data-path="section-7.html"><a href="section-7.html#section-7.3.2"><i class="fa fa-check"></i><b>7.3.2</b> 重抽样</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="section-7.html"><a href="section-7.html#-2"><i class="fa fa-check"></i><b>7.4</b> 本章总结</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="section-8.html"><a href="section-8.html"><i class="fa fa-check"></i><b>8</b> 模型评估度量</a><ul>
<li class="chapter" data-level="8.1" data-path="section-8.html"><a href="section-8.html#section-8.1"><i class="fa fa-check"></i><b>8.1</b> 回归模型评估度量</a></li>
<li class="chapter" data-level="8.2" data-path="section-8.html"><a href="section-8.html#section-8.2"><i class="fa fa-check"></i><b>8.2</b> 分类模型评估度量</a><ul>
<li class="chapter" data-level="8.2.1" data-path="section-8.html"><a href="section-8.html#kappa"><i class="fa fa-check"></i><b>8.2.1</b> Kappa统计量</a></li>
<li class="chapter" data-level="8.2.2" data-path="section-8.html"><a href="section-8.html#roc"><i class="fa fa-check"></i><b>8.2.2</b> ROC曲线</a></li>
<li class="chapter" data-level="8.2.3" data-path="section-8.html"><a href="section-8.html#section-8.2.3"><i class="fa fa-check"></i><b>8.2.3</b> 提升图</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="section-8.html"><a href="section-8.html#-3"><i class="fa fa-check"></i><b>8.3</b> 本章总结</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="section-9.html"><a href="section-9.html"><i class="fa fa-check"></i><b>9</b> 特征工程</a><ul>
<li class="chapter" data-level="9.1" data-path="section-9.html"><a href="section-9.html#section-9.1"><i class="fa fa-check"></i><b>9.1</b> 特征构建</a></li>
<li class="chapter" data-level="9.2" data-path="section-9.html"><a href="section-9.html#section-9.2"><i class="fa fa-check"></i><b>9.2</b> 特征提取</a><ul>
<li class="chapter" data-level="9.2.1" data-path="section-9.html"><a href="section-9.html#section-9.2.1"><i class="fa fa-check"></i><b>9.2.1</b> 初步探索数据</a></li>
<li class="chapter" data-level="9.2.2" data-path="section-9.html"><a href="section-9.html#section-9.2.2"><i class="fa fa-check"></i><b>9.2.2</b> 主成分分析</a></li>
<li class="chapter" data-level="9.2.3" data-path="section-9.html"><a href="section-9.html#section-9.2.3"><i class="fa fa-check"></i><b>9.2.3</b> 探索性因子分析</a></li>
<li class="chapter" data-level="9.2.4" data-path="section-9.html"><a href="section-9.html#section-9.2.4"><i class="fa fa-check"></i><b>9.2.4</b> 高维标度化</a></li>
<li class="chapter" data-level="9.2.5" data-path="section-9.html"><a href="section-9.html#section-9.2.5"><i class="fa fa-check"></i><b>9.2.5</b> 知识扩展</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="section-9.html"><a href="section-9.html#section-9.3"><i class="fa fa-check"></i><b>9.3</b> 特征选择</a><ul>
<li class="chapter" data-level="9.3.1" data-path="section-9.html"><a href="section-9.html#section-9.3.1"><i class="fa fa-check"></i><b>9.3.1</b> 过滤法</a></li>
<li class="chapter" data-level="9.3.2" data-path="section-9.html"><a href="section-9.html#section-9.3.2"><i class="fa fa-check"></i><b>9.3.2</b> 绕封法</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="section-10.html"><a href="section-10.html"><i class="fa fa-check"></i><b>10</b> 线性回归极其衍生</a><ul>
<li class="chapter" data-level="10.1" data-path="section-10.html"><a href="section-10.html#section-10.1"><i class="fa fa-check"></i><b>10.1</b> 普通线性回归</a><ul>
<li class="chapter" data-level="10.1.1" data-path="section-10.html"><a href="section-10.html#section-10.1.1"><i class="fa fa-check"></i><b>10.1.1</b> 最小二乘线性模型</a></li>
<li class="chapter" data-level="10.1.2" data-path="section-10.html"><a href="section-10.html#section-10.1.2"><i class="fa fa-check"></i><b>10.1.2</b> 回归诊断</a></li>
<li class="chapter" data-level="10.1.3" data-path="section-10.html"><a href="section-10.html#section-10.1.3"><i class="fa fa-check"></i><b>10.1.3</b> 离群点，高杠杆点和强影响点</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="section-10.html"><a href="section-10.html#section-10.2"><i class="fa fa-check"></i><b>10.2</b> 收缩方法</a><ul>
<li class="chapter" data-level="10.2.1" data-path="section-10.html"><a href="section-10.html#section-10.2.1"><i class="fa fa-check"></i><b>10.2.1</b> 岭回归</a></li>
<li class="chapter" data-level="10.2.2" data-path="section-10.html"><a href="section-10.html#lasso"><i class="fa fa-check"></i><b>10.2.2</b> Lasso</a></li>
<li class="chapter" data-level="10.2.3" data-path="section-10.html"><a href="section-10.html#section-10.2.3"><i class="fa fa-check"></i><b>10.2.3</b> 弹性网络</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="section-10.html"><a href="section-10.html#lasso"><i class="fa fa-check"></i><b>10.3</b> 知识扩展：Lasso的变量选择功能</a></li>
<li class="chapter" data-level="10.4" data-path="section-10.html"><a href="section-10.html#section-10.4"><i class="fa fa-check"></i><b>10.4</b> 主成分和偏最小二乘回归</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="section-11.html"><a href="section-11.html"><i class="fa fa-check"></i><b>11</b> 广义线性模型压缩方法</a><ul>
<li class="chapter" data-level="11.1" data-path="section-11.html"><a href="section-11.html#glmnet"><i class="fa fa-check"></i><b>11.1</b> 初识<code>glmnet</code></a></li>
<li class="chapter" data-level="11.2" data-path="section-11.html"><a href="section-11.html#section-11.2"><i class="fa fa-check"></i><b>11.2</b> 收缩线性回归</a></li>
<li class="chapter" data-level="11.3" data-path="section-11.html"><a href="section-11.html#section-11.3"><i class="fa fa-check"></i><b>11.3</b> 逻辑回归</a><ul>
<li class="chapter" data-level="11.3.1" data-path="section-11.html"><a href="section-11.html#section-11.3.1"><i class="fa fa-check"></i><b>11.3.1</b> 普通逻辑回归</a></li>
<li class="chapter" data-level="11.3.2" data-path="section-11.html"><a href="section-11.html#section-11.3.2"><i class="fa fa-check"></i><b>11.3.2</b> 收缩逻辑回归</a></li>
<li class="chapter" data-level="11.3.3" data-path="section-10.html"><a href="section-10.html#lasso"><i class="fa fa-check"></i><b>11.3.3</b> 知识扩展：群组lasso逻辑回归</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="section-11.html"><a href="section-11.html#section-11.4"><i class="fa fa-check"></i><b>11.4</b> 收缩多项回归</a></li>
<li class="chapter" data-level="11.5" data-path="section-11.html"><a href="section-11.html#section-11.5"><i class="fa fa-check"></i><b>11.5</b> 泊松收缩回归</a></li>
<li class="chapter" data-level="11.6" data-path="section-11.html"><a href="section-11.html#-4"><i class="fa fa-check"></i><b>11.6</b> 本章总结</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="section-12.html"><a href="section-12.html"><i class="fa fa-check"></i><b>12</b> 树模型</a><ul>
<li class="chapter" data-level="12.1" data-path="section-12.html"><a href="section-12.html#section-12.1"><i class="fa fa-check"></i><b>12.1</b> 分裂准则</a></li>
<li class="chapter" data-level="12.2" data-path="section-12.html"><a href="section-12.html#section-12.2"><i class="fa fa-check"></i><b>12.2</b> 树的修剪</a></li>
<li class="chapter" data-level="12.3" data-path="section-12.html"><a href="section-12.html#section-12.3"><i class="fa fa-check"></i><b>12.3</b> 回归树和决策树</a><ul>
<li class="chapter" data-level="12.3.1" data-path="section-12.html"><a href="section-12.html#section-12.3.1"><i class="fa fa-check"></i><b>12.3.1</b> 回归树</a></li>
<li class="chapter" data-level="12.3.2" data-path="section-12.html"><a href="section-12.html#section-12.3.2"><i class="fa fa-check"></i><b>12.3.2</b> 决策树</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="section-12.html"><a href="section-12.html#section-12.4"><i class="fa fa-check"></i><b>12.4</b> 装袋树</a></li>
<li class="chapter" data-level="12.5" data-path="section-12.html"><a href="section-12.html#section-12.5"><i class="fa fa-check"></i><b>12.5</b> 随机森林</a></li>
<li class="chapter" data-level="12.6" data-path="section-12.html"><a href="section-12.html#section-12.6"><i class="fa fa-check"></i><b>12.6</b> 助推法</a></li>
<li class="chapter" data-level="12.7" data-path="section-12.html"><a href="section-12.html#section-12.7"><i class="fa fa-check"></i><b>12.7</b> 知识扩展：助推法的可加模型框架</a></li>
<li class="chapter" data-level="12.8" data-path="section-12.html"><a href="section-12.html#section-12.8"><i class="fa fa-check"></i><b>12.8</b> 知识扩展：助推树的数学框架</a><ul>
<li class="chapter" data-level="12.8.1" data-path="section-12.html"><a href="section-12.html#section-12.8.1"><i class="fa fa-check"></i><b>12.8.1</b> 数学表达</a></li>
<li class="chapter" data-level="12.8.2" data-path="section-12.html"><a href="section-12.html#section-12.8.2"><i class="fa fa-check"></i><b>12.8.2</b> 梯度助推数值优化</a></li>
</ul></li>
<li class="chapter" data-level="12.9" data-path="section-12.html"><a href="section-12.html#-5"><i class="fa fa-check"></i><b>12.9</b> 本章总结</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="section-13.html"><a href="section-13.html"><i class="fa fa-check"></i><b>13</b> 深度学习</a><ul>
<li class="chapter" data-level="13.1" data-path="section-6.html"><a href="section-6.html#-1"><i class="fa fa-check"></i><b>13.1</b> 介绍</a></li>
<li class="chapter" data-level="13.2" data-path="section-13.html"><a href="section-13.html#r"><i class="fa fa-check"></i><b>13.2</b> R中深度学习包</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>14</b> References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">数据科学家：R语言</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="references" class="section level1">
<h1><span class="header-section-number">第14章</span> References</h1>

<div id="refs" class="references">
<div>
<p>1. Tukey, J.: The future of data analysis. The Annals of Mathematical Statistics. 1–67 (1962).</p>
</div>
<div>
<p>2. Berkeley: What is data science? (2015).</p>
</div>
<div>
<p>3. Donoho, D.: 50 years of data science. (2015).</p>
</div>
<div>
<p>4. Ton de Waal, S.S., Jeroen Pannekoek: Handbook of statistical data editing and imputation. John Wiley &amp; Sons (2011).</p>
</div>
<div>
<p>5. Saar-Tsechansky M, P.F.: Handling missing values when applying classification models. Journal of Machine Learning Research. 8, 1625–1657 (2007b).</p>
</div>
<div>
<p>6. L, B.: Bagging predictors. Machine Learning. 24, 123–140 (1966a).</p>
</div>
<div>
<p>7. Jolliffe, I.: Principla component analysis. New York: Springer (2002).</p>
</div>
<div>
<p>8. Geladi P, K.B.: Partial least-squares regression: A tutorial. Analytica Chimica Acta. 1–17 (1986).</p>
</div>
<div>
<p>9. Mulaik, S.: Foundations of factor analysis. Boca Raton: Chapman&amp;Hall/CRC (2009).</p>
</div>
<div>
<p>10. Box G, C.D.: An analysis of transformations. Journal of the Royal Statistical Society. Series B (Methodological), 211–252 (1964).</p>
</div>
<div>
<p>11. Iglewicz, B., Hoaglin, D.: How to detect and handle outliers. The ASQC Basic References in Quality Control: Statistical Techniques. 16, (1993).</p>
</div>
<div>
<p>12. Serneels S, E.P., Nolf ED: Spatial sign pre-processing: A simple way to impart moderate robustness to multivariate estimators. Journal of Chemical Information and Modeling. 46, 1402–1409 (2006).</p>
</div>
<div>
<p>13. Max Kuhn, K.J.: Applied predictive modeling. Springer (2013).</p>
</div>
<div>
<p>14. Nolan, D., Lang, D.T.: XML and web technologies for data sciences with r. Springer (2014).</p>
</div>
<div>
<p>15. H, W.: Estimation of principal components and related models by iterative least squares. In P Krishnaiah (ed.), Journal of Multivariate Analysis , Academic Press, New York. (1966).</p>
</div>
<div>
<p>16. H, W.: Soft modeling: The basic design and some extensions. In K Joreskog, H Wold (eds.), “Systems Under Indirect Observation: Causality, Structure, Prediction,” North–Holland, Amsterdam. (1982).</p>
</div>
<div>
<p>17. Willett, P.: Dissimilarity-based algorithms for selecting structurally diverse sets of compounds. Journal of Computational Biology. 6(3-4), 447–457 (2004).</p>
</div>
<div>
<p>18. Hyndman, R., Athanasopoulos, G.: Forecasting: Principles and practice. OTect: Melbourne, Australia (2013).</p>
</div>
<div>
<p>19. Efron, B., Tibshirani, R.: Bootstrap methods for standard errors, confidence intervals, and other measures of statistical accuracy. Statistical Science. 54–75 (1986).</p>
</div>
<div>
<p>20. Efron, B.: Estimating the error rate of a prediction rule: Improvement on cross-validation. Journal of the American Statistical Association. 316–331 (1983).</p>
</div>
<div>
<p>21. Efron, B., Tibshirani, R.: Improvements on cross-validation: The 632+ bootstrap method. Journal of the American Statistical Association. 92, 548–560 (1997).</p>
</div>
<div>
<p>22. Provost F, K.R., Fawcett T: The case against accuracy esti- mation for comparing induction algorithms. Proceedings of the Fifteenth International Conference on Machine Learning. 445–453 (1998).</p>
</div>
<div>
<p>23. J, C.: A coefficient of agreement for nominal data. Educational and Psychological Measurement. 20, 37–46 (1960).</p>
</div>
<div>
<p>24. Landis JR, K.G.: The measurement of observer agreement for categorical data. Biometrics. 33, 159–174 (1977).</p>
</div>
<div>
<p>25. E.R. DeLong, D.C.-P., D.M. DeLong: Comparing the areas under two or more correlated receiver operating characteristics curves: A nonparametric approach. Biometrics. 44, 837–845 (1988).</p>
</div>
<div>
<p>26. Hall P, F.Y., Hyndman R: Nonparametric confidence intervals for receiver operating characteristic curves. Biometrika. 91, 743–750 (2004).</p>
</div>
<div>
<p>27. T, F.: An introduction to roc analysis. Pattern Recognition Letters. 27, 861–874 (2006).</p>
</div>
<div>
<p>28. D, M.: Analyzing a portion of the roc curve. Medical Decision Making. 9, 190–195 (1989).</p>
</div>
<div>
<p>29. Hand D, T.R.: A simple generalisation of the area under the roc curve for multiple class classification problems. Machine Learning. 45, 171–186 (2001).</p>
</div>
<div>
<p>30. Lachiche N, F.P.: Improving accuracy and cost of two–Class and multi–Class probabilistic classifiers using roc curves. In “Proceed- ings of the Twentieth International Conference on Machine Learning. 20, (2003).</p>
</div>
<div>
<p>31. Li J, F.J.: ROC analysis with multiple classes and multiple tests: Methodology and its application in microarray studies. Biostatistics. 9, 566–576 (2008).</p>
</div>
<div>
<p>32. Spearman, C.: “General intelligence,” objectively determined and measured. The American Journal of Psychology. 15, 201–292 (1904).</p>
</div>
<div>
<p>33. Gower, J.C.: A general coefficient of similarity and some of its properties. Biometrics. 27, (1971).</p>
</div>
<div>
<p>34. Oded Netzer, J.G., Ronen Feldman: Mine your own business: Market-structure surveillance through text mining. Marketing Science. 31, 521–543 (2012).</p>
</div>
<div>
<p>35. Kaiser, H.F.: The varimax criterion for analytic rotation in factor analysis. Psychometrika. 23, (1958).</p>
</div>
<div>
<p>36. Jackson, E.: A user’s guide to principal components. John Wiley &amp; Sons (2003).</p>
</div>
<div>
<p>37. Mardia, K.: Some properties of classical multidimensional scaling. Communications on Statistics – Theory and Methods. A7, (1978).</p>
</div>
<div>
<p>38. Borg, G., I.: Applied multidimensional scaling. Springer (2012).</p>
</div>
<div>
<p>39. Borg, I., Groenen, P.J.: Modern multidimensional scaling: Theory and applications. Springer (2005).</p>
</div>
<div>
<p>40. Guyon I, E.A.: An introduction to variable and feature selec- tion. The Journal of Machine Learning Research. 3, 1157–1182 (2003).</p>
</div>
<div>
<p>41. Cleveland W, D.S.: Locally weighted regression: An approach to regression analysis by local fitting. Journal of the American Statistical Association. 596–610 (1988).</p>
</div>
<div>
<p>42. Bland J, A.D.: Statistics notes: Multiple significance tests: The bonferroni method. British Medical Journal. 310, 170–170 (1995).</p>
</div>
<div>
<p>43. J, H., B, M.: The meaning and use of the area under a receiver operating (roc) curvel characteristic. Radiology. 143, (1982).</p>
</div>
<div>
<p>44. E, V.: A permutation test to compare receiver operating characteristic curves. Biometrics. 56, 1134–1138 (2000).</p>
</div>
<div>
<p>45. J, B., D, A.: The odds ratio. British Medical Journal. 320, 1468 (2000).</p>
</div>
<div>
<p>46. A, A.: Categorical data analysis. Wiley (2002).</p>
</div>
<div>
<p>47. K, K., L, R.: The feature selection problem: Traditional methods and a new algorithm. Proceedings of the National Conference on Artificial Intelligence. 129–129 (1992).</p>
</div>
<div>
<p>48. I, K.: Estimating attributes: Analysis and extensions of relief.” in f bergadano, l de raedt (eds.), “machine learning: ECML–94&quot;. Springer Berlin / Heidelberg (1994).</p>
</div>
<div>
<p>49. M, R.-S., I, K.: An adaptation of relief for attribute estimation in regression. Proceedings of the Fourteenth International Conference on Machine Learning. 296–304 (1997).</p>
</div>
<div>
<p>50. Robnik-Sˇikonja, M., Kononenko, I.: Theoretical and empirical analysis of relieff and rrelieff. Machine Learning Journal. 53, (2003).</p>
</div>
<div>
<p>51. Ronald L. Wassersteina, N.A.L.: Position on p-values: Context, process, and purpose. (2016).</p>
</div>
<div>
<p>52. Fox, J., Weisberg, S.: An r companion to applied regression. Sage. (2011).</p>
</div>
<div>
<p>53. Williams, D.A.: Generalized linear model diagnostics using the deviance and single case deletions. Applied Statistics. 36, 181–191. (1987).</p>
</div>
<div>
<p>54. Fox, J.: Applied regression analysis and generalized linear models. Sage. (2008).</p>
</div>
<div>
<p>55. Cook, R.D., Weisberg, S.: Residuals and influence in regression. Chapman; Hall. (1982).</p>
</div>
<div>
<p>56. Weisberg, S.: Applied linear regression. Wiley (2014).</p>
</div>
<div>
<p>57. A, H.: Ridge regression: Biased estimation for nonorthogonal problems. Technometrics. 12, 55–67 (1970).</p>
</div>
<div>
<p>58. R, T.: Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society Series B (Methodological). 58, 267–288 (1996).</p>
</div>
<div>
<p>59. Clemmensen L, W.D., Hastie T: Sparse discriminant analysis. Technometrics. 53, 406–413 (2011).</p>
</div>
<div>
<p>60. H, C., S, K.: Sparse partial least squares regression for simultaneous dimension reduction and variable selection. Journal of the Royal Statistical Society: Series B (Statistical Methodology). 72, 3–25 (2010).</p>
</div>
<div>
<p>61. Efron B, J.I., Hastie T: Least angle regression. The Annals of Statistics. 32, 407–499 (2014).</p>
</div>
<div>
<p>62. H, Z., T, H.: Regularization and variable selection via the elastic net. Journal of the Royal Statistical Society, Series B, 67, 301–320 (2005).</p>
</div>
<div>
<p>63. W, M.: Principal components regression in exploratory sta- tistical research. Journal of the American Statistical Association. 60, 234–246 (1965).</p>
</div>
<div>
<p>64. H, W.: Multivariate analyses. Presented at the (1966).</p>
</div>
<div>
<p>65. H, W.: Systems under indirect observation: Causal- ity, structure, prediction. Presented at the (1982).</p>
</div>
<div>
<p>66. Wold S, W.H., Martens H: The multivariate calibration problem in chemistry solved by the pls method, (1983).</p>
</div>
<div>
<p>67. Wedderburn, R.W.M.: On the existence and uniqueness of the maximum likelihood estimates for certain generalized linear models. Biometrika. 63, 27–32 (1976).</p>
</div>
<div>
<p>68. A, A., J, A.A.: On the existence of the maximum likelihood estimates in logistic regression models. Biometrika. 71, 1–10 (1984).</p>
</div>
<div>
<p>69. L. Meier, S. van de Geer, Buhlmann, P.: The group lasso for logistic regression. J. R. Stat. Soc. Ser. B Stat. Methodol. 70, 53–71 (2008).</p>
</div>
<div>
<p>70. Yeo, G., Burge, C.: Maximum entropy modeling of short sequence motifs with applications to rna splicing signals. Journal of Computational Biology. 475–494 (2004).</p>
</div>
<div>
<p>71. Noah Simon, J.F., Hastie, T.: A blockwise descent algorithm for group-penalized multiresponse and multinomial regression. arXiv:1311.6529.</p>
</div>
<div>
<p>72. al, L.B. et.: Classification and regression trees. Monterey, CA: Wadsworth &amp; Brooks/Cole Advanced Books &amp; Software (1984).</p>
</div>
<div>
<p>73. Hastie T, F.J., Tibshirani R: The elements of statistical learning: Data mining, inference and prediction. Springer (2008).</p>
</div>
<div>
<p>74. Quinlan, J.: Simplifying decision trees. International Journal of Human-Computer Studies. 61, (1999).</p>
</div>
<div>
<p>75. F. Espoito, D.M., Semeraro, G.: A comparative analysis of methods for pruning decision trees. IEEE Transactions on Pattern Analysis and Machine Intelligence. 19, 476–491 (1997).</p>
</div>
<div>
<p>76. Patel, N., Upadhyay, S.: Study of various decision tree pruning methods with their empirical comparison in weka. International Journal of Computer Applications. 60, (2012).</p>
</div>
<div>
<p>77. B. Cestnik, Bratko, I.: Estimating probabilities in tree pruning. EWSL. 138–150 (1991).</p>
</div>
<div>
<p>78. Gareth James, T.H., Daniela Witten, Tibshirani, R.: An introduction to statistical learning. Springer (2015).</p>
</div>
<div>
<p>79. B, E., R, T.: Bootstrap methods for standard errors, con- fidence intervals, and other measures of statistical accuracy. Statistical Science. 54–75 (1986).</p>
</div>
<div>
<p>80. T, D.: An experimental comparison of three methods for constructing ensembles of decision trees: Bagging, boosting, and randomization. Machine Learning. 40, 139–158 (2000).</p>
</div>
<div>
<p>81. T, H.: The random subspace method for constructing decision forests. IEEE Transactions on Pattern Analysis and Machine Intelligence. 13, 340–354 (1998).</p>
</div>
<div>
<p>82. Y, A., D, G.: Shape quantization and recognition with randomized trees. Neural Computation. 9, 1545–1588 (1997).</p>
</div>
<div>
<p>83. L, B.: Randomizing outputs to increase prediction accuracy. Machine Learning. 40, 229–242 (2000).</p>
</div>
<div>
<p>84. L, B.: Random forests. Machine Learning. 45, 5–32 (2001).</p>
</div>
<div>
<p>85. L, V.: A theory of the learnable. Communications of the ACM. 27, 1134–1142 (1984).</p>
</div>
<div>
<p>86. M, K., L, V.: Cryptographic limitations on learning boolean formulae and finite automata, (1989).</p>
</div>
<div>
<p>87. Dudoit S, F.J., T, S.: Comparison of discrimination meth- ods for the classification of tumors using gene expression data. Journal of the American Statistical Association. 97, 77–87 (2002).</p>
</div>
<div>
<p>88. Ben-Dor A, F.N., Bruhn L, Z, Y.: Tissue classification with gene expression profiles. Journal of Computational Biology. 7, 559–583 (2000).</p>
</div>
<div>
<p>89. Varmuza K, H.P., K, F.: Boosting applied to classification of mass spectral data. Journal of Data Science. 1, (2003).</p>
</div>
<div>
<p>90. Bergstra J, E.D., Casagrande N, B, K. ́egl: Aggregate features and adaboost for music classification. Machine Learning. 65, 473–484 (2006).</p>
</div>
<div>
<p>91. YFR, S.: Adaptive game playing using multiplicative weights. Games and Economic Behavior. 29, 79–103 (1999).</p>
</div>
<div>
<p>92. Friedman J, H.T., R, T.: Additive logistic regression: A statistical view of boosting. Annals of Statistics. 38, 337–374 (2000).</p>
</div>
<div>
<p>93. Freund, Y., Schapire, R.: A decision-theoretic generalization of online learning and an application to boosting. Journal of Computer and System Sciences. 55, 119–139 (1997).</p>
</div>
<div>
<p>94. L, B.: Arcing classifiers. The Annals of Statistics. 26, 123–140 (1998).</p>
</div>
<div>
<p>95. Y, F., R, S.: Experiments with a new boosting algorithm, (1996).</p>
</div>
<div>
<p>96. E, B., R, K.: An empirical comparison of voting classifica- tion algorithms: Bagging, boosting, and variants. Machine Learning. 36, 105–142.</p>
</div>
</div>
</div>






            </section>

          </div>
        </div>
      </div>
<a href="section-13.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>


<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/15-cankaowenxian.Rmd",
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
