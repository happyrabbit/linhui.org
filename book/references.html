<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>数据科学家：R语言</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="This is my first book on data science">
  <meta name="generator" content="bookdown 0.1 and GitBook 2.6.7">

  <meta property="og:title" content="数据科学家：R语言" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is my first book on data science" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="数据科学家：R语言" />
  
  <meta name="twitter:description" content="This is my first book on data science" />
  

<meta name="author" content="林荟">

<meta name="date" content="2016-10-19">

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="section-14.html">


<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">数据科学家：R语言</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> 介绍</a></li>
<li class="chapter" data-level="2" data-path="section-2.html"><a href="section-2.html"><i class="fa fa-check"></i><b>2</b> 数据科学</a><ul>
<li class="chapter" data-level="2.1" data-path="section-2.html"><a href="section-2.html#section-2.1"><i class="fa fa-check"></i><b>2.1</b> 什么是数据科学？</a></li>
<li class="chapter" data-level="2.2" data-path="section-2.html"><a href="section-2.html#section-2.2"><i class="fa fa-check"></i><b>2.2</b> 什么是数据科学家？</a></li>
<li class="chapter" data-level="2.3" data-path="section-2.html"><a href="section-2.html#section-2.3"><i class="fa fa-check"></i><b>2.3</b> 数据科学家需要的技能</a></li>
<li class="chapter" data-level="2.4" data-path="section-2.html"><a href="section-2.html#section-2.4"><i class="fa fa-check"></i><b>2.4</b> 数据科学可以解决什么问题？</a><ul>
<li class="chapter" data-level="2.4.1" data-path="section-2.html"><a href="section-2.html#section-2.4.1"><i class="fa fa-check"></i><b>2.4.1</b> 前提要求</a></li>
<li class="chapter" data-level="2.4.2" data-path="section-2.html"><a href="section-2.html#section-2.4.2"><i class="fa fa-check"></i><b>2.4.2</b> 问题种类</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="section-3.html"><a href="section-3.html"><i class="fa fa-check"></i><b>3</b> 数据集模拟和背景介绍</a><ul>
<li class="chapter" data-level="3.1" data-path="section-3.html"><a href="section-3.html#section-3.1"><i class="fa fa-check"></i><b>3.1</b> 服装消费者数据</a></li>
<li class="chapter" data-level="3.2" data-path="section-3.html"><a href="section-3.html#section-3.2"><i class="fa fa-check"></i><b>3.2</b> 航空公司满意度调查</a></li>
<li class="chapter" data-level="3.3" data-path="section-3.html"><a href="section-3.html#section-3.3"><i class="fa fa-check"></i><b>3.3</b> 生猪疫情风险预测数据</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="section-4.html"><a href="section-4.html"><i class="fa fa-check"></i><b>4</b> 数据分析一般流程</a><ul>
<li class="chapter" data-level="4.1" data-path="section-4.html"><a href="section-4.html#section-4.1"><i class="fa fa-check"></i><b>4.1</b> 问题到数据</a></li>
<li class="chapter" data-level="4.2" data-path="section-4.html"><a href="section-4.html#section-4.2"><i class="fa fa-check"></i><b>4.2</b> 数据到信息</a></li>
<li class="chapter" data-level="4.3" data-path="section-4.html"><a href="section-4.html#section-4.3"><i class="fa fa-check"></i><b>4.3</b> 信息到行动</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="section-5.html"><a href="section-5.html"><i class="fa fa-check"></i><b>5</b> 数据预处理</a><ul>
<li class="chapter" data-level="5.1" data-path="section-5.html"><a href="section-5.html#section-5.1"><i class="fa fa-check"></i><b>5.1</b> 介绍</a></li>
<li class="chapter" data-level="5.2" data-path="section-5.html"><a href="section-5.html#section-5.2"><i class="fa fa-check"></i><b>5.2</b> 数据清理</a></li>
<li class="chapter" data-level="5.3" data-path="section-5.html"><a href="section-5.html#section-5.3"><i class="fa fa-check"></i><b>5.3</b> 缺失值填补</a><ul>
<li class="chapter" data-level="5.3.1" data-path="section-5.html"><a href="section-5.html#section-5.3.1"><i class="fa fa-check"></i><b>5.3.1</b> 中位数或众数填补</a></li>
<li class="chapter" data-level="5.3.2" data-path="section-5.html"><a href="section-5.html#k-"><i class="fa fa-check"></i><b>5.3.2</b> K-近邻填补</a></li>
<li class="chapter" data-level="5.3.3" data-path="section-5.html"><a href="section-5.html#section-5.3.3"><i class="fa fa-check"></i><b>5.3.3</b> 袋状树填补</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="section-5.html"><a href="section-5.html#section-5.4"><i class="fa fa-check"></i><b>5.4</b> 中心化和标量化</a></li>
<li class="chapter" data-level="5.5" data-path="section-5.html"><a href="section-5.html#section-5.5"><i class="fa fa-check"></i><b>5.5</b> 有偏分布</a></li>
<li class="chapter" data-level="5.6" data-path="section-5.html"><a href="section-5.html#section-5.6"><i class="fa fa-check"></i><b>5.6</b> 处理离群点</a></li>
<li class="chapter" data-level="5.7" data-path="section-5.html"><a href="section-5.html#section-5.7"><i class="fa fa-check"></i><b>5.7</b> 共线性</a></li>
<li class="chapter" data-level="5.8" data-path="section-5.html"><a href="section-5.html#section-5.8"><i class="fa fa-check"></i><b>5.8</b> 稀疏变量</a></li>
<li class="chapter" data-level="5.9" data-path="section-5.html"><a href="section-5.html#section-5.9"><i class="fa fa-check"></i><b>5.9</b> 编码名义变量</a></li>
<li class="chapter" data-level="5.10" data-path="section-5.html"><a href="section-5.html#section-5.10"><i class="fa fa-check"></i><b>5.10</b> 本章总结</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="section-6.html"><a href="section-6.html"><i class="fa fa-check"></i><b>6</b> 数据操作</a><ul>
<li class="chapter" data-level="6.1" data-path="section-6.html"><a href="section-6.html#section-6.1"><i class="fa fa-check"></i><b>6.1</b> 数据读写</a><ul>
<li class="chapter" data-level="6.1.1" data-path="section-6.html"><a href="section-6.html#tibble"><i class="fa fa-check"></i><b>6.1.1</b> 取代传统数据框的<code>tibble</code>对象</a></li>
<li class="chapter" data-level="6.1.2" data-path="section-6.html"><a href="section-6.html#readr"><i class="fa fa-check"></i><b>6.1.2</b> 高效数据读写：<code>readr</code>包</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="section-6.html"><a href="section-6.html#section-6.2"><i class="fa fa-check"></i><b>6.2</b> 数据整合</a><ul>
<li class="chapter" data-level="6.2.1" data-path="section-6.html"><a href="section-6.html#baseapply"><i class="fa fa-check"></i><b>6.2.1</b> base包：apply()</a></li>
<li class="chapter" data-level="6.2.2" data-path="section-6.html"><a href="section-6.html#plyrddply"><i class="fa fa-check"></i><b>6.2.2</b> plyr包：ddply()函数</a></li>
<li class="chapter" data-level="6.2.3" data-path="section-6.html"><a href="section-6.html#dplyr"><i class="fa fa-check"></i><b>6.2.3</b> dplyr包</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="section-6.html"><a href="section-6.html#section-6.3"><i class="fa fa-check"></i><b>6.3</b> 数据整形</a><ul>
<li class="chapter" data-level="6.3.1" data-path="section-6.html"><a href="section-6.html#reshape2"><i class="fa fa-check"></i><b>6.3.1</b> <code>reshape2</code>包</a></li>
<li class="chapter" data-level="6.3.2" data-path="section-6.html"><a href="section-6.html#tidyr"><i class="fa fa-check"></i><b>6.3.2</b> <code>tidyr</code>包</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="section-6.html"><a href="section-6.html#-1"><i class="fa fa-check"></i><b>6.4</b> 本章总结</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="section-7.html"><a href="section-7.html"><i class="fa fa-check"></i><b>7</b> 基础建模技术</a><ul>
<li class="chapter" data-level="7.1" data-path="section-7.html"><a href="section-7.html#section-7.1"><i class="fa fa-check"></i><b>7.1</b> 有监督和无监督</a></li>
<li class="chapter" data-level="7.2" data-path="section-7.html"><a href="section-7.html#section-7.2"><i class="fa fa-check"></i><b>7.2</b> 误差及其来源</a><ul>
<li class="chapter" data-level="7.2.1" data-path="section-7.html"><a href="section-7.html#section-7.2.1"><i class="fa fa-check"></i><b>7.2.1</b> 系统误差和随机误差</a></li>
<li class="chapter" data-level="7.2.2" data-path="section-7.html"><a href="section-7.html#section-7.2.2"><i class="fa fa-check"></i><b>7.2.2</b> 应变量误差</a></li>
<li class="chapter" data-level="7.2.3" data-path="section-7.html"><a href="section-7.html#section-7.2.3"><i class="fa fa-check"></i><b>7.2.3</b> 自变量误差</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="section-7.html"><a href="section-7.html#section-7.3"><i class="fa fa-check"></i><b>7.3</b> 数据划分和再抽样</a><ul>
<li class="chapter" data-level="7.3.1" data-path="section-7.html"><a href="section-7.html#section-7.3.1"><i class="fa fa-check"></i><b>7.3.1</b> 划分训练集和测试集</a></li>
<li class="chapter" data-level="7.3.2" data-path="section-7.html"><a href="section-7.html#section-7.3.2"><i class="fa fa-check"></i><b>7.3.2</b> 重抽样</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="section-7.html"><a href="section-7.html#-2"><i class="fa fa-check"></i><b>7.4</b> 本章总结</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="section-8.html"><a href="section-8.html"><i class="fa fa-check"></i><b>8</b> 模型评估度量</a><ul>
<li class="chapter" data-level="8.1" data-path="section-8.html"><a href="section-8.html#section-8.1"><i class="fa fa-check"></i><b>8.1</b> 回归模型评估度量</a></li>
<li class="chapter" data-level="8.2" data-path="section-8.html"><a href="section-8.html#section-8.2"><i class="fa fa-check"></i><b>8.2</b> 分类模型评估度量</a><ul>
<li class="chapter" data-level="8.2.1" data-path="section-8.html"><a href="section-8.html#kappa"><i class="fa fa-check"></i><b>8.2.1</b> Kappa统计量</a></li>
<li class="chapter" data-level="8.2.2" data-path="section-8.html"><a href="section-8.html#roc"><i class="fa fa-check"></i><b>8.2.2</b> ROC曲线</a></li>
<li class="chapter" data-level="8.2.3" data-path="section-8.html"><a href="section-8.html#section-8.2.3"><i class="fa fa-check"></i><b>8.2.3</b> 提升图</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="section-8.html"><a href="section-8.html#-3"><i class="fa fa-check"></i><b>8.3</b> 本章总结</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="section-9.html"><a href="section-9.html"><i class="fa fa-check"></i><b>9</b> 特征工程</a><ul>
<li class="chapter" data-level="9.1" data-path="section-9.html"><a href="section-9.html#section-9.1"><i class="fa fa-check"></i><b>9.1</b> 特征构建</a></li>
<li class="chapter" data-level="9.2" data-path="section-9.html"><a href="section-9.html#section-9.2"><i class="fa fa-check"></i><b>9.2</b> 特征提取</a><ul>
<li class="chapter" data-level="9.2.1" data-path="section-9.html"><a href="section-9.html#section-9.2.1"><i class="fa fa-check"></i><b>9.2.1</b> 初步探索数据</a></li>
<li class="chapter" data-level="9.2.2" data-path="section-9.html"><a href="section-9.html#section-9.2.2"><i class="fa fa-check"></i><b>9.2.2</b> 主成分分析</a></li>
<li class="chapter" data-level="9.2.3" data-path="section-9.html"><a href="section-9.html#section-9.2.3"><i class="fa fa-check"></i><b>9.2.3</b> 探索性因子分析</a></li>
<li class="chapter" data-level="9.2.4" data-path="section-9.html"><a href="section-9.html#section-9.2.4"><i class="fa fa-check"></i><b>9.2.4</b> 高维标度化</a></li>
<li class="chapter" data-level="9.2.5" data-path="section-9.html"><a href="section-9.html#section-9.2.5"><i class="fa fa-check"></i><b>9.2.5</b> 知识扩展</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="section-9.html"><a href="section-9.html#section-9.3"><i class="fa fa-check"></i><b>9.3</b> 特征选择</a><ul>
<li class="chapter" data-level="9.3.1" data-path="section-9.html"><a href="section-9.html#section-9.3.1"><i class="fa fa-check"></i><b>9.3.1</b> 过滤法</a></li>
<li class="chapter" data-level="9.3.2" data-path="section-9.html"><a href="section-9.html#section-9.3.2"><i class="fa fa-check"></i><b>9.3.2</b> 绕封法</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="section-10.html"><a href="section-10.html"><i class="fa fa-check"></i><b>10</b> 线性回归极其衍生</a><ul>
<li class="chapter" data-level="10.1" data-path="section-10.html"><a href="section-10.html#section-10.1"><i class="fa fa-check"></i><b>10.1</b> 普通线性回归</a><ul>
<li class="chapter" data-level="10.1.1" data-path="section-10.html"><a href="section-10.html#section-10.1.1"><i class="fa fa-check"></i><b>10.1.1</b> 最小二乘线性模型</a></li>
<li class="chapter" data-level="10.1.2" data-path="section-10.html"><a href="section-10.html#section-10.1.2"><i class="fa fa-check"></i><b>10.1.2</b> 回归诊断</a></li>
<li class="chapter" data-level="10.1.3" data-path="section-10.html"><a href="section-10.html#section-10.1.3"><i class="fa fa-check"></i><b>10.1.3</b> 离群点，高杠杆点和强影响点</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="section-10.html"><a href="section-10.html#section-10.2"><i class="fa fa-check"></i><b>10.2</b> 收缩方法</a><ul>
<li class="chapter" data-level="10.2.1" data-path="section-10.html"><a href="section-10.html#section-10.2.1"><i class="fa fa-check"></i><b>10.2.1</b> 岭回归</a></li>
<li class="chapter" data-level="10.2.2" data-path="section-10.html"><a href="section-10.html#lasso"><i class="fa fa-check"></i><b>10.2.2</b> Lasso</a></li>
<li class="chapter" data-level="10.2.3" data-path="section-10.html"><a href="section-10.html#section-10.2.3"><i class="fa fa-check"></i><b>10.2.3</b> 弹性网络</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="section-10.html"><a href="section-10.html#lasso"><i class="fa fa-check"></i><b>10.3</b> 知识扩展：Lasso的变量选择功能</a></li>
<li class="chapter" data-level="10.4" data-path="section-10.html"><a href="section-10.html#section-10.4"><i class="fa fa-check"></i><b>10.4</b> 主成分和偏最小二乘回归</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="glmnet.html"><a href="glmnet.html"><i class="fa fa-check"></i><b>11</b> 广义线性模型压缩方法及<code>glmnet</code>包</a><ul>
<li class="chapter" data-level="11.1" data-path="glmnet.html"><a href="glmnet.html#glmnet"><i class="fa fa-check"></i><b>11.1</b> 初识<code>glmnet</code></a></li>
<li class="chapter" data-level="11.2" data-path="glmnet.html"><a href="glmnet.html#section-11.2"><i class="fa fa-check"></i><b>11.2</b> 收缩线性回归</a></li>
<li class="chapter" data-level="11.3" data-path="glmnet.html"><a href="glmnet.html#section-11.3"><i class="fa fa-check"></i><b>11.3</b> 逻辑回归</a><ul>
<li class="chapter" data-level="11.3.1" data-path="glmnet.html"><a href="glmnet.html#section-11.3.1"><i class="fa fa-check"></i><b>11.3.1</b> 普通逻辑回归</a></li>
<li class="chapter" data-level="11.3.2" data-path="glmnet.html"><a href="glmnet.html#section-11.3.2"><i class="fa fa-check"></i><b>11.3.2</b> 收缩逻辑回归</a></li>
<li class="chapter" data-level="11.3.3" data-path="section-10.html"><a href="section-10.html#lasso"><i class="fa fa-check"></i><b>11.3.3</b> 知识扩展：群组lasso逻辑回归</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="section-12.html"><a href="section-12.html"><i class="fa fa-check"></i><b>12</b> 聚类判别分析</a><ul>
<li class="chapter" data-level="12.1" data-path="section-12.html"><a href="section-12.html#section-12.1"><i class="fa fa-check"></i><b>12.1</b> 聚类分析</a></li>
<li class="chapter" data-level="12.2" data-path="section-12.html"><a href="section-12.html#section-12.2"><i class="fa fa-check"></i><b>12.2</b> 判别分析</a><ul>
<li class="chapter" data-level="12.2.1" data-path="section-12.html"><a href="section-12.html#section-12.2.1"><i class="fa fa-check"></i><b>12.2.1</b> 线性判别分析</a></li>
<li class="chapter" data-level="12.2.2" data-path="section-12.html"><a href="section-12.html#section-12.2.2"><i class="fa fa-check"></i><b>12.2.2</b> 最小二乘判别分析</a></li>
<li class="chapter" data-level="12.2.3" data-path="section-12.html"><a href="section-12.html#section-12.2.3"><i class="fa fa-check"></i><b>12.2.3</b> 朴素贝叶斯</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="section-12.html"><a href="section-12.html#section-12.3"><i class="fa fa-check"></i><b>12.3</b> 案例：客户分组</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="section-13.html"><a href="section-13.html"><i class="fa fa-check"></i><b>13</b> 树模型</a><ul>
<li class="chapter" data-level="13.1" data-path="section-13.html"><a href="section-13.html#section-13.1"><i class="fa fa-check"></i><b>13.1</b> 分裂准则</a></li>
<li class="chapter" data-level="13.2" data-path="section-13.html"><a href="section-13.html#section-13.2"><i class="fa fa-check"></i><b>13.2</b> 树模型的参数</a></li>
<li class="chapter" data-level="13.3" data-path="section-13.html"><a href="section-13.html#section-13.3"><i class="fa fa-check"></i><b>13.3</b> 装袋树</a></li>
<li class="chapter" data-level="13.4" data-path="section-13.html"><a href="section-13.html#section-13.4"><i class="fa fa-check"></i><b>13.4</b> 随机森林</a></li>
<li class="chapter" data-level="13.5" data-path="section-13.html"><a href="section-13.html#section-13.5"><i class="fa fa-check"></i><b>13.5</b> 其它树话题</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="section-14.html"><a href="section-14.html"><i class="fa fa-check"></i><b>14</b> 深度学习</a><ul>
<li class="chapter" data-level="14.1" data-path="section-6.html"><a href="section-6.html#-1"><i class="fa fa-check"></i><b>14.1</b> 介绍</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>15</b> References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">数据科学家：R语言</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="references" class="section level1">
<h1><span class="header-section-number">第15章</span> References</h1>

<div id="refs" class="references">
<div>
<p>A, Agresti. 2002. <em>Categorical Data Analysis</em>. <em>Wiley–Interscience</em>. Wiley.</p>
</div>
<div>
<p>A, Albert, and Anderson A. J. 1984. “On the Existence of the Maximum Likelihood Estimates in Logistic Regression Models.” <em>Biometrika</em> 71 (1): 1–10.</p>
</div>
<div>
<p>A, Hoerl. 1970. “Ridge Regression: Biased Estimation for Nonorthogonal Problems.” <em>Technometrics</em> 12 (1): 55–67.</p>
</div>
<div>
<p>al, Leo Breiman et. 1984. <em>Classification and Regression Trees.</em> ISBN 978-0-412-04841-8. Monterey, CA: Wadsworth &amp; Brooks/Cole Advanced Books &amp; Software.</p>
</div>
<div>
<p>Berkeley. 2015. “What Is Data Science?” <a href="https://datascience.berkeley.edu/about/what-is-data-science/" class="uri">https://datascience.berkeley.edu/about/what-is-data-science/</a>.</p>
</div>
<div>
<p>Bland J, Altman D. 1995. “Statistics Notes: Multiple Significance Tests: The Bonferroni Method.” <em>British Medical Journal</em> 310 (6973): 170–70.</p>
</div>
<div>
<p>Borg, Groenen, I. 2012. <em>Applied Multidimensional Scaling</em>. Springer.</p>
</div>
<div>
<p>Borg, I., and P. J. Groenen. 2005. <em>Modern Multidimensional Scaling: Theory and Applications</em>. Springer.</p>
</div>
<div>
<p>Box G, Cox D. 1964. “An Analysis of Transformations.” <em>Journal of the Royal Statistical Society</em> Series B (Methodological): 211–52.</p>
</div>
<div>
<p>Clemmensen L, Witten D, Hastie T. 2011. “Sparse Discriminant Analysis.” <em>Technometrics</em> 53 (4): 406–13.</p>
</div>
<div>
<p>Cleveland W, Devlin S. 1988. “Locally Weighted Regression: An Approach to Regression Analysis by Local Fitting.” <em>Journal of the American Statistical Association</em>, 596–610.</p>
</div>
<div>
<p>Cook, R. D., and S Weisberg. 1982. <em>Residuals and Influence in Regression</em>. Chapman; Hall.</p>
</div>
<div>
<p>D, McClish. 1989. “Analyzing a Portion of the Roc Curve.” <em>Medical Decision Making</em> 9: 190–95.</p>
</div>
<div>
<p>Donoho, David. 2015. “50 Years of Data Science.”</p>
</div>
<div>
<p>E, Venkatraman. 2000. “A Permutation Test to Compare Receiver Operating Characteristic Curves.” <em>Biometrics</em> 56 (4): 1134–8.</p>
</div>
<div>
<p>E.R. DeLong, D.L. Clarke-Pearson, D.M. DeLong. 1988. “Comparing the Areas Under Two or More Correlated Receiver Operating Characteristics Curves: A Nonparametric Approach.” <em>Biometrics</em> 44: 837–45.</p>
</div>
<div>
<p>Efron, B. 1983. “Estimating the Error Rate of a Prediction Rule: Improvement on Cross-Validation.” <em>Journal of the American Statistical Association</em>, 316–31.</p>
</div>
<div>
<p>Efron B, Johnstone I, Hastie T. 2014. “Least Angle Regression.” <em>The Annals of Statistics</em> 32 (2): 407–99.</p>
</div>
<div>
<p>Efron, B, and R Tibshirani. 1986. “Bootstrap Methods for Standard Errors, Confidence Intervals, and Other Measures of Statistical Accuracy.” <em>Statistical Science</em>, 54–75.</p>
</div>
<div>
<p>———. 1997. “Improvements on Cross-Validation: The 632+ Bootstrap Method.” <em>Journal of the American Statistical Association</em> 92 (438): 548–60.</p>
</div>
<div>
<p>Fox, J. 2008. “Applied Regression Analysis and Generalized Linear Models.” <em>Sage</em>.</p>
</div>
<div>
<p>Fox, John, and Sanford Weisberg. 2011. “An R Companion to Applied Regression.” <em>Sage</em>.</p>
</div>
<div>
<p>Geladi P, Kowalski B. 1986. “Partial Least-Squares Regression: A Tutorial.” <em>Analytica Chimica Acta</em>, no. 185: 1–17.</p>
</div>
<div>
<p>Gower, J. C. 1971. “A General Coefficient of Similarity and Some of Its Properties.” <em>Biometrics</em> 27 (857–874).</p>
</div>
<div>
<p>Guyon I, Elisseeff A. 2003. “An Introduction to Variable and Feature Selec- Tion.” <em>The Journal of Machine Learning Research</em> 3: 1157–82.</p>
</div>
<div>
<p>H, Chun, and Kele S. 2010. “Sparse Partial Least Squares Regression for Simultaneous Dimension Reduction and Variable Selection.” <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em> 72 (1): 3–25.</p>
</div>
<div>
<p>H, Wold. 1966a. <em>Estimation of Principal Components and Related Models by Iterative Least Squares.</em> pp. 391-420. Academic Press, New York.: In P Krishnaiah (ed.), Journal of Multivariate Analysis ,</p>
</div>
<div>
<p>———. 1966b. “Multivariate Analyses.” In, In P Krishnaiah (ed.), 391–420. New York: Academic Press.</p>
</div>
<div>
<p>———. 1982a. <em>Soft Modeling: The Basic Design and Some Extensions</em>. pt. 2, pp. 1–54. North–Holland, Amsterdam.: In K Joreskog, H Wold (eds.), “Systems Under Indirect Observation: Causality, Structure, Prediction,”</p>
</div>
<div>
<p>———. 1982b. “Systems Under Indirect Observation: Causal- Ity, Structure, Prediction.” In, In K Joreskog, H Wold (eds.), 1–54. North–Holland, Amsterdam.</p>
</div>
<div>
<p>H, Zou, and Hastie T. 2005. “Regularization and Variable Selection via the Elastic Net.” <em>Journal of the Royal Statistical Society, Series B,</em> 67 (2): 301–20.</p>
</div>
<div>
<p>Hall P, Fan Y, Hyndman R. 2004. “Nonparametric Confidence Intervals for Receiver Operating Characteristic Curves.” <em>Biometrika</em> 91: 743–50.</p>
</div>
<div>
<p>Hand D, Till R. 2001. “A Simple Generalisation of the Area Under the Roc Curve for Multiple Class Classification Problems.” <em>Machine Learning</em> 45 (2): 171–86.</p>
</div>
<div>
<p>Hastie T, Friedman J, Tibshirani R. 2008. <em>The Elements of Statistical Learning: Data Mining, Inference and Prediction</em>. 2nd ed. Springer.</p>
</div>
<div>
<p>Hyndman, R.J., and G. Athanasopoulos. 2013. <em>Forecasting: Principles and Practice</em>. Vol. Section 2/5. OTect: Melbourne, Australia.</p>
</div>
<div>
<p>I, Kononenko. 1994. <em>Estimating Attributes: Analysis and Extensions of Relief.” in F Bergadano, L de Raedt (Eds.), “Machine Learning: ECML–94&quot;</em>. Vol. 784. 171–182. Springer Berlin / Heidelberg.</p>
</div>
<div>
<p>Iglewicz, Boris, and David Hoaglin. 1993. “How to Detect and Handle Outliers.” <em>The ASQC Basic References in Quality Control: Statistical Techniques</em> 16.</p>
</div>
<div>
<p>J, Bland, and Altman D. 2000. “The Odds Ratio.” <em>British Medical Journal</em> 320 (7247): 1468.</p>
</div>
<div>
<p>J, Cohen. 1960. “A Coefficient of Agreement for Nominal Data.” <em>Educational and Psychological Measurement</em> 20: 37–46.</p>
</div>
<div>
<p>J, Hanley, and McNeil B. 1982. “The Meaning and Use of the Area Under a Receiver Operating (Roc) Curvel Characteristic.” <em>Radiology</em> 143 (1).</p>
</div>
<div>
<p>Jackson, Edward. 2003. <em>A User’s Guide to Principal Components</em>. John Wiley &amp; Sons.</p>
</div>
<div>
<p>Jolliffe, I.T. 2002. <em>Principla Component Analysis</em>. 2nd ed. New York: Springer.</p>
</div>
<div>
<p>K, Kira, and Rendell L. 1992. “The Feature Selection Problem: Traditional Methods and a New Algorithm.” <em>Proceedings of the National Conference on Artificial Intelligence</em>, 129–29.</p>
</div>
<div>
<p>Kaiser, Henry F. 1958. “The Varimax Criterion for Analytic Rotation in Factor Analysis.” <em>Psychometrika</em> 23 (3).</p>
</div>
<div>
<p>L, Breiman. 1966a. “Bagging Predictors.” <em>Machine Learning</em> 24 (2): 123–40.</p>
</div>
<div>
<p>Lachiche N, Flach P. 2003. “Improving Accuracy and Cost of Two–Class and Multi–Class Probabilistic Classifiers Using Roc Curves.” <em>In “Proceed- Ings of the Twentieth International Conference on Machine Learning</em> 20 (416–424).</p>
</div>
<div>
<p>Landis JR, Koch GG. 1977. “The Measurement of Observer Agreement for Categorical Data.” <em>Biometrics</em> 33: 159–74.</p>
</div>
<div>
<p>Li J, Fine JP. 2008. “ROC Analysis with Multiple Classes and Multiple Tests: Methodology and Its Application in Microarray Studies.” <em>Biostatistics</em> 9 (3): 566–76.</p>
</div>
<div>
<p>M, Robnik-Sikonja, and Kononenko I. 1997. “An Adaptation of Relief for Attribute Estimation in Regression.” <em>Proceedings of the Fourteenth International Conference on Machine Learning</em>, 296–304.</p>
</div>
<div>
<p>Mardia, K.V. 1978. “Some Properties of Classical Multidimensional Scaling.” <em>Communications on Statistics – Theory and Methods</em> A7 (1233–1241).</p>
</div>
<div>
<p>Max Kuhn, Kjell Johnston. 2013. <em>Applied Predictive Modeling</em>. Springer.</p>
</div>
<div>
<p>Mulaik, S.A. 2009. <em>Foundations of Factor Analysis</em>. 2ND ed. Boca Raton: Chapman&amp;Hall/CRC.</p>
</div>
<div>
<p>Nolan, Deborah, and Duncan Temple Lang. 2014. <em>XML and Web Technologies for Data Sciences with R</em>. Springer.</p>
</div>
<div>
<p>Oded Netzer, Jacob Goldenberg, Ronen Feldman. 2012. “Mine Your Own Business: Market-Structure Surveillance Through Text Mining.” <em>Marketing Science</em> 31 (3): 521–43.</p>
</div>
<div>
<p>Provost F, Kohavi R, Fawcett T. 1998. “The Case Against Accuracy Esti- Mation for Comparing Induction Algorithms.” <em>Proceedings of the Fifteenth International Conference on Machine Learning</em>, 445–53.</p>
</div>
<div>
<p>R, Tibshirani. 1996. “Regression Shrinkage and Selection via the Lasso.” <em>Journal of the Royal Statistical Society Series B (Methodological)</em> 58 (1): 267–88.</p>
</div>
<div>
<p>Robnik-Sˇikonja, Marko, and Igor Kononenko. 2003. “Theoretical and Empirical Analysis of Relieff and Rrelieff.” <em>Machine Learning Journal</em> 53 (23-69).</p>
</div>
<div>
<p>Ronald L. Wassersteina, Nicole A. Lazara. 2016. “Position on P-Values: Context, Process, and Purpose.”</p>
</div>
<div>
<p>Saar-Tsechansky M, Provost F. 2007b. “Handling Missing Values When Applying Classification Models.” <em>Journal of Machine Learning Research</em> 8: 1625–57.</p>
</div>
<div>
<p>Serneels S, Espen PV, Nolf ED. 2006. “Spatial Sign Pre-Processing: A Simple Way to Impart Moderate Robustness to Multivariate Estimators.” <em>Journal of Chemical Information and Modeling</em> 46 (3): 1402–9.</p>
</div>
<div>
<p>Spearman, C. 1904. “‘General Intelligence,’ Objectively Determined and Measured.” <em>The American Journal of Psychology</em> 15 (2): 201–92.</p>
</div>
<div>
<p>T, Fawcett. 2006. “An Introduction to Roc Analysis.” <em>Pattern Recognition Letters</em> 27 (8): 861–74.</p>
</div>
<div>
<p>Ton de Waal, Sander Scholtus, Jeroen Pannekoek. 2011. <em>Handbook of Statistical Data Editing and Imputation</em>. John Wiley &amp; Sons.</p>
</div>
<div>
<p>Tukey, John. 1962. “The Future of Data Analysis.” <em>The Annals of Mathematical Statistics</em>, 1–67.</p>
</div>
<div>
<p>W, Massy. 1965. “Principal Components Regression in Exploratory Sta- Tistical Research.” <em>Journal of the American Statistical Association</em> 60: 234–46.</p>
</div>
<div>
<p>Wedderburn, R. W. M. 1976. “On the Existence and Uniqueness of the Maximum Likelihood Estimates for Certain Generalized Linear Models.” <em>Biometrika</em> 63: 27–32.</p>
</div>
<div>
<p>Weisberg, S. 2014. <em>Applied Linear Regression</em>. <em>Applied Linear Regression</em>. Fourth Edition. Wiley.</p>
</div>
<div>
<p>Willett, Peter. 2004. “Dissimilarity-Based Algorithms for Selecting Structurally Diverse Sets of Compounds.” <em>Journal of Computational Biology</em> 6(3-4) (doi:10.1089/106652799318382): 447–57.</p>
</div>
<div>
<p>Williams, D. A. 1987. “Generalized Linear Model Diagnostics Using the Deviance and Single Case Deletions.” <em>Applied Statistics</em> 36: 181–191.</p>
</div>
<div>
<p>Wold S, Wold H, Martens H. 1983. “The Multivariate Calibration Problem in Chemistry Solved by the Pls Method.” Springer–Verlag, Heidelberg.</p>
</div>
</div>
</div>






            </section>

          </div>
        </div>
      </div>
<a href="section-14.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>


<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/15-cankaowenxian.Rmd",
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
