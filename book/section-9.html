<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>数据科学家：R语言</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="This is my first book on data science">
  <meta name="generator" content="bookdown 0.1 and GitBook 2.6.7">

  <meta property="og:title" content="数据科学家：R语言" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is my first book on data science" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="数据科学家：R语言" />
  
  <meta name="twitter:description" content="This is my first book on data science" />
  

<meta name="author" content="林荟">

<meta name="date" content="2016-07-16">

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="section-8.html">
<link rel="next" href="section-10.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">数据科学家：R语言</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> 介绍</a></li>
<li class="chapter" data-level="2" data-path="section-2.html"><a href="section-2.html"><i class="fa fa-check"></i><b>2</b> 数据科学</a><ul>
<li class="chapter" data-level="2.1" data-path="section-2.html"><a href="section-2.html#section-2.1"><i class="fa fa-check"></i><b>2.1</b> 什么是数据科学？</a></li>
<li class="chapter" data-level="2.2" data-path="section-2.html"><a href="section-2.html#section-2.2"><i class="fa fa-check"></i><b>2.2</b> 什么是数据科学家？</a></li>
<li class="chapter" data-level="2.3" data-path="section-2.html"><a href="section-2.html#section-2.3"><i class="fa fa-check"></i><b>2.3</b> 数据科学家需要的技能</a></li>
<li class="chapter" data-level="2.4" data-path="section-2.html"><a href="section-2.html#section-2.4"><i class="fa fa-check"></i><b>2.4</b> 数据科学可以解决什么问题？</a><ul>
<li class="chapter" data-level="2.4.1" data-path="section-2.html"><a href="section-2.html#section-2.4.1"><i class="fa fa-check"></i><b>2.4.1</b> 前提要求</a></li>
<li class="chapter" data-level="2.4.2" data-path="section-2.html"><a href="section-2.html#section-2.4.2"><i class="fa fa-check"></i><b>2.4.2</b> 问题种类</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="section-3.html"><a href="section-3.html"><i class="fa fa-check"></i><b>3</b> 数据集模拟和背景介绍</a><ul>
<li class="chapter" data-level="3.1" data-path="section-3.html"><a href="section-3.html#section-3.1"><i class="fa fa-check"></i><b>3.1</b> 服装消费者数据</a></li>
<li class="chapter" data-level="3.2" data-path="section-3.html"><a href="section-3.html#section-3.2"><i class="fa fa-check"></i><b>3.2</b> 航空公司满意度调查</a></li>
<li class="chapter" data-level="3.3" data-path="section-3.html"><a href="section-3.html#section-3.3"><i class="fa fa-check"></i><b>3.3</b> 生猪疫情风险预测数据</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="section-4.html"><a href="section-4.html"><i class="fa fa-check"></i><b>4</b> 数据分析一般流程</a><ul>
<li class="chapter" data-level="4.1" data-path="section-4.html"><a href="section-4.html#section-4.1"><i class="fa fa-check"></i><b>4.1</b> 问题到数据</a></li>
<li class="chapter" data-level="4.2" data-path="section-4.html"><a href="section-4.html#section-4.2"><i class="fa fa-check"></i><b>4.2</b> 数据到信息</a></li>
<li class="chapter" data-level="4.3" data-path="section-4.html"><a href="section-4.html#section-4.3"><i class="fa fa-check"></i><b>4.3</b> 信息到行动</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="section-5.html"><a href="section-5.html"><i class="fa fa-check"></i><b>5</b> 数据预处理</a><ul>
<li class="chapter" data-level="5.1" data-path="section-5.html"><a href="section-5.html#section-5.1"><i class="fa fa-check"></i><b>5.1</b> 介绍</a></li>
<li class="chapter" data-level="5.2" data-path="section-5.html"><a href="section-5.html#section-5.2"><i class="fa fa-check"></i><b>5.2</b> 数据清理</a></li>
<li class="chapter" data-level="5.3" data-path="section-5.html"><a href="section-5.html#section-5.3"><i class="fa fa-check"></i><b>5.3</b> 缺失值填补</a><ul>
<li class="chapter" data-level="5.3.1" data-path="section-5.html"><a href="section-5.html#section-5.3.1"><i class="fa fa-check"></i><b>5.3.1</b> 中位数或众数填补</a></li>
<li class="chapter" data-level="5.3.2" data-path="section-5.html"><a href="section-5.html#k-"><i class="fa fa-check"></i><b>5.3.2</b> K-近邻填补</a></li>
<li class="chapter" data-level="5.3.3" data-path="section-5.html"><a href="section-5.html#section-5.3.3"><i class="fa fa-check"></i><b>5.3.3</b> 袋状树填补</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="section-5.html"><a href="section-5.html#section-5.4"><i class="fa fa-check"></i><b>5.4</b> 中心化和标量化</a></li>
<li class="chapter" data-level="5.5" data-path="section-5.html"><a href="section-5.html#section-5.5"><i class="fa fa-check"></i><b>5.5</b> 有偏分布</a></li>
<li class="chapter" data-level="5.6" data-path="section-5.html"><a href="section-5.html#section-5.6"><i class="fa fa-check"></i><b>5.6</b> 处理离群点</a></li>
<li class="chapter" data-level="5.7" data-path="section-5.html"><a href="section-5.html#section-5.7"><i class="fa fa-check"></i><b>5.7</b> 共线性</a></li>
<li class="chapter" data-level="5.8" data-path="section-5.html"><a href="section-5.html#section-5.8"><i class="fa fa-check"></i><b>5.8</b> 稀疏变量</a></li>
<li class="chapter" data-level="5.9" data-path="section-5.html"><a href="section-5.html#section-5.9"><i class="fa fa-check"></i><b>5.9</b> 编码名义变量</a></li>
<li class="chapter" data-level="5.10" data-path="section-5.html"><a href="section-5.html#section-5.10"><i class="fa fa-check"></i><b>5.10</b> 本章总结</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="section-6.html"><a href="section-6.html"><i class="fa fa-check"></i><b>6</b> 数据整合和整形</a><ul>
<li class="chapter" data-level="6.1" data-path="section-6.html"><a href="section-6.html#readr"><i class="fa fa-check"></i><b>6.1</b> 高效数据读写：<code>readr</code>包</a></li>
<li class="chapter" data-level="6.2" data-path="section-6.html"><a href="section-6.html#section-6.2"><i class="fa fa-check"></i><b>6.2</b> 数据整合</a><ul>
<li class="chapter" data-level="6.2.1" data-path="section-6.html"><a href="section-6.html#baseapply"><i class="fa fa-check"></i><b>6.2.1</b> base包：apply()</a></li>
<li class="chapter" data-level="6.2.2" data-path="section-6.html"><a href="section-6.html#plyrddply"><i class="fa fa-check"></i><b>6.2.2</b> plyr包：ddply()函数</a></li>
<li class="chapter" data-level="6.2.3" data-path="section-6.html"><a href="section-6.html#dplyr"><i class="fa fa-check"></i><b>6.2.3</b> dplyr包</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="section-6.html"><a href="section-6.html#section-6.3"><i class="fa fa-check"></i><b>6.3</b> 数据整形</a><ul>
<li class="chapter" data-level="6.3.1" data-path="section-6.html"><a href="section-6.html#reshape2"><i class="fa fa-check"></i><b>6.3.1</b> <code>reshape2</code>包</a></li>
<li class="chapter" data-level="6.3.2" data-path="section-6.html"><a href="section-6.html#tidyr"><i class="fa fa-check"></i><b>6.3.2</b> <code>tidyr</code>包</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="section-6.html"><a href="section-6.html#-1"><i class="fa fa-check"></i><b>6.4</b> 本章总结</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="section-7.html"><a href="section-7.html"><i class="fa fa-check"></i><b>7</b> 基础建模技术</a><ul>
<li class="chapter" data-level="7.1" data-path="section-7.html"><a href="section-7.html#section-7.1"><i class="fa fa-check"></i><b>7.1</b> 有监督和无监督</a></li>
<li class="chapter" data-level="7.2" data-path="section-7.html"><a href="section-7.html#section-7.2"><i class="fa fa-check"></i><b>7.2</b> 误差及其来源</a><ul>
<li class="chapter" data-level="7.2.1" data-path="section-7.html"><a href="section-7.html#section-7.2.1"><i class="fa fa-check"></i><b>7.2.1</b> 系统误差和随机误差</a></li>
<li class="chapter" data-level="7.2.2" data-path="section-7.html"><a href="section-7.html#section-7.2.2"><i class="fa fa-check"></i><b>7.2.2</b> 应变量误差</a></li>
<li class="chapter" data-level="7.2.3" data-path="section-7.html"><a href="section-7.html#section-7.2.3"><i class="fa fa-check"></i><b>7.2.3</b> 自变量误差</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="section-7.html"><a href="section-7.html#section-7.3"><i class="fa fa-check"></i><b>7.3</b> 数据划分和再抽样</a><ul>
<li class="chapter" data-level="7.3.1" data-path="section-7.html"><a href="section-7.html#section-7.3.1"><i class="fa fa-check"></i><b>7.3.1</b> 划分训练集和测试集</a></li>
<li class="chapter" data-level="7.3.2" data-path="section-7.html"><a href="section-7.html#section-7.3.2"><i class="fa fa-check"></i><b>7.3.2</b> 重抽样</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="section-7.html"><a href="section-7.html#-2"><i class="fa fa-check"></i><b>7.4</b> 本章总结</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="section-8.html"><a href="section-8.html"><i class="fa fa-check"></i><b>8</b> 模型评估度量</a><ul>
<li class="chapter" data-level="8.1" data-path="section-8.html"><a href="section-8.html#section-8.1"><i class="fa fa-check"></i><b>8.1</b> 回归模型评估度量</a></li>
<li class="chapter" data-level="8.2" data-path="section-8.html"><a href="section-8.html#section-8.2"><i class="fa fa-check"></i><b>8.2</b> 分类模型评估度量</a><ul>
<li class="chapter" data-level="8.2.1" data-path="section-8.html"><a href="section-8.html#kappa"><i class="fa fa-check"></i><b>8.2.1</b> Kappa统计量</a></li>
<li class="chapter" data-level="8.2.2" data-path="section-8.html"><a href="section-8.html#roc"><i class="fa fa-check"></i><b>8.2.2</b> ROC曲线</a></li>
<li class="chapter" data-level="8.2.3" data-path="section-8.html"><a href="section-8.html#section-8.2.3"><i class="fa fa-check"></i><b>8.2.3</b> 提升图</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="section-8.html"><a href="section-8.html#-3"><i class="fa fa-check"></i><b>8.3</b> 本章总结</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="section-9.html"><a href="section-9.html"><i class="fa fa-check"></i><b>9</b> 特征工程</a><ul>
<li class="chapter" data-level="9.1" data-path="section-9.html"><a href="section-9.html#section-9.1"><i class="fa fa-check"></i><b>9.1</b> 特征构建</a></li>
<li class="chapter" data-level="9.2" data-path="section-9.html"><a href="section-9.html#section-9.2"><i class="fa fa-check"></i><b>9.2</b> 特征提取</a><ul>
<li class="chapter" data-level="9.2.1" data-path="section-9.html"><a href="section-9.html#section-9.2.1"><i class="fa fa-check"></i><b>9.2.1</b> 初步探索数据</a></li>
<li class="chapter" data-level="9.2.2" data-path="section-9.html"><a href="section-9.html#section-9.2.2"><i class="fa fa-check"></i><b>9.2.2</b> 主成分分析</a></li>
<li class="chapter" data-level="9.2.3" data-path="section-9.html"><a href="section-9.html#section-9.2.3"><i class="fa fa-check"></i><b>9.2.3</b> 探索性因子分析</a></li>
<li class="chapter" data-level="9.2.4" data-path="section-9.html"><a href="section-9.html#section-9.2.4"><i class="fa fa-check"></i><b>9.2.4</b> 理论背景</a></li>
<li class="chapter" data-level="9.2.5" data-path="section-9.html"><a href="section-9.html#section-9.2.5"><i class="fa fa-check"></i><b>9.2.5</b> 高维标度化</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="section-9.html"><a href="section-9.html#section-9.3"><i class="fa fa-check"></i><b>9.3</b> 变量选择</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="section-10.html"><a href="section-10.html"><i class="fa fa-check"></i><b>10</b> 线性回归极其衍生</a><ul>
<li class="chapter" data-level="10.1" data-path="section-10.html"><a href="section-10.html#section-10.1"><i class="fa fa-check"></i><b>10.1</b> 普通线性回归</a><ul>
<li class="chapter" data-level="10.1.1" data-path="section-10.html"><a href="section-10.html#section-10.1.1"><i class="fa fa-check"></i><b>10.1.1</b> 最小二乘线性模型</a></li>
<li class="chapter" data-level="10.1.2" data-path="section-10.html"><a href="section-10.html#section-10.1.2"><i class="fa fa-check"></i><b>10.1.2</b> 回归诊断</a></li>
<li class="chapter" data-level="10.1.3" data-path="section-10.html"><a href="section-10.html#section-10.1.3"><i class="fa fa-check"></i><b>10.1.3</b> 离群点，高杠杆点和强影响点</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="section-10.html"><a href="section-10.html#section-10.2"><i class="fa fa-check"></i><b>10.2</b> 收缩方法</a></li>
<li class="chapter" data-level="10.3" data-path="section-10.html"><a href="section-10.html#section-10.3"><i class="fa fa-check"></i><b>10.3</b> 分层线性回归</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="section-11.html"><a href="section-11.html"><i class="fa fa-check"></i><b>11</b> 树模型</a><ul>
<li class="chapter" data-level="11.1" data-path="section-11.html"><a href="section-11.html#section-11.1"><i class="fa fa-check"></i><b>11.1</b> 基本树模型</a></li>
<li class="chapter" data-level="11.2" data-path="section-11.html"><a href="section-11.html#section-11.2"><i class="fa fa-check"></i><b>11.2</b> 装袋树</a></li>
<li class="chapter" data-level="11.3" data-path="section-11.html"><a href="section-11.html#section-11.3"><i class="fa fa-check"></i><b>11.3</b> 随机森林</a></li>
<li class="chapter" data-level="11.4" data-path="section-11.html"><a href="section-11.html#section-11.4"><i class="fa fa-check"></i><b>11.4</b> 其它树话题</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="section-12.html"><a href="section-12.html"><i class="fa fa-check"></i><b>12</b> 聚类判别分析</a><ul>
<li class="chapter" data-level="12.1" data-path="section-12.html"><a href="section-12.html#section-12.1"><i class="fa fa-check"></i><b>12.1</b> 聚类分析</a></li>
<li class="chapter" data-level="12.2" data-path="section-12.html"><a href="section-12.html#section-12.2"><i class="fa fa-check"></i><b>12.2</b> 判别分析</a><ul>
<li class="chapter" data-level="12.2.1" data-path="section-12.html"><a href="section-12.html#section-12.2.1"><i class="fa fa-check"></i><b>12.2.1</b> 逻辑回归</a></li>
<li class="chapter" data-level="12.2.2" data-path="section-12.html"><a href="section-12.html#section-12.2.2"><i class="fa fa-check"></i><b>12.2.2</b> 线性判别分析</a></li>
<li class="chapter" data-level="12.2.3" data-path="section-12.html"><a href="section-12.html#section-12.2.3"><i class="fa fa-check"></i><b>12.2.3</b> 最小二乘判别分析</a></li>
<li class="chapter" data-level="12.2.4" data-path="section-12.html"><a href="section-12.html#section-12.2.4"><i class="fa fa-check"></i><b>12.2.4</b> 朴素贝叶斯</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="section-12.html"><a href="section-12.html#section-12.3"><i class="fa fa-check"></i><b>12.3</b> 案例：客户分组</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="section-13.html"><a href="section-13.html"><i class="fa fa-check"></i><b>13</b> 关联法则分析</a><ul>
<li class="chapter" data-level="13.1" data-path="section-13.html"><a href="section-13.html#section-13.1"><i class="fa fa-check"></i><b>13.1</b> 关联法则简介</a></li>
<li class="chapter" data-level="13.2" data-path="section-13.html"><a href="section-13.html#section-13.2"><i class="fa fa-check"></i><b>13.2</b> 案例：商业购物篮分析</a></li>
<li class="chapter" data-level="13.3" data-path="section-13.html"><a href="section-13.html#section-13.3"><i class="fa fa-check"></i><b>13.3</b> 关联法则可视化</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="section-14.html"><a href="section-14.html"><i class="fa fa-check"></i><b>14</b> 数据可视化和结果展示</a><ul>
<li class="chapter" data-level="14.1" data-path="section-14.html"><a href="section-14.html#r-markdown"><i class="fa fa-check"></i><b>14.1</b> R Markdown</a><ul>
<li class="chapter" data-level="14.1.1" data-path="section-14.html"><a href="section-14.html#r-markdown"><i class="fa fa-check"></i><b>14.1.1</b> 什么是R Markdown?</a></li>
<li class="chapter" data-level="14.1.2" data-path="section-14.html"><a href="section-14.html#how-to-start"><i class="fa fa-check"></i><b>14.1.2</b> How to Start?</a></li>
<li class="chapter" data-level="14.1.3" data-path="section-14.html"><a href="section-14.html#interactive-r-markdown-document"><i class="fa fa-check"></i><b>14.1.3</b> Interactive R Markdown Document</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="15" data-path="section-15.html"><a href="section-15.html"><i class="fa fa-check"></i><b>15</b> 数据科学的科学</a></li>
<li class="chapter" data-level="16" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>16</b> References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">数据科学家：R语言</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="section-9" class="section level1">
<h1><span class="header-section-number">第9章</span> 特征工程</h1>
<p>对自变量进行编码和筛选的过程称为特征工程（Feature Engineering），其目的就是获取更好的训练数据，提高模型表现。例如，有时用自变量的组合能够比使用单独的自变量更有效；用两个变量的比值可能比用两个单独的变量更有效等等。通常最有效的编码数据的方法来自于建模者对问题的理解，而不是通过任何数学方法。在原始数据基础上，通过该工程得到的优化特征可以更好的描述数据关系。从数学的角度上就是优化自变量矩阵<span class="math inline">\(\mathbf{X}\)</span>。</p>
<p>特征工程在机器学习中起着举足轻重的作用，如果你能找到有效的特征，其实未必需要复杂的算法。很遗憾，大多数的书中并没有专门花一章来讲特征工程，更常提到的是特征选择（Feature Selection）。很多机器学习的书都是以介绍算法为主，目的在于理解算法本身，所以特征工程通常不是重点。这和特征工程在实际应用中的重要性极不相称。所以在这里，我们专门花一章来介绍特征工程。在特征工程下面有3个主要的子问题，我们会一次讨论这三个问题：</p>
<ol style="list-style-type: decimal">
<li>特征构建（Feature Construction）：从原始数据中构建新变量</li>
<li>特征提取（Feature Extraction）：将原始变量按照某种标准变换得到能够更好反映数据关系的变量</li>
<li>特征选择（Feature Selection）：在整个自变量集中找到和因变量有关的变量子集，从而达到降维且增加模型估计稳定性和可解释性的效果</li>
</ol>
<p>这三者的大概顺序是：特征构建 -&gt; 特征提取 -&gt; 特征选择。如果特征构建做的不好，那么它会直接影响特征提取，进而影响了特征选择，最终影响模型的性能。事实上，特征工程是一个迭代过程，我们需要不断的设计特征、选择特征、建立模型、评估模型，然后得到反馈在回头优化特征的设计。</p>
<div id="section-9.1" class="section level2">
<h2><span class="header-section-number">9.1</span> 特征构建</h2>
<p>在实际应用中，显然是不可能凭空而来的，需要我们手工去构建特征。关于特征构建的定义，可以这么说：特征构建指的是从原始数据中人工的构建新的特征。我们需要人工的创建它们。这需要我们花大量的时间去研究真实的数据样本，思考问题的潜在形式和数据结构，同时能够更好地应用到预测模型中。</p>
<p>特征构建需要很强的洞察力和分析能力，要求我们能够从原始数据中找出一些具有物理意义的特征。假设原始数据是表格数据，一般你可以使用混合属性或者组合属性来创建新的特征，或是分解或切分原有的特征来创建新的特征。比如之前在数据预处理那章中讲编码名义变量时，我们将分类变量<code>gender</code>(性别)转化为两个名义变量：<code>Female</code>和<code>Male</code>。之后考虑收入和性别的交互效应(<code>income:gender</code>)也是一种特征构建。另外再举一个例子，假设你有一个日期时间 (2006-04-01 02:26:00)该如何转换呢？对于这种时间的数据，我们可以根据需求提取出多种属性。比如下面这个从农业论坛爬取的文本数据。</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(readr)
<span class="kw">library</span>(dplyr)
topic&lt;-<span class="kw">read_csv</span>(<span class="st">&quot;/Users/happyrabbit/Documents/Hui/Project/NLP/RawData/newagtalk-20150607/topic.csv&quot;</span>)
<span class="kw">glimpse</span>(topic)</code></pre></div>
<pre><code>## Observations: 209,607
## Variables: 6
## $ tid           &lt;int&gt; 242, 259, 270, 281, 301, 312, 333, 367, 386, 387...
## $ fid           &lt;int&gt; 5, 5, 5, 5, 5, 5, 5, 7, 5, 5, 5, 5, 5, 5, 5, 7, ...
## $ title         &lt;chr&gt; &quot;0&quot;, &quot;Adobe Reader&quot;, &quot;? about this forum veiw?&quot;,...
## $ posted_at     &lt;time&gt; 2006-04-01 02:26:00, 2006-04-01 08:39:00, 2006-...
## $ user_name     &lt;chr&gt; &quot;Rich&quot;, &quot;Larry NCKS&quot;, &quot;Hay Hud Ohio&quot;, &quot;jakescia&quot;...
## $ user_location &lt;chr&gt; &quot;Kansas&quot;, &quot;Washington, Kansas  &amp; Lincoln, Nebras...</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># 将发帖时间提取出来，存在posted_at2对象中</span>
posted_at2&lt;-topic$posted_at</code></pre></div>
<p>数据由6列，这里只解释其中2列。<code>posted_at</code>是发帖时间，<code>user_name</code>是论坛用户名。对于发帖时间，我们可以将其拆分成不同的变量，这个过程类似于探索性数据分析。这里时间观测是按照年、月、日、时、分、秒的顺序，以这样的格式<code>2006-04-01 02:26:00</code>排列的。在具体分析中，我们通常希望知道用户的发帖规律。在事先不知道怎样划分能够看到规律的情况下该怎么办？尝试不同的划分方法。比如我们可以研究每年的发帖量，可以通过<code>substr()</code>函数截取字符串的固定位置得到年份：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># 将截取的年份存在名为year的列中</span>
topic$year&lt;-<span class="kw">substr</span>(posted_at2,<span class="dv">1</span>,<span class="dv">4</span>)
<span class="co"># 看下结果如何</span>
car::<span class="kw">some</span>(topic$year)</code></pre></div>
<pre><code>##  [1] &quot;2011&quot; &quot;2012&quot; &quot;2013&quot; &quot;2013&quot; &quot;2013&quot; &quot;2013&quot; &quot;2014&quot; &quot;2014&quot; &quot;2014&quot; &quot;2015&quot;</code></pre>
<p>接下来我们可以查看下每年发帖数目的变化情况：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">barplot</span>(<span class="kw">table</span>(topic$year),<span class="dt">family =</span><span class="st">&quot;Songti SC&quot;</span>, <span class="dt">main=</span><span class="st">&quot;年度发帖数目频数直方图&quot;</span>)</code></pre></div>
<p><img src="DS_R_files/figure-html/unnamed-chunk-95-1.png" width="672" /></p>
<p>图中我们可以看到，从2006年论坛创建以来，帖子的数目几乎呈指数上升，2015年貌似不符合规律，其实是因为当前数据中只饱含到2015年5月的论坛数据，也就是说如果我们由所有2015年的数据，最后的直方条应该会超过2014年。这么简单的统计能够告诉我们什么呢？这样的统计量对于公司的市场预算是很重要的。假如某农业公司要决定是否投入人力和财力去挖掘这个农业论坛数据，首先需要明确的就是这些数据是不是有代表性，该论坛是不是活跃。上图就表明该论坛是处在高速发展阶段的，而且考虑到论坛的用户是农民，这样的活跃度是非常高的。所以相关决策人员或许可以将该论坛当作一个消费者评论信息的来源。只考虑年是不够的，我们可能还想类似的检查下月度发帖分布，我们可以类似的截取时间字符串中的月份，绘制直方图：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">topic$month&lt;-<span class="kw">substr</span>(posted_at2,<span class="dv">6</span>,<span class="dv">7</span>)
<span class="kw">barplot</span>(<span class="kw">table</span>(topic$month),<span class="dt">family =</span><span class="st">&quot;Songti SC&quot;</span>, <span class="dt">main=</span><span class="st">&quot;月度发帖数目频数直方图&quot;</span>)</code></pre></div>
<p><img src="DS_R_files/figure-html/unnamed-chunk-96-1.png" width="672" /></p>
<p>大家可以看到明显的季节效应。通常北半球12-4月是农闲时节，这个时候发帖数远高于开春（5月）之后。基于此，我们可以假设12-4月的帖子或许更多的是关于去年购买的种子收获情况以及一些下一年的耕种计划的信息，5月到11月间或许更多的是当下遇到的问题，比如播种时种子发芽情况，生长中雨水，作物抗旱性能等等。这些为农业公司提供了消费者对其产品的体验信息。这样简单的统计能够给我们之后的分析指引放下。再次强调，分析的整个流程是个渐进且协同的过程，前一步中获得的信息可能有助于我们明确之后要如何进行分析，下一步分析的结果可能又让我们返回之前的步骤，比如收集构建新变量等等。</p>
<p>再看看每天发帖的规律：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">topic$time&lt;-<span class="kw">substr</span>(posted_at2,<span class="dv">12</span>,<span class="dv">13</span>)
<span class="kw">barplot</span>(<span class="kw">table</span>(topic$time),<span class="dt">family =</span><span class="st">&quot;Songti SC&quot;</span>,<span class="dt">main=</span><span class="st">&quot;每日不同时间段发帖频数直方图&quot;</span>)</code></pre></div>
<p><img src="DS_R_files/figure-html/unnamed-chunk-97-1.png" width="672" /></p>
<p>可以看到，每天有两个发帖高峰，中午一阵，晚上一阵（这里不考虑北美不同纬度的时差影响）。这能给我们什么信息呢？很有可能农民习惯于在这两个时间段上网查询信息等。如果农业公司想要发营销广告，这或许是最佳的时间。</p>
<p>上面是一个简单的关于构建特征的例子，在具体应用中特征构建的方法可以非常灵活，没有一个黄金标准。随着分析经验的增长，以及对相关领域的了解加深，构建有意义特征的能力也会随之提高。特征构建是特征工程中艺术成分最高的部分，接下来我们要介绍的特征提取和特征选择就有更强的技术性。</p>
</div>
<div id="section-9.2" class="section level2">
<h2><span class="header-section-number">9.2</span> 特征提取</h2>
<p>特征提取是一项用不同变量的组合代替原变量的技术。它的目的是自动地构建新的特征，将原始特征转换为一组具有明显物理意义或者统计意义的特征。比如通过变换特征取值来减少原始数据中某个特征的取值个数等。我们考虑3个常用的可以对数据降维的特征提取方法。主成分分析（PCA）试图找到原变量的不相关线性组合，这些线性组合能够最大限度的解释原数据中变量方差。解释性因子分析（EFA）同样试图在尽量小的维度上解释原数据中尽可能多的方差。高维标度化（MDS）将观测见的相似度映射到低维度上，如2维平面。MDS能够作用于非数值型变量，如分类变量或者有序数据预测变量。接下来我们通过模拟的航空公司数据集来展示不同的特征提取方法。在市场营销中这类消费者调查问卷中，虽然初始问题很多，但通常存在多个调查项共同反应少数几个潜在因子。比如航空公司满意度调查数据中下面四个问题：购票容易度（<code>Easy_Reservation</code>）、座椅选择（<code>Preferred_Seats</code>）、航班选择（<code>Flight_Options</code>）和票价（<code>Ticket_Prices</code>）都和购票体验有关。</p>
<div id="section-9.2.1" class="section level3">
<h3><span class="header-section-number">9.2.1</span> 初步探索数据</h3>
<p>我们先读入该数据：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># 可以从网站下载该数据</span>
airline&lt;-<span class="kw">read_csv</span>(<span class="st">&quot;/Users/happyrabbit/Documents/GitHub/DataScientistR/Data/AirlineRating.csv&quot;</span>)</code></pre></div>
<p>可以用<code>glimpse()</code>函数检查该数据：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">glimpse</span>(airline)</code></pre></div>
<pre><code>## Observations: 3,000
## Variables: 17
## $ Easy_Reservation &lt;int&gt; 6, 5, 6, 5, 4, 5, 6, 4, 6, 4, 5, 5, 6, 5, 5, ...
## $ Preferred_Seats  &lt;int&gt; 5, 7, 6, 6, 5, 6, 6, 6, 5, 4, 7, 5, 7, 6, 6, ...
## $ Flight_Options   &lt;int&gt; 4, 7, 5, 5, 3, 4, 6, 3, 4, 5, 6, 6, 6, 5, 6, ...
## $ Ticket_Prices    &lt;int&gt; 5, 6, 6, 5, 6, 5, 5, 5, 5, 6, 7, 7, 6, 7, 7, ...
## $ Seat_Comfort     &lt;int&gt; 5, 6, 7, 7, 6, 6, 6, 4, 6, 9, 7, 7, 6, 6, 6, ...
## $ Seat_Roominess   &lt;int&gt; 7, 8, 6, 8, 7, 8, 6, 5, 7, 8, 8, 9, 7, 8, 6, ...
## $ Overhead_Storage &lt;int&gt; 5, 5, 7, 6, 5, 4, 4, 4, 5, 7, 6, 6, 7, 5, 4, ...
## $ Clean_Aircraft   &lt;int&gt; 7, 6, 7, 7, 7, 7, 6, 4, 6, 7, 7, 7, 7, 7, 6, ...
## $ Courtesy         &lt;int&gt; 5, 6, 6, 4, 2, 5, 5, 4, 5, 6, 4, 6, 4, 5, 5, ...
## $ Friendliness     &lt;int&gt; 4, 6, 6, 6, 3, 4, 5, 5, 4, 5, 6, 7, 5, 4, 4, ...
## $ Helpfulness      &lt;int&gt; 6, 5, 6, 4, 4, 5, 5, 4, 3, 5, 5, 6, 5, 4, 5, ...
## $ Service          &lt;int&gt; 6, 5, 6, 5, 3, 5, 5, 5, 3, 5, 6, 6, 5, 5, 4, ...
## $ Satisfaction     &lt;int&gt; 6, 7, 7, 5, 4, 6, 5, 5, 4, 7, 6, 7, 6, 4, 4, ...
## $ Fly_Again        &lt;int&gt; 6, 6, 6, 7, 4, 5, 3, 4, 7, 6, 8, 6, 5, 4, 6, ...
## $ Recommend        &lt;int&gt; 3, 6, 5, 5, 4, 5, 6, 5, 8, 6, 8, 7, 6, 5, 6, ...
## $ ID               &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...
## $ Airline          &lt;chr&gt; &quot;AirlineCo.1&quot;, &quot;AirlineCo.1&quot;, &quot;AirlineCo.1&quot;, ...</code></pre>
<p>数据的前15列都是问卷调查的各种问题，问题格式如下：对该航空公司的<问卷项>你的满意度是？从1到9，分值越大满意度越高。可以看到前15列评分在1-9之间，是整数型了。<code>ID</code>代表受访者编号，不同编号代表不同受访者。每个受访者需要评估3家航空公司，列<code>Airline</code>指出相应的航空公司。一共有1000名受访者，因此观测的总行数为1000x3=3000。关于数据各列变量的解释，大家可以参考“数据集模拟和背景介绍”中相关小节。</p>
<p>我们用<code>corrplot()</code>函数检查问卷调查问题的相关性：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(corrplot)
<span class="co"># 选取其中的问卷调查项</span>
<span class="kw">select</span>(airline,Easy_Reservation:Recommend)%&gt;%
<span class="st">  </span><span class="co"># 得到相关矩阵</span>
<span class="st">  </span><span class="kw">cor</span>()%&gt;%
<span class="st">  </span><span class="co"># 用corrplot()绘制相关图</span>
<span class="st">  </span><span class="co"># 选项order=&quot;hclust&quot;按照变量的相似度，基于系统聚类的结果对行列进行重新排列</span>
<span class="st">  </span><span class="kw">corrplot</span>(,<span class="dt">order=</span><span class="st">&quot;hclust&quot;</span>)</code></pre></div>
<p><img src="DS_R_files/figure-html/unnamed-chunk-100-1.png" width="672" /></p>
<p>由相关矩阵图可以看到，这些问卷项大致分成3类：</p>
<ol style="list-style-type: decimal">
<li>空航服务相关
<ul>
<li>礼貌（Courtesy）</li>
<li>友善（Friendliness）</li>
<li>能够提供需要的帮助（Helpfulness）</li>
<li>食物饮料服务（Service）</li>
</ul></li>
<li>购票体验相关
<ul>
<li>购票容易度（Easy_Reservation）</li>
<li>座椅选择（Preferred_Seats）</li>
<li>航班选择（Flight_Options）</li>
<li>票价（Ticket_Prices）</li>
</ul></li>
<li>机舱设施和总体评估指数
<ul>
<li>座椅舒适度（Seat_Comfort）</li>
<li>位置前后空间（Seat_Roominess）</li>
<li>随机行李存放（Overhead_Storage）</li>
<li>机舱清洁（Clean_Aircraft）</li>
<li>总体满意度（Satisfaction）</li>
<li>再次选择次航空公司（Fly_Again）</li>
<li>向朋友推荐此航空公司（Recommend）</li>
</ul></li>
</ol>
<p>而且机舱设施和总体满意度相关性较高。空航服务和购票体验貌似负相关，也就是说航空公司目前没有做到让乘客对这两类体验都感到满意，这也可能是潜在需要提高的地方。这里简单的检查数据能够给我们一些基本的信息，让我们能够做出一些假设，然后在之后的分析中尝试证实这些假设。</p>
<p>对于这样的数据初步探索，一个非常自然的问题是：每个航空公司对应的各项评分均值是多少？我们可以用之前介绍的<code>dplyr</code>包中的各种函数，以及使用之前讲到的管道操作<code>%&gt;%</code>让代码更易读：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># 选取其中的问卷调查项和航空公司因子信息</span>
<span class="co"># 即删除ID项</span>
airline.mean&lt;-<span class="kw">select</span>(airline,-ID)%&gt;%
<span class="st">  </span><span class="co"># 按Airline对数据进行分组总结</span>
<span class="st">  </span><span class="kw">group_by</span>(Airline)%&gt;%
<span class="st">  </span><span class="co"># 对每个数值</span>
<span class="st">  </span><span class="kw">summarise_each</span>(<span class="kw">funs</span>(mean))%&gt;%
<span class="st">  </span><span class="co"># 显示数据</span>
<span class="st">  </span><span class="kw">glimpse</span>()</code></pre></div>
<pre><code>## Observations: 3
## Variables: 16
## $ Airline          &lt;chr&gt; &quot;AirlineCo.1&quot;, &quot;AirlineCo.2&quot;, &quot;AirlineCo.3&quot;
## $ Easy_Reservation &lt;dbl&gt; 5.031, 2.939, 2.038
## $ Preferred_Seats  &lt;dbl&gt; 6.025, 2.995, 2.019
## $ Flight_Options   &lt;dbl&gt; 4.996, 2.033, 2.067
## $ Ticket_Prices    &lt;dbl&gt; 5.997, 3.016, 2.058
## $ Seat_Comfort     &lt;dbl&gt; 6.988, 5.009, 7.918
## $ Seat_Roominess   &lt;dbl&gt; 7.895, 3.970, 7.908
## $ Overhead_Storage &lt;dbl&gt; 5.967, 4.974, 7.924
## $ Clean_Aircraft   &lt;dbl&gt; 6.947, 6.050, 7.882
## $ Courtesy         &lt;dbl&gt; 5.016, 7.937, 7.942
## $ Friendliness     &lt;dbl&gt; 4.997, 7.946, 7.914
## $ Helpfulness      &lt;dbl&gt; 5.017, 7.962, 7.954
## $ Service          &lt;dbl&gt; 5.019, 7.956, 7.906
## $ Satisfaction     &lt;dbl&gt; 5.944, 3.011, 7.903
## $ Fly_Again        &lt;dbl&gt; 5.983, 3.008, 7.920
## $ Recommend        &lt;dbl&gt; 6.008, 2.997, 7.929</code></pre>
<p>上面的数值结果可以看到乘客对各个航空公司的满意度情况有明显的区别。总的来说购票体验相关的项满意度偏低（购票容易度（<code>Easy_Reservation</code>）、座椅选择（<code>Preferred_Seats</code>）、航班选择（<code>Flight_Options</code>）和票价（<code>Ticket_Prices</code>）），相较而言第1个航空公司在购票体验方面优于竞争对手，但在其它方面并没有优势。第2个航空公司在空航服务方面做的比较好，在其它方面也没有优势。第3个航空公司除了购票体验较差以外，在其它方面都至少和竞争对手相当，或者优于竞争对手。这里的数据分类和之前相关矩阵图展示出的信息有一致性，但也提醒我们各个航空公司对应的问卷回复项之间的关系可能不一样。对于上面各航空公司评分均值结果使用热图进行可视化是很好的方式。我们用<code>gplots</code>包中的<code>heatmap.2()</code>函数绘制热图，用<code>RColorBrewer</code>包对图形着色：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># gplots是可视化包</span>
<span class="kw">library</span>(gplots)
<span class="co"># RColorBrewer包用于设计图形的调色盘</span>
<span class="co"># 相关信息见：http://colorbrewer2.org</span>
<span class="kw">library</span>(RColorBrewer)
<span class="co"># 将航空公司设置成行名称然后将对应的字符列删除</span>
<span class="kw">row.names</span>(airline.mean)&lt;-airline.mean$Airline
airline.mean&lt;-<span class="kw">select</span>(airline.mean,-Airline)
<span class="co"># 绘制热图</span>
<span class="kw">heatmap.2</span>(<span class="kw">as.matrix</span>(airline.mean),
          <span class="dt">col=</span><span class="kw">brewer.pal</span>(<span class="dv">9</span>,<span class="st">&quot;YlGn&quot;</span>),<span class="dt">trace=</span><span class="st">&quot;none&quot;</span>,<span class="dt">key=</span><span class="ot">FALSE</span>,<span class="dt">dend=</span><span class="st">&quot;none&quot;</span>,<span class="dt">cexCol=</span><span class="fl">0.6</span>,<span class="dt">cexRow =</span><span class="dv">1</span>)
<span class="kw">title</span>(<span class="dt">family =</span><span class="st">&quot;Songti SC&quot;</span>,
          <span class="dt">main=</span><span class="st">&quot;航空公司问卷调查均值热图&quot;</span>)</code></pre></div>
<p><img src="DS_R_files/figure-html/unnamed-chunk-102-1.png" width="672" /></p>
<p>在上面代码中，我们将数据框<code>airline.mean</code>转化成矩阵传递给<code>heatmap.2()</code>，因为函数要求。我们通过<code>RColorBrewer</code>包中的<code>YlGn</code>调色盘，用黄色和绿色对热图着色，并且取消了一些将热图变得复杂的选项（<code>trace</code>、<code>key</code>和<code>dend</code>）。在结果图中，绿色表示高观测值，黄色表示低观测值，处于中间的值对应的颜色较浅。乘客对不同航空公司的满意度分值分布很显然有区别。航空公司3和2有明显让人满意和不满意的地方，而公司1总体来说比较平均，除了乘客对其位置前后空间（Seat_Roominess）特别满意。如果考虑和竞争对手的差距，1和2需要改进的地方显然比3要多。</p>
<p>通过观测目前得到的一些探索性结果，我们可以猜测各个问题可能的聚类情况，以及它们之间的关系。但我们最好使用更严格正规的统计模型验证这些猜测。接下来我们就开始介绍模型。</p>
</div>
<div id="section-9.2.2" class="section level3">
<h3><span class="header-section-number">9.2.2</span> 主成分分析</h3>
<p>主成分分析（PCA）是寻找变量的线性组合，得到新的组合变量叫做“成分”，其指导思想是尽量捕捉原数据中方差。在统计学中，方差通常也被认为是信息。第一个成分捕捉的方差最大。第二个成分尽可能多的捕捉前一个成分没有解释的方差。依次类推，直到成分的数目和原变量数目一样多。我们通过使用前几个成分取代原变量来达到降维的目的，同时要保证这些成分能够解释大部分原始数据集中的方差。这里要提醒一点，如果数据观测不在一个标度上时需要对数据进行标准化，因为PCA是基于变量方差矩阵。这里因为各个变量的观测分布没有很大差异，是不是标准化没有太大的影响。</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">airline.pc&lt;-<span class="kw">select</span>(airline,Easy_Reservation:Recommend)%&gt;%
<span class="st">  </span><span class="kw">prcomp</span>()
<span class="kw">summary</span>(airline.pc)</code></pre></div>
<pre><code>## Importance of components:
##                          PC1    PC2     PC3     PC4     PC5     PC6
## Standard deviation     4.693 4.2836 1.68335 1.03625 0.88896 0.82333
## Proportion of Variance 0.435 0.3624 0.05596 0.02121 0.01561 0.01339
## Cumulative Proportion  0.435 0.7974 0.85338 0.87458 0.89019 0.90358
##                            PC7     PC8     PC9    PC10    PC11    PC12
## Standard deviation     0.80349 0.78694 0.77536 0.77020 0.74612 0.71831
## Proportion of Variance 0.01275 0.01223 0.01187 0.01172 0.01099 0.01019
## Cumulative Proportion  0.91633 0.92856 0.94043 0.95215 0.96314 0.97333
##                           PC13    PC14    PC15
## Standard deviation     0.69417 0.66650 0.65131
## Proportion of Variance 0.00952 0.00877 0.00838
## Cumulative Proportion  0.98285 0.99162 1.00000</code></pre>
<p>由上面输出结果的第一行是主成分的标准差，第二行对应成分单独解释方差的比例，第三行是累计解释方差。可见，前两个主成分解释的大部分原变量方差（80%）。<code>plot()</code>函数作用在PCA结果上默认绘制陡坡图，该图展示每个成分额外解释的方差。我们可用累计解释方差比例或者陡坡图来判断需要的主成分。我们通过下面代码绘制图形：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(airline.pc,<span class="dt">type=</span><span class="st">&quot;l&quot;</span>,<span class="dt">family =</span><span class="st">&quot;Songti SC&quot;</span>,<span class="dt">main=</span><span class="st">&quot;主成分分析陡坡图&quot;</span>)</code></pre></div>
<p><img src="DS_R_files/figure-html/unnamed-chunk-104-1.png" width="672" /></p>
<p>图中由陡到缓的转折点能告诉我们从哪个主成分开始，继续添加更多的成分对解释更多方差没有太大帮助，反而增加模型复杂度。关于拐点的判断是主观的。图中显示拐点大约在2或者3。这表明前2到3个主成分解释了大部分方差。将各个原始变量映射到前两个主成分张成的平面上能够揭示这些变量之间的关系。这样的图称为“双标图”，可以用<code>biplot()</code>函数绘制：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">biplot</span>(airline.pc,<span class="dt">family =</span><span class="st">&quot;Songti SC&quot;</span>,<span class="dt">main=</span><span class="st">&quot;PCA结果双标图&quot;</span>,<span class="dt">cex=</span><span class="kw">c</span>(<span class="fl">0.5</span>,<span class="dv">1</span>),<span class="dt">xlim=</span><span class="kw">c</span>(-<span class="fl">0.06</span>,<span class="fl">0.04</span>))</code></pre></div>
<p><img src="DS_R_files/figure-html/unnamed-chunk-105-1.png" width="672" /></p>
<p>双标图的横坐标是第一个主成分（PC1），纵坐标是第二个主成分（PC2）。</p>
<p>A biplot uses points to represent the scores of the observations on the principal components, and it uses vectors to represent the coefficients of the variables on the principal components. In this example, the points represent automobiles, and the vectors represents judges.</p>
<p>Interpreting Points: The relative location of the points can be interpreted. Points that are close together correspond to observations that have similar scores on the components displayed in the plot. To the extent that these components fit the data well, the points also correspond to observations that have similar values on the variables.</p>
<p>In this example, cars that are close together are ones that have similar profiles of preference judgements: Most judges have the same kind of preference ratings for Cadillac and Lincoln Whether or not they like them we don’t know, its just that is a judge likes one s/he tends to like the other, and if a judge dislikes one, then the othe is likely to be disliked. The same is true for Pinto and Chevette, although the judgments about Pinto and Chevette are likely to be rather different then those about Cadillac and Lincoln, since the two pairs of points are relatively far apart.</p>
<p>Interpreting Vectors: Both the direction and length of the vectors can be interpreted. Vectors point away from the origin in some direction.</p>
<p>A vector points in the direction which is most like the variable represented by the vector. This is the direction which has the highest squared multiple correlation with the principal components. The length of the vector is proportional to the squared multiple correlation between the fitted values for the variable and the variable itself.</p>
<p>The fitted values for a variable are the result of projecting the points in the space orthogonally onto the variable’s vector (to do this, you must imagine extending the vector in both directions). The observations whose points project furthest in the direction in which the vector points are the observations that have the most of whatever the variable measures. Those points that project at the other end have the least. Those projecting in the middle have an average ammount. then the</p>
<p>Thus, vectors that point in the same direction correspond to variables that have similar response profiles, and can be interpreted as having similar meaning in the context set by the data.</p>
<p>满意度评分PCA结果的双标图中红色的箭头中看到不同调查问题的聚类情况，并且可以大致感觉样本的聚类情况，大致成为3类，一个合理的猜测是针对3个不同的航空公司。从之前的热图可以看到，3个公司有各自的优势和劣势，评分分布显然是不同的。但这样的图有个问题：基于所有评分样本导致图形非常稠密，难以识别。如果是基于各个公司聚合后的数据，得到的图或许会更清晰。</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">airline.mean.pc&lt;-<span class="kw">select</span>(airline.mean,Easy_Reservation:Recommend)%&gt;%
<span class="st">  </span><span class="kw">prcomp</span>()
<span class="kw">biplot</span>(airline.mean.pc,<span class="dt">family =</span><span class="st">&quot;Songti SC&quot;</span>,<span class="dt">main=</span><span class="st">&quot;聚合后PCA结果双标图&quot;</span>,
       <span class="dt">cex=</span><span class="fl">0.7</span>, <span class="dt">expand=</span><span class="dv">2</span>,<span class="dt">xlim=</span><span class="kw">c</span>(-<span class="fl">0.8</span>, <span class="dv">1</span>),<span class="dt">ylim=</span><span class="kw">c</span>(-<span class="fl">0.7</span>,<span class="fl">0.8</span>))</code></pre></div>
<p><img src="DS_R_files/figure-html/unnamed-chunk-106-1.png" width="672" /></p>
<p>按航空公司聚合后的结果双标图提供了可解释的乘客<strong>感知图</strong>，该图展示了各个航空公司在前两个主成分上的定位。我们先和聚合前后的感知图进行比较。注意，感知图的空间旋转是任意的，重要的是箭头的相对位置。比如，在两个感知图中，<code>Courtesy</code>、<code>Friendliness</code>、<code>Service</code>和<code>Helpfulness</code>都几乎重叠。 <code>Seat_Comfort</code>、<code>Seat_Roominess</code>、<code>Overhead_Storage</code>、<code>Clean_Aircraft</code>、<code>Satisfaction</code>、<code>Fly_Again</code>和<code>Recommend</code>大致指向相同的方向。剩下的<code>Easy_Reservation</code>、<code>Preferred_Seats</code>、<code>Flight_Options</code>和<code>Ticket_Prices</code>紧密相连。因此聚合后变量在主成分纬度上的分布位置和用原始观测得到的一致，但基于公司平均分值的结果更清晰的展示了公司相对定位情况。航空公司3在机舱设施、总体满意度和空航服务上得分都较高。航空公司2在空航服务方面得分较高。航空公司1在购票体验上表现较好。且在购票体验上满意度高的乘客更不满空航服务。如果你是航空公司3的商业数据分析师，看到这样的结果你可以得到什么结果？</p>
<ul>
<li>公司在很多方面具有竞争优势，客户满意度总体高于竞争对手</li>
<li>公司在购票体验上有明显劣势，这是需要努力改进的地方</li>
<li>我们什么在购票体验上满意度高的乘客更不满空航服务？是因为乘客本身的特质，或是由于某种原因重视空航服务的公司容易忽视购票体验？</li>
<li>需要进一步研究购票体验差的原因，以及评估其可能带来的影响：如果购票体验差并不会影响当前总体满意度以及票的销售情况，那我们需要投入多少改进该问题？</li>
</ul>
<p>如果航空公司1只是一个很小的公司，并不是主要竞争对手。你的主要竞争对手是航空公司3，那你可以进一步检查你们公司和航空公司2的得分差别：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">airline.mean[<span class="dv">3</span>,]-airline.mean[<span class="dv">1</span>,]</code></pre></div>
<pre><code>##   Easy_Reservation Preferred_Seats Flight_Options Ticket_Prices
## 1           -2.993          -4.006         -2.929        -3.939
##   Seat_Comfort Seat_Roominess Overhead_Storage Clean_Aircraft Courtesy
## 1         0.93          0.013            1.957          0.935    2.926
##   Friendliness Helpfulness Service Satisfaction Fly_Again Recommend
## 1        2.917       2.937   2.887        1.959     1.937     1.921</code></pre>
<p>从上面结果可以看出，和主要竞争对手相比，我们主要的劣势在于购票容易度（<code>Easy_Reservation</code>）、座椅选择（<code>Preferred_Seats</code>）、航班选择（<code>Flight_Options</code>）和票价（<code>Ticket_Prices</code>）上。在座椅舒适度（<code>Seat_Comfort</code>）、座椅空间（<code>Seat_Roominess</code>）、机舱清洁（<code>Clean_Aircraft</code>）和随机行李存放（<code>Overhead_Storage</code>）上两者相当。在剩余方面我们有优势。</p>
<p>通常情况下，对这样的问卷调查数据，你需要在不同维度上比较各个公司。可以通过陡坡图或者直接观察累计方差来决定该用多少的主成分。两个主成分张成的平面上绘制感知图能够解释观测在主成分维度上的分布。对这样多维度的市场调查数据，用PCA进行可视化是理解各个公司或者品牌在消费者认知中的分布的有效手段。关于该方法有几个需要注意的地方：</p>
<ol style="list-style-type: decimal">
<li><p>这里我们选择用均值对各个公司的评分进行聚合。但这并不是唯一的方式，取决于你的数据和问题，也可以使用中位数。在解释聚合后结果双标图之前，应该先确保聚合前后双标图上主成分相对位置分布一致。</p></li>
<li><p>这里所说的位置分布都是只相对位置，而不是具体的位置。主成分是基于所有变量的线性组合，因此从图上无法看出某个公司在特定问卷调查项上的具体强度。比如由图可以看出，公司3和公司2在机舱设施和总体满意度这个大方向上分布不同。检查具体的平均评分你会发现，总体上3确实在这两个方面好于2，但这并不代表3在其中每一个问卷调查项上都由明显优势，事实上，这三个公司在<code>Clean_Aircraft</code>这个选项上差别都不是很大。这里我们研究的是在一个更高层面上的消费者满意度分布。</p></li>
<li><p>这里得到的各项相对分布位置和你考虑的公司，还有问卷调查项有关。如果对另外3个不同的公司进行同样的问卷调查，可能结果会不同，或者添加新的调查项也可能改变原来调查项的相对位置。一个评估模型敏感性的方法是抽取一个样本子集进行类似分析，或者删除一些问卷调查项，看看结果是不是有很大变化。如果这些随机干扰下得到的双标图中各项的相对位置相似的话，你对在该项目中使用这个模型就更加自信。</p></li>
<li><p>最后一点和建模没有直接关系，但是在应用中很重要。问题出现在问卷调查上不代表问题就重要。取决于不同公司对定义“重要”的定义。通常情况下，在有限的市场研究经费下我们希望问和用户购买行为最相关的问题。比如在购票体验方面，对购买决定影响最大的可能是票价，而了解乘客是否对座椅选择满意本身当然没有坏处，但是考虑到进行调查研究的成本，我们不得不问：是不是需要问这个问题？有没有更好的问题取代当前问题？这需要你首先定义一个衡量“重要性”的标准（如，总体满意度，购买行为），然后据此尽量寻找对该标准影响最大的问题。</p></li>
</ol>
<p>从上面的例子中我们可以看到调查项大致分成几个类，每类问题对应一个可能的潜在变量。比如购票容易度（<code>Easy_Reservation</code>）、座椅选择（<code>Preferred_Seats</code>）、航班选择（<code>Flight_Options</code>）和票价（<code>Ticket_Prices</code>）就和购票体验有关。接下来我们可能会想知道如何用更科学的方式分析出调查项背后的潜变量，以及衡量受访者对某公司或品牌针对某个潜变量的总体得分（比如对总体购票体验），如果需要提高消费者对某方面的认知，商家需要关注哪些具体问题（比如，提高价格竞争力可能对购票体验的提高帮助最大）。下面我们要讲的探索性因子分析就可以帮助我们实现这一点。</p>
</div>
<div id="section-9.2.3" class="section level3">
<h3><span class="header-section-number">9.2.3</span> 探索性因子分析</h3>
<p>探索性因子分析（EFA）可以用来获取抽样调查中问题之间的构造。这里的因子就是无法观测到的<strong>潜变量</strong>，或者隐变量。因子分析的形成和早期发展一般认为是从Charles Spearman在1904年发表的文章开始<span class="citation">(Spearman <a href="#ref-spearman1904">1904</a>)</span>。关于因子分析的经典案例是心理学和教育学中的测试。例如“智力”，人格依恋类型（安全型，焦虑－矛盾型和回避型），人格特点（外倾性，宜人性，尽责性，神经质和开放性），这些都是抽象的概念或者说构造，它们都是无法直接观测到的。取而代之的，我们可以用不同的行为反映这些变量。这些观测到的行为变量称为<strong>显变量</strong>，比如测试分数，问卷调查回复以及其它观测到的行为。EFA的目标是找到能最大限度解释显变量方差的隐因子（即潜变量）。</p>
<p>比如在此例中，我们不能直接观测到客户总体满意度的构成，但我们可以通过问卷获知客户对各项具体活动的满意度，然后通过数据分析，尽可能的揭示导致客户总体印象背后的原因，这样可以将资金投入到能最有效改善客户满意度的项目。在本小节中，我们通过EFA进一步探索评分数据下的潜在机制，然后根据得到的隐因子估计比较不同的公司。</p>
<p>EFA的结果是一个因子矩阵，其目标是使一小部分变量对应较高的因子载荷，其余的因子载荷都很低。这样的因子能由少数几个变量解释。其通过旋转正交矩阵改变变量对应的因子载荷，在旋转过程中不改变解释方差。好比小时候切生日蛋糕，总想尽量保持蛋糕上图样的完整，坚决不要把奶油塑成的花对半切开，当然还有生肖图案，每次不得不切开的时候都无比纠结。这个纠结权衡的过程就喝EFA类似，这样旋转切分后得到的结果比随机切分要让强迫症患者舒服的多。其中一些每块蛋糕上面一朵完整的花，有一大块表面是完整的的生肖图案，有的上面是水果等等。这一块块蛋糕就好比隐因子，这些因子的定义不是唯一的，没有一种切分方式严格优于另外一种，但是总有一些比另外一些让你更舒服。对于EFA的结果，总有一些因子结果比另外一些更有用，能更好的解释实际现状。</p>
<p>和PCA相比，能产生可解释和实践的结果是EFA的一大优势。如果我们只对EFA得出的某因子感兴趣，可以保留该因子对应载荷高的问题，舍去其它问题，优化问卷调查。之后在例子中我们会展示EFA还能用来探索调查项相互联系的方式是不是符合我们的期待。可能真实的维度比我们想的少，也可能因子分析结果反映了一些我们不知道的维度。由于EFA是探索性的，没有非黑即白的标准答案，这里一定要特别关注结果的可解释性。如果结果难以解释，得到的因子对实际工作也没有作用。</p>
<p>下面我们还是以航空公司满意度调查的数据为例展示如何应用EFA。其中第一步就是决定要估计的因子数目。通常的方法是通过之前提到的陡坡图，或者根据对应特征值来决定因子数目（大于1的特征值数目）。R包<code>nFactors</code>中的函数<code>nScree()</code>能够应用几种方法通过陡坡检验估计因子数目。</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(nFactors)
<span class="kw">subset</span>(airline,<span class="dt">select=</span>Easy_Reservation:Recommend)%&gt;%
<span class="st">  </span><span class="co"># 转化成数据框格式传递给nScree()函数</span>
<span class="st">  </span><span class="kw">data.frame</span>()%&gt;%
<span class="kw">nScree</span>()</code></pre></div>
<pre><code>##   noc naf nparallel nkaiser
## 1   2   2         2       2</code></pre>
<p>结果显示，当前的4个方法建议的因子数目都是2。我们也可以通过特征值来决定因子数目。<code>eigen()</code>函数可以用来得到特征值：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># 得到变量相关矩阵的特征值</span>
eigenvalue&lt;-<span class="kw">subset</span>(airline,<span class="dt">select=</span>Easy_Reservation:Recommend)%&gt;%
<span class="st">  </span><span class="kw">cor</span>()%&gt;%
<span class="st">  </span><span class="kw">eigen</span>()
eigenvalue$values</code></pre></div>
<pre><code>##  [1] 6.13519681 5.47501966 0.94251965 0.46623604 0.26506447 0.22711198
##  [7] 0.21221230 0.21044797 0.19462556 0.17989084 0.16075997 0.15678382
## [13] 0.15513097 0.13273209 0.08626788</code></pre>
<p>可以看到开始2个特征值都大于1，因此，这里给出的因子数目建议还是2。再次强调，最后模型是不是有效取决于其可解释性。在实际应用中，最好检测多个因子数目。比如，2个或者3个。这里我们使用2个因子。接下来通过<code>factanal()</code>函数拟合EFA模型：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">airline%&gt;%
<span class="st">  </span><span class="kw">subset</span>(<span class="dt">select=</span>Easy_Reservation:Recommend)%&gt;%
<span class="st">  </span><span class="kw">factanal</span>(<span class="dt">factors=</span><span class="dv">2</span>)</code></pre></div>
<pre><code>## 
## Call:
## factanal(x = ., factors = 2)
## 
## Uniquenesses:
## Easy_Reservation  Preferred_Seats   Flight_Options    Ticket_Prices 
##            0.428            0.307            0.335            0.327 
##     Seat_Comfort   Seat_Roominess Overhead_Storage   Clean_Aircraft 
##            0.251            0.164            0.252            0.495 
##         Courtesy     Friendliness      Helpfulness          Service 
##            0.253            0.244            0.248            0.240 
##     Satisfaction        Fly_Again        Recommend 
##            0.152            0.112            0.113 
## 
## Loadings:
##                  Factor1 Factor2
## Easy_Reservation -0.754         
## Preferred_Seats  -0.831         
## Flight_Options   -0.801   0.155 
## Ticket_Prices    -0.819         
## Seat_Comfort              0.863 
## Seat_Roominess   -0.346   0.846 
## Overhead_Storage  0.233   0.833 
## Clean_Aircraft            0.707 
## Courtesy          0.864         
## Friendliness      0.869         
## Helpfulness       0.867         
## Service           0.872         
## Satisfaction              0.921 
## Fly_Again                 0.942 
## Recommend                 0.941 
## 
##                Factor1 Factor2
## SS loadings      5.769   5.309
## Proportion Var   0.385   0.354
## Cumulative Var   0.385   0.739
## 
## Test of the hypothesis that 2 factors are sufficient.
## The chi square statistic is 5416.28 on 76 degrees of freedom.
## The p-value is 0</code></pre>
<p>其中<code>factors=2</code>设置因子数目为2. 我们看看结果输出的载荷<code>Loadings</code>，这是最需要解释的部分。结果中没有显示接近0的因子载荷。在此方案中，因子1对应载荷高的变量为：</p>
<ul>
<li>购票体验相关变量：<code>Easy_Reservation</code>，<code>Preferred_Seats</code>，<code>Flight_Options</code>，<code>Ticket_Prices</code></li>
<li>机舱服务相关变量：<code>Courtesy</code>，<code>Friendliness</code>，<code>Helpfulness</code>，<code>Service</code></li>
</ul>
<p>因子2对应载荷高的变量为：</p>
<ul>
<li>机舱设施相关变量：<code>Seat_Comfort</code>，<code>Seat_Roominess</code>，<code>Overhead_Storage</code>，<code>Clean_Aircraft</code></li>
<li>总体满意度相关变量：<code>Satisfaction</code>，<code>Fly_Again</code>，<code>Recommend</code></li>
</ul>
<p>也就是说这些结果大致反映出两类因子，其中一类和购票体验和机舱服务相关，另外一类和机舱设施和总体满意度相关。回忆之前得到的相关矩阵图，购票体验和机舱服务在图像强烈负相关，这里因子分析将它们看作受同一个因子影响，但载荷符号相反。这里很自然的提出一个实际问题，为什么对购票体验满意度高时对机舱服务满意度就低？这是需要和营销人员沟通的地方。同时，从统计的角度，我们可能会想，如果我们设置3个因子，是否会将购票体验和机舱服务分开呢？下面我们试着将因子个数设置成3个：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">airline%&gt;%
<span class="st">  </span><span class="kw">subset</span>(<span class="dt">select=</span>Easy_Reservation:Recommend)%&gt;%
<span class="st">  </span><span class="kw">factanal</span>(<span class="dt">factors=</span><span class="dv">3</span>)</code></pre></div>
<pre><code>## 
## Call:
## factanal(x = ., factors = 3)
## 
## Uniquenesses:
## Easy_Reservation  Preferred_Seats   Flight_Options    Ticket_Prices 
##            0.233            0.157            0.222            0.173 
##     Seat_Comfort   Seat_Roominess Overhead_Storage   Clean_Aircraft 
##            0.251            0.165            0.253            0.495 
##         Courtesy     Friendliness      Helpfulness          Service 
##            0.219            0.191            0.153            0.161 
##     Satisfaction        Fly_Again        Recommend 
##            0.151            0.111            0.113 
## 
## Loadings:
##                  Factor1 Factor2 Factor3
## Easy_Reservation         -0.318   0.814 
## Preferred_Seats          -0.422   0.815 
## Flight_Options    0.163  -0.419   0.759 
## Ticket_Prices            -0.406   0.813 
## Seat_Comfort      0.864                 
## Seat_Roominess    0.847  -0.286   0.189 
## Overhead_Storage  0.832   0.160  -0.172 
## Clean_Aircraft    0.706                 
## Courtesy                  0.786  -0.403 
## Friendliness              0.813  -0.385 
## Helpfulness               0.854  -0.342 
## Service                   0.842  -0.362 
## Satisfaction      0.921                 
## Fly_Again         0.942                 
## Recommend         0.941                 
## 
##                Factor1 Factor2 Factor3
## SS loadings      5.309   3.452   3.192
## Proportion Var   0.354   0.230   0.213
## Cumulative Var   0.354   0.584   0.797
## 
## Test of the hypothesis that 3 factors are sufficient.
## The chi square statistic is 769.65 on 63 degrees of freedom.
## The p-value is 3.9e-122</code></pre>
<p>结果如我们所料，将之前机舱服务和购票体验对应的因子进一步分开成为两个因子。这个结果和之前基于主成分分析得到的双标图中问题对应箭头方向的聚类情况一致。</p>
<p>了解一些关于EFA背后理论的读者应该知道，因子载荷估计并不是唯一的，我们可以通过旋转产生新的载荷，所谓旋转，就是只用一个矩阵右乘因子载荷矩阵。当你使用的是正交矩阵时，就是正交旋转，否者是斜交旋转。比如有一种常用的正交旋转叫做方差最大正交旋转（varimax rotation），其目的是使得旋转后的各因子载荷的平方按列向0和1两级分化，以便每个因子具有实际的解释，该方法寻找的是不相关的因子。斜交旋转的方法允许因子之间相关。基于之前的讨论可知，购票体验和机舱服务是负相关的，更合理的方法是允许各因子间相关。下面我们用一种常见的斜交变换（<code>oblimin</code>）重复3因子模型：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(GPArotation)
airline%&gt;%
<span class="st">  </span><span class="kw">subset</span>(<span class="dt">select=</span>Easy_Reservation:Recommend)%&gt;%
<span class="st">  </span><span class="kw">factanal</span>(<span class="dt">factors=</span><span class="dv">3</span>,<span class="dt">rotation=</span><span class="st">&quot;oblimin&quot;</span>)</code></pre></div>
<pre><code>## 
## Call:
## factanal(x = ., factors = 3, rotation = &quot;oblimin&quot;)
## 
## Uniquenesses:
## Easy_Reservation  Preferred_Seats   Flight_Options    Ticket_Prices 
##            0.233            0.157            0.222            0.173 
##     Seat_Comfort   Seat_Roominess Overhead_Storage   Clean_Aircraft 
##            0.251            0.165            0.253            0.495 
##         Courtesy     Friendliness      Helpfulness          Service 
##            0.219            0.191            0.153            0.161 
##     Satisfaction        Fly_Again        Recommend 
##            0.151            0.111            0.113 
## 
## Loadings:
##                  Factor1 Factor2 Factor3
## Easy_Reservation                  0.941 
## Preferred_Seats                   0.880 
## Flight_Options    0.167           0.803 
## Ticket_Prices                     0.887 
## Seat_Comfort      0.865                 
## Seat_Roominess    0.844  -0.242         
## Overhead_Storage  0.833   0.137  -0.142 
## Clean_Aircraft    0.708                 
## Courtesy                  0.818         
## Friendliness              0.868         
## Helpfulness               0.953         
## Service                   0.922         
## Satisfaction      0.921                 
## Fly_Again         0.943                 
## Recommend         0.942                 
## 
##                Factor1 Factor2 Factor3
## SS loadings      5.316   3.285   3.135
## Proportion Var   0.354   0.219   0.209
## Cumulative Var   0.354   0.573   0.782
## 
## Factor Correlations:
##         Factor1 Factor2 Factor3
## Factor1  1.0000 -0.0494  0.0188
## Factor2 -0.0494  1.0000 -0.7535
## Factor3  0.0188 -0.7535  1.0000
## 
## Test of the hypothesis that 3 factors are sufficient.
## The chi square statistic is 769.65 on 63 degrees of freedom.
## The p-value is 3.9e-122</code></pre>
<p>从载荷中可以看到，允许因子之间相关更好的区分了购票体验和机舱服务，这样能够更加清晰的解释这两个感知项。这里就牵扯到一个问题，我们如何决定因子是该独立还是相关呢？你可能觉得这是数据决定的。但其实各因子间的相关性不是一个数据问题，而是你对潜因子的构想，或者假设。从认知概念的角度，你觉得在当前语境下因子独立有意义还是相关有意义？</p>
<p>因子分析在特征提取方面的优势体现在下面3个方面：</p>
<ol style="list-style-type: decimal">
<li>我们可以用维度更低的因子分值取代大量原始调查问题</li>
<li>将反映相同认知维度的问卷分值联合起来比使用其中任何一项含有更多的信息。也在某种程度上缓解单个变量评分中的不确定性。从人脑认知的角度看，我们的购买决策常取决于一个总体的印象，而我们自己也无法解释其中具体的组成成分。所以在比较不同的公司或品牌时，应该关注总体感知。</li>
<li>通常人们会有一些感兴趣的因子，通过因子载荷，我们可以删除贡献小的变量从而简化收集过程。</li>
</ol>
</div>
<div id="section-9.2.4" class="section level3">
<h3><span class="header-section-number">9.2.4</span> 理论背景</h3>
<p>之前只介绍如何应用主成分分析和因子分析。虽然我尽量用非技术的语言解释模型背后的思想，但要很好的解释模型结果，还是需要对理论背景有所了解。本小节主要介绍这两种方法背后的数学理论，建议有一定数学基础的读者能够花些时间理解这些知识。</p>
<div id="-1" class="section level4">
<h4><span class="header-section-number">9.2.4.1</span> 主成分分析</h4>
<p>假设我们有随机变量组成的向量<span class="math inline">\(\mathbf{X}=[X_{1},X_{2},...,X_{p}]^{T}\)</span>，其对应的协方差矩阵为<span class="math inline">\(\Sigma\)</span>。</p>
<p>主成分分析的目的就是通过对原变量进行线性组合找到<strong>彼此不相关</strong>的新变量 <span class="math inline">\((Z_{1} , Z_{2} , \ldots , Z_{p})\)</span>，依次最大化每个变量的方差:</p>
<p><span class="math display">\[
\begin{array}{ccccc}
Z_{1} &amp; = &amp; \mathbf{a_{1}^{T}X} &amp; = &amp; a_{11}X_{1}+a_{12}X_{2}+\cdots+a_{1p}X_{p}\\
Z_{2} &amp; = &amp; \mathbf{a_{2}^{T}X} &amp; = &amp; a_{21}X_{1}+a_{22}X_{2}+\cdots+a_{2p}X_{p}\\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots\\
Z_{P} &amp; = &amp; \mathbf{a_{p}^{T}X} &amp; = &amp; a_{p1}X_{1}+a_{p2}X_{2}+\cdots+a_{pp}X_{p}
\end{array}
\]</span></p>
<p>其中：</p>
<p><span class="math display">\[Var(Z_{i})=\mathbf{a_{i}^{T}\Sigma a_{i}},\ \ \ \ \ \ \ \ \ i=1...p\]</span> <span class="math display">\[Cov(Z_{i},Z_{k})=\mathbf{a_{i}^{T}\Sigma a_{k}},\ \ \ \ \ \ \ \ \ i,k=1...p\]</span></p>
<p>从几何学上讲，主成分分析中生成各个主成分的过程可以看成是将<span class="math inline">\((X_{1},X_{2},\ldots,X_{p})\)</span>定义的坐标系旋转成新坐标系。新坐标的坐标轴代表了观测方差最大的方向。主成分的推导依赖于 <span class="math inline">\(\mathbf{X}\)</span>的相关矩阵<span class="math inline">\(\Sigma\)</span>，这是典型的无监督学习，并且不要求变量服从多元正态分布。PCA过程按如下依次寻找主成分：</p>
<ol style="list-style-type: decimal">
<li>在控制条件<span class="math inline">\(\mathbf{a_{1}^{T}a_{1}}=1\)</span>下，通过最大化方差<span class="math inline">\(Var(Z_{1})=\mathbf{a_{1}^{T}\Sigma a_{1}}\)</span>得到第一个主成分<span class="math inline">\(Z_{1}\)</span></li>
<li>在控制条件<span class="math inline">\(\mathbf{a_{2}^{T}a_{2}}=1\)</span>和<span class="math inline">\(Cov(Z_{1},Z_{2})=0\)</span>下，通过最大化<span class="math inline">\(Var(Z_{1})=\mathbf{a_{1}^{T}\Sigma a_{1}}\)</span>得到第二个主成分<span class="math inline">\(Z_{2}\)</span></li>
<li>依次类推，对第i个主成分，在控制条件<span class="math inline">\(\mathbf{a_{i}^{T}a_{i}}=1\)</span>和<span class="math inline">\(Cov(Z_{i},Z_{k})=0\)</span>（任意<span class="math inline">\(k&lt;i\)</span>）下，通过最大化<span class="math inline">\(Var(Z_{i})=\mathbf{a_{i}^{T}\Sigma a_{i}}\)</span>得到第二个主成分<span class="math inline">\(Z_{i}\)</span></li>
</ol>
<p>对于上面第1条，可以证明 <span class="math display">\[Var(Z_{1})=\mathbf{a_{1}^{T}\Sigma a_{1}}=\underset{\mathbf{e^{T}e=1}}{max}\mathbf{e^{T}\Sigma e}=\lambda_{1}\]</span></p>
<p>其中，<span class="math inline">\(\lambda_{1}\geqslant\lambda_{2}\geqslant...\geqslant\lambda_{p}\)</span>是<span class="math inline">\(\Sigma\)</span>的特征值， <span class="math inline">\(\mathbf{a_{1},a_{2},...,a_{p}}\)</span>是相应的单位正交特征向量。证明需要用到谱分解定理，但并不复杂。</p>
<p>由于主成分分析基于变量方差矩阵<span class="math inline">\(\Sigma\)</span>，我们需要特别注意各个变量的标度，如果差别很大（比如年龄和收入），那需要将观测标准化后再进行分析。关于如何用R进行数据标准化，我们在之前数据预处理的部分已经详细介绍过了。当前例子中所有问题的回复都在1-9分的量表上，所以是不是标准化差别不太大。</p>
</div>
<div id="section-9.2.4.2" class="section level4">
<h4><span class="header-section-number">9.2.4.2</span> 因子分析</h4>
<p>因子分析是研究相关阵或者协方差矩阵的内部关系，它将多个变量综合为少数几个因子，以再现原始变量和因子之间的相关关系。因子分析和主成分分析有联系，是其推广和发展。1904年Charles Spearman指出，如果第i个变量（第i门功课）上的分数由两部分组成的：</p>
<p><span class="math display">\[X_{i}=l_{i}F+\epsilon_{i}\]</span></p>
<p>其中F是对所有变量都起作用的公因子，那么就可以说明各门功课相关的“效应”。每门课程的考试成绩可以看作由一个公因子（智力因子）和一个特殊因子之和组成。这是最早的最简单的因子模型。可以将这个因子模型进一步推广到多个因子的情况，即全体科目所共有的因子有m个，如数学推导因子、记忆因子、计算因子等，分别记为<span class="math inline">\(F_{1},...,F_{m}\)</span>：</p>
<p><span class="math display">\[
\begin{array}{ccc}
X_{1}-\mu_{1} &amp; = &amp; l_{11}F_{1}+l_{12}F_{2}+\cdots+l_{1m}F_{m}+\epsilon_{1}\\
X_{2}-\mu_{2} &amp; = &amp; l_{21}F_{1}+l_{22}F_{2}+\cdots+l_{2m}F_{m}+\epsilon_{2}\\
\vdots &amp; \vdots &amp; \vdots\\
X_{p}-\mu_{p} &amp; = &amp; l_{p1}F_{1}+l_{p2}F_{2}+\cdots+l_{pm}F_{m}+\epsilon_{p}
\end{array}
\]</span></p>
<p>其中<span class="math inline">\(l_{ij}\)</span>就是因子载荷，第i个变量在第j个因子上的载荷。<span class="math inline">\(\mathbf{X}=[X_{1},...,X_{p}]^{T}\)</span>是p个观测属性组成的向量，均值向量为<span class="math inline">\(\mathbf{\mu}\)</span>，协方差矩阵为<span class="math inline">\(\Sigma\)</span>。用矩阵标记表示就是：</p>
<p><span class="math display">\[\mathbf{(X}-\mathbf{\mu)_{p\times1}}=L_{p\times m}\mathbf{F}_{m\times1}+\mathbf{\epsilon_{p\times1}}\]</span></p>
<p>其中<span class="math inline">\(L\)</span>是因子载荷矩阵，<span class="math inline">\(\mathbf{F}\)</span>是<span class="math inline">\(m\)</span>个无法观测到的潜因子组成的向量。这里模型看上去有点像一般线性回归。由于在这里等式右边是无法观测到的，如果我们没有其它的限制条件的话无法给出任何载荷估计。正交因子模型假设：</p>
<p><span class="math display">\[E(\mathbf{F})=\mathbf{0}\]</span> <span class="math display">\[Var(\mathbf{F})=E(\mathbf{FF^{T}})=I\]</span> <span class="math display">\[E(\mathbf{\epsilon})=\mathbf{0}\]</span> <span class="math display">\[Var(\mathbf{\epsilon})=E(\mathbf{\epsilon\epsilon^{T}})=\Psi=diag(\psi_{i}),\ \ i=1,\cdots,p\]</span> <span class="math display">\[cov(\mathbf{F},\mathbf{\epsilon})=0\]</span></p>
<p>这里假设每个因子的方差都是1不是一个限制条件，因为我们总是可以通过放大缩小因子载荷进行调整。正交因子模型真正的限制条件是假设各个因子之间不相关，且共因子和特殊因子之间不相关。这些假设暗示了<span class="math inline">\(\Sigma\)</span>的结构。如果：</p>
<p><span class="math display">\[\mathbf{(X}-\mathbf{\mu)_{p\times1}}=L_{p\times m}\mathbf{F}_{m\times1}+\mathbf{\epsilon_{p\times1}}\]</span></p>
<p>那么：</p>
<p><span class="math display">\[
\begin{array}{ccc}
\mathbf{(X}-\mathbf{\mu)\mathbf{(X}-\mathbf{\mu)^{T}}} &amp; = &amp; (L\mathbf{F}+\mathbf{\epsilon})(L\mathbf{F}+\mathbf{\epsilon})^{T}\\
 &amp; = &amp; (L\mathbf{F}+\mathbf{\epsilon})(\mathbf{F^{T}L^{T}}+\mathbf{\epsilon^{T}})\\
 &amp; = &amp; \mathbf{LFF^{T}L^{T}+LF\epsilon^{T}+\epsilon F^{T}L^{T}+\epsilon\epsilon^{T}}
\end{array}
\]</span></p>
<p>对上述等式两边取期望可以得到：</p>
<p><span class="math display">\[
$\begin{array}{ccc}
\Sigma &amp; = &amp; E\{\mathbf{(X}-\mathbf{\mu)\mathbf{(X}-\mathbf{\mu)^{T}}}\}\\
 &amp; = &amp; E\{LFF^{T}L^{T}+LF\epsilon^{T}+\epsilon F^{T}L^{T}+\epsilon\epsilon^{T}\}\\
 &amp; = &amp; \mathbf{LL^{T}+\Psi}
\end{array}$
\]</span></p>
<p>因为 <span class="math inline">\(E(\mathbf{FF^{T}})\text{＝Var(\mathbf{F})}=I\)</span>并且<span class="math inline">\(\ensuremath{E(\mathbf{\epsilon^{T}F})=cov(\mathbf{F},\mathbf{\epsilon})=0}\)</span>。此外还有：</p>
<p><span class="math display">\[\text{\ensuremath{\mathbf{(X-\mu)F^{T}=LFF^{T}+\epsilon F^{T}}}}\]</span> <span class="math display">\[cov(\mathbf{X,F})=E[(\mathbf{X-\mu})F^{T}]=L\]</span></p>
<p>因此在正交因子模型中，进一步有：</p>
<p><span class="math display">\[Var(X_{i})=\sigma_{ii}=l_{i1}^{2}+l_{i2}^{2}+\cdots+l_{im}^{2}+\psi_{i}\]</span> <span class="math display">\[Cov(X_{i},X_{K})=\sigma_{ik}=l_{i1}l_{k1}+l_{i2}l_{k2}+l_{im}l_{km}\]</span></p>
<p>我们将方差<span class="math inline">\(\sigma_{i}\)</span>分解成两部分：</p>
<p><span class="math display">\[\sigma_{ii}=l_{i1}^{2}+l_{i2}^{2}+\cdots+l_{im}^{2}+\psi_{i}=h_{i}^{2}+\psi_{i}\]</span></p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(h_{i}^{2}\)</span>是全部公因子对变量<span class="math inline">\(X_{i}\)</span>的总方差所作出的贡献，称为共因子方差（共同度）</li>
<li><span class="math inline">\(\psi_{i}\)</span>由特殊因子<span class="math inline">\(\epsilon_{i}\)</span>产生的方差，它仅与变量<span class="math inline">\(X_{i}\)</span>，也称为剩余方差</li>
</ol>
<p><span class="math inline">\(h_{i}^{2}\)</span>反映了变量<span class="math inline">\(X_{i}\)</span>对公因子F依赖程度。</p>
<!-- 
3.1 特征选择Feature Selection

首先，从特征开始说起，假设你现在有一个标准的Excel表格数据，它的每一行表示的是一个观测样本数据，表格数据中的每一列就是一个特征。在这些特征中，有的特征携带的信息量丰富，有的（或许很少）则属于无关数据（irrelevant data），我们可以通过特征项和类别项之间的相关性（特征重要性）来衡量。比如，在实际应用中，常用的方法就是使用一些评价指标单独地计算出单个特征跟类别变量之间的关系。如Pearson相关系数，Gini-index（基尼指数），IG（信息增益）等，下面举Pearson指数为例，它的计算方式如下：


r2xy=(con(x,y)var(x)var(y)−−−−−−−−−−√)
其中，x属于X，X表一个特征的多个观测值，y表示这个特征观测值对应的类别列表。

Pearson相关系数的取值在0到1之间，如果你使用这个评价指标来计算所有特征和类别标号的相关性，那么得到这些相关性之后，你可以将它们从高到低进行排名，然后选择一个子集作为特征子集（比如top 10%），接着用这些特征进行训练，看看性能如何。此外，你还可以画出不同子集的一个精度图，根据绘制的图形来找出性能最好的一组特征。

这就是特征工程的子问题之一——特征选择，它的目的是从特征集合中挑选一组最具统计意义的特征子集，从而达到降维的效果。

做特征选择的原因是因为这些特征对于目标类别的作用并不是相等的，一些无关的数据需要删掉。做特征选择的方法有多种，上面提到的这种特征子集选择的方法属于filter（刷选器）方法，它主要侧重于单个特征跟目标变量的相关性。优点是计算时间上较高效,对于过拟合问题也具有较高的鲁棒性。缺点就是倾向于选择冗余的特征,因为他们不考虑特征之间的相关性,有可能某一个特征的分类能力很差，但是它和某些其它特征组合起来会得到不错的效果。另外做特征子集选取的方法还有wrapper（封装器）和Embeded(集成方法)。wrapper方法实质上是一个分类器，封装器用选取的特征子集对样本集进行分类，分类的精度作为衡量特征子集好坏的标准,经过比较选出最好的特征子集。常用的有逐步回归（Stepwise regression）、向前选择（Forward selection）和向后选择（Backward selection）。它的优点是考虑了特征与特征之间的关联性，缺点是：当观测数据较少时容易过拟合，而当特征数量较多时,计算时间又会增长。对于Embeded集成方法，它是学习器自身自主选择特征，如使用Regularization做特征选择，或者使用决策树思想，细节这里就不做介绍了。这里还提一下，在做实验的时候，我们有时候会用Random Forest和Gradient boosting做特征选择，本质上都是基于决策树来做的特征选择，只是细节上有些区别。

综上所述，特征选择过程一般包括产生过程，评价函数，停止准则，验证过程，这4个部分。如下图所示：


这里写图片描述 
(1) 产生过程( Generation Procedure )：产生过程是搜索特征子集的过程，负责为评价函数提供特征子集。搜索特征子集的过程有多种，将在2.2小节展开介绍。 
(2) 评价函数( Evaluation Function )：评价函数是评价一个特征子集好坏程度的一个准则。评价函数将在2.3小节展开介绍。 
(3) 停止准则( Stopping Criterion )：停止准则是与评价函数相关的，一般是一个阈值，当评价函数值达到这个阈值后就可停止搜索。 
(4) 验证过程( Validation Procedure ) ：在验证数据集上验证选出来的特征子集的有效性。

3.2 特征提取

特征提取的子问题之二——特征提取。



4、特征工程处理过程

那么问题来了，特征工程具体是在哪个步骤做呢？

具体的机器学习过程是这样的一个过程：


我们发现，特征工程和数据转换其实是等价的。事实上，下面是特征工程的一个迭代过程：

1.头脑风暴式特征：意思就是进你可能的从原始数据中提取特征，暂时不考虑其重要性，对应于特征构建；
2.设计特征：根据你的问题，你可以使用自动地特征提取，或者是手工构造特征，或者两者混合使用；
3.选择特征：使用不同的特征重要性评分和特征选择方法进行特征选择；
4.评估模型：使用你选择的特征进行建模，同时使用未知的数据来评估你的模型精度。
By the way, 在做feature selection的时候，会涉及到特征学习（Feature Learning），这里说下特征学习的概念，一般而言，特征学习（Feature Learning）是指学习输入特征和一个训练实例真是类别之间的关系。



-->
</div>
</div>
<div id="section-9.2.5" class="section level3">
<h3><span class="header-section-number">9.2.5</span> 高维标度化</h3>
</div>
</div>
<div id="section-9.3" class="section level2">
<h2><span class="header-section-number">9.3</span> 变量选择</h2>
<p>我们建立统计模型常常会对下面的几个问题感兴趣：</p>
<ol style="list-style-type: decimal">
<li>模型拟合情况如何？</li>
<li>模型在新样本上预测的情况如何？</li>
<li>所有的自变量都有助于解释应变量（<span class="math inline">\(\mathbf{y}\)</span>），还是只有其中部分重要的自变量？</li>
</ol>
<p>回答这三个问题的共同前提是得先有一个评判模型“好”和“坏”的标准。前两个问题在建模技术那章已经给出了一般性的解答，通常使用数据划分和再抽样的方式，根据建模目的不同选择不同的评判标准，在此基础上检验模型拟合和预测情况。这章我们主要回答第3个问题。虽然所有自变量对于解释因变量来说都是重要的这样的情况可能发生，但更常见的是因变量只和一部分自变量有关。</p>
<!--作为展示，下面我们基于刚才建立的线性模型用`stepAIC()`进行变量选择（这里省略了模型选择过程的输出）。

```r
library(MASS)
stepAIC(fit)
```

最后模型选出的模型如下：


```r
fit2<-lm(formula = income ~ store_exp + online_exp + store_trans + online_trans + age, data = sim.dat)
summary(fit2)
```

```
## 
## Call:
## lm(formula = income ~ store_exp + online_exp + store_trans + 
##     online_trans + age, data = sim.dat)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -124039  -15317     967   12917  152086 
## 
## Coefficients:
##                Estimate Std. Error t value Pr(>|t|)    
## (Intercept)  67274.7949  6449.7606  10.431  < 2e-16 ***
## store_exp        3.1906     0.4722   6.757 2.70e-11 ***
## online_exp       9.5312     0.9018  10.570  < 2e-16 ***
## store_trans   4420.0094   437.8654  10.094  < 2e-16 ***
## online_trans -1026.6919   215.9450  -4.754 2.36e-06 ***
## age            327.8788    94.8446   3.457 0.000575 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 31320 on 810 degrees of freedom
##   (184 observations deleted due to missingness)
## Multiple R-squared:  0.6076, Adjusted R-squared:  0.6052 
## F-statistic: 250.8 on 5 and 810 DF,  p-value: < 2.2e-16
```

和原模型相比，该过程 -->

</div>
</div>
<h3> References</h3>
<div id="refs" class="references">
<div id="ref-spearman1904">
<p>Spearman, C. 1904. “‘General Intelligence,’ Objectively Determined and Measured.” <em>The American Journal of Psychology</em> 15 (2): 201–92.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="section-8.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="section-10.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/09-tezhenggongcheng.Rmd",
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
