[
["index.html", "数据科学家：R语言 第1章 介绍", " 数据科学家：R语言 林荟 2016-07-15 第1章 介绍 数据科学家目前是北美最热门的职业之一，平均年薪突破10万美元。但数据科学并不是一个低门槛的行业，所有数据科学家都会认同这一点。除了对数学，统计，计算机等相关科学领域的学位要求以外，还要需要相关应用领域的知识。这个职业听起来很酷，但如果你对数据分析没有兴趣的话，你也会觉得这个行业很苦。这里我默认本书的读者都至少是对这个行业有兴趣和激情的。本书的写作对象是那些现在从事数据分析相关行业，或者之后想从事数据分析行业的人，意在为实践者提供数据科学家这门职业的相关信息。读者可以从阅读中了解到数据科学家需要的技能，及背后的“分析哲学”。书中会对部分最常用，有效的模型加以展开。关于模型技术部分，我希望读者有初步统计知识，最好知道线性回归。 数据科学家这个行业的本质是应用。市面上有很多文章，出版物介绍各种数据模型，大多数此类书籍并不能让读者重复书中所述的分析过程，对于书中介绍的知识，读者真正实践起来会遇到很多困难。本书着重在于数据科学的实际应用，让读者能够重复书中的结果，这也用到了统计软件R的自动化报告功能。可能有读者会问，为什么要可重复？根据个人经验，学习数据分析技能最好的方式是实践：动手重复分析的过程，检查分析结果，发现问题后再去查询相关模型的背景技术知识。这一过程得到的学习效果远远超过死磕一本大部头的技术理论书籍。磕了一年之后发现碰到实际问题不知道该用什么工具实践这些书中讲到的模型方法。而且对于新手而言，一开始就直奔艰深的理论，很容易因为困难而失去兴趣最终放弃。本书倡导的是一种循序渐进的启发性教学路径，从实际问题入手，抽丝剥茧进入技术内核。此外，分析师通常需要写很多分析报告，描述分析结果。这样的报告如果可以重复的话将极大提高工作效率。我们会在“数据可视化和结果展示”这一章中对此做更详细的介绍。 本书主要部分将避免过多的数学公式，但难免有例外。我们在一些地方提到方法背后的技术细节是为了帮助读者理解模型的长处和弱点，而非单纯的介绍数理统计知识。这并不意味着这些数理背景知识不重要，相反尽可能多的了解模型背后的数学重要且有意义，为了平衡理论和应用，我们会在有的章中加一些选学小节，用来介绍更多的模型数理背景或给出必要的参考资料来源，如果不感兴趣的读者可以跳过这些小节，不会影响本书主要部分的阅读。书中的每一章都只是冰山一角，我并不试图彻底的介绍模型，而是有选择性的解释其中部分我觉得重要的地方。我会尽量将想要强调的概念和内容在分析数据的过程中体现出来，而不仅仅是数学公式符号表达。想要成为数据科学家，仅靠阅读本书是远远不够的，读者需要进一步查阅书中提到的参考资料，或者选修相关课程。 随着计算机科学的发展，不仅收集存储的数据增加了，分析数据的软件包也不断推陈出新，这极大的降低了应用统计学习方法的壁垒。现在不管会建模的不会建模的，大都听过线性回归，这个经典统计模型可追根溯源至19世纪Legendre和Gauss发表的若干关于最小二乘的论文。现在你要通过最小二乘拟合一个线性模型那就动动指头两秒钟的事情。可在那个计算器都没有的时代，能优化误差平方和这样的东西的大牛都会被认为是火星人。那会美国宪法规定每十年必须进行一次人口普查，1880年排山倒海的普查资料花了8年时间处理分析，一个名叫Herman Hollerith的品学兼优的美国少年跳出来，在1890年发明了一种排序机，利用打孔卡储存资料，再由机器感测卡片，协助人口调查局对统计资料进行自动化制表，结果不出3年就完成了户口普查工作，Herman同学也顺带用这个发明拿个了工程学博士学位。你可能要问，计算能力这么落后那这伙数学家捣鼓出来的方法谁用？天文学家用。线性模型最早用在天文学研究中。研究使用统计方法的，那会绝对是小众边缘群体，全都可以贴上火星制造的标签。然后盼星星盼月亮我们终于在1912年6月等到了图灵这个天才的降临。 若不是图灵这个孩子被性取向拖了后腿，数据科学家这个行业早几十年可能就火了。当然，统计泰斗们也没有闲着，Fisher在1936年提出了线性判别分析。在1940s，又一家喻户晓的经典统计模型——逻辑回归——问世了！在1970s早期，Nelder和Wedderburn发明了广义线性模型这个词，这是一个更大的统计模型框架，它将随机分布函数和系统效应（非随机效应）通过一个连接函数（link function）连起来，之前的线性模型和逻辑回归都是该框架下的特例。到1970s末，可以用来分析数据的方法已经有好些了，但这些方法几乎都是线性模型，因为在那时，拟合非线性关系的计算量相对当时计算机水平来说还是太大了。等到1980s，计算机技术终于发展到可以使用非线性模型了。Breiman, Fridman, Olshen和Stone提出了分类回归树。随后的一些机器学习方法进一步丰富了数据科学家可以使用的工具集。计算机软件的飞速发展使得这些方法模型得以应用在更加广泛的领域，应用涵盖了商业，健康，基因，社会心理学研究和政策分析等等。数据科学家这个行业随着数据量的增加和分析软件的进步不断向前发展。 关于分析软件，本书使用R。选择R语言的原因如下： R免费，且可以在不同操作系统上使用。 R开源、可扩展：它在通用公共许可（General Public License）下发行，在此构架下任何人可以检查修改源程序。含有很多最新的模型。 R有强大图形可视化和自动化报告功能 笔者10年使用R的经验证明无论在学术还是业界，这都是非常有效的工具。 网上有大量的R入门教程，关于用R进行数据分析的书也有好些，所以这里就不重复造轮子，不熟悉R的读者可以先学习相关资料，这里我假设读者已经有一定R语言基础。 本书布局如下，本书第2章，第3章主要介绍数据科学家这个行业和“分析哲学”和数据分析的一般流程。这是非技术的部分，但对于从业者来说非常重要，它帮助你对这个职业设定一个合理的预期。其中会讨论数据科学家需要的技能。之后的章节会对这里提到的部分我觉得重要的技能进一步展开讨论，由于篇幅所限，不可能详细讨论开始这几章中提到的所有技能。第4章开始进入技术部分，本章讲的是分析环节的第一步——数据预处理，这一步虽然不是正式建模，但却是整个分析过程中最耗时的一个环节。这步没有到位将严重影响模型质量。也正是因为预处理重要，所以单独作为一个章节，没有和第5章其它建模技术合并起来。第5章介绍的是一些在建模过程中需要的辅助性的技术以及建模需要注意的问题。第6章到第10章正式介绍各种笔者在从业过程中经常用到的模型。后面几个章节末尾有比较完整的案例，案例放在这些章节是因为其中主要的模型方法是该章节的主题，但些案例都综合性的用到之前提到的许多方法，对读者理解模型应用和建模背后的思路很有帮助。 本书用来展示模型的数据大部分是通过R得到的模拟数据集。为什么用模拟数据而不是真实数据呢？原因如下： 你可以控制数据生成过程，免去了传输下载数据的麻烦。 你可以根据需要改变生成数据的代码，得到新的数据，观察数据变化对模型结果的影响。 对于自己创建的数据，我们知道数据要表达的真实信息，那么就可以评估分析使用的模型的准确性，然后再用于真实数据。 可以通过使用模拟数据在拿到真实数据前准备好代码模板，这样，当你有真实数据时就可以迅速进行分析。 通过重复数据模拟的过程可以加深对模型假设的理解。 同一章后面的代码通常建立在之前代码上，但每章的代码自成系统，也就是说你不需要以其它章节代码运行结果为前提重复某章的代码。有一定R语言基础的读者可以通过学习生成数据的代码了解数据的结构，以及模型假设。R语言的新手学习这些代码可能会觉得太困难，没有关系，你们可以跳过生成数据的细节，只需要了解数据的语境，都有哪些变量以及变量类型。你可以直接从网站上读取这些数据。书中的代码和数据可以在这个github页面上找到：https://github.com/happyrabbit/happyrabbit.github.com/tree/master/Book/DataScientistR 现在开始我们的旅程吧！ "],
["section-2.html", "第2章 数据科学 2.1 什么是数据科学？ 2.2 什么是数据科学家？ 2.3 数据科学家需要的技能 2.4 数据科学可以解决什么问题？", " 第2章 数据科学 数据科学和数据科学家成为了流行词汇。当有人问你干什么，你回答说数据科学家。对方会恍然大悟，觉得特别高大上，奥，数据科学家啊，听说过。是啊，没听说过数据科学家那就out了。如果接着问，数据科学家具体干什么的？然后就没有然后了。不知道你们有没有听过这样一则轶事，美国最高法院法官Potter Stewart被问到什么是淫秽时，他回答：“看下才知道。”这和数据科学很类似，很多概念，在大而化之的时候都可以存在，大家口耳相传，聊的不亦乐乎，但一追究细节，立即土崩瓦解。那么什么是数据科学家呢？我谷歌了一下数据科学家的定义，下面是其中的一些： 住在加州的数据分析师 数据科学家是商业（数据）分析师的进化版 比软件学家更懂统计，比统计学家更懂软件科学的人 拥有出众数据分析能力的BI咨询师，尤其是能用大量数据增加商业竞争力的人 会编程，懂统计，能通过多种方式从数据中掘金的人 此外，很多其它职位其职责都和“从数据中获取信息”有关，比如：数据分析师，BI咨询师，统计学家，金融分析师、商业分析师，预测分析师……这些不同职业有什么区别？即便都是数据科学家，教育背景等等也是千差万别。由于媒体的炒作以及对“数据科学家”这个名称的滥用，尽管总的分析行业正在飞速发展，但大家对这个行业从业人员的认识却越来越混乱。现在大部分商业领域所谓的分析都达不到“科学”的程度，而仅仅是加减乘除的游戏。这些不同的职位要求有何不同？在北美总体说来： 金融分析师一般有金融方向的MBA学位。他／她会用电子表格，知道会计软件，分析各部门的预算数据，分析实际经营结果和预测之间的差别，做一些预测，但这里的预测不会涉及复杂的机器学习，统计模型。 数据分析师一般有MBA学位，有一些计算机背景，很擅长使用电子表格，会用高阶的电子表格编程功能如VBA，自定义函数，宏。根据情况，会使用一些BI的软件，如Tableau，主要都是用鼠标点拖的方式。会用SQL从数据库中读取数据。我所见的商业分析师拥有很少（或没有）统计知识。所以这部分人有处理数据的知识，但是没有统计学的知识，能做的分析非常有限。 统计学家一般多在药厂，生物技术公司，做一些非常传统的混合效应模型，方差分析等生物统计分析。由于行业要求，多用SAS而非开源软件R。 BI咨询师，一般也是工商管理专业，有MBA学位，受传统的商学院教育（熟悉4Ps或6Ps,4Cs,使用SWOT法分析市场），熟练使用电子表格，很少或没有其它技术背景。 数据科学家，多是数学／统计，计算机，工程学专业出身，会使用R,Python等多种编程语言，熟悉数据可视化。大多数在入职前没有太多市场营销知识。掌握高等概率统计，熟悉如下概念：抽样，概率分布，假设检验，方差分析，拟合优度检验，回归，时间序列预测模型，非参数估计，实验设计，决策树，马尔可夫链，贝叶斯统计（很快就能在白板上写下贝叶斯定理） 数据科学家都分布在哪些行业呢？根据Burtch Works Executive Recruiting在2015年4月发布的“数据科学家薪资调查报告”，科技公司（包括互联网）是数据科学家最大的雇主。其次是一些为其它公司提供如广告，市场调查，市场分析等商业服务的公司。这两者之和超过了50%。2014年创业公司雇佣了29.4%的数据科学家，2015年这个比例降至14.3%，原因不是创业公司招的数据科学家职位少了，而是大公司招入的数据科学家增长迅速，整体基数变大。总体来说数据科学家就业前景在北美是非常好的。调查还显示，在北美，大部分数据科学家（70%）工作经验小于10年。因此数据科学还是个很年轻的行业。现在，大家对数据科学领域应该有个大致的感觉了。下面我们对其进一步探讨。 2.1 什么是数据科学？ 50年前，John Tukey他老人家就预言有个类似今天的数据科学的东西会出现。早在1962年，他在“数据分析的未来（The Future of Data Analysis）”中(Tukey 1962)就嚷着要对学术统计进行改革。这篇文章当时发表在“数理统计年鉴（The Annals of Mathematical Statistics）”上，他的观点震惊了许多统计界的同事，这都是一群根正苗红的数理统计出身的大神们，那会数理统计年鉴中的文章都是满满的数学公式推导，从定义，定理到证明，逻辑缜密，理论精确。当然牛人最大的特点就是可以随时任性。John推了大半辈子公式突然有天发现统计不是这么玩的，于是他跳出来说： “很长一段时间我觉得自己是统计学家，对统计推断情有独钟，将从小样本上研究得到的结论推广到更大的群体。但随着数理统计的发展，我越发觉得这个路数不大对…总的来说，我觉得自己感兴趣的是数据分析，它包括：分析数据的过程，解释该过程得到结果的技术，合理计划收集数据的方案使得之后的分析过程更方便准确，以及所有分析中需要用到的仪器和数学理论。” 用简短的一句话概括就是：仅仅研究数学理论不是数据科学，数据科学的内容涵盖更广。 美国密歇根大学在2015年9月宣布了一个1亿美金的“数据科学项目（Data Science Initiative）”，计划在未来4年聘请35名新教授，支持与数据相关的跨学科研究。大学媒体大胆的宣称： “数据科学已经成为第4大科学发现手段，前三个为：实验，模型和计算。” 这里的数据科学指的是什么？该项目的网站上有如下对数据科学的描述： “数据科学是科学发现和实践的结合，其包括对大量类型各异的数据进行收集，管理，清理，分析，可视化和结果解释。其应用遍及各种科学，平移和交叉领域。” 如前所述，数据科学是一个新兴领域。在美国，对数据分析类专业人才的需求不断上升。研究估计 (Berkeley 2015)，从2015到2018年，美国预计有400-500万工作岗位要求数据分析技能，大部分这些岗位的人才需要经过特殊训练。前面已经介绍过各种和数据分析相关的行业，这些行业对专业训练的要求参差不齐。其中数据科学家的门槛是最高的。成为一个数据科学家不是容易的事。不可否认，即使是数据科学家这个职业名称，当前也被滥用了。这些工作的本质都是从数据中获取信息。但不是每个都能称为“科学”。什么样的东西能够称为科学？我们看看John Tukey在50年前是怎么说的(Tukey 1962)： 怎样才能称为科学呢？回答因人而异。但下面3点大多数人都同意： 1. 学术知识（intellectual content） 2. 用能让人理解的方式组织起来 3. 实践是检验其结果的最终标准 也就是说，数据分析要通过上面3条检验才能称为数据科学。我是这样定义数据科学的： 数据科学=数据+科学=从数据中获取信息的科学 这是一门新的科学，有各种因素推动了这门科学的产生。John提到了4个驱动因素： 正统统计学理论 计算机和电子显示设备的高速发展 很多领域内更多更大的数据提出的挑战 定量分析在更广的领域受到重视 很难想象这些观点是在1962年提出的，现在看来一点也不过时。7年之后，Tukey和Wilk在1969年又将这门科学和已经存在的科学进行对比，进一步限定了统计学在数据科学中所扮演的角色： “…数据科学是一个困难的领域。它需要和人们能用数据做什么和想用数据做什么这样的外在条件相适应。从某种意义上说，生物比物理困难，行为科学比这两者都难，很可能总体数据科学的问题比这三者还要难。无论在现在还是短期的将来，要建立一个正式的能够给数据分析实践提供高效指导的数据科学的结构还有很长的路要走。数据科学可以从正规正统统计学那里获得很多，但它们之间也需要保持适当的距离。” 数据科学不仅是个科学领域，而且和其它已经存在很久的科学领域一样困难。统计理论只在数据科学中扮演了部分角色，因为数据科学还有艺术的一面，艺术部分的发挥就需要数据科学家啦！ 2.2 什么是数据科学家？ 数据科学家=数据+科学+艺术家=用数据和科学从事艺术创作的人 数据科学家立足于科学，但不止于科学。从数据中提取出信息无疑是重要且有意义的过程，但这还不够。因为分析的终极目标是能够解决问题，实现价值。而从信息到具体应用领域的知识，进而应用所得知识创造价值，这两步都是需要一些艺术的，需要一点想象力。在之后“数据分析一般流程”那章中我会进一步讨论这个职业中艺术的部分。科学家需要不断学习，数据科学家是一个需要终身学习职业，其实很多职业都要求这一点。当然，你进入这个领域之前有一个门槛得要跨过去，有些基本的技能需要掌握。上面关于数据科学以及数据科学家的定义听起来非常高大上，可能有些抽象，感觉自己是个文艺女青年。其实也可以用一种更接地气的方式表达： 数据科学=从数据中得到问题答案的科学 数据科学家=通过科学方法从数据中得到有实际意义 的问题答案的人 数据科学结合了一整套科学工具与技术（数学，计算，视觉，分析，统计，试验，问题界定，模型建立与检验等），用于从数据收集中获得新发现、洞察与价值。使用数据科学的根本目的是解决实际问题。David Donoho在他2015年的文章“数据科学50年（50 years of Data Science）”中(Donoho 2015)讨论了当今数据科学的全貌，其中他将数据科学这个大领域分成6块： 数据探索和准备 数据表示和变换 数据编程计算 数据建模 数据可视化和展示 数据科学的科学 而一个合格的数据科学家，应该掌握这6个子领域的相关技能。我们会在本书剩余的部分围绕这6个方面展开讨论。 2.3 数据科学家需要的技能 我们在之前介绍北美各种和数据分析相关职位要求的时候，从技术层面上列举了一些数据科学家需要的技能。我们现在进一步讨论下这个职业需要的不同方面技能。 首先谈谈数据科学家的教育背景。数学、统计、计算机或其它定量分析学科（电子工程，运筹学等）的本科以上学历是必须的。根据2015年的统计数据，美国的数据科学家有48%有博士学位，44%有硕士学位，只有8%是本科。研究生博士期间的课题最好偏向机器学习，数据挖掘或预测模型。其次需要的是数据库操作技能。在工作中通常需要用SQL从数据库读取数据。所以能熟练使用SQL是基础。对于统计或者数学专业的学生，在校期间可能不需要使用SQL，因此不太熟悉。这没有关系，我也是工作以后才开始使用SQL的。但你要确保自己至少精通一种程序语言，之后遇到需要用到的新语言可以迅速学习。在学校期间的主要目的不是学会毕业后所需的全部技能，这是不可能完成的任务。高等教育（本科，研究生和博士）后应该具有的是基本的专业知识和自学能力。数据科学和很多其它领域一样，需要终身学习。有很多人问，要成为优秀的数据科学家是不是一定需要博士？这个问题很难用简单的是或者不是来回答。我看到的大多数优秀数据科学家确实都有博士学位，其余也都是硕士。我并不是要说高学历是成为优秀数据科学家的必要条件，其实真正重要的不是那个学历本身，而是拿到那个学历的过程，以及会选择获许这些学位的人共有的一些特质。 在美国，一般情况下，如果你拿到数理专业的博士学位，至少说明一个问题，就是你对学习的东西有兴趣。这样成天在电脑前面分析数据，编写程序的生活，对于那些对此不感兴趣的人来说必定是难以想象的痛苦。其次是研究生期间系统的理论训练。很多人可能觉得模型背后的数理知识不重要，只要会用模型就可以。统计软件使得很多模型使用者不需要知道具体的模型原理。了解模型原理是否能够帮助你更好的使用模型？当然会有帮助。但问题是这个帮助有多大？是不是值得我们花几年时间去学习？学习很多东西的好处是很难用短期去衡量的。我没有严格的分析，只是个人觉得了解模型原理是必要的。我很喜欢一个词“匠人精神”，也很乐意将“数据科学家”称为匠人，这是一种精益求精的精神。当然这种精神和学位没有必然联系，有本科毕业而对数据科学很感兴趣，自己学习也能够对这个学科有很深的理解。但大多数对这个领域感兴趣又具有“匠人精神”的人都有相关领域的更高等学历。最后，当然就是学习的能力。即使拿到博士学位，也不意味着学完了所有知识，而是具备进一步自学的能力，可以自己看懂数新方法的论文，也就是具备了在这个领域发展的自学能力。总的来说，这个领域的高学历现象并不能说明学历是必要条件，也不是充分条件。真正重要的是兴趣、匠人精神和自学能力。 编程能力也是数据科学家需要的基本技能。熟练使用一种编程语言是必须的，如R，Phython，C等。有人可能会问，只会SAS够不够？个人意见是：不够。这里不想对SAS过多评价。我的建议是大家至少要熟悉一门开源语言。当然，这些都只是工具，工具是解决问题的手段，而非目的。你必须要有一个能用来进行数据分析的工具，偏好因人而异，但你选择工具的时候最好考虑工具的灵活性和可扩展性。 接下来就要提到具体的分析技能。数据科学家应该掌握高等概率统计，能够熟练进行t检验，开方检验，拟合优度检验，方差分析。能够清楚的解释Spearman秩相关和Pearson相关之间的区别。熟悉抽样，概率分布，实验设计相关概念。了解贝叶斯统计（很快就能在白板上写下贝叶斯定理）。知道什么是有监督学习，什么是无监督学习。知道重要的聚类，判别和回归方法。知道基于罚函数的模型，关联法则分析。如果从事心理相关的应用的话（如消费者认知调查），还需要知道基本的潜变量模型，如探索性因子分析，验证性因子分析，结构方程模型。这个单子还可以一直列下去。看起来是不是不只一点吓人？我说过，数据科学家不是一个低门槛的行业，之前需要接受的训练对于没有兴趣的人来说是无比痛苦的。还有，单子是动态的，因为你在工作过程中还是需要不断学习。这些技能只是让你能够很好的开始。再次强调自学能力和成为一个终生学习者是优秀的数据科学家的必要条件。 除了技术能力以外，还需要其它一些非技术的能力。这些包括将实际问题转化成数据问题的能力，这一过程需要交流，也就要求良好的交流沟通能力。关注细节，分析是一个需要细心和耐心的职业。还有就是展示结果的能力，如何让没有分析背景的客户理解模型的结果，并且最终在实践中应用模型的结论。“数据科学家技能表”中总结了数据科学家需要的各方面技能。 总而言之，关于数据科学家有三个关键词：数据，科学和艺术。数据是基础；科学是工具；艺术是纽带，最终通过艺术将数据和科学结合得出的结果转化成相关领域的可应用知识，解决问题，真正产生价值。在实际应用中，以需要解决的问题为导向的思维方式很重要，否则分析很容易沦落为手段淹没目的的过程，很多分析行业的人就会犯这个错误，一味追求高大上的模型，酷炫的可视化，而忘了分析的根本目的是为了解决问题。说到这里，大家应该对这个行业有了一些概念性的了解，可能有读者会问：你这么强调数据科学是为了解决问题的，那么都解决哪些问题呢？在接下来的一章中，我们就来看看数据学科家都使用什么技术，解决哪些问题。 2.4 数据科学可以解决什么问题？ 2.4.1 前提要求 数据科学不是万能药，数据科学家也不是魔术师，有些问题我们无法用数据科学解决，最好在一开始就对问题做出判断，对于那些无法解决的问题，诚实的告诉对方并解释原因，那我们对问题有什么要求呢？ 你的问题需要尽可能具体 来看两个例子： 问题1: 如何提高产品销售量？ 问题2: 今年年初推出的新促销手段是不是提高了先锋先玉696玉米种子在西南地区的销售量？ 比较上面这两个问题，大家是不是很快能发现它们的差别？问题1从语法上是个正确的问题，但从解决的角度，并不是一个能够用分析给出答案的问题。为什么？因为问题太泛了，根本无从定义该问题背后的自变量和应变量。而问题2就是一个恰当的问题。从分析的角度，应变量很明显是“先锋先玉696玉米种子在西南地区的销售量”，感兴趣的自变量是“今年年初推出的新促销手段”，我们想要研究的就是这两者之间的关系。从这里开始再去寻找其它变量，这样就慢慢的进入分析流程了。当然，问题具体不代表就能够回答。比如我曾经遇到一个很具体的供应链问题，问的是针对某一个特定产品在特定区域的库存该是多少。这个问题为什么无法回答呢？这个项目一开始我通过多元自适应回归样条（MARS）模型以为找到了一个合理的答案，但到项目的最后才发现，他们给我的供应相关的数据极其不准确，很多地区的供应量都只是估计。这是我从业生涯中的一次教训，这告诉我们下面将要提到的一点。 你要有和问题相关的必要数据 巧妇难为无米之炊，这老古人的话放在那里时刻闪闪发光。艺术源于生活，所以你首先得要有数据，之所以数据科学会火也是因为计算机的发展，使数据的收集更容易。上面提到的供给问题就是一个很好的例子，没有相对准确的数据，之后任何模型都没有意义。当然，任何数据都是存在误差的，但是误差必须在一定范围内。尤其是感兴趣的自变量（如之前问题2里的“新促销手段”相关的数据）和应变量（“先锋先玉696玉米种子在西南地区的销售量”），如果这些必要的变量有很大缺失，或者不准确的话，模型是无法发挥作用的。再如，你要预测某个产品的消费者中谁最可能在接下来的3个月内购买该产品。要解决这个问题，你需要有目标消费者群体历史购买行为的信息：上一次购买的时间，消费量，优惠券使用情况等等。如果你仅仅知道这些客户的银行卡号，身份证号，出生月份之类的信息是不会对你的预测有任何帮助的。 很多时候数据的质量比数量重要，但数量也是不容忽视的。在能保证数据质量的前提下，数量越多越好。样本量越大，你能够回答的问题也就更细，且模型发现的置信度也更高。如果你有一个具体合理的问题，有足够大，质量合理的相关数据集，那么恭喜你，可以开始玩数据科学啦！ 2.4.2 问题种类 很多数据科学的书籍都从技术的角度对各种模型分类。比如有监督模型和无监督模型，线性模型和非线性模型，参数模型和参数模型等等。这里我们换而使用之前提到的“问题导向”的思维方式，对数据科学回答的问题进行分类，然后介绍哪些模型可以用于回答相应类别的问题，希望这些分组能在你面对自己的问题时帮助思考。 比较 第一类常见的问题是比较组之间不同的问题。常见的句式是：A在某方面是不是比B好？或者多者比较：A、B、C之间在某方面有没有差别？下面是一些问题的例子： 参与促销活动和没有参与促销活动消费者购买量有差异么？ 男性是不是比女性更倾向于购买我们的产品？ 用户满意度在不同商业区是不是有不同？ 服用某种药物的老鼠比没有服用药物的老鼠体重增长的是否更快？ 携带某种基因的大豆是不是比普通大豆产油量高？ 对于这类数据，通常从各组观测的基本统计量和可视化开始初步探索数据。在对数据分布和组之间的差异有个初步直观了解之后，通过统计检验测试组间是否在感兴趣的变量上有显著不同。处理这类问题常用的是经典统计推断：开方检验，t检验和方差分析。放在贝叶斯框架下也有一种比较组间不同的方法。如果因子增加，结构变得复杂（如在生物医药领域的复杂实验设计有随机效应因子），则需要使用更加复杂的混合效应模型。 描述 在分析中不可避免的要描述数据。比如聚类问题。当你通过算法找到不同的样本分类后，就需要对类进行定义，这要通过比较各类中变量的描述统计量得到。常用的描述问题有： 样本中家庭年观测的收入是不是无偏的？ 某产品在不同区域的月销售量均值／方差是多少？ 变量的量级差异大么？（决定是否需要对数据标准化） 模型中的预测变量观测缺失情况如何？ 问卷调查回复者的年龄分布范围是多少？ 这类数据描述常用于检查数据，找到合适的数据预处理方法，以及拟合模型后对结果的分析和展示。 聚类 聚类是一个极其常见的问题，其通常和判别联系在一起。聚类模型回答的问题是： 哪些消费者有相似的产品偏好？（市场营销） 哪些打印机损坏的模式相同？（质量控制） 公司员工在对公司评价上可以分为几类？（人力资源） 哪些词更经常同时出现？（自然语义处理） 哪些文档可能有相似的主题？（自然语义处理） 聚类是无监督分析。 判别 判别是另外一个经典的分析问题。通常用类别已知的样本作为训练集拟合判别器，然后用训练好的判别器预测新样本的类别。下面是一些关于判别的问题： 哪些新客户最有可能转化（购买）？ 当前的压力度数是正常的么？ 某贷款人有不还款的风险么？ 这个消费者还可能喜欢什么产品？ 这本书的作者可能是谁？ 这封邮件是不是垃圾邮件？ 关于判别的模型有数百种，在实践中我们其实不必要尝试所有的模型而只要拟合其中几种在大部分情况下表现最好的模型即可。我们在后面判别的章节还会介绍。 回归 当你感兴趣的量是一个数值而非类别时，通常就是一个回归问题。比如： 明天的气温可能会是多少？ 公司今年第4季度的销售额会是多少？ 某品牌打印机明年上半年在北京市的销量会是多少？ 该引擎还能工作多久？ 这次活动中需要准备多少啤酒？ 通常情况下，回归能够给出一个数值答案。回归通常解决“…是多少？”这样的问题。在有些时候模型给出的负数结果可能需要解释为0，或者有小数点的结果需要解释为最近的整数。 References "],
["section-3.html", "第3章 数据集模拟和背景介绍 3.1 服装消费者数据 3.2 航空公司满意度调查 3.3 生猪疫情风险预测数据", " 第3章 数据集模拟和背景介绍 之后的章节将通过案例讨论建模的各个方面。在进入正题之前，我先用本章介绍书中使用的数据，包括模拟数据的代码，数据的获取以及数据语境背景。很多R包里有现成的数据，网上也有各种机器学习竞赛的数据，但本书用来展示模型的数据大部分是通过R得到的模拟数据集。其原因我在第1章开始已经讲过了。 3.1 服装消费者数据 我们先模拟一个关于某品牌服装消费者的数据，这个数据会在之后的章节中反复用到。数据中包含N=1000个观测，我们将模拟3类变量（括号内是变量对应的模拟数据框中的列标签名）： （1）人口统计学变量。 年龄（age） 性别（gender） 有房还是租房（house） （2）消费者行为变量。 2015年实体店购买该品牌服装花销（store_exp） 2015年在线购买该品牌服装花销（online_exp） 2015年实体店交易次数（store_trans） 2015年在线交易次数（online_trans） （3）客户认知问卷调查。为了进一步了解消费者，商家时常对消费者进行问卷调查，然后对调查结果进行分组，其目标是寻找在产品兴趣，市场参与度或营销反应的重要方面有显著差异的客户群。通过了解组间的不同，市场营销人员可以优化产品定位，进行更加精准的营销。这里我们假设该服装品牌对消费者进行了下面的调查，并模拟该调查问卷的回复。 你是否同意下面的申明？ 问题 1（非常不同意） 2（有点不同意） 3（中立/不知道） 4（有点同意） 5（非常同意） （Q1）：我喜欢买不同品牌的服装，比较它们 （Q2）：我喜欢买同一个品牌的服装 （Q3）：品牌的知名度对我来说非常重要 （Q4）：服装质量对我来说非常重要 （Q5）：服装风格我喜欢的风格 （Q6）：我喜欢在实体店购买 （Q7）：我喜欢在网上购买 （Q8）：价格对我来说很重要 （Q9）：我喜欢不同风格的衣服 （Q10）：我喜欢自己挑选服装，不需要周围人的建议 我们进一步假设这些根据问卷调查的结果可以将消费者分成4组：价格敏感（Price），炫耀性消费（Conspicuous），质量（Quality），风格（Style）。 （本章我们不会提到如何得到这些分组；我们假设这些已知。我们会在第9章中介绍聚类分析时会更详细的说明。） 你也可以重复下面的代码，自己创建该数据。我们强烈建议读者重复数据模拟的过程，这样能加深对模型方法的理解。如果你对此不感兴趣，也可以从本书网站上直接下载数据： sim.dat&lt;-read.csv(&quot;https://raw.githubusercontent.com/happyrabbit/DataScientistR/master/Data/SegData.csv&quot;) 获得该数据集后你可以跳过本小节后半部分直接跳到下一节对分析流程的讲解。否则，继续本节。 模拟该数据的过程有些复杂，我们先模拟描述客户的变量。模拟该数据的代码分为3部分： 定义数据结构：定义变量名，变量类型，消费者分组名，各组大小。 变量分布参数，如各自的均值和方差。 在各组和各个变量上迭代，基于定义和参数设置抽取随机数。 通过这种方式组织代码，如果我们要改变部分模拟方式重新抽取数据就比较容易。例如，如果我们想要加一个组，或者改变其中某个人口统计变量的均值，只要稍微改变代码就好。我们也想通过这个结构介绍新的R代码，生成数据的第3个步骤中将用到这些代码。 # 设置随机种子，使数据模拟过程可重复 set.seed(12345) # 定义观测数目 ncust&lt;-1000 # 建立数据框存放模拟观测，初始数据框中只有一列id，即消费者编号 seg_dat&lt;-data.frame(id=as.factor(c(1:ncust))) # 指定要生成的变量，并为变量命名 vars&lt;-c(&quot;age&quot;,&quot;gender&quot;,&quot;income&quot;,&quot;house&quot;,&quot;store_exp&quot;,&quot;online_exp&quot;,&quot;store_trans&quot;,&quot;online_trans&quot;) # 每个变量对应的数据类型 # norm： 正态分布 # binom: 二项分布 # pois： 泊松分布 vartype&lt;-c(&quot;norm&quot;,&quot;binom&quot;,&quot;norm&quot;,&quot;binom&quot;,&quot;norm&quot;,&quot;norm&quot;,&quot;pois&quot;,&quot;pois&quot;) # 四个消费者分组的名称 group_name&lt;-c(&quot;Price&quot;,&quot;Conspicuous&quot;,&quot;Quality&quot;,&quot;Style&quot;) # 各消费者群组的大小 group_size&lt;-c(250,200,200,350) # group_name和group_size的第一个元素表明，对于“Price”这组消费者，我们将模拟N=250个观测。 定义好了数据的基本结构之后，我们下一步是定义分布参数，用这些参数来抽取相应数据。 这里我们要 模拟的数据有4个样本类，8个非抽样调查变量，因此我们创建一个4×8的均值矩阵，因为不同类别的 消费者对应不同的分布参数。下面代码用来创建均值矩阵： # 定义均值矩阵 mus &lt;- matrix( c( # 价格敏感（Price）类对应均值 60, 0.5, 120000,0.9, 500,200,5,2, # 炫耀性消费（Conspicuous）类对应均值 40, 0.7, 200000,0.9, 5000,5000,10,10, # 质量（Quality）类对应均值 36, 0.5, 70000, 0.4, 300, 2000,2,15, # 风格（Style）类对应均值 25, 0.2, 90000, 0.2, 200, 2000,2,20), ncol=length(vars), byrow=TRUE) 具体过程是怎样的？ 均值矩阵mus指定，例如，价格敏感（Price）类群体的第一个变量（这里是年龄age）均值为60，炫耀性消费（Conspicuous）类群体的年龄均值为40依次类推。正态分布变量需要指定均值和方差，如年龄（age）， 实体店花销（store_exp）和在线花销（online_exp）。对于二项分布（只有两个可能取值）和泊松分布变量，我们只需要规定均值。其中，性别（gender），有房还是租房（house）是二项数据，生成这样的数据需要指定得到其中某一观测值的概率，比如矩阵mus中。实体店交易次数（store_trans）和线交易次数（online_trans）是泊松变量（频数），泊松分布只有一个参数——分布均值。所以在下面的标准差矩阵sds中，非正态分布变量对应的标准差为缺失值NA。（注意这里我们只是用这些分布为例生成数据，并不意味着这些是最好的拟合变量观测的分布。例如，真实的收入数据更可能是一个有偏的分布而非正态）。 下面我们对正态分布变量创建标准差矩阵： # 每类的标准差 (NA = 标准差无定义) sds&lt;- matrix( c( # 价格敏感（Price）类对应均值 3,NA,8000,NA,100,50,NA,NA, # 炫耀性消费（Conspicuous）类对应均值 5,NA,50000,NA,1000,1500,NA,NA, # 质量（Quality）类对应均值 7,NA,10000,NA,50,200,NA,NA, # 风格（Style）类对应均值 2,NA,5000,NA,10,500,NA,NA), ncol=length(vars), byrow=TRUE) 将这两个矩阵加在一起我们就能够完全定义各个类的分布了。例如，我们来看下每个矩阵的第1行，其代表第1类群体（价格敏感）的分布参数。这些值规定该类群体的年龄（age）均值为60（见第一个矩阵第1行第1列），标准差为3（第二个矩阵第1行第1列）。另外，其中大约有50%的男性（第一个矩阵第1行第2列），年收入（income）均值为120000元，标准差为8000元。将这些设置分开存在不同表格中，将来想要修改十分容易。将数据定义和抽样过程分开是个很好的习惯。下面开始抽取数据： # 抽取非抽样调查数据 sim.dat&lt;-NULL set.seed(2016) # 对消费者类别进行循环（i） for (i in seq_along(group_name)){ # 为了核实代码，展示循环运行过程，我们在循环中添加了这样一行代码 # 函数运行时会打印出正在抽取的样本类名 cat (i, group_name[i],&quot;\\n&quot;) # 创建一个空矩阵用于存放该类消费者相关数据 seg&lt;-data.frame(matrix(NA,nrow=group_size[i], ncol=length(vars))) # 在这个类之内，对不同变量迭代，抽取相应的随机数据 for (j in seq_along(vars)){ # 在每个变量上迭代 if (vartype[j]==&quot;norm&quot;){ # 抽取正态分布变量 seg[,j]&lt;-rnorm(group_size[i], mean=mus[i,j], sd=sds[i,j]) } else if (vartype[j]==&quot;pois&quot;) { # 抽取泊松分布变量 seg[,j]&lt;-rpois(group_size[i], lambda=mus[i,j]) } else if (vartype[j]==&quot;binom&quot;){ # 抽取二项分布变量 seg[,j]&lt;-rbinom(group_size[i],size=1,prob=mus[i,j]) } else{ # 如果变量类型不是上述几种，程序停止运行并提示信息 stop (&quot;Don&#39;t have type:&quot;,vartype[j]) } } # 将该消费者类的数据依行添加到总数据集 sim.dat&lt;-rbind(sim.dat,seg) } 上面的代码是随机抽样的主要过程，其中cat()函数使得循环运行时会打印出正在抽取的样本类名，最后得到的sim.dat是初始描述客户的变量部分的数据，在对数据进行润色前，提醒大家注意两个关于R的技巧： 第一、在i循环内，我们事先定义一个有着相应行数和列数的没有元素值的数据框seg，之后每迭代一次就将样本赋值到事先定义的seg的特定行。这么做的原因是由于只要R在某个对象上添加东西——如在数据框上增加一行——它都会将原对象拷贝一份。这将使用两倍的内存，减慢运行速度。通过这种方法可以避免对内存的浪费。对这里的小数据可能感觉不出差别，但对于大数据，运行速度会有极大不同。 第二、对循环指针范围的设定用的是seq_along()而非1:length()。这是为了够避免一些常见的错误，如指针向量长度为0或者不经意将向量方向弄反了。 之后我们对描述客户的这部分数据进行完善，添加合适的列标签，将二项（0/1）变量转化为贴有标签的因子变量。 # 指定数据框的列名为我们定义的变量名 names(sim.dat)&lt;-vars # 加上一个因子列表明每个观测的对应的消费者类别 sim.dat$segment&lt;-factor(rep(group_name,times=group_size)) # 将二项变量转化为贴有标签的因子变量 # Female: 女性 # Male: 男性 sim.dat$gender&lt;-factor(sim.dat$gender, labels=c(&quot;Female&quot;,&quot;Male&quot;)) sim.dat$house&lt;-factor(sim.dat$house, labels=c(&quot;No&quot;,&quot;Yes&quot;)) # 假设在线购买和在实体店购买的次数至少为1，所以这里在原随机值上加1 sim.dat$store_trans&lt;-sim.dat$store_trans+1 sim.dat$online_trans&lt;-sim.dat$online_trans+1 # 年龄为整数 sim.dat$age&lt;-floor(sim.dat$age) 真实市场营销数据往往没有这么干净，数据缺失，以及错误输入等问题常常发生。我们最后对模拟的数据做一点“破坏”，使其更像真实的数据。我们假设一些人不愿意给出关于收入（income）的信息。我们建立一个逻辑变量idxm，然后将逻辑变量idxm值为真的对应位置消费者收入观测设为缺失值NA（R用NA表示缺失值）。我们假设年龄（age）越大的消费者对应缺失值的概率越大： # 加入缺失值 idxm &lt;- as.logical(rbinom(ncust, size=1, prob=sim.dat$age/200)) sim.dat$income[idxm]&lt;-NA 真实的数据中可能有错误的输入，或者离群点： # 错误输入，离群点 set.seed(123) idx&lt;-sample(1:ncust,5) sim.dat$age[idx[1]]&lt;-300 sim.dat$store_exp[idx[2]]&lt;- -500 sim.dat$store_exp[idx[3:5]]&lt;-c(50000,30000,30000) 到目前为止我们已经建立了一部分数据，你可以通过summary(sim.dat)检查数据。下面我们接着抽取问卷调查回复数据。我们先通过rnorm()生成正态分布随机数。但从上面的问卷调查表格中可以看到，这是一个1-5分量级的问卷，1代表非常不同意，5代表非常同意。于是接下来我们通过floor()函数将连续值转化成离散整数。 # 抽取问卷调查回复 # 问卷问题数目 nq&lt;-10 # 各类消费者对问卷回复的正态分布均值矩阵 mus2 &lt;- matrix( c( # 价格敏感（Price）类对应均值 5,2,1,3,1,4,1,4,2,4, # 炫耀性消费（Conspicuous）类对应均值 1,4,5,4,4,4,4,1,4,2, # 质量（Quality）类对应均值 5,2,3,4,3,2,4,2,3,3, # 风格（Style）类对应均值 3,1,1,2,4,1,5,3,4,2), ncol=nq, byrow=TRUE) # 方差假设都是0.2 sd2&lt;-0.2 sim.dat2&lt;-NULL set.seed(1000) # 对消费者类别进行循环（i） for (i in seq_along(group_name)){ # 为了核实代码，展示循环运行过程，我们在循环中添加了这样一行代码 # 函数运行时会打印出正在抽取的样本类名，这里不再显示输出 # cat (i, group_name[i],&quot;\\n&quot;) # 创建一个空矩阵用于存放该类消费者相关数据 seg&lt;-data.frame(matrix(NA,nrow=group_size[i], ncol=nq)) # 在这个类之内，对不同变量迭代，抽取相应的随机数据 for (j in 1:nq){ # 抽取正态分布变量 res&lt;-rnorm(group_size[i], mean=mus2[i,j], sd=sd2) # 设置上下限度 res[res&gt;5]&lt;-5 res[res&lt;1]&lt;-1 # 通过 floor()函数将连续值转化成离散整数。 seg[,j]&lt;-floor(res) } # 将该消费者类的数据添加到总数据集 sim.dat2&lt;-rbind(sim.dat2,seg) } # 为数据框添加列标签 names(sim.dat2)&lt;-paste(&quot;Q&quot;,1:10,sep=&quot;&quot;) # 合并两部分数据 sim.dat&lt;-cbind(sim.dat,sim.dat2) # 加上一个因子列表明每个观测的对应的消费者类别 sim.dat$segment&lt;-factor(rep(group_name,times=group_size)) 至此为止我们得到了需要的数据集。让我们检查一下抽取的数据集： str(sim.dat,vec.len=3) ## &#39;data.frame&#39;: 1000 obs. of 19 variables: ## $ age : int 57 63 59 60 51 59 57 57 ... ## $ gender : Factor w/ 2 levels &quot;Female&quot;,&quot;Male&quot;: 1 1 2 2 2 2 2 2 ... ## $ income : num 120963 122008 114202 113616 ... ## $ house : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 2 2 2 2 2 2 2 2 ... ## $ store_exp : num 529 478 491 348 ... ## $ online_exp : num 304 110 279 142 ... ## $ store_trans : int 2 4 7 10 4 4 5 11 ... ## $ online_trans: int 2 2 2 2 4 5 3 5 ... ## $ Q1 : int 4 4 5 5 4 4 4 5 ... ## $ Q2 : int 2 1 2 2 1 2 1 2 ... ## $ Q3 : int 1 1 1 1 1 1 1 1 ... ## $ Q4 : int 2 2 2 3 3 2 2 3 ... ## $ Q5 : int 1 1 1 1 1 1 1 1 ... ## $ Q6 : int 4 4 4 4 4 4 4 4 ... ## $ Q7 : int 1 1 1 1 1 1 1 1 ... ## $ Q8 : int 4 4 4 4 4 4 4 4 ... ## $ Q9 : int 2 1 1 2 2 1 1 2 ... ## $ Q10 : int 4 4 4 4 4 4 4 4 ... ## $ segment : Factor w/ 4 levels &quot;Conspicuous&quot;,..: 2 2 2 2 2 2 2 2 ... 可以看到，服装消费者数据有1000个观测，19个变量。前8个变量是关于样本的人口统计学和购买行为描述。Q1-Q10是关于消费者选择偏好的问卷调查回复，问卷分值量表为1-5分（最常见的市场调查设计）。最后一列是消费者类别，样本观测的模拟是根据消费者类别进行的，因此这些可以当作“真实”的消费者类别。使用随机模拟的一个重要优点就是能够通过这种方式验证模型的效果。而实际生活中的数据样本真正所属类别通常是未知的。我们在之后对聚类和判别分析进行介绍的时候会使用样本类别信息。下面我们会反复用该数据集为例。 3.2 航空公司满意度调查 这一小节我们模拟一个航空公司满意度调查数据。数据中包含N=1000个受访者，每个受访者基于最近一次航班体验对3个航空公司进行评分，问卷调查一共15项，每项评分从1-9，分值越大满意度越高。这15个调查项分为4类（括号中为相应数据集中的变量名）： 购票体验 购票容易度（Easy_Reservation） 座椅选择（Preferred_Seats） 航班选择（Flight_Options） 票价（Ticket_Prices） 机舱设施 座椅舒适度（Seat_Comfort） 位置前后空间（Seat_Roominess） 随机行李存放（Overhead_Storage） 机舱清洁（Clean_Aircraft） 空航服务 礼貌（Courtesy） 友善（Friendliness） 能够提供需要的帮助（Helpfulness） 食物饮料服务（Service） 总体指数 总体满意度（Satisfaction） 再次选择次航空公司（Fly_Again） 向朋友推荐此航空公司（Recommend） # 先建立因子载荷矩阵 # 其中前12项符合双因子结构，因为每项对应一个总体因子载荷和某特定因子的载荷 # 比如购票容易度对应总体因子载荷0.33，对因特定购票因子载荷0.58 # 我们可以将结果评分看成是总体因子和特定因子共同作用的结果 loadings &lt;- matrix(c ( # 购票体验 .33, .58, .00, .00, # 购票容易度 .35, .55, .00, .00, # 座椅选择 .30, .52, .00, .00, # 航班选择 .40, .50, .00, .00, # 票价 # 机舱设施 .50, .00, .55, .00, # 座椅舒适度 .41, .00, .51, .00, # 位置前后空间 .45, .00, .57, .00, # 随机行李存放 .32, .00, .54, .00, # 机舱清洁 # 空航服务 .35, .00, .00, .50, # 礼貌 .38, .00, .00, .57, # 友善 .60, .00, .00, .50, # 能够提供需要的帮助 .52, .00, .00, .58, # 食物饮料服务 # 总体指数 .43, .10, .30, .30, # 总体满意度 .35, .50, .40, .20, # 再次选择次航空公司 .25, .50, .50, .20), # 向朋友推荐此航空公司 nrow=15,ncol=4, byrow=TRUE) # 将载荷矩阵乘以它的转秩，然后将对角线元素设置为1得到相关矩阵 cor_matrix&lt;-loadings %*% t(loadings) # Diagonal set to ones. diag(cor_matrix)&lt;-1 # 我们通过mvtnorm包模拟有特定相关矩阵的数据集 library(mvtnorm) # 设置3个航空公司对应的评分均值向量 mu1=c(5,6,5,6, 7,8,6,7, 5,5,5,5, 6,6,6) mu2=c(3,3,2,3, 5,4,5,6, 8,8,8,8, 3,3,3) mu3=c(2,2,2,2, 8,8,8,8, 8,8,8,8, 8,8,8) #设置随机种子 set.seed(123456) # 受访者ID resp.id &lt;- 1:1000 library(MASS) rating1 &lt;- mvrnorm(length(resp.id), mu=mu1, Sigma=cor_matrix) rating2 &lt;- mvrnorm(length(resp.id), mu=mu2, Sigma=cor_matrix) rating3 &lt;- mvrnorm(length(resp.id), mu=mu3, Sigma=cor_matrix) # 将分值限定在1到9之间 rating1[rating1&gt;9]&lt;-9 rating1[rating1&lt;1]&lt;-1 rating2[rating2&gt;9]&lt;-9 rating2[rating2&lt;1]&lt;-1 rating3[rating3&gt;9]&lt;-9 rating3[rating3&lt;1]&lt;-1 # 将分值转化为整数 rating1&lt;-data.frame(round(rating1,0)) rating2&lt;-data.frame(round(rating2,0)) rating3&lt;-data.frame(round(rating3,0)) rating1$ID&lt;-resp.id rating2$ID&lt;-resp.id rating3$ID&lt;-resp.id rating1$Airline&lt;-rep(&quot;AirlineCo.1&quot;,length(resp.id)) rating2$Airline&lt;-rep(&quot;AirlineCo.2&quot;,length(resp.id)) rating3$Airline&lt;-rep(&quot;AirlineCo.3&quot;,length(resp.id)) rating&lt;-rbind(rating1,rating2,rating3) # 为数据集的各列命名 names(rating)&lt;-c( &quot;Easy_Reservation&quot;, &quot;Preferred_Seats&quot;, &quot;Flight_Options&quot;, &quot;Ticket_Prices&quot;, &quot;Seat_Comfort&quot;, &quot;Seat_Roominess&quot;, &quot;Overhead_Storage&quot;, &quot;Clean_Aircraft&quot;, &quot;Courtesy&quot;, &quot;Friendliness&quot;, &quot;Helpfulness&quot;, &quot;Service&quot;, &quot;Satisfaction&quot;, &quot;Fly_Again&quot;, &quot;Recommend&quot;, &quot;ID&quot;, &quot;Airline&quot;) 让我们检查一下抽取的数据集： str(rating,vec.len=3) ## &#39;data.frame&#39;: 3000 obs. of 17 variables: ## $ Easy_Reservation: int 6 5 6 5 4 5 6 4 ... ## $ Preferred_Seats : int 5 7 6 6 5 6 6 6 ... ## $ Flight_Options : int 4 7 5 5 3 4 6 3 ... ## $ Ticket_Prices : int 5 6 6 5 6 5 5 5 ... ## $ Seat_Comfort : int 5 6 7 7 6 6 6 4 ... ## $ Seat_Roominess : int 7 8 6 8 7 8 6 5 ... ## $ Overhead_Storage: int 5 5 7 6 5 4 4 4 ... ## $ Clean_Aircraft : int 7 6 7 7 7 7 6 4 ... ## $ Courtesy : int 5 6 6 4 2 5 5 4 ... ## $ Friendliness : int 4 6 6 6 3 4 5 5 ... ## $ Helpfulness : int 6 5 6 4 4 5 5 4 ... ## $ Service : int 6 5 6 5 3 5 5 5 ... ## $ Satisfaction : int 6 7 7 5 4 6 5 5 ... ## $ Fly_Again : int 6 6 6 7 4 5 3 4 ... ## $ Recommend : int 3 6 5 5 4 5 6 5 ... ## $ ID : int 1 2 3 4 5 6 7 8 ... ## $ Airline : Factor w/ 3 levels &quot;AirlineCo.1&quot;,..: 1 1 1 1 1 1 1 1 ... 3.3 生猪疫情风险预测数据 本小节中我们将模拟一个生猪疫情数据。假设研究人员对800个养猪场进行和某生猪疫情有关的问卷调查，问卷由120个问题组成。每个问题有3个可能选项。目的是根据问卷调查回复得到每个养猪场在未来爆发疫情的概率。每个养猪场在问卷问题的3个可选项中等概率选择。第\\(i\\)个养猪场对应的疫情爆发概率服从\\(Bernoulli(1,p_{i})\\)分布。其中 \\[ln(\\frac{p_{i}}{1-p_{i}})=\\beta_{0}+\\sum_{g=1}^{G}\\mathbf{x_{i,g}^{T}}\\beta_{g}\\] \\(\\beta_{0}\\)是截距项，\\(\\mathbf{x_{i,g}}\\)是第\\(i\\)观测对应第\\(g\\)个问题的回复。这里将问题回复转化为0/1虚拟变量，因为每个问题有3个可能选项，所以\\(\\mathbf{x_{i,g}}\\)是一个取值为0/1的含有三个元素的向量。\\(\\mathbf{\\beta_{g}}\\)是对应的参数。 我们在这里考虑3类问题。第1类（问题1到问题40）问题中有两个选项对应变量有预测能力。第2类（问题41到问题80）问题中只有一个选项对结果有预测能力。第3类（问题81到问题120）对结果预测没有帮助，也就是我们希望能够去除的变量。模拟数据的参数设置如下： \\[\\mathbf{\\beta^{T}}=\\left(\\underset{question\\ 1}{\\frac{40}{3},\\underbrace{1,0,-1}},...,\\underset{question\\ 40}{\\underbrace{1,0,-1}},\\underset{question\\ 41}{\\underbrace{1,0,0}},...,\\underset{question\\ 80}{\\underbrace{1,0,0}},\\underset{question\\ 81}{\\underbrace{0,0,0}},...,\\underset{question\\ 120}{\\underbrace{0,0,0}}\\right)*\\gamma\\] 这里我们通过设置5个\\(\\gamma\\)值（\\(\\gamma \\in \\{0.1,0.25,0.5,1,2\\}\\) ）模拟了5种参数情况下的数据。 \\(\\gamma\\)越大，参数值越大，也就意味着有效问题对结果的预测性越强。对于每个参数设定模拟了20个数据集，之后我们会以这些数据为例展示不同模型变量选择的效果。模拟多个数据集是为了研究一些估值的稳定性。 # sim1_da1.csv 模拟的第一个数据集 # similar sim1_da2 and sim1_da3 # sim1.csv simulated data, the first simulation # dummy.sim1.csv dummy variables for the first simulated data with all the baseline in #code for simulation # setwd(dirname(file.choose())) # library(grplasso) nf&lt;-800 for (j in 1:20){ set.seed(19870+j) x&lt;-c(&#39;A&#39;,&#39;B&#39;,&#39;C&#39;) sim.da1&lt;-NULL for (i in 1:nf){ # sample(x, 120, replace=TRUE)-&gt;sam sim.da1&lt;-rbind(sim.da1,sample(x, 120, replace=TRUE)) } data.frame(sim.da1)-&gt;sim.da1 paste(&quot;Q&quot;, 1:120, sep = &quot;&quot;)-&gt;col paste(&quot;Farm&quot;, 1:nf, sep = &quot;&quot;)-&gt;row colnames(sim.da1)&lt;-col rownames(sim.da1)&lt;-row # 用nnet包中的class.ind()函数将问题回复编码为名义变量 library(nnet) dummy.sim1&lt;-NULL for (k in 1:ncol(sim.da1)) { tmp=class.ind(sim.da1[,k]) colnames(tmp)=paste(col[k],colnames(tmp)) dummy.sim1=cbind(dummy.sim1,tmp) } data.frame(dummy.sim1)-&gt;dummy.sim1 # 每个问题对应的3个名义变量中有重复信息 # 将C选项设置为基线回复 # 删除基线名义变量 base.idx&lt;-3*c(1:120) dummy1&lt;-dummy.sim1[,-base.idx] # 对每个r设置依次抽取相应的因变量 # 每次只对一个r值抽取，将其余代码注释掉 # 得到r=0.1 时每个农场对应的连接函数值 c(rep(c(1/10,0,-1/10),40),rep(c(1/10,0,0),40),rep(c(0,0,0),40))-&gt;s1 as.matrix(dummy.sim1)%*%s1-40/3/10-&gt;link1 # r=0.25 # c(rep(c(1/4,0,-1/4),40),rep(c(1/4,0,0),40),rep(c(0,0,0),40))-&gt;s1 # as.matrix(dummy.sim1)%*%s1-40/3/4-&gt;link1 # r=0.5 # c(rep(c(1/2,0,-1/2),40),rep(c(1/2,0,0),40),rep(c(0,0,0),40))-&gt;s1 # as.matrix(dummy.sim1)%*%s1-40/3/2-&gt;link1 # r=1 # c(rep(c(1,0,-1),40),rep(c(1,0,0),40),rep(c(0,0,0),40))-&gt;s1 # as.matrix(dummy.sim1)%*%s1-40/3-&gt;link1 # r=2 # c(rep(c(2,0,-2),40),rep(c(2,0,0),40),rep(c(0,0,0),40))-&gt;s1 # as.matrix(dummy.sim1)%*%s1-40/3/0.5-&gt;link1 # 在连接函数的基础上计算每个农场对应的爆发概率 exp(link1)/(exp(link1)+1)-&gt;hp1 # 基于爆发概率hp1，抽取相应的因变量res res&lt;-rep(9,nf) for (i in 1:nf){ sample( c(1,0),1,prob=c(hp1[i],1-hp1[i]))-&gt;res[i] } # 这里将数据存成3个不同的版本，只是为了之后不同模型使用方便 # 3个数据集都含有所有120个问题的回复，但彼此稍微有不同 # da1 含有因变量，但没有名义变量所属问题的信息 # da2 没有因变量，但最后一行包括的名义变量所属的问题 # da3 没有因变量，没有名义变量所属问题的信息 dummy1$y&lt;-res da1&lt;-dummy1 y&lt;-da1$y ind&lt;-NULL for (i in 1:120){ c(ind,rep(i,2))-&gt;ind } da2&lt;-rbind(da1[,1:240],ind) da3&lt;-da1[,1:240] # 将数据集储存起来 write.csv(da1,paste(&#39;sim&#39;,j,&#39;_da&#39;,1,&#39;.csv&#39;,sep=&#39;&#39;),row.names=F) write.csv(da2,paste(&#39;sim&#39;,j,&#39;_da&#39;,2,&#39;.csv&#39;,sep=&#39;&#39;),row.names=F) write.csv(da3,paste(&#39;sim&#39;,j,&#39;_da&#39;,3,&#39;.csv&#39;,sep=&#39;&#39;),row.names=F) write.csv(sim.da1,paste(&#39;sim&#39;,j,&#39;.csv&#39;,sep=&#39;&#39;),row.names=F) write.csv(dummy.sim1,paste(&#39;dummy.sim&#39;,j,&#39;.csv&#39;,sep=&#39;&#39;),row.names=F) } 要理解这里数据模拟的代码，读者需要了解逻辑回归和分组lasso的理论知识，这超出了本书的范围。这里的代码仅供大家参考。可以重复上面的代码生成相应的数据集。因为这里生成的数据量较大，在网上只有\\(\\gamma=2\\)对应的一次模拟的数据集。我们看下得到的数据集： library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union disease_dat&lt;-readr::read_csv(&quot;/Users/happyrabbit/Documents/GitHub/DataScientistR/Data/sim1_da1.csv&quot;) # 这里只截取最后的7列 head(subset(disease_dat,select=c( &quot;Q118.A&quot;,&quot;Q118.B&quot;,&quot;Q119.A&quot;,&quot;Q119.B&quot;,&quot;Q120.A&quot;,&quot;Q120.B&quot;,&quot;y&quot;))) ## # A tibble: 6 x 7 ## Q118.A Q118.B Q119.A Q119.B Q120.A Q120.B y ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1 0 0 0 0 1 1 ## 2 0 1 0 1 0 0 1 ## 3 1 0 0 0 1 0 1 ## 4 1 0 0 0 0 1 1 ## 5 1 0 0 0 1 0 0 ## 6 1 0 0 1 1 0 1 其中最后一列y代表相应农场疫情爆发情况，y=1代表从问卷调查之后5年内有疫情爆发。剩余的列表示农场问卷调查结果，如Q120.A=1对应问卷调查中第120个问题选择A的农场，类似的Q120.B=1对应第120个问题中选择B的农场，我们将选项C作为基准选项。之后我会用这个数据集展示一些相关的模型。 "],
["section-4.html", "第4章 数据分析一般流程 4.1 问题到数据 4.2 数据到信息 4.3 信息到行动", " 第4章 数据分析一般流程 数据科学的目的是找到数据背后潜藏的模式，这个模式就是我们所说的信息。仅仅挖掘出信息是不够的，你还要利用信息作出具体的行动，最终将信息转化成知识，然后从知识到行动从而创造价值。这个数据分析流程将数据分析的技术部分放在了一个更大的应用语境下，重点从技术建模到根据发现采取行动，这章我们要重点讨论得到可实践的数据分析结果。当前有太多介绍分析技术的书籍对数据分析定义很狭隘，好像只要将当前最华丽的模型（如神经网络，随机森林，支持向量机）通过某种统计软件套用在数据上，得到结果报告就结束了。这些方法模型都是很重要的，但是分析的过程可不仅仅是一系列功能强大的模型和结构完整的数据。我们要在恰当的应用领域，将正确的方法用在合适的数据上。整个数据分析的流程是个迭代学习的过程，每一步都在前一步的结果之上更深入的理解（挖掘）数据。积累的经验反馈又可用于优化整个过程。因此这是一个自反馈的生态环。该流程具体是怎样的呢？“数据分析的一般流程”图展示了这个生态环的6个阶段，图中每一个步骤旁边方括号内的百分比是该步骤大致占用的时间范围。整个分析成功的关键在于将数据分析嵌入实际问题的语境中，最终分析师能够将数据中提取的信息通过交流传达给客户，指导推动用户付出行动创造价值。下面我会逐一对每个阶段进行讲解。但在此之前，我们先模拟一个关于某品牌服装消费者的数据，本章会用该数据为例展示数据分析的流程，该数据及也会在之后的章节中反复用到。 4.1 问题到数据 分析的流程开始于一个具体的问题，一定确保准确理解客户的问题。这里所说的“客户”是广义上的，如果你是在专门的统计咨询公司工作，那客户可以是其它需要统计分析帮助的组织机构，如果你在企业内部从事相关分析工作，那客户可以是你所支持的部门的同事，如产品经理，市场总监，科研部门化学、生物领域的科学家等等。分析的新手很容易犯的一个错误就是在没有确保准确理解客户问题的情况下就开始收集分析数据，最后白忙一场，对于各方都造成时间上的损失。如何避免这样的情况发生？交流！通过交流，将问题尽可能具体化。这个在之前讲到过，你的问题需要尽可能具体到： 知道问题的类别：是比较，描述，聚类，判别还是回归 知道需要什么样的数据：我们有哪些数据，数据的质量如何，还需要收集哪些数据，哪些是自变量，哪些是应变量。 以上面服装消费者为例。假设某服装公司的营销人员找到你，希望知道该如何通过更精准的营销提高效率。该服装公司旗下有多个品牌。这是一个非常泛的商业问题，遇到这样的问题你下一步该怎么办？对，将问题具体化。在我看来，问题的关键在于清晰的定义客户所谓的精准具体指哪方面？是针对某一品牌的线上广告希望能准确锁定消费者的注意力？还是某个特定款式的服装打算打折，想要知道谁最可能对这个打折信息感兴趣？或是不同品牌有不同价位，不同款式，他们想知道关于这些不同类型的服装的产品信息的目标客户群是什么？可能之前他们没有对客户进行区分，广告信息对所有客户都是相同的，现在他们希望更加精确的对不同类型的客户提供不同的服装信息，如可以在用户浏览网页时对不同客户显示不同的服装推荐，可以在不同的杂志刊登不同的广告（不同杂志可能对应不同的读者群），或者对系统内客户发送不同的邮件促销广告。 假设通过进一步交流知道客户感兴趣的是最后一个问题：跨品牌的精准营销。那么接下来我会继续问客户下面的问题： 你们都有什么品牌？这些品牌的价位都是怎样的？风格质量如何？ 你们有哪些关于消费者的数据？（人口统计学数据，在线消费，实体店消费） 是不是还有其它关于消费者选择偏好的数据？ 反复交流过后，我们或许可以得到类似模拟的服装消费者数据，其中包括1000个消费者的人口统计学数据，1年内的在线/实体店消费量，在线购/实体店购买次数，以及关于选择偏好的问卷调查数据。此外，我们可以将该问题定义为聚类问题。在你阅读了全书之后，遇到这种情况可能脑子里已经有一些候选模型了。你还需要和客户协商制定一个结果交付的计划，什么时候交付结果，结果是书面报告还是需要一个演讲展示，在项目进行的过程中是不是需要设定几个时间点双方交流讨论进展情况，有什么新数据，或客户有什么需要补充的，这些和数据分析不直接相关，但对于一个合格的数据科学家而言，项目管理能力同样不可缺少。至此，在这个例子中我们完成从问题到数据这一步骤了。关于这一步骤最后强调一点，在和客户交流的过程中，保证交流的重点是客户要解决的问题，而不是你要使用的分析技术和算法。除非客户对你要用的模型方法有兴趣，一般情况下不需要提及具体可能用到的模型。 4.2 数据到信息 数据到信息的过程牵扯到流程图中的四步：数据准备，数据清理，建模和模型评估。对于数据准备这里不过多介绍，对于非实验环境（如市场的案例），这一步主要是通过SQL从数据库中读取相应数据，或者有些客户以电子表格的形式给你的数据（如调查问卷回复），你需要将不同来源的数据合并起来。对于实验室环境下，如医药，生物相关实验，数据科学家（更常见的称呼是“统计师”，这些几乎全都是根正苗红的统计学背景的研究人员）或许会涉入收集数据之前的实验设计环节，确保收集到的数据能够用来回答相应的问题。 对于从数据预处理到模型检验这些步骤的具体细节和常用方法我会在之后专门章节更详细的介绍。但这里我想先对数据分析这个过程进行一个大体描述。有几个方面值得深入进行探讨。首先大家可以从图上看到，数据预处理占用相当大比例的时间。预处理技术一般指数据的清理、变换或者缺失值填补，这些我们在下面数据预处理的章节会更详细的介绍。虽然数据预处理不是正式建模，但这一步能够极大的影响模型的精确度（对于预测模型）和可解释性。一个简单的例子是通过回归系数估计比较回归模型中各参数对结果变量的贡献，在这个问题中，需要保证数据中各个变量的尺度一致，我们模拟的服装消费者数据中的变量尺度就不一致： # 选取数据框中的列“age”和“income”，得到子数据框 sdat&lt;-subset(sim.dat,select=c(&quot;age&quot;,&quot;income&quot;)) # 对子数据框进行总结 summary(sdat) ## age income ## Min. : 16.00 Min. : 41776 ## 1st Qu.: 25.00 1st Qu.: 85832 ## Median : 36.00 Median : 93869 ## Mean : 38.84 Mean :113543 ## 3rd Qu.: 53.00 3rd Qu.:124572 ## Max. :300.00 Max. :319704 ## NA&#39;s :184 从上面通过summary()函数的输出可以看到（这里我们只展示其中两个变量，想要检查整个数据框的总结信息，可以键入 summary(sim.dat)），收入（income）的观测尺度明显大于年龄（age）。如果没有数据变换，拟合模型的变量系数之间是没有可比性的。 其次，建立和评估模型通常离不开数据划分（交互校验）在服装消费者数据的例子中，如果研究兴趣在于预测一个新的消费者在未来一年里在这个公司服装上的消费额，这种情况下预测的样本和用来建模的样本属于不同的群体。这意味着，我们不仅需要检查模型对当前数据的拟合程度，还需要评估模型外推至新样本的能力。为了做到这一点，我们需要将样本划分成训练集和测试集。训练集用于拟合模型，测试集用于评估模型在新样本上预测的能力。我们会在之后的建模技术模块进一步介绍划分训练集和测试集的不同方法。 实际应用中的数据有时含有许多变量（多维度），这可能导致一系列问题。如由于自变量之间存在相关性导致模型估计不稳定，如果模型是以预测为目的，则会降低预测准确性。太多变量还会导致模型难以解释，极大削弱其应用价值，尤其在市场营销这样的领域，模型的解释性和其应用价值紧密联系。例如，在服装消费者调查问卷中有10个关于消费者对服装不同方面的在意程度的问题，可能在这些问题的回复背后有几个潜在的概念，如对价格的敏感，炫耀性心理的满足，产品质量和风格，这些概念是潜藏在背后促使消费者对表面上10个问题做出回复的因子。如果我们能将这样的问卷调查观测降至少数潜在因子的维度，便可更清晰的识别人们在哪些因子上存在不同，进而更有针对性的进行品牌营销。因为这个话题在实际应用中非常重要，我们会用专门一个章节（特征工程）进行讲解。 关于这部分流程还有一个重要的问题是选择合适的模型评估标准。在使用测试集之前，有两种方法可以用来决定模型的效能。首先，可以使用一些模型表现的度量。根据具体数据评估度量可能不同，比如对离散应变量，我们通常使用AUC，每类的错误率来作为评估标准；对于连续应变量，可以使用\\(R^2\\)，RMSE等方式。另外一个工具是进行简单的模型可视化，如作出观测数据与预测数据的散点图，发掘出模型表现特别好或者特别差的区域。这些定性的信息对改进模型非常关键，如果只是用总结性的统计量去衡量就会失去这些信息。 根据经验，一些建模者会依赖于自己钟爱的某个模型。然而天下没有免费的午餐，关于你要解决的问题信息不足的情况下，没有一个模型总是比其它模型更有效。因此，建模者应该在一个具体问题上尝试各种方法，之后再选择一个特定的模型。 有两种意义上的模型选择。我们可以选择一类模型而放弃另一类，比如因为线性回归模型拟合效果不好，因而用多元自适应回归样条（MARS）。在这种情况下，我们其实是在模型之间进行选择。模型选择还有另外一种形式，就是在同类模型中选择。比如在拟合MARS时，我们通过交叉验证选择模型中的调优参数，这也是一种模型选择。这里我们选择的是MARS模型的具体种类，即是在MARS模型这个大类之内进行选择。在两种情形下，我们都依赖于交叉检验，模型评估标准和测试集得到定量的模型评估，从而帮助我们作出选择。 4.3 信息到行动 业界普遍反映数据科学家欠缺将数据中的信息转化为商业知识的能力，大部分数据科学家止于从数据到信息这一步。如何做到从信息到知识呢？这个需要你对所在行业有较深入的了解，具有应用该行业说故事的能力。这是艺术的部分。这也是一个矛盾的地方，因为这意味着数据科学家不如很多人之前所设想的可以单独成为一个职能，应用在不同的领域。当从技术角度上说这是可行的，但是从实践的角度，仅是应用技术得到一个模型结果远远不够，将结果转化为真正的市场营销的决策建议并和相关营销人员交流，保证结果应用在市场营销中提高效率，是非常关键的收尾环节，这一步决定了你工作的价值。但这一步要求深入了解行业，做到这点需要在某个特定行业中从业较长时间，因此从实践上讲，一个数据分析师很难细致了解每一个行业，即使“从数据到信息”这个步骤可以一般化到不同领域，但从“信息到知识”将数据科学家从某种程度上限定于某一领域。 我想以一个中学时代就听过的故事结束本章：美国福特公司的一台高级马达坏了，公司所有技术人员都束手无策，公司只好请来了德国籍电机专家维修。他经过研究和计算后在电机上划了一条线，说：“打开电机，把划线处线圈减去16圈。”果然，电机很快恢复正常了。福特公司问需要多少酬金。专家回答：1万美元。在场的人都惊呆了：划条线这么贵！德国专家坦然地说：“划条线1美元，知道在哪里划线9999美元。” 现在很多机器学习和统计模型都可以轻易用代码实现，就好像是在电机上划条线，但前提是你知道在什么情况下用什么模型能够解决问题。数据科学家不仅仅是那个编写执行代码的人，更重要的是成为数据科学行业中那个“知道在哪里划线”的人，这是也是本书最重要的写作目的。 "],
["section-5.html", "第5章 数据预处理 5.1 介绍 5.2 数据清理 5.3 缺失值填补 5.4 中心化和标量化 5.5 有偏分布 5.6 处理离群点 5.7 共线性 5.8 稀疏变量 5.9 编码名义变量 5.10 本章总结", " 第5章 数据预处理 5.1 介绍 许多数据分析相关书籍着重介绍模型，算法和统计推断。但在实际应用中，刚到手的原始数据通常都不能直接用于建模。数据预处理是将原始数据转化成能够用于建模的一致数据的过程。建模失败的原因有多种，其中之一就是在建模前没有对数据进行恰当的预处理。数据预处理会极大的影响建模结果，如缺失值填补和对离群点的处理显然会影响统计分析的结果。因此这是整个分析流程中非常关键的一个环节，这一步没有到位，之后的分析就如同在沙地上建房，及其不稳固。 在实际分析项目中，根据数据清理的不同阶段，有下面几类数据： 原始数据 技术上正确的数据 可以用于模型的数据 整合后的数据 设置了固定格式的数据 原始数据是刚开始得到的第一手数据，可以是从数据库读取出的销售数据，市场调查的同事给你的调查问卷回复，研发部门收集的实验数据等等。这些数据集可能很粗糙，不一定能直接读入R。 比如多行表格标题，或者格式不符合要求： 用50%表示百分比而不是0.5，这样R读入时会将其当作字符型； 销售额的缺失值用“-”表示，而不是空格，这样R会将销售额当成字符型； 数据是在幻灯片文档中，或者电子表格不是“.csv”而是“.xlsx”格式； … 总而言之，这样的数据有时不能直接读入R，需要进行一些清理；有些格式的数据需要安装特定的包读取。当对数据进行必要的清理，格式变换后，可以顺利用R读入成数据框时，就是技术上正确的数据。这样的数据载入R后有合理的列标签，变量格式等等。但这不意味着这时的数据是完全无误的，如年龄变量可能是负数，折扣百分比可能大于1，没到法定年龄的某人可能有驾照号码，或者数据缺失。取决于你的具体情况，数据可能存在各种问题。你在建模之前需要对这样的数据进行进一步清理。除了数据本身是否合理的角度对数据进行清理以外，取决于你要使用的模型，还需要从技术层面对数据进行预处理，使之尽可能不要太过偏离模型假设。比如有偏数据，变量量级不同，离群值，变量间的共线性，需要将分类变量转化为数值变量等等。对技术上正确的数据进行进一步清理后就得到可以用于建模的数据。 有时我们需要对数据进行整合。比如你可能需要展示某个产品在不同价格下的年销售量，这时就需要将每天的销售量相加得到相应的年销售量；在聚类分析中，当你得到各个分类后，通常需要对不同类进行描述，这时也需要整合各个类的描述统计量（比如平均年龄，平均收入，年龄标准差等等）。数据整合通常是为了给人展示，或者进一步用于可视化。 对整合好的数据，根据不同客户的要求我们需要更改数据格式。比如简洁明了的行列标签，单元格颜色，对一些需要强调的数据高亮等等。 这里我们建议大家分别储存每一步得到的数据，以及各个处理过程使用的R代码，使得这个过程尽可能可重复。如果需要检查更改某个环节，也相对容易。在本章的剩余部分我们针对建模前的数据预处理以及数据整合。本章我们将介绍一系列的数据清理技术，并用R进行实践。除了技术以外，我们还会讲到如何针对不同的情况选择合适的数据预处理方式。 前面有提到，数据预处理一般指数据的清理、变换或者缺失值填补。不同的模型对于预测变量的类型有不同的敏感度，此外变量以什么形式进入模型也很重要，如5分量表的调查问卷回复是当作因子变量还是数值变量。 本章将介绍无监督数据处理，有监督的方法会在其它章节中讨论。例如，偏最小二乘回归（PLS）模型本质上是主成分分析（PCA）的有监督版本。我们还将描述不考虑因变量时移除自变量的方法。 先载入本章需要的R包（我将在相应代码注释中解释每个包的用途）： # 先安装这些包才能用library()函数载入 # caret: 提供获取、使用、评估成百上千个机器学习模型及其拟合效果的系统交互界面 # 为机器学习提供了结构化的方法并且对一系列机器学习过程进行评估 library(caret) # e1071: 各类计量经济和机器学习的延伸；我们通过naiveBayes()函数进行朴素贝叶斯判别 library(e1071) # gridExtra: 绘图辅助功能，讲不同的图形组合在一起成为图表 library(gridExtra) # lattice: 建立在核心绘图能力上的格子框架图形 library(lattice) # imputeMissings: 填补缺失值 library(imputeMissings) # RANN: 应用k邻近算法 library(RANN) # corrplot: 相关矩阵的高级可视化 library(corrplot) # nnet: 拟合单个潜层级的神经网络模型 library(nnet) # car: 回归模型解释和可视化工具，其它附加功能； 其中包括some()和scatterplotMatrix()函数 library(car) # gpairs: 广义散点图；对混合类别和连续变量产生散点图矩阵 library(gpairs) # reshape2: 灵活重构和整合数据，主要有两个函数melt()和dcast() library(reshape2) # psych: 心理计量学方法和抽样调查分析，尤其是因子分析和项目反应模型； # 我们会使用包中的describe()函数 library(psych) # plyr: 可以将数据分割成更小的数据，然后对分割后的数据进行些操作，最后把操作的结果汇总 library(plyr) # tidyr: 清理揉合数据的包，主要函数是spread()和gather() library(tidyr) 5.2 数据清理 当你得到上述技术上正确的数据后，第一步就是检查数据，看看都有哪些变量，这些变量分布如何，是不是存在错误的观测。我们先读取并且检查服装消费者数据： sim.dat&lt;-read.csv(&quot;/Users/happyrabbit/Documents/GitHub/DataScientistR/Data/SegData.csv&quot;) summary(sim.dat) ## age gender income house ## Min. : 16.00 Female:554 Min. : 41776 No :432 ## 1st Qu.: 25.00 Male :446 1st Qu.: 85832 Yes:568 ## Median : 36.00 Median : 93869 ## Mean : 38.84 Mean :113543 ## 3rd Qu.: 53.00 3rd Qu.:124572 ## Max. :300.00 Max. :319704 ## NA&#39;s :184 ## store_exp online_exp store_trans online_trans ## Min. : -500.0 Min. : 68.82 Min. : 1.00 Min. : 1.00 ## 1st Qu.: 205.0 1st Qu.: 420.34 1st Qu.: 3.00 1st Qu.: 6.00 ## Median : 329.0 Median :1941.86 Median : 4.00 Median :14.00 ## Mean : 1356.8 Mean :2120.18 Mean : 5.35 Mean :13.55 ## 3rd Qu.: 597.3 3rd Qu.:2440.78 3rd Qu.: 7.00 3rd Qu.:20.00 ## Max. :50000.0 Max. :9479.44 Max. :20.00 Max. :36.00 ## ## Q1 Q2 Q3 Q4 ## Min. :1.000 Min. :1.000 Min. :1.000 Min. :1.000 ## 1st Qu.:2.000 1st Qu.:1.000 1st Qu.:1.000 1st Qu.:2.000 ## Median :3.000 Median :1.000 Median :1.000 Median :3.000 ## Mean :3.101 Mean :1.823 Mean :1.992 Mean :2.763 ## 3rd Qu.:4.000 3rd Qu.:2.000 3rd Qu.:3.000 3rd Qu.:4.000 ## Max. :5.000 Max. :5.000 Max. :5.000 Max. :5.000 ## ## Q5 Q6 Q7 Q8 ## Min. :1.000 Min. :1.000 Min. :1.000 Min. :1.000 ## 1st Qu.:1.750 1st Qu.:1.000 1st Qu.:2.500 1st Qu.:1.000 ## Median :4.000 Median :2.000 Median :4.000 Median :2.000 ## Mean :2.945 Mean :2.448 Mean :3.434 Mean :2.396 ## 3rd Qu.:4.000 3rd Qu.:4.000 3rd Qu.:4.000 3rd Qu.:3.000 ## Max. :5.000 Max. :5.000 Max. :5.000 Max. :5.000 ## ## Q9 Q10 segment ## Min. :1.000 Min. :1.00 Conspicuous:200 ## 1st Qu.:2.000 1st Qu.:1.00 Price :250 ## Median :4.000 Median :2.00 Quality :200 ## Mean :3.085 Mean :2.32 Style :350 ## 3rd Qu.:4.000 3rd Qu.:3.00 ## Max. :5.000 Max. :5.00 ## 发现什么问题没有？问卷调查回复Q1-Q10貌似合理，最小值都是1，最大值是5，因为问卷分值表是1-5。在实体店交易次数（store_trans）和在线交易次数（store_trans）看上去也合理。收入（income）有缺失值，处理缺失值的问题我们在下一小节会介绍。在线花销（online_exp）分布看上去没什么问题。实体店花销（store_exp）存在离群值，最大值有50000人民币，有个别受访者有着浓浓的土豪味。还有呢？大家可能已经发现其中有负值（最小值是－500），花销不可能是负数，这里你就要怀疑存在输入错误。类似的，age也存在不大可能的观测值，最大年龄是300，如果不是真的碰到青春期的千年老妖的话，这个年龄值应该是错误的。那怎么处理这些错误的值呢？取决于你的实际情况，如果你的样本量很大，不在乎这几个样本，可以删除这些不合理的值。在这里我们一共就1000个观测，而且问卷调查的数据在现实中通常都不容易获得，需要花费人力财力，故若因为其中某一个变量的值错误就将整个样本删去在此情况下有些可惜，所以我们可以将这些值设置成缺失状态，在下一步介绍缺失值处理的时候进行填补。 # 将错误的年龄观测设置为缺失值 sim.dat$age[which(sim.dat$age&gt;100)]&lt;-NA # 将错误的实体店购买观测设置为缺失值 sim.dat$store_exp[which(sim.dat$store_exp&lt;0)]&lt;-NA # 通过summary()函数检查清理情况 summary(subset(sim.dat,select=c(&quot;age&quot;,&quot;income&quot;))) ## age income ## Min. :16.00 Min. : 41776 ## 1st Qu.:25.00 1st Qu.: 85832 ## Median :36.00 Median : 93869 ## Mean :38.58 Mean :113543 ## 3rd Qu.:53.00 3rd Qu.:124572 ## Max. :69.00 Max. :319704 ## NA&#39;s :1 NA&#39;s :184 现在我们接着处理数据中的缺失值。 5.3 缺失值填补 缺失值填补是个大话题，单这一个就可以写本书。这里不会详细给大家介绍这个子领域，而是展示一些常用的缺失值处理方法，并且根据个人的工作经验对这些方法进行一些说明。对想进一步学习的读者，可以参考本小节中提到的相关参考资料。缺失值填补是对缺失的数据观测进行估计的过程。De Waal, Pannekoek和Scholtus的书(Ton de Waal 2011)中第7章对现存的一些缺失值填补方法做了简洁的概述。具体填补方法的选择取决于你的实际应用情况：关于缺失数据你有哪些辅助信息（比如如果客户管理系统将没有购买的客户对应的购买量设置为缺失的话，那你就该用0填补），变量之间是不是有些相关性限制（比如关于是否有驾照信息的缺失就可能受到年龄的限制，如果某人年龄低于16周岁，那就不可能有驾照信息）。因为情况各不相同，所以没有某个方法永远比其它方法好。 决定处理缺失值的方法之前要先了解缺失的原因等关于缺失的辅助信息。缺失是随机发生的么？如果这样的话，可以用中位数/众数填补，也可以使用均值填补。或者缺失其实是有潜在发生机制的？比如年龄大的人在问卷调查中更不愿意透露年龄，这样关于年龄的缺失就不是随机发生的，如果用均值或者中位数填补可能产生很大偏差。这时需要利用年龄和其它自变量的关系对缺失的值进行估计。比如可以基于那些没有缺失值的数据，用是否有子女，收入，问卷调查的回复这些信息来对年龄建模，然后用拟合的模型来进行预测（如用树模型）。 此外，建模的目的对于选择缺失值填补方法也很重要。若建模目的是对传统统计模型结果进行解释和推断，那么仔细研究缺失机制，尽可能用非缺失信息建模来估计缺失值就显得尤为重要。相反，如果建模的目的是预测，大部分情况下不会很严格的研究缺失机制（缺失机制很明显的时候除外），在缺失机制不太清楚的情况下，可以当成随机缺失进行填补（使用均值，中位数或者用K-近邻）。由于统计推断对缺失值更加敏感，所以抽样调查统计学对各种缺失值填补方法进行了深入的研究，这些研究着重于有效的统计推断。而在预测模型中的缺失值填补的问题和抽样调查中的有所不同，因为预测模型的主要目的是预测而非推断。因此关于预测模型中的缺失值填补方面的研究文献没有传统抽样调查统计那么多。想深入学习这方面知识的读者可以参考Saar-Tsechansky 和 Provost对判别模型中不同缺失值方法的比较(Saar-Tsechansky M 2007b)。还有之前提到的De Waal, Pannekoek和Scholtus的书(Ton de Waal 2011)。 5.3.1 中位数或众数填补 在假设随机缺失的情况下，一个常用的填补方法是可以用含有缺失观测变量的中位数（连续变量）或者众数（分类变量）填补缺失值。 imputeMissings包中的函数impute()可以实现这类填补。我们用该函数对sim.dat数据框填补缺失值： # 将填补后的数据存在另外一个数据框中 demo_imp&lt;-impute(sim.dat,method=&quot;median/mode&quot;) # 只检查前5列，因为后面没有缺失值 summary(demo_imp[,1:5]) ## age gender income house store_exp ## Min. :16.00 Female:554 Min. : 41776 No :432 Min. : 155.8 ## 1st Qu.:25.00 Male :446 1st Qu.: 87896 Yes:568 1st Qu.: 205.1 ## Median :36.00 Median : 93869 Median : 329.8 ## Mean :38.58 Mean :109923 Mean : 1357.7 ## 3rd Qu.:53.00 3rd Qu.:119456 3rd Qu.: 597.3 ## Max. :69.00 Max. :319704 Max. :50000.0 从上面输出可以看到，填补后的数据框demo_imp没有缺失值。这个方法简单迅速，在工作中经常使用。但其有一个缺点，单独对每个变量进行缺失值填补而没有考虑到变量之间的关系，因而有的时候不太准确，如果缺失的比例较大，且你的建模目的是统计推断，建议进一步研究变量之间的关系，发觉缺失机制，通过对缺失变量建模来进行填补。上面的例子中缺失变量都是数值变量，如果缺失变量是分类／因子变量的话，impute()函数会用众数进行填补。 你也可以用上面讲到的preProcess()函数进行中位数填补，但该函数只针对数值变量，不能对分类变量进行众数填补。由于这里含有缺失值的都是数值变量，可以使用preProcess()函数。得到的结果和之前impute()函数相同。preProcess()函数的功能很强大，你可以将其想象成连接各种数据预处理方法的接口，我们之后会介绍该函数其它数据预处理功能。 imp&lt;-preProcess(sim.dat,method=&quot;medianImpute&quot;) demo_imp2&lt;-predict(imp,sim.dat) summary(demo_imp2[,1:5]) ## age gender income house store_exp ## Min. :16.00 Female:554 Min. : 41776 No :432 Min. : 155.8 ## 1st Qu.:25.00 Male :446 1st Qu.: 87896 Yes:568 1st Qu.: 205.1 ## Median :36.00 Median : 93869 Median : 329.8 ## Mean :38.58 Mean :109923 Mean : 1357.7 ## 3rd Qu.:53.00 3rd Qu.:119456 3rd Qu.: 597.3 ## Max. :69.00 Max. :319704 Max. :50000.0 5.3.2 K-近邻填补 直观的讲，K-邻近方法（也称为KNN）就是“物以类聚”这一思想的统计学表达。如果某人留着海藻般的长发，喜欢安妮宝贝的“现世安稳，岁月静好”，一会明亮一会忧伤，问题是你要知道这个人喜欢什么样的鞋子怎么办？去看看其他那些留着同样长发喜欢同样句子又明亮又忧伤的人都穿什么鞋子，然后你很可能得出结论：此人喜欢棉布鞋。 技术上讲，K-近邻方法建立在距离的定义之上（通常时欧几里德距离），其基本思路是对于含有缺失值的样本，寻找该离样本最近的K个样本（邻居），然后用这些邻居的观测均值对该样本的缺失值进行填补。由于这里找邻居根据的是样本点之间的距离，各个变量的标度需要统一，不然尺度大的变量在决定距离上会占主导作用。 我们使用preProcess()实现KNN填补。 imp&lt;-preProcess(sim.dat,method=&quot;knnImpute&quot;,k=5) # 用predict()函数进行KNN填补 demo_imp&lt;-predict(imp,sim.dat) Error in `[.data.frame`(old, , non_missing_cols, drop = FALSE) : undefined columns selected 程序报错说“undefined columns selected（选择了无法定义的列）”。这是因为sim.dat中有函数无法处理的非数值型变量，在上面代码的第一行使用preProcess()时，函数会自动忽略非数值型变量，所以你运行第一行代码没有问题。但是在第二行代码，通过predict()函数对数据框进行KNN填补时，数据框有非数值变量就会导致填补无法进行。我们移除这些变量，再进行填补就没有问题了。 # 找到因子变量 imp&lt;-preProcess(sim.dat,method=&quot;knnImpute&quot;,k=5) idx&lt;-which(lapply(sim.dat,class)==&quot;factor&quot;) demo_imp&lt;-predict(imp,sim.dat[,-idx]) summary(demo_imp[,1:3]) ## age income store_exp ## Min. :-1.5910972 Min. :-1.43989 Min. :-0.43345 ## 1st Qu.:-0.9568733 1st Qu.:-0.53732 1st Qu.:-0.41574 ## Median :-0.1817107 Median :-0.37606 Median :-0.37105 ## Mean : 0.0000156 Mean : 0.02389 Mean :-0.00042 ## 3rd Qu.: 1.0162678 3rd Qu.: 0.21540 3rd Qu.:-0.27437 ## Max. : 2.1437770 Max. : 4.13627 Max. :17.52734 当前的例子中非数值型的变量只有因子，你在自己应用该方法时要注意是否还有字符型（character）变量。 lapply(data,class)命令可以返回一个数据框各列对应类别的列表，你只要将data换成你处理的数据框名即可。这里我们的数据框是sim.dat，于是你可以这样获得数据框各列的类别： # 这里只显示前3个元素 lapply(sim.dat,class)[1:3] ## $age ## [1] &quot;integer&quot; ## ## $gender ## [1] &quot;factor&quot; ## ## $income ## [1] &quot;numeric&quot; 将KNN填补的结果和之前中位数填补结果进行比较大家发现什么了？结果好像完全不一样。KNN返回的数据框整个尺度都变了。这是因为当你告诉preProcess()函数进行KNN填补时（选项method=&quot;knnImpute&quot;），函数会自动对数据进行标准化（下一小节将提到的中心化和标量化）。另外一种方法是使用下一小节中介绍的袋状树填补。关于KNN填补还有一点需要注意，算法无法对整行缺失的观测进行填补。这个并不难从直观上理解，既然改算法是通过邻近点的取值平均来填补，如果某一样本所有值都缺失，那怎么定义该样本的邻近点呢？下面我们在原数据框上添加一个所有观测都缺失的样本，将这个新的数据框储存在temp对象中，然后对temp进行KNN填补，看看会发生什么： temp&lt;-rbind(sim.dat,rep(NA,ncol(sim.dat))) imp&lt;-preProcess(sim.dat,method=&quot;knnImpute&quot;,k=5) idx&lt;-which(lapply(temp,class)==&quot;factor&quot;) demo_imp&lt;-predict(imp,temp[,-idx]) Error in FUN(newX[, i], ...) : cannot impute when all predictors are missing in the new data point 运行结果中有错误警告“cannot impute when all predictors are missing in the new data point（当样本对应的所有观测都缺失时无法填补）”。我们在拿到数据的时可以查找并删除这些完全缺失的样本。我们可以通过下面代码找到这样的行： idx&lt;-apply(temp,1,function(x) sum(is.na(x)) ) as.vector(which(idx==ncol(temp))) ## [1] 1001 结果显示第1001行所有的观测都缺失。你可以进一步删除所有观测都缺失的行。 5.3.3 袋状树填补 另外一种填补方法是装袋树（Bagging，bootstrap aggregation 的缩写），最初由 Leo Breiman 提出，它是最早发展起来的集成方法之一(L 1966a)。对于数据中需要填补的变量，我们使用剩下的变量训练装袋树，然后再用训练出的树来对缺失值进行预测。虽然理论上说该方法更加强大，但计算量比KNN大了许多。在实际应用中，你一定要根据自己的具体情况选择填补的方式。因为你可以不断的探索使用理论上更加精确的填补方式，但也要考虑在这上面花的时间成本是不是值得，如果一个中位数，或者均值填补就可以满足建模需要，即使用袋状树填补可以提高些许精度，但是提升很可能非常小，没有太大的实际意义，尤其在样本量很大的时候。袋状树本身就是一个模型，可以用于回归和判别，我们之后在介绍树相关的方法时会进一步介绍。下面我们用preProcess()对数据框sim.dat进行袋状树填补。 imp&lt;-preProcess(sim.dat,method=&quot;bagImpute&quot;) demo_imp&lt;-predict(imp,sim.dat) summary(demo_imp[,1:5]) age gender income house store_exp Min. :16.00 Female:554 Min. : 41776 No :432 Min. : 155.8 1st Qu.:25.00 Male :446 1st Qu.: 86762 Yes:568 1st Qu.: 205.1 Median :36.00 Median : 94739 Median : 329.0 Mean :38.58 Mean :114629 Mean : 1357.6 3rd Qu.:53.00 3rd Qu.:123726 3rd Qu.: 597.3 Max. :69.00 Max. :319704 Max. :50000.0 5.4 中心化和标量化 这是最基本的数据变换。中心化是通过将变量的每个观测减去该变量均值，这样中心化后的变量观测均值为0。标量化是将变量观测除以变量标准差，标量化后的变量标准差为1。对于一些要对变量进行线性组合的模型，中心化和标量化保证了变量的线性组合是基于组合后的新变量能够解释的原始变量中的方差。统计学上的方差，从直观的角度讲就是信息。试想若上面的消费者数据中，所有人的在线交易次数都是相同的，那么在线交易次数这个变量方差就是0，从应用的角度这个变量没有给我们提供可以区分这些消费者的信息。用到基于方差的变量线性组合的模型有主成分分析（PCA）(Jolliffe 2002)，偏最小二乘分析（PLS）(Geladi P 1986)，探索性因子分析（EFA）(Mulaik 2009)等等。这两个变换可以很容易自己编写程序实施，我们以服装装消费者数据（sim.dat）中的收入（income）变量为例： income&lt;-sim.dat$income # 变量income的均值，na.rm=T告诉R忽略缺失值 mux&lt;-mean(income,na.rm=T) # 变量income的标准差，na.rm=T告诉R忽略缺失值 sdx&lt;-sd(income,na.rm=T) # 中心化 tr1&lt;-income-mux # 标量化 tr2&lt;-tr1/sdx 上面代码中，tr1是income中心化后的结果。tr2是对中心化后的结果tr1进一步标量化的结果。下面是关于这三步分别得到结果的总结： summary(data.frame(cbind(income,tr1,tr2))) ## income tr1 tr2 ## Min. : 41776 Min. :-71767 Min. :-1.4399 ## 1st Qu.: 85832 1st Qu.:-27711 1st Qu.:-0.5560 ## Median : 93869 Median :-19674 Median :-0.3947 ## Mean :113543 Mean : 0 Mean : 0.0000 ## 3rd Qu.:124572 3rd Qu.: 11029 3rd Qu.: 0.2213 ## Max. :319704 Max. :206161 Max. : 4.1363 ## NA&#39;s :184 NA&#39;s :184 NA&#39;s :184 可以看到，中心化后的tr1均值为0，但取值跨度依旧很大。接着标量化后的tr2就是一个均值为0，标准差为1的向量。 你也可以直接用caret包中的函数preProcess()对多个变量同时进行中心化和标量化，这里我选取其中2个变量进行展示： sdat&lt;-subset(sim.dat,select=c(&quot;age&quot;,&quot;income&quot;)) # method选项用于设置变换的方式，你可以同时进行一系列变换。 # center：中心化 # scale：标量化 trans&lt;-preProcess(sdat,method=c(&quot;center&quot;,&quot;scale&quot;)) # preProcess函数的给出的还不是变换后的结果 # 你需要通过predict函数对你想要变换的数据应用preProcess的结果才能够得到变换后的数据框 transformed&lt;-predict(trans,sdat) 变换后这两个变量就分布在相似的尺度上： summary(transformed) ## age income ## Min. :-1.5911 Min. :-1.4399 ## 1st Qu.:-0.9569 1st Qu.:-0.5560 ## Median :-0.1817 Median :-0.3947 ## Mean : 0.0000 Mean : 0.0000 ## 3rd Qu.: 1.0163 3rd Qu.: 0.2213 ## Max. : 2.1438 Max. : 4.1363 ## NA&#39;s :1 NA&#39;s :184 有时只要标量化数据而不一定要中心化。例如，如果模型中有针对参数估计绝对值的罚函数和且通过调优参数来进行变量选择（如LASSO）的话，变量大体在一个量级范围内使得能确保对参数“公平”的变量选择。之后在对于收缩方法（shrinkage method）的介绍能够帮助你更清楚的理解这一点。当然，如果你要通过参数估计衡量各个自变量和应变量之间关系强度的话，必须要对变量观测标量化。我是收缩方法的重度使用者，在工作中，我针对自己的分析项目设计了下面这种变换方式，该方式对我处理的问题非常有效，称其为分位数变换吧： \\[\\label{eq:quantile1} x_{ij}^{*}=\\frac{x_{ij}-quantile(x_{.j},0.01)}{quantile(x_{.j}-0.99)-quantile(x_{.j},0.01)}\\] 这里\\(x_{ij}\\)代表第i个样本的第j个变量观测，\\(quantile(x_{.j},0.01)\\)指的是第j个变量所有样本观测组成的向量的1%分位数，类似的\\(quantile(x_{.j},0.99)\\)是99%分位数，的这里之所以使用99%和1%分位数，而非最大值和最小值是为了减弱离群点的影响。编写函数进行分位数变换 非常容易： qscale&lt;-function(dat){ for (i in 1:ncol(dat)){ up&lt;-quantile(dat[,i],0.99) low&lt;-quantile(dat[,i],0.01) diff&lt;-up-low dat[,i]&lt;-(dat[,i]-low)/diff } return(dat) } 这里我们对中位数填补后的数据集demo_imp2中的变量收入（income），实体店消费（store_exp）和在线消费（online_exp）进行上面的分位数变换： demo_imp3&lt;-qscale(subset(demo_imp2,select=c(&quot;income&quot;,&quot;store_exp&quot;,&quot;online_exp&quot;))) summary(demo_imp3) ## income store_exp online_exp ## Min. :-0.05776 Min. :-0.003407 Min. :-0.006023 ## 1st Qu.: 0.15736 1st Qu.: 0.003984 1st Qu.: 0.042719 ## Median : 0.18521 Median : 0.022704 Median : 0.253691 ## Mean : 0.26009 Mean : 0.176965 Mean : 0.278417 ## 3rd Qu.: 0.30456 3rd Qu.: 0.062849 3rd Qu.: 0.322871 ## Max. : 1.23857 Max. : 7.476996 Max. : 1.298845 变换后的变量取值基本分布在0-1的范围之内。 5.5 有偏分布 如果模型要求变量服从一定的对称分布（如正态分布）时，则需要进行数据变换去除分布的偏度。偏度是3阶标准化中心矩，是用来衡量分布不对称程度的，该统计量的数学定义如下： \\[偏度=\\frac{\\sum(x_{i}+\\bar{x})^{3}}{(n-1)v^{3/2}}\\] \\[v=\\frac{\\sum(x_{i}=\\bar{x})^{2}}{(n-1)}\\] # 需要使用e1071包中的偏度计算函数skewness() set.seed(1000) par(mfrow=c(1,2),oma=c(2,2,2,2)) # 抽取1000个自由度为2的开方分布，右偏分布 x1&lt;-rchisq(1000,2, ncp = 0) # 通过x1得到对应的左偏分布变量x2 x2&lt;-max(x1)-x1 plot(density(x2),family =&quot;Songti SC&quot;,main=paste(&quot;左偏，偏度＝&quot;,round(skewness(x2),2)), xlab=&quot;X2&quot;) plot(density(x1),family =&quot;Songti SC&quot;,main=paste(&quot;右偏，偏度＝&quot;,round(skewness(x1),2)), xlab=&quot;X1&quot;) Figure 5.1: 有偏分布展示 分布是否有偏可以很容易从图上看到。图5.1显示了两种类型的不对称分布。分布对称时偏度=0，分布左偏时偏度&lt;0，分布右偏时偏度&gt;0，且偏离程度越大，偏度统计量的绝对值越大。有很多变换有助于去除偏度，如log变换，平方根或者取倒数。但是仅仅靠观察图形无法知道哪种变换方法最好。大家有没有注意到，常用的一些变换都和指数函数有关，如 \\(log(x)\\)（这个是对数函数，但也是指数函数的近亲嘛：））、\\(x^2\\) 和 \\(\\frac{1}{x}\\)。于是Box和Cox（1964）(Box G 1964)提出了含有一个参数\\(\\lambda\\)的指数变换族： \\[x^{*}=\\begin{cases} \\begin{array}{c} \\frac{x^{\\lambda}-1}{\\lambda}\\\\ log(x) \\end{array} &amp;amp; \\begin{array}{c} if\\ \\lambda\\neq0\\\\ if\\ \\lambda=0 \\end{array}\\end{cases}\\] 很容易看出这个变换族群包含了\\(log(x)\\)变换（\\(\\lambda\\)=0），\\(x^2\\)变换（\\(\\lambda\\)=2），\\(sqrt(x)\\)变换（\\(\\lambda\\)=0.5）以及\\(frac{1}{x}\\)变换（\\(\\lambda\\)=-1）等常用的变换。Box-Cox覆盖的面更加广，变换指数可能是任意实数。caret包中有两个函数可以进行该变换，一个是BoxCoxTrans()。另外一个是我们之前用到的preProcess()，这里推荐大家使用后者，因为我们可以通过更改其中method选项对数据进行不同的变换，可以把该函数看作是不同预处理方法的接口，熟练使用后大家会发现其功能强大且及其方便。这里给大家插播一则广告，关于psych包中的describe()函数。我很喜欢用这个函数来在处理数据的不同阶段检查各个变量的情况，偏度，峰度，是不是可能有离群值，取值范围，均值等等，这个函数比summary()好用多了。当然这因人而异，有的人喜欢通过散点图矩阵来看各个变量的分布情况，个人还是喜欢看变量分布的各个统计量。 describe(sim.dat) ## vars n mean sd median trimmed mad ## age 1 999 38.58 14.19 36.00 37.67 16.31 ## gender* 2 1000 1.45 0.50 1.00 1.43 0.00 ## income 3 816 113543.07 49842.29 93868.68 104841.94 28989.47 ## house* 4 1000 1.57 0.50 2.00 1.58 0.00 ## store_exp 5 999 1358.71 2775.17 329.80 845.14 197.47 ## online_exp 6 1000 2120.18 1731.22 1941.86 1874.51 1015.21 ## store_trans 7 1000 5.35 3.70 4.00 4.89 2.97 ## online_trans 8 1000 13.55 7.96 14.00 13.42 10.38 ## Q1 9 1000 3.10 1.45 3.00 3.13 1.48 ## Q2 10 1000 1.82 1.17 1.00 1.65 0.00 ## Q3 11 1000 1.99 1.40 1.00 1.75 0.00 ## Q4 12 1000 2.76 1.16 3.00 2.83 1.48 ## Q5 13 1000 2.94 1.28 4.00 3.05 0.00 ## Q6 14 1000 2.45 1.44 2.00 2.43 1.48 ## Q7 15 1000 3.43 1.46 4.00 3.54 0.00 ## Q8 16 1000 2.40 1.15 2.00 2.36 1.48 ## Q9 17 1000 3.08 1.12 4.00 3.23 0.00 ## Q10 18 1000 2.32 1.14 2.00 2.27 1.48 ## segment* 19 1000 2.70 1.15 3.00 2.75 1.48 ## min max range skew kurtosis se ## age 16.00 69.00 53.00 0.47 -1.18 0.45 ## gender* 1.00 2.00 1.00 0.22 -1.95 0.02 ## income 41775.64 319704.34 277928.70 1.69 2.57 1744.83 ## house* 1.00 2.00 1.00 -0.27 -1.93 0.02 ## store_exp 155.81 50000.00 49844.19 8.08 115.04 87.80 ## online_exp 68.82 9479.44 9410.63 1.18 1.31 54.75 ## store_trans 1.00 20.00 19.00 1.11 0.69 0.12 ## online_trans 1.00 36.00 35.00 0.03 -0.98 0.25 ## Q1 1.00 5.00 4.00 -0.12 -1.36 0.05 ## Q2 1.00 5.00 4.00 1.13 -0.32 0.04 ## Q3 1.00 5.00 4.00 1.06 -0.40 0.04 ## Q4 1.00 5.00 4.00 -0.18 -1.46 0.04 ## Q5 1.00 5.00 4.00 -0.60 -1.40 0.04 ## Q6 1.00 5.00 4.00 0.11 -1.89 0.05 ## Q7 1.00 5.00 4.00 -0.90 -0.79 0.05 ## Q8 1.00 5.00 4.00 0.21 -1.33 0.04 ## Q9 1.00 5.00 4.00 -0.68 -1.10 0.04 ## Q10 1.00 5.00 4.00 0.39 -1.23 0.04 ## segment* 1.00 4.00 3.00 -0.20 -1.41 0.04 大家可以很清楚的看到哪些变量有偏（skew），通过比较均值（mean）和修剪后的均值（trimmed），大概知道哪些变量可能存在离群值。修剪后的均值是去除最大和最小的一部分观测点后得出的均值。这两个值差距很大说明对应变量存在离群点，这里很明显在线消费（store_exp）存在离群点。 下面我们以数据集sim_dat中的变量store_trans和online_trans为例展示Box-Cox变换函数的使用方法： # 选取需要的两列，存在dat_bc中 dat_bc&lt;-subset(sim.dat,select=c(&quot;store_trans&quot;,&quot;online_trans&quot;)) (trans&lt;-preProcess(dat_bc,method=c(&quot;BoxCox&quot;))) ## Created from 1000 samples and 2 variables ## ## Pre-processing: ## - Box-Cox transformation (2) ## - ignored (0) ## ## Lambda estimates for Box-Cox transformation: ## 0.1, 0.7 上面的输出的第一行显示了样本量是1000，有2个变量。输出的最后一行表明每个变量对应的参数\\(\\lambda\\)估计值（0.1和0.7）。和之前一样，我们接着用predict()函数对数据框应用估计的变换： transformed&lt;-predict(trans,dat_bc) par(mfrow=c(1,2),oma=c(2,2,2,2)) hist(dat_bc$store_trans,main=&quot;原始商店消费次数&quot;,xlab=&quot;store_trans&quot;,family =&quot;Songti SC&quot;) hist(transformed$store_trans,main=&quot;变换后商店消费次数&quot;,xlab=&quot;store_trans&quot;,family =&quot;Songti SC&quot;) Figure 5.2: Box-Cox变换展示 从图5.2可以看到，变换前商店消费量分布明显右偏，变换后情况显著改善，基本对称。BoxCoxTrans() 也可以进行Box-Cox变换，使用方法类似。但要注意的是BoxCoxTrans()只能作用于单个数值变量，不能像之前那样对一个数据框中的列一次性进行变换。 (trans&lt;-BoxCoxTrans(dat_bc$store_trans)) ## Box-Cox Transformation ## ## 1000 data points used to estimate Lambda ## ## Input data summary: ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.00 3.00 4.00 5.35 7.00 20.00 ## ## Largest/Smallest: 20 ## Sample Skewness: 1.11 ## ## Estimated Lambda: 0.1 ## With fudge factor, Lambda = 0 will be used for transformations transformed&lt;-predict(trans,dat_bc$store_trans) skewness(transformed) ## [1] -0.2154708 参数\\(\\lambda\\)的估计和之前相同（0.1），原始观测的偏度为1.1，变换后偏度为－0.2，虽然不严格为0，但和之前比较有极大改善，可以用来计算变量间线性相关性，用于回归等。 5.6 处理离群点 有时判断离群点并不是那么容易，因为没有一个非黑即白的标准。箱线图和直方图等一些基本的可视化可以用来初步检查是否有离群点。举个例子，我们可以对服装消费者数据中的数值型非问卷调查变量进行可视化： # 选取数值型非问卷调查变量 sdat&lt;-subset(sim.dat,select=c(&quot;age&quot;,&quot;income&quot;,&quot;store_exp&quot;,&quot;online_exp&quot;,&quot;store_trans&quot;,&quot;online_trans&quot; )) # 用car包中的函数scatterplotMatrix()绘制散点图矩阵 par(oma=c(2,2,1,2)) scatterplotMatrix(sdat,diagonal=&quot;boxplot&quot;,smoother=FALSE) Figure 5.3: 数值型非问卷调查变量散点图矩阵 从图5.3，商店消费量（store_exp）明显有离群点（记得之前的土豪么）。你也可以从中看到一些变量两两之间的关系。年龄和在线交易次数负相关，但和实体店交易次数正相关，貌似年纪大的人更倾向于实体店购买。当然，还有消费量和收入正相关。这样的散点图简单但是有效，可以在建模之前告诉你很多关于数据的信息。 除了可视化这样直观的方式外，在一定的假设条件下，有一些统计学的定义离群值的方法。如常用Z分值来判断可能的离群点。对于某观测变量\\(\\mathbf{Y}\\)的Z分值定义为： \\[Z_{i}=\\frac{Y_{i}-\\bar{Y}}{s}\\] 其中\\(\\bar{Y}\\)和\\(s\\)分别为观测列的均值和标准差。直观的理解Z分值就对观测离均值的距离的度量（多少个标准差单位）。这种方法可能具有误导性，尤其是在样本量小的时候。Iglewicz和Hoaglin提出使用修正后的Z分值来判断离群点(Iglewicz and Hoaglin 1993)： \\[M_{i}=\\frac{0.6745(Y_{i}-\\bar{Y})}{MAD}\\] 其中MAD是一系列\\(|Y_{i}-\\bar{Y}|\\)的中位数，称为绝对离差中位数。他们建议将上面修正后的Z分值大于3.5的点标记为可能的离群点。我们来检查下商店消费量观测对应的Z分值： # 计算商店消费量的绝对离差中位数，这里用na.omit()告诉R忽略缺失值 ymad&lt;-mad(na.omit(sdat$income)) # 计算Z分值 zs&lt;-(sdat$income-mean(na.omit(sdat$income)))/ymad # 看看有多少个离群点 sum(na.omit(zs&gt;3.5)) ## [1] 59 用该标准，商店消费量对应的离群点有59个。关于离群点还有其它不同的检测。更多关于不同检测方法可以参考(Iglewicz and Hoaglin 1993)。 很重要的一点是离群点的影响取决于你使用的模型。有的模型对离群值很敏感，如线性回归，逻辑回归。有的对离群点具有抗性，如基于树的模型，支持向量机。此外离群点和错误的观测不一样，它是真实的观测，其中包含信息，所以不能随意的删除。如果你使用的模型对离群点非常敏感，可以使用空间表示变换(Serneels S 2006)。该变换将自变量取值映射到高维的球面上。变换公式如下： \\[x_{ij}^{*}=\\frac{x_{ij}}{\\sqrt{\\sum_{j=1}^{p}x_{ij}^{2}}}\\] 其中\\(x_{ij}\\)表示第i个样本对应第j个变量的观测。由公式可见，每个样本都除以了它们的平方模。公式的分母其实可以看作是该样本到p维空间0点的欧几里德距离，有三点需要特别注意： 在该变换前需要对自变量标准化 与中心化和标准化不同，这个变换操作的对象是所有自变量 如果需要移除变量（之后会提到移除高度相关变量），这一步必须要在空间表示变换之前，否者会导致一系列问题 作为例子，我们用caret包中的spatialSign()函数对收入和年龄两个变量进行空间表示变换： # 用KNN填补缺失值 sdat&lt;-sim.dat[,c(&quot;income&quot;,&quot;age&quot;)] imp&lt;-preProcess(sdat,method=c(&quot;knnImpute&quot;),k=5) sdat&lt;-predict(imp,sdat) transformed &lt;- spatialSign(sdat) transformed &lt;- as.data.frame(transformed) par(mfrow=c(1,2),oma=c(2,2,2,2)) plot(income ~ age,data = sdat,col=&quot;blue&quot;,main=&quot;变换前&quot;,family =&quot;Songti SC&quot;) plot(income ~ age,data = transformed,col=&quot;blue&quot;,main=&quot;变换后&quot;,family =&quot;Songti SC&quot;) Figure 5.4: 空间表示变换图 细心的读者可能已经发现，上面的代码中貌似并没有对数据进行标准化。如果你还记得，在介绍KNN填补的时候讲过，preProcess()在进行KNN的同时默认会对数据框进行标准化，所以上面代码没有特地标准化数据。 5.7 共线性 共线性可能是大多数地球人都听过的一个词了，即使在传统企业市场部的营销人员都知道这个词，这也可能是他们唯一可以拿出来显摆的技术词汇。关于变量间的相关性，有个非常好用的包corrplot，其中同名函数corrplot()能够对变量相关矩阵进行可视化。函数中有一个选项可以设置变量的排序方式，使得相关性高的变量排列在一起。和之前一样，我们选取其中数值类的非问卷调查变量为例子，展示如何使用该函数探索变量之间共线性： # 选取数值型非问卷调查变量 sdat&lt;-subset(sim.dat,select=c(&quot;age&quot;,&quot;income&quot;,&quot;store_exp&quot;,&quot;online_exp&quot;,&quot;store_trans&quot;,&quot;online_trans&quot; )) # 用装袋树填补，换着用，帮大家练练手：） imp&lt;-preProcess(sdat,method=&quot;bagImpute&quot;) sdat&lt;-predict(imp,sdat) # 得到相关矩阵 correlation&lt;-cor(sdat) # 对相关矩阵作图 par(oma=c(2,2,2,2)) corrplot.mixed(correlation,order=&quot;hclust&quot;,tl.pos=&quot;lt&quot;,upper=&quot;ellipse&quot;) Figure 5.5: 相关矩阵可视化 这里我们使用的是corrplot.mixed()函数对相关矩阵可视化（图5.5）。其中相关性越接近0颜色越浅且形状越接近圆，相关性不等于0的用椭圆表示（因为我们设置了选项upper=“ellipse”），相关性越大椭圆越窄，蓝色代表正相关，红色代表负相关，椭圆的方向也随着相关性正负变化。 相关系数在矩阵的下三角显示。可以看到，我们之前在散点图矩阵（图5.3）中看到的变量相关性在这里很明显的展现出来，年龄和在线购物次数负相关，消费量和收入正相关。且有些线性相关性非常强（online_trans和age的相关系数是－0.85）。这会导致什么问题呢？这其实很容易理解，两个变量高度相关意味着它们含有重复的信息，我们其实不需要讲两个变量同时留在模型中。你可能会问，那重复未必有害处不是么？变量高度相关会倒是参数估计极为不稳定。举个例子，我们可以对收入进行简单的线性回归： cfit1&lt;-lm(income~age+online_trans+store_exp+store_trans,data=sdat) summary(cfit1) ## ## Call: ## lm(formula = income ~ age + online_trans + store_exp + store_trans, ## data = sdat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -183840 -14126 -914 10944 156106 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 67790.0372 8640.8338 7.845 1.11e-14 *** ## age 221.5832 135.9644 1.630 0.103 ## online_trans -270.9619 249.5672 -1.086 0.278 ## store_exp 5.7269 0.4356 13.147 &lt; 2e-16 *** ## store_trans 6399.5252 357.8722 17.882 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 31650 on 995 degrees of freedom ## Multiple R-squared: 0.5763, Adjusted R-squared: 0.5746 ## F-statistic: 338.3 on 4 and 995 DF, p-value: &lt; 2.2e-16 大家可以看到，年龄和在线购买的方差都比较大。因为这两个变量高度相关，模型也拿不准到底该如和分配这两个变量的系数。下面我们将年龄（age）删除，再进行一次回归： cfit2&lt;-lm(income~online_trans+store_exp+store_trans,data=sdat) summary(cfit2) ## ## Call: ## lm(formula = income ~ online_trans + store_exp + store_trans, ## data = sdat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -182372 -14498 -1179 11127 156121 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 80861.0786 3217.6962 25.130 &lt; 2e-16 *** ## online_trans -606.4452 141.2126 -4.295 1.92e-05 *** ## store_exp 5.6427 0.4329 13.035 &lt; 2e-16 *** ## store_trans 6425.0185 357.8273 17.956 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 31670 on 996 degrees of freedom ## Multiple R-squared: 0.5752, Adjusted R-squared: 0.5739 ## F-statistic: 449.5 on 3 and 996 DF, p-value: &lt; 2.2e-16 比较cfit1和cfit2中online_trans对应的系数估计之间巨大的变化，就可以很容易看出高度共线的变量同时出现在回归模型中对参数估计可能带来的影响了。于此同时store_exp和store_trans的参数估计在两个结果中差异不大。所以我们在进行回归之前需要移除一些高度相关的变量，使得模型中变量相关性在一定范围之内。我个人比较喜欢用《应用预测模型》(Max Kuhn 2013)书中3.5小节中展示的算法，其核心思想是在删除尽可能少的变量的情况下将变量两两相关性控制在人为设定的一个阈值内： 算法：处理高度相关变量 计算自变量的相关系数矩阵 找出相关系数绝对值最大的那对自变量（记为自变量A和B） 计算A和其他自变量相关系数的均值。对B也做同样的计算 如果A的平均相关系数更大，则将A移除；如若不然，移除B 重复步骤2到4，直至所有相关系数的绝对值都低于设定的阈值 caret中的findCorrelation()函数能够实施上面的算法： (highCorr&lt;-findCorrelation(cor(sdat),cutoff=.75)) ## [1] 1 结果返回的是需要删除的列号，这里算法告诉我们若要使得变量相关性在0.75内，需要删除第1列。 # 将高相关的变量删除 sdat&lt;-sdat[-highCorr] # 查看新的相关矩阵 cor(sdat) ## income store_exp online_exp store_trans online_trans ## income 1.0000000 0.6004006 0.5198623 0.7069595 -0.3572884 ## store_exp 0.6004006 1.0000000 0.5349527 0.5399121 -0.1367411 ## online_exp 0.5198623 0.5349527 1.0000000 0.4420638 0.2256370 ## store_trans 0.7069595 0.5399121 0.4420638 1.0000000 -0.4367544 ## online_trans -0.3572884 -0.1367411 0.2256370 -0.4367544 1.0000000 移除变量后相关矩阵中元素的绝对值都低于0.75。这里需要提醒大家一点，关于这个相关性阈值的选取强烈建议大家将这个阈值成一个调优参数，试验不同的值，看哪个对应的模型精度最高。建议在0.6-0.8范围内寻找最优的阈值。关于共线性的处理还有一些其它方法，如主成分分析和因子分析，这些方法我们将在特征工程那一章进行讲解。 5.8 稀疏变量 除了高度相关的变量以外，我们还需要移除那些观测非常稀疏的变量。一个极端的例子是某变量观测只有一个取值，我们可以将其称为0方差变量。有的可能只有若干取值，我们称其为近0方差变量。这里所讲的处理稀疏变量的方法无法解决大规模基因表达研究中的问题，在基因表达研究中的一个常见问题是观测少，同时变量数目远远超过观测，几乎所有变量都很稀疏，这种情况下需要很多新的高维模型，这是当前非常活跃的一个研究领域，但这不在本书讨论范围之内。这里讨论情况是由于各种原因，出现某些稀疏变量的情况。我们需要做的是识别这些变量然后将其删除。这些变量的存在对如线性回归和逻辑回归这样的模型拟合的表现和稳定性会有很大影响，但决策树模型没有影响。 通常识别这样的变量有两个法则： 不同取值数目和样本量的比值 最常见的取值频数和第二常见的取值频数之间的比值 我们可以设定上面的规则，然后用caret包中的nearZeroVar()函数过滤近0方差变量。为了展示该方法，我们在数据中加入一些这样的变量，然后应用该函数查找它们： # 先备份数据 zero_demo&lt;-sim.dat # 加上两个稀疏变量 # zero1 的取值全是1 # zero2 除了第一个元素是1以外其余全是0 zero_demo$zero1&lt;-rep(1,nrow(zero_demo)) zero_demo$zero2&lt;-c(1,rep(0,nrow(zero_demo)-1)) 上面代码中我们生成了两个新的变量（zero1和zero2），它们分别为0方差和近0方差变量，然后将这两个变量添加到数据框zero_demo上。 nearZeroVar(zero_demo,freqCut = 95/5, uniqueCut = 10) 我们将函数nearZeroVar()应用在含有稀疏变量的数据框上，和之前查找高共线性变量一样，函数返回的是稀疏变量对应的列号。这里返回的是我们生成的两个稀疏变量所在的列。你可以接下来删除这两列。注意这里的两个选项分别对应我们上面提到的两个定义稀疏变量的标准。uniqueCut =是不同取值数目和样本量的比值； freqCut =是最常见的取值频数和第二常见的取值频数之间的比值。你可以根据具体情况提高或者降低这些标准。一个很自然的问题是该怎么选？你要是在这个行业久了就会发现，即使是这样一个和数学相关的技术性行业，非黑即白的事情也是很少存在的。目前为止在我看来（我也还在这个领域不停的学习），所有关于标准，参数，模型选择的问题，最好的答案只有一个，看哪种选择能帮你达成建模目标。比如你的建模目标预测，那你就要看看不同选择下给出的预测精度哪个更高。这个标准甚至超过的p值， AIC等等一堆统计学的测量。我们在下一章讨论建模技术的时候会进一步展开。 5.9 编码名义变量 名义变量，又称虚设变量，是一个指标性质的变量，通常取值0或1。有时你需要将分类变量转化成名义变量。比如一些问卷调查每个问题都有A,B,C,D,E五个选项，你得到数据后通常要将每个问题对应的分类变量转化成5个名义变量，然后将其中一个选项当作基准选项。我们在介绍逻辑回归及其衍生模型的那节中会展示一个这样的案例分析。我们还是用服装消费者的数据为例，假设我们要将性别（gender）和房产拥有情况（house）这两个变量转化为名义变量，R中有两个函数可以进行这项操作，nnet包中的｀class.ind()｀函数，但该函数有个局限是一次只能处理一个变量： dumVar&lt;-class.ind(sim.dat$gender) head(dumVar) ## Female Male ## [1,] 1 0 ## [2,] 1 0 ## [3,] 0 1 ## [4,] 0 1 ## [5,] 0 1 ## [6,] 0 1 我们可以看到性别这个变量被重新编码为2个名义变量，你在建模时需要删除其中一个，因为它们之间有重复信息（正常情况下非男即女嘛）。这样在分类变量多的时候得写个循环来进行重新编码还是有些麻烦。另外一个更加方便的方法是caret包（功能强大的包，简直是居家旅行必备神器！）中的dummyVars() 函数： dumMod&lt;-dummyVars(~gender+house+income, data=sim.dat, # 用原变量名加上因子层级的名称作为新的名义变量名 levelsOnly=F) head(predict(dumMod,sim.dat)) ## gender.Female gender.Male house.No house.Yes income ## 1 1 0 0 1 120963.4 ## 2 1 0 0 1 122008.1 ## 3 0 1 0 1 114202.3 ## 4 0 1 0 1 113616.3 ## 5 0 1 0 1 124252.6 ## 6 0 1 0 1 107661.5 dummyVars()可以用类似模型公式的表达方式对任何变量同时进行转化。而且公式右边不一定要是分类变量，你发现我将收入（income）这个变量也加上去了，对于数值型变量，函数会保持原变量，这样的好处在于你不需要特地选定其中的因子变量，转换后再添加到原数据框上，还要删除原变量，省去了很多麻烦。不仅仅如此，该函数还可以添加交互效应，比如： dumMod&lt;-dummyVars(~gender+house+income+income:gender, data=sim.dat, levelsOnly=F) head(predict(dumMod,sim.dat)) ## gender.Female gender.Male house.No house.Yes income ## 1 1 0 0 1 120963.4 ## 2 1 0 0 1 122008.1 ## 3 0 1 0 1 114202.3 ## 4 0 1 0 1 113616.3 ## 5 0 1 0 1 124252.6 ## 6 0 1 0 1 107661.5 ## gender.Female:income gender.Male:income ## 1 120963.4 0.0 ## 2 122008.1 0.0 ## 3 0.0 114202.3 ## 4 0.0 113616.3 ## 5 0.0 124252.6 ## 6 0.0 107661.5 这里，如果你觉得男性中收入水平对行为的影响和女性收入水平对行为的影响不一样，女性高收入的人在服装购买上大花销更大，而男性收入高低对服装花销影响不大，这就要检测收入和性别的交互效应，可以在公式中加入income:gender得到交互效应编码。是不是很方便？ 到目前为止，我们讲了几种常用的数据预处理方式，这些处理的目的是为了得到我们在本章开始所讲的可以用于模型的数据。本章的最后一部分想讲讲一些数据整合的方法。数据整合可能用在任何一个阶段，可能数据行列安排的方式不符合建模要求，或者之后的数据展示需要对比不同的群体，这些都涉及到数据整合。下面我介绍一些自己工作中常用到的数据整合方法。 5.10 本章总结 本章介绍了常用的建模前的数据预处理方法。需要补充一点，这里我没有讲到是连续变量的离散化，也称为区间化自变量。比如将年龄转变为由&lt;25，25-40，40-60和&gt;60组成的分类变量。个人不赞成分析师自行将连续变量离散化，如果客户或相关领域专家给出明确的理由，在该领域这么划分是通常惯例，或者只有划分成某种区间模型结果才能够解释，那么你可以根据对方的观点划分。而从分析的角度，手动区间化连续型数据是不推荐的。连续变量的效能通常比区间变量高。你需要权衡将连续变量离散化对可解释性的提升和对模型精确度的损害。注意这里指的是人为主观的将一些连续变量转变为分类变量而非模型检测出的截断点。有一些模型，如分类/回归树和多元自适性回归样条，它们在建模过程中能够估计合适的截断点。这些模型使用了所有自变量的信息，对不同变量进行评估，使用可靠的统计方法，并且是基于某个准则（我们在下一章会讲到）来得到合适的区间划分。这样的划分是可以的，但这属于建模，而非数据预处理。 下面总结一下通常情况下得到技术上正确的数据（对数据进行必要的清理，格式变换后，可以顺利用R读入成数据框时，就是技术上正确的数据）后需要经历的数据预处理流程： 检查数据：变量分布，是不是存在错误的观测 缺失值填补：了解缺失原因，选择填补方式 数据变换：取决于需要建立的模型，对不符合正态分布假设，变量尺度差异大，有离群值的数据进行变换 检查共线性：找到高度线性相关的变量，决定删除变量，还是使用PCA，CFA这类非监督方法得到不相关的变量线性组合 稀疏变量：查找并且删除稀疏变量 编码名义变量：对于不能作用于分类变量的模型，将分类变量转化成0/1名义变量 我们将在下一章介绍一些数据整合和整形的方法，以及R中能够进行高效数据操作的包。得到可以用于模型的数据后，接下来就该进入建模阶段了。 References "],
["section-6.html", "第6章 数据整合和整形 6.1 高效数据读写：readr包 6.2 数据整合 6.3 数据整形 6.4 本章总结", " 第6章 数据整合和整形 这章专门介绍一些经常用到的有效数据整合方法，和R中可以实现这些整合的函数。在进行分析之前用描述统计量（均值，标准差等等）和数据可视化总结探索数据集很重要，在分析之后对结果进行总结也很重要，此外我们还经常需要对数据的格式和排列方式进行变换，使得数据结构符合模型的要求。 这一章我们主要介绍常用的数据整合和整形的方式，以及如何用R实现这些操作。我们会跳过一些基础的变量水平的描述，比如对于离散变量，我们通常会用频数表格稍微在需要时查看变量各个层级观测的频数（table()），或者两个变量的交叉表格。还可以通过对离散变量绘制条形图。对于连续变量，我们时不时需要查看某个变量的均值（mean()），标准差（sd()）,分位数（quantile()）之类的。此外还有一些像summary()，str()和describe()（这个函数是’psych’包里的，之前有用过）这样的函数能给出关于一个数据框的总结。以上提到的这些都是一些最基础的探索各个阶段数据（包括模型结果）的方法。但仅仅这些是不够的，这些方法的灵活性不高，输出的信息是固定的。我们可能不想要summary()函数输出的全部信息，而有些我们想要的信息它却没有。比如如果我们想要知道每个类别的客户收入，在线花销等的均值；或者我们想要在每个类别中找到收入最高的人，然后将他们提取出来集合在一起；又或者我们希望有一个新的变量指示购买的渠道（是在线还是实体店），并且将这个变量用于建模，这时就需要对数据进行整形，将在线购买的记录和实体店购买的记录逐行排列而非现在的逐列。这些操作如果仅仅使用初级函数会非常繁琐，且运算效率也不高。R中有一些其它包可以非常高效简洁的完成这些看似复杂的任务。初次接触这些函数的小伙伴会觉得它们不太好学，那是自然，功能强的东西选项自然多些，灵活性越强的工具，你学习掌握的时间自然长些，但这就是一个熟能生巧的事情。 本章内容安排如下，在介绍数据整合和整形操作之前，先介绍一个高效读取数据集的包readr。虽然这个包并不用来进行数据变换操作，但数据读写也是分析的一个必要基础环节。当数据大时，读取数据会成为一个耗时的环节，使用一些高效的数据读写包在数据量大时显著提高效率。此外，和R基础包中相应的读写函数相比，readr包还有一些友好的新特征。之后我们将介绍基础包中用于数据整合的apply()函数，以及更高级的plyr包和它专门针对数据框的优化版dplyr包，后两个包能够进行高效的数据整合操作。最后介绍两个用于数据整形的包：reshape2包和tidyr包。所有这些包都是数据处理的利器，使用频率非常高。大家需要通过实践不断熟悉。 6.1 高效数据读写：readr包 RStudio在2015年推出的readr包可以取代传统的read.csv()和read.table()等函数。使用该包中对应的read_csv()和write_csv()函数读写数据的效率大幅度提高。readr包基于 C++ 的SourceFile，SourceString，SourceRaw的文件API接口，避免了数据的复制和分配，能更加快速读取格式化数据（和传统基础R中的包相比，速度快乐超过10倍）。此外，readr不会将字符串转化成因子变量，能够解析时间观测，不会修改列名。 6.2 数据整合 6.2.1 base包：apply() R基础包中有几个强大的函数，apply()、lapply()和sapply()等。它们做的事情类似，只是对应的对象，或返回对象的格式不同。这些函数对于R的初学者来说可能有些难，但一旦熟悉以后会发现它们非常有效。它们是干什么的？简单来说就是依次对某一对象的某一部分重复应用一个指定的函数。它们的不同在于，apply()将你指定的函数作用于数据框对象的行或列，返回一个向量。‘lapply()’ 将指定的函数作用于列表或者数据框对象，返回一个长度相同的列表。sapply()更加便捷，且算是对lapply()进行了包装，若sapply()中参数simplify=FALSE，那么其返回的值和lapply()是一样的。若simplify=TRUE，则sapply()的返回值不是一个列表，而是一个矩阵。因为在平常工作中通常处理的都是数据框，所以这里主要介绍函数apply()的用法。大家可能会觉得很抽象，觉得抽象很正常，因为这几兄弟确实不太接地气，我现在用这些函数几乎每次都得去查帮助文档。回到我们的服装消费者数据。如果我想知道样本中所有数值变量的均值和标准差该怎么办？这在建模前检查数据和建模后展示数据时都经常用到。 记得我们之前通过preProcess()函数对数据进行KNN填补时，需要提取数据框中所有的数值变量用到的是lapply()函数。这里不能用apply()函数是因为apply()函数自动将对象转化成矩阵，这样就会丢失每列的类别信息，而lapply()不会对对象进行转化（这样细微的差别需要常用R渐渐熟悉了才会慢慢了解，我也是写书的时候发现这个，方才请教了一位R高人才明白个中缘由：））。我们再次用该函数选取其中数值型的变量： sdat&lt;-sim.dat[,!lapply(sim.dat,class)==&quot;factor&quot;] 现在的数据框sdat中只包括数值型的变量。这样我们就可以用apply()函数对每列求均值和标准差了： apply(sdat, MARGIN=2,function(x) mean(na.omit(x))) ## age income store_exp online_exp store_trans ## 38.57858 113543.06522 1358.70923 2120.18119 5.35000 ## online_trans Q1 Q2 Q3 Q4 ## 13.54600 3.10100 1.82300 1.99200 2.76300 ## Q5 Q6 Q7 Q8 Q9 ## 2.94500 2.44800 3.43400 2.39600 3.08500 ## Q10 ## 2.32000 这里我们定义了一个函数function(x) mean(na.omit(x))，这个函数很简单，就是对任何向量求均值同时忽略其中的缺失值。MARGIN=2告诉函数逐列应用定义的函数。如果要计算行均值，只要简单的将margin参数设置为1即可。结果可见，平均说来在线购买次数和消费量都要高于实体店购买。对于10个问卷调查，第二个问题（Q2）平均得分最低。 apply(sdat, MARGIN=2,function(x) sd(na.omit(x))) ## age income store_exp online_exp store_trans ## 14.190572 49842.287197 2775.166414 1731.224308 3.695559 ## online_trans Q1 Q2 Q3 Q4 ## 7.956959 1.450139 1.168348 1.402106 1.155061 ## Q5 Q6 Q7 Q8 Q9 ## 1.284377 1.438529 1.455941 1.154347 1.118493 ## Q10 ## 1.136174 让我们再来看看标准差，代码和计算均值几乎是一样的，只是将均值函数mean()换成标准差函数sd()。虽然在线花费的均值高过实体店花费，但是实体店花费的标准差比在线花费高很多，认真看过本章开头的读者应该知道，这是由于个把土豪在实体店买了上万的衣服。还有问卷调查的第二个问题，虽然之前看到的均值很小，但是标准差也很小，这说明了什么？说明总体对该问题的评分都偏低，如果用于客户分组的话该问题可能不具有太高的区分度。关于这点，我们可以在客户分组那个章节以该数据集为例进行核实。虽然这都是一些简单的统计量，但能够使你在建模前了解更多的关于数据的信息，这对于模型选择和结果解释都有无形的帮助。 6.2.2 plyr包：ddply()函数 之前讲过一个居家旅行必备神器是caret包，希望大家还记得，这个包实在是太强大了，我们之后会反复用到里面的各个函数，但这里不会详细介绍这个包的所有功能，因为具体介绍R中函数的用法不是本书主要目的，而且关于各种包网上有很多文档材料，如果你真的知道你要对数据做什么，找到相应的R包并在网上查看相应帮助资料学会如何使用并不困难（对于有一定经验的R用来说），但很多时候，我们的问题在于即使不知道该如何解决实际问题，本书的重点是给大家展示数据科学家是如何通过这些技术手段解决问题的，R是一个功能强大的工具，但是手段不是目的。回归主题，现在要再讲第二个必备神器，plyr包。同样我不会详细介绍这个包的用法，而是展示如何用这个包中的ddply()函数帮助我们进行数据分析。还是服装消费者的数据（我知道这很没有创意，但太有创意了老换数据集对你们理解模型方法没有什么好处：）），之前我们都没有用到数据框中的最后一列指明消费者类别的变量segment。已经忘记了消费者类别定义的小伙伴们请回到生成数据的那小节，复习下。有客户分组项目经验的人都知道，在通过聚类（在聚类的章节会说到）得到分组以后，下一件事情就是看看每组客户都是些什么样的人，也就是建立各组客户档案。下面我们就来看看各组客户的人口统计学和消费行为档案吧： ddply(sim.dat,&quot;segment&quot;,summarize, Age=round(mean(na.omit(age)),0), FemalePct=round(mean(gender==&quot;Female&quot;),2), HouseYes=round(mean(house==&quot;Yes&quot;),2), store_exp=round(mean(na.omit(store_exp),trim=0.1),0), online_exp=round(mean(online_exp),0), store_trans=round(mean(store_trans),1), online_trans=round(mean(online_trans),1)) ## segment Age FemalePct HouseYes store_exp online_exp store_trans ## 1 Conspicuous 40 0.32 0.86 4990 4898 10.9 ## 2 Price 60 0.45 0.94 501 205 6.1 ## 3 Quality 35 0.47 0.34 301 2013 2.9 ## 4 Style 24 0.81 0.27 200 1962 3.0 ## online_trans ## 1 11.1 ## 2 3.0 ## 3 16.0 ## 4 21.1 这结果信息量太大了，不过并不奇怪。在实际应用中，真实客户分组是未知的并且分析的目标就是找到这样的组。也就是说在实际客户分组的项目中，这是我们希望的到的分析上的最终结果。在对这个结果进行解读之前，我们先看看上面ddply()代码。函数的第一个参数是数据集（sim.dat），其次是告诉函数要按照哪个分类变量进行总结，这里我们只想对不同类别的消费者进行总结，但也可以是多个变量，如你可以将该参数设置成ddply(sim.dat, c(“segment”,”house”), …)，结果读者试着自行脑补下：）。接下来summarize是说我们希望对数据框分组做总结，你可以设置其它功能比如transform（在组内进行数据变换）和subset（在组内进行数据选择）。接下来分别是： Age：计算每组的年龄均值 FemalePct：计算每组女性的比例 HouseYes：计算每组内有房的人的比例 store_exp：计算每组实体店消费均值，这里我们用修剪后的（trim=0.1）均值，因为通过之前的数据探索我们知道这个变量观测有些土豪离群点。 online_exp：计算每组在线消费均值 store_trans：计算每组实体店消费次数均值 online_trans：计算每组在线消费次数均值 了解这些以后，我们看看消费者群体之间有何不同。 炫耀性消费（Conspicuous）人群年龄平均40岁左右，基本中年土豪，女性大概占了1/3另外2/3是男性（基本是大叔控的目标），土豪在哪里都买的多，在线消费量和实体店消费量都远大于其他人，在线和实体店消费的量和次数都差不多，反正有钱不在乎在哪买，看到好的就买，有钱任性嘛！基本有房（0.86），剩下14%没房的如果不是由于观点坚决不买房的话，那或许是在北上广这样的地方高不成低不就，买房不够消费有余，这也提醒我们如果这些样本是来自不同城市的话，我们可能还需要收集消费者所在城市的信息，城市的生活水平很大程度上影响了消费行为。 对价格敏感的人（Price）年龄大（60），基本有房（0.94）这和他们的年龄有关，房奴是后来时代发展的产物。这类人在线消费比其他人都少，还是倾向于在实体店消费（平均在线交易次数是3，而实体店消费次数是6），这也是唯一一类在线消费低于实体店消费的。 注重服装质量的人（Quality）平均年龄居中，可能和炫耀性消费人群没有显著差别，男女比例基本一半一半。明显偏爱在线消费，消费量和土豪比差远了，但位居其次，估计是中产。有房的人不是很多（0.34），这代人很不幸已经进入房奴的时代。 风格类（Style），这些无疑是年轻人了，平均年龄只有24，大学生或者刚工作不久的白领，绝大多数是女性（0.81），有房的不多（0.27）或者可以说能完全靠啃老的不多：），也是典型的在线一族，在线购买的次数比Quality和Conspicuous的人多但是消费额却没有他们大。 这里就提醒我们需要计算平均每次购买的花销，这样可以了解各个群体大概都买什么价位的东西。大家看到没有，分析是一个迭代学习的过程，我们在探索数据的过程中可能会发现一些问题，促使我们去检查某部分数据或者计算一些新的变量，使用新的可视化来探索数据。我们接着用ddply()补充计算这两个统计量： ddply(sim.dat,&quot;segment&quot;,summarize,avg_online=round(sum(online_exp)/sum(online_trans),2), avg_store=round(sum(store_exp)/sum(store_trans),2)) ## segment avg_online avg_store ## 1 Conspicuous 442.27 479.25 ## 2 Price 69.28 81.30 ## 3 Quality 126.05 105.12 ## 4 Style 92.83 121.07 结果显示价格敏感的人群果然买的价位最低，其次是风格类人群，这些人不一定是对价格敏感，但或许钱包不允许他们买太贵的。注重质量的买的东西价格比风格类的高些，但远不及土豪组，物美价廉的东西毕竟少，这些人可能更看重性价比，不会炫耀性消费但也不会买低质廉价的东西。我们在之后客户分组的时候还会进行类似的总结，那时我们会加上关于问卷调查的回复。大家看到了么，短短的几行代码就可以得到这么有信息量的总结。你可能会说这样的总结通过excel的数据透视表（pivot table）也能完成，但是用R代码要快得多，而且这只是其中一部分功能，如果之前说的，你可以设置其它功能比如transform（在组内进行数据变换）和subset（在组内进行数据选择），并且计算的东西也是可以自己定义。 为了方便，我们按照消费者类别比例随机抽取11个样本，选择3个变量（age，store_exp和segment）用于展示（数据框：examp）。这里用于分层抽样的函数在之后介绍建模辅助技术时会讲到，所以这里不做介绍。 library(caret) set.seed(2016) trainIndex&lt;-createDataPartition(sim.dat$segment,p=0.01,list=F,times=1) examp&lt;-sim.dat[trainIndex,c(&quot;age&quot;,&quot;store_exp&quot;,&quot;segment&quot;)] examp数据集只有11行3列。我们先看看transform设置的作用： ddply(examp,&quot;segment&quot;,transform,store_pct=round(store_exp/sum(store_exp),2)) ## age store_exp segment store_pct ## 1 42 6319.0718 Conspicuous 0.55 ## 2 42 5106.4816 Conspicuous 0.45 ## 3 55 595.2520 Price 0.42 ## 4 64 399.3550 Price 0.28 ## 5 64 426.6653 Price 0.30 ## 6 39 362.4795 Quality 0.58 ## 7 35 260.5065 Quality 0.42 ## 8 23 205.6099 Style 0.25 ## 9 24 212.3040 Style 0.26 ## 10 24 202.1017 Style 0.25 ## 11 28 200.1906 Style 0.24 可以看到，设置transform使得函数对数据集按照指定分类变量（segment）在组内进行数据变换，并将变换后得到的新变量添加到原数据集后。再来看看subset设置： ddply(examp,&quot;segment&quot;,subset,store_exp&gt;median(store_exp)) ## age store_exp segment ## 1 42 6319.0718 Conspicuous ## 2 55 595.2520 Price ## 3 39 362.4795 Quality ## 4 23 205.6099 Style ## 5 24 212.3040 Style 上面代码可以获取每个消费者类别（segment）中实体店消费（store_exp）大于该类别中位数的样本。 6.2.3 dplyr包 dplyr包是plyr包中的ddply()等函数的强化版，专门处理数据框（dataframe）对象，大幅提高了速度, 并且提供了更稳健的与其它数据库对象间的接口。由于分析中绝大多数是处理数据框，这个包尤其好用。这里我对这个包进行比较详细的介绍。接下来会按顺序介绍该包的几块重要功能： 数据框显示 数据截选（按行／列） 数据总结 生成新变量 合并数据集 6.2.3.1 数据框显示 tbl_df()函数: 能将数据转化成tbl类，这样查看起来更加方便，输出会调整适应当前窗口 # 这里不展示输出 dplyr::tbl_df(sim.dat) glimpse()函数：类似之前的tbl_df()函数，只是转了方向。变量由列变成行。输出结果同样可以自动调整以适应窗口。 # 这里不展示输出 dplyr::glimpse(sim.dat) 6.2.3.2 数据截选（按行／列） 先介绍按行截选。 # 提取出满足条件的行：收入大于30万的样本 library(magrittr) library(dplyr) dplyr::filter(sim.dat, income &gt;300000) %&gt;% dplyr::tbl_df() ## # A tibble: 4 x 19 ## age gender income house store_exp online_exp store_trans ## &lt;int&gt; &lt;fctr&gt; &lt;dbl&gt; &lt;fctr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 40 Male 301398.0 Yes 4840.461 3618.212 10 ## 2 33 Male 319704.3 Yes 5998.305 4395.923 9 ## 3 41 Male 317476.2 Yes 3029.844 4179.671 11 ## 4 37 Female 315697.2 Yes 6548.970 4284.065 13 ## # ... with 12 more variables: online_trans &lt;int&gt;, Q1 &lt;int&gt;, Q2 &lt;int&gt;, ## # Q3 &lt;int&gt;, Q4 &lt;int&gt;, Q5 &lt;int&gt;, Q6 &lt;int&gt;, Q7 &lt;int&gt;, Q8 &lt;int&gt;, Q9 &lt;int&gt;, ## # Q10 &lt;int&gt;, segment &lt;fctr&gt; 这里用到了一个可能大家之间没有见过的操作符号%&gt;%，这是管道操作，其意思是将%&gt;%左边的对象传递给右边的函数，作为第一个选项的设置（或剩下唯一一个选项的设置）。比如： x %&gt;% f(y) 等同于 f(x, y) y %&gt;% f(x, ., z) 等同于 f(x, y, z ) 管道操作来自于magrittr包，它能够极大简化代码，增加代码可读性。尤其对于dplyr包中的函数操作。大家看下面这段代码，能够知道都干了什么么？ ave_exp &lt;- filter( summarise( group_by( filter( sim.dat, !is.na(income) ), segment ), ave_online_exp = mean(online_exp), n = n() ), n &gt; 200 ) 再看看用管道操作符进行相同操作的代码： avg_exp &lt;- sim.dat %&gt;% filter(!is.na(income)) %&gt;% group_by(segment) %&gt;% summarise( ave_online_exp = mean(online_exp), n = n() ) %&gt;% filter(n &gt; 200) 用%&gt;%的代码是不是简洁清晰的多？我们从上掉下依次读下代码都干了什么： 选出数据框sim.dat中收入未缺失的观测 按照segment变量对观测分组 对每组数据求在线消费额的平均值，并赋予新的变量ave_online_exp 对每组计算观测个数，赋值为n 选出结果中观测个数大于200的行 上面代码中用到的一些没有讲到的函数马上就会介绍。 distinct()函数可以删除数据框中重复的行。可以说是unique()函数在数据框上的扩展。 ## 删除重复的行 ## 这里没有重复的行 dplyr::distinct(sim.dat) sample_frac()函数可以随机选取一定比例的行。sample_n()函数可以随机选取一定数目的行。 dplyr::sample_frac(sim.dat, 0.5, replace = TRUE) dplyr::sample_n(sim.dat, 10, replace = TRUE) slice()可以选取指定位置的行。和sim.dat[10:15,]类似。 # 选取sim.dat的10到15行 dplyr::slice(sim.dat, 10:15) top_n()可以选取某变量取值最高的若干观测。如果有指定组的话，可以对每组选择相应变量取值最高的观测。 # 选取收入最高的两个观测 dplyr::top_n(sim.dat,2,income) 对列变量的选择使用的是select()函数。下面我展示一些代码，并在相应的注释中指出该代码的功能。大家自己运行下看看结果。更多信息，键入?select查阅该函数的帮助文档。 # 通过列名选取变量 # 选取 sim.dat数据框中的income，age和store_exp列 dplyr::select(sim.dat,income,age,store_exp) # 选取列名中含有某字符串（_）的列 # 该命令将选取store_exp，online_exp，store_trans和online_trans dplyr::select(sim.dat, contains(&quot;_&quot;)) # 选取以某字符串（e）结尾的列 # 结果选取了age，income和house # 类似的starts_with指以某字符串开始的列 dplyr::select(sim.dat, ends_with(&quot;e&quot;)) # 选取列Q1，Q2，Q3，Q4和Q5 select(sim.dat, num_range(&quot;Q&quot;, 1:5)) # 选取列名在某字符串中的列 dplyr::select(sim.dat, one_of(c(&quot;age&quot;, &quot;income&quot;))) # 选取两个列名之间的列，包含头尾两列 dplyr::select(sim.dat, age:online_exp) # 选出出了某列（age）以外的其它列 dplyr::select(sim.dat, -age) 6.2.3.3 数据总结 这里的操作类似于apply()和ddply()， 可以对数据框的每一列进行某个函数操作；或者按照某个分类变量将观测分组，然后对每组观测按列进行函数操作。 # 对列online_trans取均值，返回的是一个单一值 dplyr::summarise(sim.dat, avg_online = mean(online_trans)) ## avg_online ## 1 13.546 # 对数据框中的每一列应用函数anyNA() # 这里可以指定一个函数向量，如c(&quot;anyNA&quot;,&quot;is.factor&quot;) dplyr::summarise_each(sim.dat, funs_(c(&quot;anyNA&quot;))) ## age gender income house store_exp online_exp store_trans online_trans ## 1 FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE ## Q1 Q2 Q3 Q4 Q5 Q6 Q7 Q8 Q9 Q10 segment ## 1 FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE 若要根据某分类变量对观测进行分组总结，可以使用group_by()函数。比如： # 对每个消费者类别对应变量应用anyNA()函数 sim.dat %&gt;% group_by(segment) %&gt;% summarise_each(funs_(c(&quot;anyNA&quot;))) ## # A tibble: 4 x 19 ## segment age gender income house store_exp online_exp store_trans ## &lt;fctr&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt; ## 1 Conspicuous FALSE FALSE TRUE FALSE FALSE FALSE FALSE ## 2 Price FALSE FALSE TRUE FALSE FALSE FALSE FALSE ## 3 Quality FALSE FALSE TRUE FALSE FALSE FALSE FALSE ## 4 Style FALSE FALSE TRUE FALSE FALSE FALSE FALSE ## # ... with 11 more variables: online_trans &lt;lgl&gt;, Q1 &lt;lgl&gt;, Q2 &lt;lgl&gt;, ## # Q3 &lt;lgl&gt;, Q4 &lt;lgl&gt;, Q5 &lt;lgl&gt;, Q6 &lt;lgl&gt;, Q7 &lt;lgl&gt;, Q8 &lt;lgl&gt;, Q9 &lt;lgl&gt;, ## # Q10 &lt;lgl&gt; 你在数据总结操作中赋予各种总结函数，如mean()，sd()等。但注意这里的总结函数是作用于向量，返回单一值。比如函数is.na()，作用于向量，但返回的也是向量，就不可以在此使用。 6.2.3.4 生成新变量 dplyr包中的mutate()函数可以进行列计算，然后将结果添加到原数据集上。 dplyr::mutate(sim.dat, total_exp = store_exp + online_exp) 对每列应用窗口函数，它们作用于一个向量然后返回一个向量。回顾刚才介绍dplyr的总结功能时讲到总结函数作用于一个向量返回一个数值。注意理解这两者的不同。 # 这里的min_rank等价于rank(ties.method = &quot;min&quot;) # mutate_each()对每列应用指定的窗口函数 dplyr::mutate_each(sim.dat, funs(min_rank)) transmute()函数和mutate()类似，区别在于它只返回新生成的列，删除原始列。 dplyr::transmute(sim.dat, total_exp = store_exp + online_exp) 这里没有显示代码的结果，大家需要自己操作看看结果，这样对理解学习这些函数很有帮助。关于R中的窗口函数，大家可以自己查找相关资料。熟悉这些常见的函数及其功能，可以提高用R做数据变换的效率（不用总查帮助）。这个过程没有什么技巧可言，纯粹是熟能生巧。 6.2.3.5 合并数据集 这里先随机抽取两个小数据集来展示数据集合并。 x&lt;-data.frame(cbind(ID=c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;),x1=c(1,2,3))) y&lt;-data.frame(cbind(ID=c(&quot;B&quot;,&quot;C&quot;,&quot;D&quot;),y1=c(T,T,F))) x ## ID x1 ## 1 A 1 ## 2 B 2 ## 3 C 3 y ## ID y1 ## 1 B TRUE ## 2 C TRUE ## 3 D FALSE 这里数据框x和y都非常简单，由一个ID变量和各自的观测变量组成。下面我们来介绍各种合并操作。 left_join()从y到x合并数据。结果保留了数据框x的3行。类似大家可以自行尝试right_join()。 left_join(x,y,by=&quot;ID&quot;) ## ID x1 y1 ## 1 A 1 &lt;NA&gt; ## 2 B 2 TRUE ## 3 C 3 TRUE inner_join()返回的是y和x中都可以匹配的观测。 inner_join(x,y,by=&quot;ID&quot;) ## ID x1 y1 ## 1 B 2 TRUE ## 2 C 3 TRUE full_join()返回的是y或者x中含有的观测。 full_join(x,y,by=&quot;ID&quot;) ## ID x1 y1 ## 1 A 1 &lt;NA&gt; ## 2 B 2 TRUE ## 3 C 3 TRUE ## 4 D &lt;NA&gt; FALSE semi_join()对x中的观测进行筛选，找到那些同时在y中可以匹配的观测，但并没有将y的变量y1合并进来。 semi_join(x,y,by=&quot;ID&quot;) ## ID x1 ## 1 B 2 ## 2 C 3 anti_join()同样对x中的观测进行筛选，找到那些在y中无法匹配的观测。 anti_join(x,y,by=&quot;ID&quot;) ## ID x1 ## 1 A 1 此外，dplyr包中还有各种针对数据框的交（intersect()）、并（union()）和补（setdiff()）运算，以及将一个数据框按照行或者列添加到另一个数据框上的操作（bind_rows()，bind_cols()）。这里就不一一介绍，大家自己用一个简单的数据框尝试下。 6.3 数据整形 6.3.1 reshape2包 终于到本章末尾了。之前提到过，如果我们希望有一个新的变量指示购买的渠道（是在线还是实体店），并且将这个变量用于建模，这时就需要对数据进行整形（也称作数据整理，或者揉数据），将在线购买的记录和实体店购买的记录逐行排列而非现在的逐列。我们可以用reshape2包中的相关函数实现这些操作。可能有的读者知道有个包叫做reshape，这是初版，后面的reshape2是重写升级版。这个数据整形的过程确实和揉面团有些类似，先将数据通过melt()函数将数据揉开，然后再通过dcast()函数将数据重塑成想要的形状，为了更清晰的展示函数对数据结构的影响，我们选取其中小部分列，和前5行： (sdat&lt;-sim.dat[1:5,1:6]) ## age gender income house store_exp online_exp ## 1 57 Female 120963.4 Yes 529.1344 303.5125 ## 2 63 Female 122008.1 Yes 478.0058 109.5297 ## 3 59 Male 114202.3 Yes 490.8107 279.2496 ## 4 60 Male 113616.3 Yes 347.8090 141.6698 ## 5 51 Male 124252.6 Yes 379.6259 112.2372 我们截取的子数据框一共5行，6列。 (mdat &lt;- melt(sdat, measure.vars=c(&quot;store_exp&quot;,&quot;online_exp&quot;))) ## age gender income house variable value ## 1 57 Female 120963.4 Yes store_exp 529.1344 ## 2 63 Female 122008.1 Yes store_exp 478.0058 ## 3 59 Male 114202.3 Yes store_exp 490.8107 ## 4 60 Male 113616.3 Yes store_exp 347.8090 ## 5 51 Male 124252.6 Yes store_exp 379.6259 ## 6 57 Female 120963.4 Yes online_exp 303.5125 ## 7 63 Female 122008.1 Yes online_exp 109.5297 ## 8 59 Male 114202.3 Yes online_exp 279.2496 ## 9 60 Male 113616.3 Yes online_exp 141.6698 ## 10 51 Male 124252.6 Yes online_exp 112.2372 我们将变量store_exp和online_exp揉合在一起，结果产生了新的两列，一列是变量variable，指代是哪个揉合变量，另外一列是取值value，即变量对应的值。我们也称这样逐行排列的方式称为长数据格式。由于这里我们并没有指定除了要揉合的变量外的id变量，于是函数默认将所有剩下的变量都当作id变量。对于这些变量保留原来对应的观测，只是对store_exp和online_exp分别重复一次。所以得到的结果有10行。如果我们要对消费量（value）建立线性模型，并且考虑购买渠道的效应的话就可以利用揉合后的数据： # 这里为了展示回归需要更多的数据，所以用原数据框的所有行 mdat&lt;-melt(sim.dat[,1:6], measure.vars=c(&quot;store_exp&quot;,&quot;online_exp&quot;)) fit&lt;-lm(value~gender+house+income+variable+age,data=mdat) summary(fit) ## ## Call: ## lm(formula = value ~ gender + house + income + variable + age, ## data = mdat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4208 -821 -275 533 44353 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -9.132e+02 1.560e+02 -5.855 5.76e-09 *** ## genderMale 3.572e+02 1.028e+02 3.475 0.000524 *** ## houseYes -5.687e+01 1.138e+02 -0.500 0.617275 ## income 2.834e-02 1.079e-03 26.268 &lt; 2e-16 *** ## variableonline_exp 8.296e+02 9.772e+01 8.489 &lt; 2e-16 *** ## age -2.793e+01 3.356e+00 -8.321 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1974 on 1626 degrees of freedom ## (368 observations deleted due to missingness) ## Multiple R-squared: 0.348, Adjusted R-squared: 0.346 ## F-statistic: 173.5 on 5 and 1626 DF, p-value: &lt; 2.2e-16 这里lm()函数自动将实体店消费store_exp设置成基准水平，只看对应的系数估计表明，在其它条件不变的情况下，在线购买的人比商店购买的人平均消费高出830元，而且购买渠道的效应非常显著。看到这样的结果，分析师需要考虑建议商家提高网上商城的购物体验，采取一些手段改变那些实体店为主的消费者改变消费习惯。当然，当靠一个线性回归系数就作出这样的建议有些仓促，还需要对模型的可靠性进行进一步检查。下一章就会讲到模型选择和评估。 如果我们将house和gender指定为id变量，结果为： # 这里用所用的数据 # 缺失值填补 demo_imp&lt;-impute(sim.dat,method=&quot;median/mode&quot;) mdat &lt;- melt(demo_imp, measure.vars=c(&quot;store_exp&quot;,&quot;online_exp&quot;),id.vars=c(&quot;house&quot;,&quot;gender&quot;)) head(mdat) ## house gender variable value ## 1 Yes Female store_exp 529.1344 ## 2 Yes Female store_exp 478.0058 ## 3 Yes Male store_exp 490.8107 ## 4 Yes Male store_exp 347.8090 ## 5 Yes Male store_exp 379.6259 ## 6 Yes Male store_exp 338.3154 melt()函数不仅能揉合数据框，还能揉合列表，矩阵，表格等。感兴趣的小伙伴可以自己在网上找相关的介绍，很容易找到案例代码，自己一步一步跟着运行一遍就很清楚了。这里揉合数据是为了建模需要，有的时候是为了进一步重塑数据结构。好比把一个积木房子拆开重新盖一个新的造型。这就要用到该包的第二个重要函数dcast()（这是升级版中的函数，初级版本的reshape包对应的是cast()函数）。比如，如果想知道对于有房和没有房的男性和女性，在线消费和实体店消费的总额分别是多少： dcast(mdat, house+gender~ variable, sum) ## house gender store_exp online_exp ## 1 No Female 171102.2 583492.4 ## 2 No Male 133130.8 332499.9 ## 3 Yes Female 355320.2 500856.9 ## 4 Yes Male 697297.3 703332.0 上面代码中~左边是你用来划分数据框的id变量，这里是house和gender，右边是你计算根据的变量（也必须是分类变量），真正用于计算的数值是你之前揉合过程中生成的value那列值。这两个函数确实不太好理解，大家需要对一个数据框自己实际操作才能真正理解。 6.3.2 tidyr包 R中还有一个能够用于数据整形的包tidyr。下面我们截取一个小数据框(sdat)展示包中一些主要函数的功能。 sdat&lt;-sim.dat[1:5,]%&gt;% dplyr::select(age,gender,store_exp,store_trans) sdat %&gt;% tbl_df() ## # A tibble: 5 x 4 ## age gender store_exp store_trans ## * &lt;int&gt; &lt;fctr&gt; &lt;dbl&gt; &lt;int&gt; ## 1 57 Female 529.1344 2 ## 2 63 Female 478.0058 4 ## 3 59 Male 490.8107 7 ## 4 60 Male 347.8090 10 ## 5 51 Male 379.6259 4 首先是gather()函数。它的作用类似于reshape2中的melt()。下面这条命令的结果和melt(sdat, measure.vars=c(&quot;store_exp&quot;,&quot;store_trans&quot;))是一样的。其中variable和value是自定义的揉合生成的两列新变量的名字。variable列对应原数据中参与揉合的变量名，value列是参与揉合的变量的取值。store_exp,store_trans告诉R对那些变量进行揉合。 msdat&lt;-tidyr::gather(sdat,&quot;variable&quot;,&quot;value&quot;,store_exp,store_trans) msdat %&gt;% tbl_df() ## # A tibble: 10 x 4 ## age gender variable value ## &lt;int&gt; &lt;fctr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 57 Female store_exp 529.1344 ## 2 63 Female store_exp 478.0058 ## 3 59 Male store_exp 490.8107 ## 4 60 Male store_exp 347.8090 ## 5 51 Male store_exp 379.6259 ## 6 57 Female store_trans 2.0000 ## 7 63 Female store_trans 4.0000 ## 8 59 Male store_trans 7.0000 ## 9 60 Male store_trans 10.0000 ## 10 51 Male store_trans 4.0000 如果用我们之前讲到的管道操作，上面代码可以等价的写成： # 这里不显示输出结果 sdat%&gt;%gather(&quot;variable&quot;,&quot;value&quot;,store_exp,store_trans) 和gather()相反的是spread()，前者将不同的列堆叠起来，后者将同一列分开。 msdat %&gt;% spread(variable,value) ## age gender store_exp store_trans ## 1 51 Male 379.6259 4 ## 2 57 Female 529.1344 2 ## 3 59 Male 490.8107 7 ## 4 60 Male 347.8090 10 ## 5 63 Female 478.0058 4 tidyr包中还有另外两个互补的函数，separate()和unite()。separate()函数可以将不同列分开成为多列。 sepdat&lt;- msdat %&gt;% separate(variable,c(&quot;Source&quot;,&quot;Type&quot;)) sepdat %&gt;% tbl_df() ## # A tibble: 10 x 5 ## age gender Source Type value ## * &lt;int&gt; &lt;fctr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 57 Female store exp 529.1344 ## 2 63 Female store exp 478.0058 ## 3 59 Male store exp 490.8107 ## 4 60 Male store exp 347.8090 ## 5 51 Male store exp 379.6259 ## 6 57 Female store trans 2.0000 ## 7 63 Female store trans 4.0000 ## 8 59 Male store trans 7.0000 ## 9 60 Male store trans 10.0000 ## 10 51 Male store trans 4.0000 可以看到，原来的变量variable被分成了两部分：Source和Type。你可以通过设置sep=来自定义用于划分字符的正则表达，默认是所有非字母和数字的字符。比如这里的“_”。 与separate()相反的函数是unite()， 它能将不同的列合并在一起。类似于paste()函数的数据框版本。 sepdat %&gt;% unite(&quot;variable&quot;,Source,Type,sep=&quot;_&quot;) ## age gender variable value ## 1 57 Female store_exp 529.1344 ## 2 63 Female store_exp 478.0058 ## 3 59 Male store_exp 490.8107 ## 4 60 Male store_exp 347.8090 ## 5 51 Male store_exp 379.6259 ## 6 57 Female store_trans 2.0000 ## 7 63 Female store_trans 4.0000 ## 8 59 Male store_trans 7.0000 ## 9 60 Male store_trans 10.0000 ## 10 51 Male store_trans 4.0000 上面的代码将原先分开的两列又合并回去了，并赋予的和之前一样的列名&quot;variable&quot;。 整形这部分可能是数据处理变换中最复杂的，这种复杂和证明数学定理不同，主要是需要时间熟悉。更像一门手艺，所以大家要发扬手艺人精神，多使用这些函数，把数据当面团一样揉来揉去的，也挺好玩的，不是么？觉得R太难有些犹豫需不需要学习的小伙伴们，磨刀不误砍柴工，再一次中国的老古话放在那里闪闪发光的有木有？ 6.4 本章总结 本章介绍数据整合和整形的方法，以及R中能够进行相应高效数据操作的包。整合方法类似于excel中的透视表格，R中有一些功能强大的函数能够有效的进行各种数据整合。我们介绍了base包中的apply()函数，还有更加高级灵活的plyr包中ddply()函数的各种用法。由于实际工作中大部分时间是处理数据框，所以这里我们介绍了plyr包的一个专门针对数据框的强化包dplyr。该包对于数据的整合非常高效。对于数据的整形（长型数据和宽型数据的转换），我们举例说明了由于模型需要将数据从宽型转化成长型的情况，并且用reshape2包实现了该过程。此外我们还介绍了另外一个数据整形的包tidyr，该包可以进行数据变量揉合，拆分，合并；还可以进行数据框的各种合并。这些包的使用需要一些时间学习，实践尤其重要，大家需要多花功夫熟悉这些的操作，这是数据科学家的基本技能。这里说的“熟悉”是指成为第二天性，再应用时不需要占用大脑的工作记忆。 "],
["section-7.html", "第7章 基础建模技术 7.1 有监督和无监督 7.2 误差及其来源 7.3 数据划分和再抽样 7.4 本章总结", " 第7章 基础建模技术 建模技术指代一系列用于理解数据的工具。本章介绍基本的统计学习术语，概念，以及一些辅助性的技能。后面章节会分别对一些特定模型进行展开。 7.1 有监督和无监督 建模技术可以粗略的分为有监督和无监督这两类。大部分统计学习方法都可以归于其中一种。广义上说有监督方法涉及根据一个或者多个输入变量（也称为自变量，解释变量，预测变量），估计或者预测一个结果变量（也称为因变量，响应变量）。而无监督方法只考虑自变量，没有应变量作为“监督”，我们通过这类方法探索观测数据中内在变量结构。我们在之前提到的方法中，袋状树，广义线性回归是有监督方法；主成分分析，探索性因子分析，对近0方差和高相关变量的筛选都是无监督方法。 下面我们先对本书之后的数学公式表达进行统一。 我们用\\(n\\)表示样本量（或者观测数目）。\\(p\\)代表自变量数目。我们用\\(\\mathbf{X}\\)表示\\(n\\times p\\)观测矩阵： \\[ \\mathbf{X}=\\left[\\begin{array}{cccc} x_{11} &amp; x_{12} &amp; \\cdots &amp; x_{1p}\\\\ x_{21} &amp; x_{22} &amp; \\cdots &amp; x_{2p}\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ x_{n1} &amp; x_{n2} &amp; \\cdots &amp; x_{np} \\end{array}\\right] \\] 其中\\(x_{ij}\\)代表第i个样本第j个变量的观测，\\(i=1, \\ldots, n\\)，\\(j=1, \\ldots, p\\)。\\(\\mathbf{x_{i.}}\\)代表第i个样本的所有变量观测组成的向量，向量统一按列排： \\[ \\mathbf{x_{i.}}=\\left[\\begin{array}{c} x_{i1}\\\\ x_{i2}\\\\ \\vdots\\\\ x_{ip} \\end{array}\\right] \\] 类似的，\\(\\mathbf{x_{.j}}\\)代表第j个变量的所有样本观测组成的向量： \\[ \\mathbf{x_{.j}}=\\left[\\begin{array}{c} x_{1j}\\\\ x_{2j}\\\\ \\vdots\\\\ x_{nj} \\end{array}\\right] \\] 于是我们有： \\[ \\mathbf{X}=\\left[\\begin{array}{cccc} x_{11} &amp; x_{12} &amp; \\cdots &amp; x_{1p}\\\\ x_{21} &amp; x_{22} &amp; \\cdots &amp; x_{2p}\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ x_{n1} &amp; x_{n2} &amp; \\cdots &amp; x_{np} \\end{array}\\right]=\\left[\\begin{array}{c} \\mathbf{x_{1.}^{T}}\\\\ \\mathbf{x_{2.}^{T}}\\\\ \\vdots\\\\ \\mathbf{x_{n.}^{T}} \\end{array}\\right]=\\left[\\begin{array}{cccc} \\mathbf{x_{.1}} &amp; \\mathbf{x_{.2}} &amp; \\ldots &amp; \\mathbf{x_{.p}}\\end{array}\\right] \\] 其中\\(^{T}\\)代表矩阵转秩。我们用\\(y_{i}\\)代表第i个样本对应的响应变量。所有\\(n\\)个响应变量组成的向量为： \\[ \\mathbf{y}=\\left[\\begin{array}{c} y_{1}\\\\ y_{2}\\\\ \\vdots\\\\ y_{n} \\end{array}\\right] \\] 自变量和应变量的关系为： \\[\\mathbf{y}=f(\\mathbf{X})+\\mathbf{\\epsilon}\\] 有监督和无监督建模技术用上面的符号语言表达就是： 无监督建模：探索\\(\\mathbf{X}\\)中的自变量之间的关系 有监督建模：估计\\(\\mathbf{y}\\)和\\(\\mathbf{X}\\)之间的关系 \\(f(\\cdot)\\) 其中\\(\\mathbf{\\epsilon}\\) 是随机误差，均值为\\(\\mathbf{0}\\)。函数\\(f(\\cdot)\\)是我们的建模目标，代表X能够提供的关于Y的系统信息（和随机性相对应）。估计\\(f(\\cdot)\\)目的主要是推断或者预测，有时兼有两者。通常情况下，模型的灵活性和可解释性之间是一种此消彼长的关系——灵活性越高的模型可解释性越弱。因此数据科学家需要把握这两者间微妙的平衡。不同的建模目的对模型解释性的要求不同，因而极大影响了模型选择。如果预测是唯一目的，那么模型的解释性就不在考虑范围内，这种情况下可以使用一些复杂的灵活度高的“黑箱”模型，装袋，助推，非线性核函数支持向量机，神经网络和随机森林等。这些模型都非常灵活，但是很难解释自变量和应变量之间的关系。人们可能会觉得这些模型的预测精度通常更高，但就个人经验来说，那些灵活性不那么高的模型预测精度更高的情况时常发生。咋一看来好像不符合逻辑，但是认真想想也并不奇怪，这些模型之所以复杂，就在于它们极力拟合当前观测数据，因此它们更有可能过度拟合（把噪声也拟合进去了），这些模型在训练集上的表现可能更好，但预测未必更准确。 7.2 误差及其来源 7.2.1 系统误差和随机误差 假设我们对于\\(\\mathbf{X}\\)得到\\(f\\)的估计\\(\\hat{f}\\)，进而得到\\(\\mathbf{y}\\)的预测 \\(\\hat{\\mathbf{y}}=\\hat{f}(\\mathbf{X})\\)。预测的误差分成两部分，系统误差和随机误差： \\[ E(\\mathbf{y}-\\hat{\\mathbf{y}})^{2}=E[f(\\mathbf{X})+\\mathbf{\\epsilon}-\\hat{f}(\\mathbf{X})]^{2}=\\underset{\\text{(1)}}{\\underbrace{[f(\\mathbf{X})-\\hat{f}(\\mathbf{X})]^{2}}}+\\underset{\\text{(2)}}{\\underbrace{Var(\\mathbf{\\epsilon})}} \\label{eq:error}\\] 其中（1）是系统误差， \\(\\hat{f}\\)通常不能彻底对\\(\\mathbf{X}\\)和\\(\\mathbf{y}\\)之间的“系统关系”建模，这里系统关系指的是在不同样本上存在的稳定关系。这一部分误差能通过改进模型得到提高；（2）是随机误差，这部分误差代表当前数据无法解释的部分，因此无法通过建立更复杂的模型来改进。那些拥有众多参数的复杂黑箱模型最大的问题就是试图通过自变量解释这部分误差，也就是过度拟合。随机误差的显著特点就是在不同的样本上是无法重复的，于是判断是否存在过度拟合的一个准则就是预留一部分样本作为测试集，然后检验训练出来的模型在测试集上的表现。这个我们随后会讲到。这里要澄清一点，过度拟合不只发生在这些黑箱模型上，其发生的根源在于参数个数太多（常超过观测个数），理论上说任何模型都可能过度拟合，只是因为黑箱模型的参数尤其多，其高灵活性和复杂度放大了过度拟合的问题。有些黑箱模型在训练的过程中会使用“袋外数据”（又称为Out of Bag [OOB]）来尽量避免过度拟合的影响。 如果建模的目的也包含推断，那么这些“黑箱”模型就不合适，这就需要在模型可以解释的范围内使用尽量灵活的模型，比如Lasso回归，多元自适应回归样条等。有人可能不同意Lasso回归是灵活的。从其本质还是传统回归的角度看，它确实没有那么灵活，受到很多模型假设的限制。但由于Lasso的罚函数能同时起到变量选择的作用，这个变量的选择的过程可以不依赖于p值之类的参数（这些参数基于数据分布假设因此具有局限性），而可以通过优化模型预测值和真实值的差距来进行变量选择，从这个角度上看，该模型是灵活的。根据笔者的应用经验，Lasso作为收缩（或变量选择）方法在实际应用中的效果非常好。对于一些市场营销或者社会心理学相关的抽样调查数据分析，分层贝叶斯是一种灵活有效的方法。 模型选择向来是非常困难的，这种困难不是数据分析行业特有的，很多专业领域都有类似的情况，比如医生判断病人所患的疾病，并在众多治疗方案中选择最合适的，这不是答案一目了然的选择题，决策的过程需要很多权衡和妥协。模型选择也类似，在选择过程中需要考虑具体的情况：项目目的，客户要求的精确度（这点很重要），计算量等等。这个选择的过程很难白纸黑字的像食谱一样写下来，这里我们只是尽己所能的介绍模型选择过程中需要考虑的点，以及评估不同模型的辅助性技术。具体的应用和“数据科学思维”还需要大家在从业过程中通过实践思考不断学习打磨。 7.2.2 应变量误差 若应变量包含可观的测量误差，那么这部分误差将反映在随机误差（\\(\\mathbf{\\epsilon}\\)）中。这部分误差使得均方根误差（RMSE）和\\(R^2\\)有相应的上下限。RMSE和\\(R^2\\)是回归模型常用的表现度量方法，我们在本章后面部分会进行介绍。因此，随机误差项不仅仅代表模型无法解释的波动，还含有测量误差。《应用预测模型（Applied Predictive Modeling）》(Max Kuhn 2013)的第20.2小节有一个例子展示了因变量的测量误差对模型表现（RMSE和\\(R^2\\)）的影响。作者在因变量上加入了不同强度的随机正态噪声，重复拟合不同的模型，研究模型均方根误差（RMSE）和\\(R^2\\)的变化。这里我们用服装消费者数据进行类似的展示。假设我们面对这样一个问题，实际中消费者的收入并不是那么容易收集，很多人不愿透露这样的私人信息。于是我们希望利用消费记录变量建立关于消费者收入的预测模型，模型可以对那些数据库中缺失收入信息的记录进行填补。我们建立下面模型： # 载入数据 sim.dat&lt;-read.csv(&quot;/Users/happyrabbit/Documents/GitHub/DataScientistR/Data/SegData.csv&quot;) ymad&lt;-mad(na.omit(sim.dat$income)) # 计算Z分值 zs&lt;-(sim.dat$income-mean(na.omit(sim.dat$income)))/ymad # which(na.omit(zs&gt;3.5)) 找到利群点 # which(is.na(zs)) 找到缺失值 idex&lt;-c(which(na.omit(zs&gt;3.5)),which(is.na(zs))) # 删除含有离群点和缺失值的行 sim.dat&lt;-sim.dat[-idex,] fit&lt;-lm(income~store_exp+online_exp+store_trans+online_trans,data=sim.dat) 由输出可见，在没有额外添加噪音时模型的均方根误差（RMSE）是 29567，\\(R^2\\)是 0.6。下面我们在应变量年收入（income）上添加不同程度的噪音（均方根误差的0到3倍）： \\[ RMSE \\times (0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0) \\] noise&lt;-matrix(rep(NA,7*nrow(sim.dat)),nrow=nrow(sim.dat),ncol=7) for (i in 1:nrow(sim.dat)){ noise[i,]&lt;-rnorm(7,rep(0,7),summary(fit)$sigma*seq(0,3,by=0.5)) } 我们接下来检查噪音强度对复杂度不同的模型拟合\\(R^2\\)的影响。 拟合的模型复杂度从低到高依次为：一般线性回归，偏最小二乘回归，多元自适应回归样条，支持向量机（核函数是径向基函数），随机森林。 # 拟合一般线性回归模型 rsq_linear&lt;-rep(0,ncol(noise)) for (i in 1:7){ withnoise&lt;-sim.dat$income+noise[,i] fit0&lt;-lm(withnoise~store_exp+online_exp+store_trans+online_trans,data=sim.dat) rsq_linear[i]&lt;-summary(fit0)$adj.r.squared } 下面我们接着拟合偏最小二乘回归（PLS）。偏最小二乘源自于Herman Wold的非线性迭代偏最小二乘（NIPALS）算法 (H 1966; H 1982)，是一种通过隐层级将非线性关系线性化的方法。该方法和主成分回归类似，不同在于主成分回归在选择成分的时候没有考虑因变量的信息，其目的是找到最大程度概括自变量空间变异性的线性组合（即，是无监督方法）。当自变量和因变量相关时，主成分回归能够很好的识别出它们之间的系统关系。然而，当存在和因变量不相关的自变量时，该方法的效果就会受到影响。而PLS最大程度概括与因变量相关性的线性组合。推荐大家用PLS解决那些自变量之间存在相关性，但不确定所有自变量都和因变量有关，同时希望用线性回归来解决的问题。在当前情况下，更加复杂的PLS表现效果并不比简单线性好，因为这里几个自变量都和因变量有关的不同信息（从前面的拟合结果看到所有变量都是显著的）。 # pls: 进行偏最小二乘回归和主成分回归 library(pls) rsq_pls&lt;-rep(0,ncol(noise)) # 拟合PLS模型 for (i in 1:7){ withnoise&lt;-sim.dat$income+noise[,i] fit0&lt;-plsr(withnoise~store_exp+online_exp+store_trans+online_trans,data=sim.dat) # plsr函数结果是mvr对象，需要用特定函数提取模型解释的应变量方差 rsq_pls[i]&lt;-max(drop(R2(fit0, estimate = &quot;train&quot;,intercept = FALSE)$val)) } # earth: 拟合多元自适应回归样条 library(earth) rsq_mars&lt;-rep(0,ncol(noise)) # 拟合多元自适应回归样条 for (i in 1:7){ withnoise&lt;-sim.dat$income+noise[,i] fit0&lt;-earth(withnoise~store_exp+online_exp+store_trans+online_trans,data=sim.dat) # 提取模型解释的应变量方差 rsq_mars[i]&lt;-fit0$rsq } # caret: 用于建立预测模型的包，可以拟合多种模型 library(caret) rsq_svm&lt;-rep(0,ncol(noise)) # 拟合支持向量机 # 注意：运行需要一些时间 for (i in 1:7){ idex&lt;-which(is.na(sim.dat$income)) withnoise&lt;-sim.dat$income+noise[,i] trainX&lt;-sim.dat[,c(&quot;store_exp&quot;,&quot;online_exp&quot;,&quot;store_trans&quot;,&quot;online_trans&quot;)] trainY&lt;-withnoise fit0&lt;-train(trainX,trainY,method=&quot;svmRadial&quot;, tuneLength=15, trControl=trainControl(method=&quot;cv&quot;)) # 提取模型解释的应变量方差 rsq_svm[i]&lt;-max(fit0$results$Rsquared) } # randomForest: 拟合随机森林模型 library(randomForest) rsq_rf&lt;-rep(0,ncol(noise)) # 拟合随机森林模型 # ntree=500 用500棵树 # na.action = na.omit 忽略缺失值 for (i in 1:7){ withnoise&lt;-sim.dat$income+noise[,i] fit0&lt;-randomForest(withnoise~store_exp+online_exp+store_trans+online_trans,data=sim.dat,ntree=500,na.action = na.omit) # 提取模型解释的应变量方差 rsq_rf[i]&lt;-tail(fit0$rsq,1) } # reshape2在之前介绍过，用于数据整形 library(reshape2) rsq&lt;-data.frame(cbind(Noise=c(0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0),rsq_linear,rsq_pls,rsq_mars,rsq_svm,rsq_rf)) # 将数据转化成长型 rsq&lt;-melt(rsq,id.vars=&quot;Noise&quot;,measure.vars=c(&quot;rsq_linear&quot;,&quot;rsq_pls&quot;,&quot;rsq_mars&quot;,&quot;rsq_svm&quot;,&quot;rsq_rf&quot;)) # 功能强大的绘图包 library(ggplot2) # 用ggplot2包进行可视化 ggplot(data=rsq, aes(x=Noise, y=value, group=variable, colour=variable)) + geom_line() + geom_point()+ ylab(&quot;R2&quot;) Figure 7.1: 模型\\(R^2\\)随应变量噪音强度变化，rsq_linear是简单线性回归，rsq_pls是偏最小二乘回归，rsq_mars是多元自适应回归样条回归，rsq_svm是支持向量机，rsq_rf是随机森林。 由图7.1中可以看到： 所有模型拟合效果随着噪音强度的增加急剧下降。对变量测量系统的理解能够帮助我们更好的预期模型的表现。这是在之前“数据分析一般流程”中说过的从问题到数据这个环节需要弄清的问题。你应该清楚当前数据库中已有的数据的质量。如果客户提供给你额外的数据，或者需要你从其它地方获得数据，数据质量是必须交流清楚的问题，笔者就曾在这里栽过跟头，希望大家可以避免类似的错误。 使用更加复杂的模型的效果不一定更好，如复杂的随机森林和支持向量机表现居中，简单线性回归和偏最小二乘回归在噪音低的时候拟合效果最差。效果最好的是多元自适应回归样条回归，该模型比简单线性回归复杂，但比剩下其它的模型的解释性都更强。 噪音增加到一定程度，复杂的随机森林模型能够发现的潜在结构变得更加模糊，模型表现不如其它更简单的模型。因此系统测量误差较大时，使用更简单的易于解释的模型可能是更好的选择，大家建模的时候要尽量多尝试几种模型，在表现相当的情况下选择最简单的模型，模型的评估和选择很好的反应了一个数据科学家的职业“成熟度”。 7.2.3 自变量误差 传统的统计模型通常假设自变量的测量无误差（或者随机性），这在实际中是不可能的，所以我们需要考虑自变量观测的随机性。自变量观测中的随机性产生的影响取决于如下几个因素：随机性的强度，相应因变量在模型中的重要性，使用模型的类别。我们选取自变量“在线消费”（online_exp）为例，用和上面相似的方法在该自变量上添加不同程度的噪音看其对模型拟合情况的影响。我们在自变量online_exp和上添加如下不同程度的噪音（标准差的0到3倍）： \\[ \\sigma_{0} \\times (0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0) \\] 其中\\(\\sigma_{0}\\)是在线消费观测的标准差。 noise&lt;-matrix(rep(NA,7*nrow(sim.dat)),nrow=nrow(sim.dat),ncol=7) for (i in 1:nrow(sim.dat)){ noise[i,]&lt;-rnorm(7,rep(0,7),sd(sim.dat$online_exp)*seq(0,3,by=0.5)) } 同样的，我们检查噪音强度对复杂度不同的模型拟合\\(R^2\\)的影响。拟合的模型复杂度从低到高依次为：一般线性回归，偏最小二乘回归，多元自适应回归样条，支持向量机（核函数是径向基函数），随机森林。代码和之前类似，这里就不重复展示。 Figure 7.2: 模型\\(R^2\\)随自变量(在线消费)噪音强度变化，rsq_linear是简单线性回归，rsq_pls是偏最小二乘回归，rsq_mars是多元自适应回归样条回归，rsq_svm是支持向量机，rsq_rf是随机森林。 比较图7.2和图7.1，可以看到自变量的误差和应变量误差对模型拟合结果的影响很不相同。应变量误差是无法克服的，对任何模型来说都是个硬伤。而自变量误差确不一定。试想极端的情况，在线消费这个变量完全是随机噪音，也就是所说的无信息变量，随机森林和支持向量机受的影响并不太大。线性模型和偏最小二乘回归的结果依旧基本重合，而且随着噪音的增加拟合效果开始下降较快，到一定程度后趋于平稳，如果噪音不断增加，最后拟合的情况实际上会趋近于移除“在线消费”这个变量的结果。总体说来，如果某个自变量含有误差，其它与之相关的变量在某种程度上可以进行弥补。线性模型对于自变量的观测误差的抗性普遍较差。 7.3 数据划分和再抽样 模型训练和选择过程都离不开数据的划分和再抽样。数据划分是将一部分数据预留出来用于模型测试，只用另外的部分数据用于模型的训练。再抽样过程牵扯到重复的从训练集中抽取样本并且在不同的样本上拟合模型，以此来得到关于拟合模型的信息。假设我们想知道某线性模型拟合度\\(R^2\\)的稳定性（也可以用其它模型拟合度量），可以重复的抽取不同的样本，然后拟合相同的线性模型，检查这些模型对应\\(R^2\\)的变化。由于牵扯到使用随机样本重复拟合模型，这个过程有一定的计算量，最近五年里，数据处理工具和技术获得了飞速的发展。除非你需要处理PB（\\(2^{50}\\)比特）级别的数据，或者每天要处理千亿级的事件，现阶段大多数技术已经能轻松满足你的需求了。 你可能会问：为什么要对数据划分和再抽样？简单的回答是避免过度拟合。在预测问题中，有时拟合的模型能很好的描述现有数据中的变量关系，但是对新样本的预测有很大的偏差，这时就发生了过度拟合。很多领域都会讨论过度拟合，如医学研究，化学计量，气象，金融和社会学研究等等。现代很多含有调优参数的分类 和回归模型有高度的灵活性，如之前提到的随机森林、支持向量机等。它们能够对复杂的关系进行建模，但是很容易过度强调不可再现的数据关系。要注意，虽然过度拟合的问题在灵活度高的模型中更加突出，所有模型（包括简单线性回归）在应用中都可能出现该问题。建模的目的是找到可重复的数据关系， 这就需要将现有数据划分成不同的数据集来调试模型参数和评估模型表现。 划分和再抽样的一般过程如下： 将样本划分成训练集和测试集 使用训练集拟合模型 将拟合的模型应用于测试集评估模型表现 关于数据划分，我们会介绍3种划分数据的方法：（1）按照结果变量划分数据；（2）按照预测变量划分数据；（3）按照时间序列划分数据。之后我们会介绍两种主要的再抽样方法： bootstrap和交互校验。 7.3.1 划分训练集和测试集 关于数据划分大家可能主要会问这三个问题：（1）为什么要划分训练集和测试集？（2）多少比例的数据用于训练集？（3）具体如何划分？我们现在就对此逐一回答。 刚接触数据科学的人常常会问为什么我们要预留一部分数据作为测试集而不是使用全部的数据用于训练。印象中传统商业智能声称的数据分析通常只是数据描述。通过从数据库中查询相关测量来回答简单的问题，如：2015年某产品每月销售量是多少？我们网站在过去一个月每天的访问量是多少？两种包装设计的同类产品在某大零售店上个月的销量差距多大？像这样的问题确实不用对数据进行划分，相反我们需要用尽可能完整的数据，然后对感兴趣的部分求和或者平均。假设数据观测准确，我们不需要怀疑问题的答案，因为这些问题本质上就是对数据进行某种描述总结，没有牵扯到任何分析推断。 数据科学家需要解决的不会是这样的问题，常是预测问题，或者同时还需要从预测模型中得到相应能够指导决策的推断。在这些情况下，分析的重心在于找到自变量\\(\\mathbf{X}\\)和应变量\\(\\mathbf{y}\\)之间的系统关系。这时我们就必须非常小心，因为我们在用一个样本得到一般化的结论，进而对将来可能出现的观测进行预测，这远远超越了描述统计的界限。根据彭加莱的理论，在预测未来的过程中，预测的越远的未来要求模型越精确，因为你的错误率会迅速上升。每向前预测一步，噪声会随着以一种非线性的方式迅速增加，因此我很难相信对5年以后某事件的定量预测。我们能够处理定性的事物，能够讨论系统的某些特点，但能够计算的东西是很局限的。在《黑天鹅》那本书中，作者以数学家Michael Berry的弹子球计算为例说明了这种放大效应。该实验是预测弹子球在球桌上的运动轨迹。如果弹子球的基本参数已知，你能够计算出桌面阻力，测量撞击量，那么就可以预测第1次撞击的结果。要预测第2次撞击就更为复杂一些，你需要小心确定球的初始状态，但不是不可能。如果要计算第9次撞击的结果你需要考虑某个站在桌子旁边的人的体重和产生的引力。要计算第56次撞击结果你需要考虑宇宙中的每一个基本粒子。注意这还只是单独的弹子球而没有牵扯到有着自由意志的人，以及不同人之间相互的影响。对现实世界的复杂局面，人的预测能力有着本质上的局限性。因此在实际预测分析当中，你需要很小心的界定这个可预测的边界，好比在弹子球实验中，你能预测第1次撞击的结果或者咬咬牙，再多杀一大片脑细胞做第2次撞击预测，但不要试图再进一步，承认自己的局限需要知识和勇气。回到实际分析中，如何找到预测的边界？（注：随着你经验的增长，你会遇到很多你无法预测（有时是分析）的情况。）目前我知道的方法就是在仔细确保当前情况基本符合假设的情况下，严格划分训练集和测试集，尽可能对模型的预测情况进行评估，检测预测模型的精确度和稳定性。划分背后隐含的假设是： 我们用于分析的数据展现的过程能够反应真实世界中事情的发展过程 我们想要对其建模的真实世界中事情的发展过程随着时间变化是相对稳定的。如，用上个月的数据建立的表现良好的模型，在接下来的一个月的观测上依旧能够有类似的良好表现 换句话说，我们想要知道如果我们用模型来对新样本进行预测时会发生什么。我们的预测和真实将观测到的值有多接近？预测值偏离真实值的误差大致是多少？模型的误差是不是单向的，即预测是不是总大于真实值？这些都是很自然的问题，但它们的答案并非那么容易获得。最简单的理解模型在将来数据集上表现的方法就是试图模拟这件事。虽然严格说来，在将来事件发生之前，我们不可能得到相应的数据，但是我们能够预留一部分当前的数据并将它们视为将来的观测。例如，如果我们要预测2016年哪些农民还会某品牌的种子，可以用之前到2015年的历史数据建立预测模型，然后预测2016年的购买情况。这是一个相当好的模拟，由于我们其实已经知道2016年实际购买情况，可以将预测和真实情况进行对比。 在商业促销活动和信用风险的案例中，我们得到的数据通常和某个时间点相连（或者时间区间：一周，一个月，一个促销活动期间）。通常称这样的数据有代表性（用某时间点或者时间段的数据代表普遍情况）。在这样的情况下我们通常将数据集随机分成不同部分，然后用一部分（训练集）建立模型，另外一部分（测试集）来评估模型表现，可能的话对模型做出调整。 如果这两条假设大致正确，那么当前数据就能够合理反映未来的情况。因此在这种情况下，预留一部分当前数据来估计模型在将来的表现是合理的。明确了预测模型的一些假设前提，以及划分训练集和测试集的必要性之后，下一个问题是我们该将多少比例的数据用于训练集。 一般这需要视具体情况而定。通常需要考虑的两个因素是：（1）样本量；（2）计算速度。当样本量较大时，在考虑计算速度的条件下，我一般会尝试60%，70％和80%这三个比例，看哪个效果更好。如果样本量很小，那么测试集其评估模型效果的能力将非常有限，并且在原本样本量就不大的情况下再分出一部分数据会极大影响模型拟合。这种情况下，使用再抽样技术更加有效。常用的再抽样方法有交互校验和Bootstrap。 我们可以用createResample()函数生成简单bootstrap样本，createFolds函数可以生成平衡的交互校验样本集。 具体如何划分? 划分训练集和测试集时需要小心避免两个数据集有系统差别。例如，我们不能简单的把前半部分数据当作训练集，后半部分当作测试集。因为数据有可能是以某种方式排列的，如，按收入从大到小，按访问次数多少排列等等。有一种避免数据集间随机差别的方法是用简单的随机抽样，如对每个样本我们都抛下硬币，人头面就归于训练集，菊花面就归于测试集。有时还有一些其它因素需要考虑，但本质都是随机抽样。要想真正理解划分数据背后的逻辑需要实践。下面我们介绍经常使用的几种划分方法。 按照结果变量划分数据 若结果变量\\(\\mathbf{y}\\)为分类变量，那么我们的到的测试集和训练集中结果变量各类的分布比例应该类似。可以使用caret包中的createDataPartition()函数平衡划分样本集。回到我们之前使用的服装消费者数据集，假设我们想要建立关于消费者类别（segment）的判别模型，这时结果变量为segment，我们用80%的样本训练模型，20%的样本做为测试集，且训练集和测试集中各类别的比例要尽可能相近。我们可以用如下R代码实现： # 载入数据 sim.dat&lt;-read.csv(&quot;/Users/happyrabbit/Documents/GitHub/DataScientistR/Data/SegData.csv&quot;) # 需要caret包 library(caret) # 设置随机种子这样能得到相同的抽样结果 set.seed(3456) trainIndex &lt;- createDataPartition(sim.dat$segment, p = .8, list = FALSE, times = 1) head(trainIndex) ## Resample1 ## [1,] 1 ## [2,] 2 ## [3,] 3 ## [4,] 4 ## [5,] 6 ## [6,] 7 list = FALSE选项使得返回的值是数据框。该函数还有一个选项times，用于设置划分的次数，你可以一次返回多次划分的结果，函数会返回一个（或多个）整数向量（指针向量），指明归于训练集的行（你可以设置times＝2再运行一下上面的代码看看输出有什么不同）。下面我们通过返回的指针向量（trainIndex）得到训练集和测试集： # 得到训练集 datTrain &lt;- sim.dat[ trainIndex,] # 得到测试集 datTest &lt;- sim.dat[-trainIndex,] 按照设置，训练集中该有800个样本，测试集中有200个样本。我来看看两个集合中消费者类别的比例分布是否相似： library(plyr) ddply(datTrain,&quot;segment&quot;,summarise,count=length(segment), percentage=round( length(segment)/nrow(datTrain),2)) ## segment count percentage ## 1 Conspicuous 160 0.20 ## 2 Price 200 0.25 ## 3 Quality 160 0.20 ## 4 Style 280 0.35 ddply(datTest,&quot;segment&quot;,summarise,count=length(segment), percentage=round(length(segment)/nrow(datTest),2)) ## segment count percentage ## 1 Conspicuous 40 0.20 ## 2 Price 50 0.25 ## 3 Quality 40 0.20 ## 4 Style 70 0.35 很明显两个集合中消费者类别比例分布是一样的（实际应用中两个集合分布不一定严格相似，但应该非常接近）。 按照自变量划分 还可以使用最大差异度法(Willett 2004)划分数据（maxDissim()函数）。假设样本集A中含有m个样本，样本集B含有n个样本，n&gt;m，且我们要从B中选出一些样本加到A中，该子集中的样本要尽量和A中的不同。要实现这一点，对B中的一个样本，计算A中样本和该样本的差异度（距离，这里会算出m个值，因为A中有m个样本）。然后将和A中样本最不相同的B的样本抽取出来加入A，重复这个过程，直到A的样本量达到要求。关于这么权衡这m个差异度找到和A“最不相似”的样本，有不同的方法，比如以最小的值为准，将所有距离求和等等。这里没有什么黄金法则，建议大家尝试几种方法，查看比较得到的训练/测试样本自变量分布，选取其中一种。用这种方式可以得到自变量分布相似的不同样本集。R中有不同的计算样本间差异度（基于自变量观测）的函数。caret包中调用的是proxy包中的函数。关于不同的差异度测量，见相关包的帮助文档。我们可以通过选项 obj设置和样本集A“最不相似”的样本的方式，其中minDiss表示以最小差异度为准，sumDiss表示使用差异度之和。 我们用服装数据的一个子集为例展示按照自变量抽样。这里选取年龄和收入这两个变量。 # 最大差异度抽样用到proxy包 library(proxy) # 用lattice包绘制散点图 library(lattice) # 选取年龄和收入这两个变量 testing&lt;-subset(sim.dat,select=c(&quot;age&quot;,&quot;income&quot; )) 我们先随机选取5个样本做为初始集（start），剩下的样本组成集合samplePool： set.seed(5) # 随机选取5个样本 startSet &lt;- sample(1:dim(testing)[1], 5) start &lt;- testing[startSet,] # 剩下的样本存在对象samplePool中 samplePool &lt;- testing[-startSet,] 通过maxDissim()函数从samplePool中抽取5个样本，这5个样本尽量和start中已有的样本不同。： # 通过最大化差异得到的样本存在数据框new内 # obj = minDiss 表示总体差异度以最小差异度为准 newSamp &lt;- maxDissim(start, samplePool,obj = minDiss, n = 5) new&lt;-samplePool[newSamp,] 我们再从samplePool中不用最大差异法，而是随机抽取5个样本，将这5个样本存在数据框new2中： newSet &lt;- sample(1:dim(samplePool)[1], 5) new2&lt;-testing[newSet,] 绘制散点图比较两种不同方法（new：用最大化差异法抽取的样本；new2：随机抽取的样本）抽取的样本和初始样本（start）有什么不同： start$group&lt;-rep(&quot;start&quot;,nrow(start)) new$group&lt;-rep(&quot;new&quot;,nrow(new)) new2$group&lt;-rep(&quot;new2&quot;,nrow(new2)) xyplot(age~income,data=rbind(start,new,new2),grid = TRUE, group = group, auto.key = TRUE ) Figure 7.3: 按自变量最大化差异抽样 由图7.3可见，通过最大化差异抽取的样本（new）和初始样本点（start）分布在图的不同位置。而随机抽取的新样本（new2）和原始样本更加接近。我们为什么希望每次抽取的样本和之前的不一样呢？因为我们希望最后得到的训练集和测试集覆盖的自变量观测区间相似。如果抽取的样本点都来自一个区域的话（比如全部都是年龄30以下，收入10万以下），如果讲这个样本用于训练的模型很可能不具有预测这个区域外样本的能力。反之要是用这个样本做为测试集，则无法检测模型在这个区域外样本上的表现。 按时间序列划分 对于时间序列数据，用简单随机抽样通常不是最好的方式。有一种按时间序列划分训练集和测试集的方法，关于该方法的讨论见(Hyndman and Athanasopoulos 2013)。我们临时抽取一个长度为100的来自1阶自回归模型［AR(1)］的时间序列样本，用来展示caret包中对时间序列样本划分测试集和训练集的函数createTimeSlices()。由于时间序列话题不在本书范围之内，这里不会进行过多讨论。 # 抽取符合AR(1)的时间序列向量 timedata = arima.sim(list(order=c(1,0,0), ar=-.9), n=100) # 对时间序列作图 plot(timedata, main=(expression(AR(1)~~~phi==-.9))) Figure 7.4: 时间序列样本图 图7.3展示了100个模拟的时间序列观测。对这样的数据，我们希望训练集和测试集都能覆盖到不同时段的观测。下面用createTimeSlices()函数对数据进行划分。该函数中有3个需要设置的参数： initialWindow: 初始训练集样本中的连续观测数目 horizon: 测试集中的观测数目 fixedWindow: 逻辑值，取值为FALSE时，训练集从第一个样本开始划分区间长度不固定。 timeSlices &lt;- createTimeSlices(1:length(timedata), initialWindow = 36, horizon = 12, fixedWindow = T) str(timeSlices,max.level = 1) ## List of 2 ## $ train:List of 53 ## $ test :List of 53 可以看到函数结果返回2个列表，分别含有训练集和测试集的样本索引。我们查看第一个训练集和测试集样本。 # 将训练集索引信息存在trainSlices对象内 trainSlices &lt;- timeSlices[[1]] # 将测试集索引信息存在testSlices对象内 testSlices &lt;- timeSlices[[2]] # 分别查看第一个训练集样本和测试集样本 trainSlices[[1]] ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 ## [24] 24 25 26 27 28 29 30 31 32 33 34 35 36 testSlices[[1]] ## [1] 37 38 39 40 41 42 43 44 45 46 47 48 第一个训练集样本是原数据中第1个观测到第36个观测（因为initialWindow = 36），接下来从第37到48这12个观测被划分到第一个测试集（因为horizon = 12）。你可以通过head(trainSlices)查看后续的样本索引。尝试着改变fixedWindow =的设置，然后重复上面的代码得到新的trainSlices和testSlices，然后键入： head(trainSlices) head(testSlices) 比较两种结果的不同就能够很容易理解该选项的作用了。 训练集和测试集的划分很容易理解和实现。但注意其中两个潜在的缺陷： 由于训练集和测试集的划分是随机的，所以重复这一过程在测试集上得到的误差会有波动。 由于训练集中只包含原始观测的一个子集，拟合模型使用的是部分数据。通常当数据量不是非常大的时候，使用更少的观测多少会对模型拟合造成负面影响。这就意味着该过程可能过度估计模型误差（即，使用所有观测拟合的模型的误差应该比当前估计的要小）。 7.3.2 重抽样 重抽样即对样本进行重复划分，所以是建立在数据划分的基础上。其基本原理是：用部分样本拟合模型，用剩下的样本评估模型。多次重复这一过程，然后对结果进行汇总。进行重抽样的目的可能有： 对于有调优参数的模型，如支持向量机，罚函数模型等，必须通过重抽样估计调优参数。这时的目的是针对一个模型表现的度量（如RMSE），找到能够优化该度量的调优参数值。 对于不含有调优参数的模型，如普通线性回归，最小二乘回归等，就模型拟合本身不需要重抽样，但可以通过重抽样考察模型拟合结果的稳定性，也可以用于检验模型在和训练集无关的样本上的表现。 这一小节将介绍几种主要的重抽样方法。 7.3.2.1 k折交叉验证 k折交叉验证的主要过程如下： 将样本随机划分为\\(k\\)个大小相当的子集 对\\(i=1…k\\) 用除了第\\(i\\)个样本集之外的样本拟合模型\\(M_{i}\\) 将\\(M_{i}\\)用在第i个样本集上，对结果进行评估 这样会得到k个模型评估结果，将这些结果进行汇总（通常是计算均值和标准差），然后基于此了解调优参数和模型表现之间的关系。联系之前介绍的不同划分方法，可以将这些划分方法应用到k折交叉验证中，使k个子集中的因变量组成尽可能平衡。k折交叉验证的一个特定是k等于样本量，这时每次只有一个预留样本，该情况也称为留一交叉验证（LOOCV），注意在这种情况下模型最终的评估结果将根据所有的预测值进行计算。通常在样本量较小的时候使用LOOCV，道理很简单，样本量小的时候我们应该用尽可能多的样本拟合模型。关于交互校验的折数，很多R函数默认设置k=10，但没有黄金标准。折数越多，每次预留在外的样本就越少，模型表现估计值和真实值之间的差距就越小。但LOOCV的计算量最大，因为其模型拟合的次数等于样本量，且每次模型拟合使用的子集样本量几乎和训练集相同。另一方面，当k值很小时（2或者3），计算效率高但是结果的方差和偏差都会增加。这意味着如果重复抽样的过程得到的结果可能很不一样。当样本量足够大时，方差和偏差的潜在影响就可以忽略不计，在这种情况下可以使用折数较低的交叉验证。这里讲到的关于计算效率和偏差之间的权衡，需要读者自己反复实践才能真正理解。 caret包中有几个关于重抽样的函数。createFolds()函数用于k折交叉验证。我们按照 消费者类别变量对服装消费者数据抽取k折交叉验证样本。 library(caret) class&lt;-sim.dat$segment #k折校验重抽样 set.seed(1) cv&lt;-createFolds(class,k=10,returnTrain=T) str(cv) ## List of 10 ## $ Fold01: int [1:900] 1 2 3 4 5 6 7 8 9 10 ... ## $ Fold02: int [1:900] 1 2 3 4 5 6 7 9 10 11 ... ## $ Fold03: int [1:900] 1 2 3 4 5 6 7 8 10 11 ... ## $ Fold04: int [1:900] 1 2 3 4 5 6 7 8 9 11 ... ## $ Fold05: int [1:900] 1 3 4 6 7 8 9 10 11 12 ... ## $ Fold06: int [1:900] 1 2 3 4 5 6 7 8 9 10 ... ## $ Fold07: int [1:900] 2 3 4 5 6 7 8 9 10 11 ... ## $ Fold08: int [1:900] 1 2 3 4 5 8 9 10 11 12 ... ## $ Fold09: int [1:900] 1 2 4 5 6 7 8 9 10 11 ... ## $ Fold10: int [1:900] 1 2 3 5 6 7 8 9 10 11 ... 结果返回10个子样本集中样本对应的行数。我们可以通过交叉验证来估计调优参数。回忆之前应变量误差的小节中拟合支持向量机模型的代码： #这里只是截取了之前的代码用于展示，并不能独立运行 fit0&lt;-train(trainX,trainY,method=&quot;svmRadial&quot;, tuneLength=15, trControl=trainControl(method=&quot;cv&quot;)) 上面代码中“method=&quot;cv&quot;”告诉R进行交叉验证，这里默认\\(k=10\\)。 7.3.2.2 重复训练/测试集划分 该方法其实就是对数据集重复多次训练集／测试集划分，用训练集建立模型，用测试集评估模型。和k折交叉验证不同，该过程生成的测试集可能有重复的样本，其通常重复更多次。对于划分比例和重复次数没有固定法则，通常将总样本的75%到80%用于训练，剩下的用于测试，用于训练的样本越接近，得到模型估计的偏差就越小。该方法中重复的次数的增加可以减少模型评估结果的不确定性，当然代价就是在模型复杂时的计算时间。当然，重复的次数也和测试集的样本占总体比例有关，如果比例小，那么得到的预测评估结果的波动性就更大，这时就需要增加重复次数来见效评估结果的不确定性。 假设我们还是按照消费者类别（segment）划分数据，这依旧可以使用之前用于划分训练集和测试集的函数createDataPartition()。记得之前该函数中的选项设置times=1么？这里只要将其设置成你想要重复的次数即可。 trainIndex &lt;- createDataPartition(sim.dat$segment, p = .8, list = FALSE, times = 5) dplyr::glimpse(trainIndex) ## int [1:800, 1:5] 1 3 4 5 6 7 8 9 10 11 ... ## - attr(*, &quot;dimnames&quot;)=List of 2 ## ..$ : NULL ## ..$ : chr [1:5] &quot;Resample1&quot; &quot;Resample2&quot; &quot;Resample3&quot; &quot;Resample4&quot; ... 类似的，对于其它划分方式，只要大家知道如何划分，重复划分应该很容易。 7.3.2.3 Bootstrap 方法 Bootstrap是一种应用及其广泛而且强大的统计工具。它可以用来定量分析参数估计或统计模型的不确定性(Efron and Tibshirani 1986)。如，可以通过Bootstrap估计线性回归模型参数拟合的标准差，这是另外一种取代p值的方法。该方法的强大在于其可以很容易应用于每个模型（说白了就是是对数据重复进行有放回随机抽样，拟合的过程），有的模型要是用传统的统计推断方法很难得到标准差和置信区间。这是一个典型的听起来很高端，其实没太多技术含量的方法。但很多天才的想法不都是这样么？之前没有人想到，之后大家都觉得这么简单粗暴有效怎么会想不到。由于是有放回抽样，一个样本可能多次被选中，且Bootstrap样本量和原数据样本量一样。这些没有被选中的样本称为“袋外（out-of-bag）样本”。选中的样本用来建立模型，带外样本用来评估模型。Efron指出，一般情况下(Efron 1983)，Bootstrap估计的模型错误率的不确定性更小。平均而言，63.2%的样本点在 Bootstrap 中出现过至少一次，因此其估计的偏差与2折交叉验证相似。如之前所述，折数越小，用于训练的样本数目越少，这意味着估计的偏差越大。增加样本量可以缓解该问题。总的来说，和交叉验证相比，Bootstrap偏差更大，不确定性更小。针对估计偏差问题，Efron对原始Bootstrap过程进行了改进，得到下面的632方法： \\[(0.632 × 原始 Bootstrap 估计错误率) + (0.368 ×显性错误率)\\] 其中显性错误率就是用所有样本进行建模，然后再作用于相同的样本集得到的模型错误率，该估计显然过度乐观。这一改进虽然在某种程度上降低了偏差，但在样本量小的时候依旧表现不佳。试想严重过度拟合的情况下显性错误率几乎是0，那么上面公式中的第二项也就不存在了，这个时候，632方法给出的错误率估计可能过度乐观。Efron 和 Tibshirani之后进一步改进了632方法，得到“632+ 方法”，进一步调整了Bootstrap 估计(Efron and Tibshirani 1997)。 7.4 本章总结 本章介绍了一些基础建模技术，包括有监督和无监督的概念。模型误差分类： 系统误差：能够通过改进模型减小这部分误差 随机误差：当前数据无法解释的部分，无法通过建立更加复杂的模型来改进 此外我们讲了误差的两个来源，应变量误差和自变量误差。其中应变量误差会反映在随机误差中，这是个硬伤，无法克服。而某些自变量的误差可能通过其它相关自变量得到弥补，其影响取决于随机性强度，相应变量在模型中的重要性，以及自变量之间的相关性。 最后，也是最重要的一个话题是数据的划分和再抽样。其主要目的是为了判断模型真实的表现。我们介绍了3种数据划分的方法： 按照结果变量划分数据； 按照预测变量划分数据； 按照时间序列划分数据。 我们还介绍了两种主要的再抽样方法：bootstrap和交互校验。重抽样是建立在数据划分的基础上，其主要目的有两个： 对于有调优参数的模型估计调优参数； 考察模型拟合结果的稳定性。 其中我们讨论了不同重抽样的影响，以及在方差，偏差和计算效率之间的权衡。这里讲的所有方法都需要大家在实践中总结，才能真正成为自己的技能。 References "],
["section-8.html", "第8章 模型评估度量 8.1 回归模型评估度量 8.2 分类模型评估度量 8.3 本章总结", " 第8章 模型评估度量 当我们问哪个模型拟合效果好的时候我们到底在问什么？很多看似明确合理的问题一旦究其细节就会发现，其定义非常模糊以至于无法直接回答。这个问题的模糊之处在于没有指明用什么来衡量“拟合效果”？要比较模型首要任务是确定一个模型表现的度量，即通过什么标准来决定两个模型谁更好。模型表现的度量方法有好几种，要想更加全面的了解模型的表现，有时需要结合多种度量方式。这里我们只是单独介绍模型表现评估的度量，真正对度量的使用是建立在数据划分和再抽样的基础上的，也就是拟合模型和评估模型使用的数据集应该不同，否则得到的度量估计将过度乐观。 8.1 回归模型评估度量 接下来我们会依次介绍下面几种回归模型的表现度量方式：RMSE、校正\\(R^2\\)、\\(C_{p}\\)、AIC和BIC。 当因变量是数值时，我们可以使用均方误差平方根（Root mean squared error, RMSE）为指标衡量模型的表现。 这个度量是模型残差的函数，其中残差即为观测值减去模型的预测值。 均方误差（Mean squared error, MSE）的计算方法是将残差平方然后取平均， 而RMSE则是取MSE的平方根，从而它与原始数据的单位相同。 \\[MSE=\\frac{1}{n}\\sum_{i=1}^{n}(y_{i}-\\hat{y}_{i})^{2}\\] \\[RMSE=\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_{i}-\\hat{y}_{i})^{2}}\\] 得到的RMSE取值通常解释为残差离0的平均距离，或者解释为观测值和模型预测值之间平均的距离。回到之前介绍误差来源时用过的例子，对服装消费者数据中的收入（income）建立一般线性模型，将消费记录变量作为自变量，结果如下所示： # 载入服装消费者数据 sim.dat&lt;-read.csv(&quot;/Users/happyrabbit/Documents/GitHub/DataScientistR/Data/SegData.csv&quot;) fit&lt;-lm(income~store_exp+online_exp+store_trans+online_trans,data=sim.dat) summary(fit) ## ## Call: ## lm(formula = income ~ store_exp + online_exp + store_trans + ## online_trans, data = sim.dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -128768 -15804 441 13375 150945 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 85711.6796 3651.5991 23.472 &lt; 2e-16 *** ## store_exp 3.1977 0.4754 6.726 3.28e-11 *** ## online_exp 8.9949 0.8943 10.058 &lt; 2e-16 *** ## store_trans 4631.7507 436.4777 10.612 &lt; 2e-16 *** ## online_trans -1451.1618 178.8355 -8.115 1.80e-15 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 31530 on 811 degrees of freedom ## (184 observations deleted due to missingness) ## Multiple R-squared: 0.6018, Adjusted R-squared: 0.5998 ## F-statistic: 306.4 on 4 and 811 DF, p-value: &lt; 2.2e-16 拟合的线性模型fit的RMSE为3.15310^{4}（输出底部“Residual standard error:”后面的值）。 另一个常用的度量是R-Squared，通常写作\\(R^2\\)。它实际上是观测值和预测值的相关系数的平方。大家可能对线性回归中的\\(R^2\\)很熟悉，但它可以用于任何回归模型。通常解释成模型能够解释的应变量总变异的比例其中R-squared＝0.6 表示模型可以解释因变量总变异的四分之三。尽管这是一个易于解释的统计量，但要注意它是一种相关性而不是准确性的度量，它依赖于应变量方差。比如虽然fit的\\(R^2\\)不低，但是RMSE为0.6，说明预测的收入和真实收入之间的平均差距为3.15310^{4}，这样的精确度并不高。在应变量的取值很大时，即使&gt;90%的\\(R^2\\)也不一定代表足够的精确度，在对公司的销售总额进行建模就常是这样的情况。之前我们在展示自变量和应变量误差对模型表现影响的时候有用过\\(R^2\\)，那时并没有考虑变量个数对\\(R^2\\)的影响（因为变量个数和观测个数相比并不多）。但事实上\\(R^2\\)会随着变量个数的增加而增大。校正\\(R^2\\)就是针对该问题对原\\(R^2\\)进行改进。原始\\(R^2\\)的定义为： \\[R^{2}=1-\\frac{RSS}{TSS}\\] 其中\\(RSS=\\sum_{i=1}^{n}(y_{i}-\\hat{y_{i}})^{2}\\)，\\(TSS=\\sum_{i=1}^{n}(y_{i}-\\bar{y})^{2}\\)。 由于RSS总是随着变量个数的增加而降低，\\(R^2\\)也就相应随着变量个数增加而增加。对于有\\(p\\)个变量的最小二乘模型，校正\\(R^2\\)定义为： \\[校正R^{2}=1-\\frac{RSS/(n-p-1)}{TSS/(n-1)}\\] 最大化\\(校正R^{2}\\)等同于最小化\\(RSS/(n-p-1)\\)。由于考虑了变量个数\\(p\\)，\\(RSS/(n-p-1)\\)随着变量个数的增加可能增加或者减少。\\(校正R^{2}\\)的直观想法是当模型中已经包含所有有用的变量后继续加入噪音变量只能略微降低\\(RSS\\)，由于变量个数增加，\\(n-p-1\\)增加进而整体\\(RSS/(n-p-1)\\)反而增加了，于是\\(校正R^{2}\\)会降低。因此，从理论上讲对应最大\\(校正R^{2}\\)的模型只包含有效变量而没有噪音变量。模型每加入一个噪音变量都会受到“惩罚”。 对于含有\\(p\\)个变量的最小二乘拟合模型，\\(C_{p}\\)的定义如下： \\[C_{p}=\\frac{1}{n}(RSS+2p\\hat{\\sigma}^{2})\\] 其中\\(\\hat{\\sigma}^{2}\\)是对模型随机项\\(\\epsilon\\)的方差的估计。本质上\\(C_{p}\\)统计量就是在训练集的\\(RSS\\)上加上惩罚\\(2p\\hat{\\sigma}^{2}\\)，对基于训练集过度乐观的误差估计做出调整。很明显，当变量个数增加时，惩罚也随之加重，这可以抵消变量个数增加导致的\\(RSS\\)减小。用于模型选择时，我们选择对应\\(C_{p}\\)更小的模型。 AIC可以用于评估很多模型，它是基于最大似然值的。在线性回归的例子里，最大似然估计和最小二乘估计是一样的： \\[AIC=n+nlog(2\\pi)+nlog(RSS/n)+2(p+1)\\] BIC 也是基于最大似然值： \\[BIC=n+nlog(2\\pi)+nlog(RSS/n)+log(n)(p+1)\\] R中的函数AIC()和BIC()就是按上面的公式分别计算AIC和BIC的。在很多教科书里通常会省略常数项\\(n+nlog(2\\pi)\\)，且用\\(p\\)代替\\(p+1\\)。但不同的公式效果相同，因为使用时只考虑相对大小。和AIC相比，BIC对参数个数进行了更加严厉的惩罚。所以通过BIC选出的模型通常参数个数比AIC少。 模型评估和变量选择要求选取一个相应的选择标准，关于变量选择，在特征工程的章节中会详细介绍。 8.2 分类模型评估度量 本小节关注判别模型（即，应变量为分类变量）的表现度量。之前对连续型变量适用的RMSE和\\(R^2\\)不适用于分类模型。分类指对给定观测样本预测其所属类别，而且类别空间已知，所以是有监督学习。这个和聚类不同，聚类分析的目的是得到类别空间，是无监督学习，这在之后聚类和判别的部分还会更详细的介绍。通常遇到的问题是二分类，比如是否有某种疾病，垃圾邮件分类器等。也有多分类问题，比如服装消费数据中的消费者类别。这里我们用生猪疫情风险预测数据为例展示分类模型的评估度量。这里我们训练一个随机森林模型对农场疫情爆发概率进行评估，之后在树模型的章节中会对模型本身进行更详细的介绍。 library(dplyr) library(randomForest) library(caret) library(readr) # 读取数据 # 载入数据 disease_dat&lt;-read.csv(&quot;/Users/happyrabbit/Documents/GitHub/DataScientistR/Data/sim1_da1.csv&quot;) # 可以用glimpse()函数查看数据 # glimpse(disease_dat) AirlineRating 划分训练集和测试集，在训练集（xTrain和yTrain）上得到模型，然后在测试集（xTest和yTest）上评估模型表现。70%的样本用于训练，剩下30%用于模型评估： # 划分训练集和测试集 set.seed(2016) trainIndex&lt;-createDataPartition(disease_dat$y,p=0.8,list=F,times=1) xTrain&lt;-disease_dat[trainIndex,]%&gt;%select(-y) xTest&lt;-disease_dat[-trainIndex,]%&gt;%select(-y) # 需要将应变量转化成因子类型 yTrain&lt;-disease_dat$y[trainIndex]%&gt;%as.factor() yTest&lt;-disease_dat$y[-trainIndex]%&gt;%as.factor() 训练随机森林模型： train_rf&lt;-randomForest(yTrain~.,data=xTrain,mtry=trunc(sqrt(ncol(xTrain)-1)),ntree=1000,importance=T) 为了展示不同的模型评估法则，我们将训练得到的随机森林模型应用到测试集，得到两种预测： 每个类别的概率预测（结果为0到1之间的连续值，可以通过在代码中添加选项prob得到预测，结果存在yhatprob对象中） 离散类别预测（结果为0/1形式，存在yhat对象中） 我们分别看看这两个预测结果： yhatprob&lt;-predict(train_rf,xTest,&quot;prob&quot;) set.seed(100) car::some(yhatprob) ## 0 1 ## 45 0.592 0.408 ## 158 0.455 0.545 ## 255 0.575 0.425 ## 291 0.513 0.487 ## 314 0.620 0.380 ## 392 0.538 0.462 ## 402 0.472 0.528 ## 443 0.542 0.458 ## 462 0.620 0.380 ## 626 0.586 0.414 yhat&lt;-predict(train_rf,xTest) car::some(yhat) ## 206 273 305 348 519 524 525 599 701 780 ## 1 0 0 0 0 0 0 0 1 0 ## Levels: 0 1 现在我们就用上面的两种预测结果为例介绍不同的预测类评估方法。 8.2.1 Kappa统计量 混淆矩阵（Confusion Matrix）是对分类结果进行详细描述的一个表，是简单的观测类和预测类的交叉表。在此例中，观测类是yTest，预测类是yhat，相应的混淆矩阵为： table(yhat,yTest) ## yTest ## yhat 0 1 ## 0 68 56 ## 1 3 33 表格中左上角和右下角分别代表预测正确的样本数目，左下角和右上角代表错误预测的样本数目。更一般的二分类混淆矩阵如下： 观测发生 观测不发生 预测发生 TP FP 预测不发生 FN TN 其中TP代表真阳性，FP代表假阳性，TN代表真阴性，FN代表假阴性。表格左上到右下对角线上的元素表示正确预测的样本数目，另一个方向的对角线上的元素代表误判的样本数。评估类预测最简单的指标是总体精确率，即预测正确的总体样本比例: \\[总体精确率=\\frac{TP+TN}{TP+TN+FP+FN}\\] 对类别数目大于2的情况，可以类似计算总体精确率。该统计量很直观，但有一些缺点，首先总体精确率没有区分错误类型。实际应用中，不同错误对应的损失可能不同，这时就无法用总体精确率衡量模型。 比如过滤垃圾邮件，误删一封重要的邮件带来的损失要高于收到一封垃圾邮件的损失。Provost等人(Provost F 1998)深入讨论了用精确率来比较不同的分类器存在的问题。其次总体精确率没有考虑真实频率。比如在保险风险分析中，有风险的概率可能只有千分之一或者更小，模型只要将所有样本都清一色判定为无风险就能达到几乎完美的精确率。有时我们将不用模型也能得到的精确率称为无信息率。这里将所有样本判定为无风险得到的无信息率至少是99.9%。在这种情况下模型的精确率需要高于无信息率才算合理。 还有一种一致性检验方法叫做Kappa统计量，最早由Cohen等人在1960年提出用于考察两个不同的诊断方法在结果上是否具有一致性(J 1960)。Kappa考虑到简单由偶然情况产生的准确性。具体公式如下： \\[Kappa=\\frac{P_{0}+P_{e}}{1-P_{e}}\\] 假设\\(n=TP+TN+FP+FN\\)为总体样本数，其中\\(P_{0}=\\frac{TP+TN}{n}\\)为实际预测一致率，\\(P_{e}=\\frac{(TP+FP)(TP+FN)+(FN+TN)(FP+TN)}{n^{2}}\\)为由简单偶然情况产生的一致率。Kappa取值从-1到1，值越高一致性越强。 Kappa = 1 时，表明完全一致。 Kappa = 0 时，则一致性与偶然预期的相同。 Kappa &lt; 0 时，一致性比偶然预期的还要弱，不过这种情况很少发生。 一般说来，Kappa值在0.3到0.5之间代表合理的一致性。假定一个模型的精确度很高（90%），但偶然预期的精确度也很高（85%），Kappa统计量为\\(\\frac{1}{3}\\)，表明预测和观测适度一致。Kappa统计量也可以扩展至评估类别大于2的情形。fmsb包中的Kappa.test()函数能用于计算Cohen的Kappa统计量。该函数还能对Kappa统计量进行统计检验，并且给出置信区间。以上面观测类yTest和预测类yhat结果为例，可以通过如下代码计算Kappa统计量： kt&lt;-fmsb::Kappa.test(table(yhat,yTest)) # 统计量估值在函数返回值的Result对象中 kt$Result$estimate ## [1] 0.3054738 上面函数返回结果中包含一个Judgement对象: kt$Judgement ## [1] &quot;Fair agreement&quot; 这是基于Landis和Koch提出的Kappa统计量的一般性解释方法(Landis JR 1977)： Kappa&lt; 0：无一致性（No agreement） Kappa在0-0.2之间：略微一致（Slignt agreement） Kappa在0.2-0.4之间：轻度一致（Fair agreement） Kappa在0.4-0.6之间：适度一致（Moderate agreement） Kappa在0.6-0.8之间：强一致（Substantial agreement） Kappa在0.8-1.0之间：几乎完全一致（Almost perfect agreement） 8.2.2 ROC曲线 相对与简单的类取值，类概率中含有更多的模型预测信息，比如之前得到的yhatprob就是连续的预测值。对于这样的预测结果，ROC曲线是一个通用评估方法。该方法确定一个有效的阈值，超过这个阈值的观测被标注为某类。比如所有yhatprob&gt;0.9的样本都判定为风格类。ROC曲线是基于灵敏度和特异度这两个统计量。考虑之前展示的二分类情况的混淆矩阵。模型的灵敏度为在所有真实观测到“发生”的样本中被准确预测为“发生”的比率： \\[灵敏度=\\frac{正确预测为“发生”的样本数目}{观测到“发生”的样本数目}=\\frac{TP}{TP+FN}\\] 特异度指的是观测到“不发生”的样本中准确预测为“不发生”的比率： \\[特异度=\\frac{正确预测为“不发生”的样本数目}{观测到“不发生”的样本数目}=\\frac{TN}{TN+FP}\\] 灵敏度也称为真阳性率，特异度也称为真阴性率。“1-特异度”为假阳性率。灵敏度和特异度对应的分母是固定的，当模型将更多样本判定为“发生”时，对应的灵敏度会增加，特异度会降低。根据不同错误类型导致的损失，通常需要建模者在灵敏度和特异度之间做出权衡。ROC曲线是权衡这二者的一个有效工具。ROC曲线是通过设定一系列预测结果阈值，得到相应的真阳性率（灵敏度）和假阳性率（1-特异度）绘制成的曲线。我们得到的关于农场疫情爆发预测概率结果yhatprob为例，展示如何用rROC包中的相应函数得到ROC曲线和相关统计量。yhatprob中的第2列代表将样本判定为疫情爆发的概率，其两列相加和为1。我们可以使用函数roc()得到相应的ROC对象rocCurve。然后将不同的函数应用在该对象上得到相应的图形结果或者统计量。下面的代码可以用来得到ROC曲线： library(pROC) rocCurve&lt;-roc(response=yTest,predictor=yhatprob[,2]) plot(rocCurve,legacy.axes=T) ## ## Call: ## roc.default(response = yTest, predictor = yhatprob[, 2]) ## ## Data: yhatprob[, 2] in 71 controls (yTest 0) &lt; 89 cases (yTest 1). ## Area under the curve: 0.8083 其中roc()函数中的第一个参数response是真实观测值，predictor是连续预测结果，这里赋予的是判定为爆发的预测概率。ROC曲线的横坐标是1-特异度，纵坐标是灵敏度。一个完美的模型能完全区分两个类，灵敏度和特异度均为100%。从图形上看，ROC曲线为通过(0,0)和(1,1)的曲线。完美模型对应的曲线还通过(0,1)点，对应的曲线下面积为1。完全无效的模型对应的曲线趋近于45度对角线，曲线下面积为0.5。可以将不同模型结果对应的ROC曲线放在一张图中，直观对比模型效果。或者通过曲线下面积（AUC）量化比较模型，对应面积越大的模型越有效。DeLong等提出了基于U统计量的估计和比较AUC的方法(E.R. DeLong 1988)，也可以通过bootstrap得到AUC的置信区间(Hall P 2004)。 在R中，我们可以通过如下代码得到基于DeLong提出的非参方法得到的AUC的估计和置信区间： # 得到AUC的估计 auc(rocCurve) ## Area under the curve: 0.8083 # DeLong方法得到的AUC置信区间 ci.auc(rocCurve) ## 95% CI: 0.7419-0.8746 (DeLong) ROC曲线和线下面积AUC是我最常用的评估分类模型的方式，由于它是灵敏度和特异度的函数，对类失衡有抗性(Provost F 1998; T 2006)。用AUC和其它单一度量类似，在用某个量总结曲线时会带来信息的损失，因为很可能没有某条曲线一致好于其它的曲线（曲线交叉）。如果我们对曲线特定的区域感兴趣，可以直接比较曲线。如果我们关心的是ROC曲线低的一端，可以使用ROC曲线下局部面积度量(D 1989)，该度量关注曲线特定部分。ROC曲线主要针对二分类定义，但之后不同人将其扩展到多分类的情况(Hand D 2001; Lachiche N 2003; Li J 2008)。 8.2.3 提升图 除了数值度量以外，还有一些对分类结果评估的可视化工具，如提升图。提升图以图形的形式表示模型预测比随机预测相比带来的改进，根据“提升”分数来选择模型，或者确定应该将数据中多大比例的样本视为目标群体可以从模型预测结果中获益。这样抽象的描述很难让大家理解提升图。因此我们将其放在应用的语境下。我们用猪场疫情数据为例。之前我们在训练集上训练随机森林模型，然后将模型应用在含有160个样本的测试集（xTest）上。我们对测试集样本的预测yhatprob来解释提升图。由于我们感兴趣的是疫情爆发的事件——yhatprob第二列（第一列是无疫情的概率）——我们将针对疫情爆发的连续概率预测存在一个新对象modelscore中。真实情况是有89个样本有疫情。如果我们将这160个样本按照模型预测分值modelscore从高到低排序，对于完美的模型，排序后的前89个样本应该正好就是那些疫情爆发的样本。当预测完全随机时，排序后前x％的样本中根据随机概率也该正好含有89个发生疫情的样本中的x％。提升图展示了通过模型预测排序筛选出的样本比随机样本对应目标类命中率的差别。 下面我们抽取了一些随机分值（randomscore），对其和模型预测结果（modelscore）绘制提升图，比较它们。 modelscore&lt;-yhatprob[,2] # 随机抽取一些分值 randomscore&lt;-rnorm(length(yTest)) labs&lt;-c(modelscore=&quot;Random Forest&quot;, randomscore=&quot;Random Number&quot;) 我们可以使用caret包中的lift()函数来绘制提升曲线。该函数用一个公式作为输入选项，公式左侧是真实类别，公式右侧是多个预测值。这里公式右侧是模型分值和随机分值： liftCurve&lt;-lift(yTest~modelscore+randomscore,class=&quot;1&quot;,labels=labs) 为了绘制多条提升图，使用lattice包中的xyplot()函数： xyplot(liftCurve,auto.key=list(columns=2,lines=T,points=F)) 提升图的横轴是累计样本百分比，纵轴是累计获取的目标类样本百分比。比如随机森林模型提升曲线上的点(6.25,11.24)表示：按照模型预测分值从高到低排序后的前6.25%的样本中含有160个疫情爆发样本中的0.1123596。和ROC曲线类似，我们可以通过比较不同模型的提升图来选择模型，曲线下面积也可作为模型效果的度量。此外我们也可能对曲线的某一部分特别感兴趣。比如在当前的例子中，如果判定一个农场在未来5年内可能爆发疫情，那么通常的措施是对该农场进行大规模的清洗消毒，这样的措施花费很高。假设我们只能对50%的农场进行清理，那么就该选择对应横坐标为50%，疫情爆发样本命中率最高的模型。在这个应用场景下，在支出预算一定时最大化效率。 8.3 本章总结 本章探讨了模型评估的度量。在数据分析项目中，评估模型是非常重要的。掌握本章和上一章介绍的数据划分和再抽样技术就具备评估模型的技术能力了。这里关于分类模型评估有一个重要话题由于篇幅所限没有介绍，就是预测概率校准和处理类失衡的问题。对此话题感兴趣的读者可以参考Max Kuhn 和 Kjell Johnston的书《Applied Predictive Modeling》中的第11章(Max Kuhn 2013)，这本书的中文版已于2016年5月由电子工业出版社出版。模型选择和评估要求分析师将模型放在具体项目语境下。这是体现科学和艺术结合的典型环节。我们在本书之后讲具体模型的时候会给出几个完整的案例分析，其中包括用之前讲到的这些建模技术进行模型选择。关于模型评估还要提两点： 尝试尽可能多的模型 当考虑该用什么模型解决某具体的问题时，应该考虑多个可能的模型。从最简单的模型开始直到你能达到的难度上限。真正尝试拟合模型时，根据个人喜好，你可以从最简单的模型开始，每拟合一次模型，对数据中变量关系的理解会有所加深，慢慢过渡到更加复杂的模型。或者从最复杂的模型开始，但要做好简化模型的准备，使得模型具有更强的解释性。实际应用中，你不知道什么模型对当前问题最有效，所以比较不同的模型对于一个合格的数据科学家来说是必须的。当然，这有一个隐藏的前提条件是你能够快速有效的拟合不同模型。如果你需要让计算机跑1个晚上的程序来拟合一个模型，尝试这样的模型不是一个好主意。 检查模型的稳定性 提高模型稳定性有各种可能的方法，收集更多的观测，除去冗余变量，如之前提到的近0方差变量和高度相关变量。检查模型拟合的稳定程度最常用的方法是再抽样。通过抽取不同的样本拟合相同的模型，然后查看拟合参数的变化范围。要是需要检查模型在某假设条件不满足的情况下的表现，可以通过模拟数据进行考察。 最后我想用George Box的那句统计学界家喻户晓的名言结束这一章： 所有模型都是错的，但其中有一些是有用的。(All models are wrong, but some are useful.) References "],
["section-9.html", "第9章 特征工程 9.1 特征构建 9.2 特征提取 9.3 变量选择", " 第9章 特征工程 对自变量进行编码和筛选的过程称为特征工程（Feature Engineering），其目的就是获取更好的训练数据，提高模型表现。例如，有时用自变量的组合能够比使用单独的自变量更有效；用两个变量的比值可能比用两个单独的变量更有效等等。通常最有效的编码数据的方法来自于建模者对问题的理解，而不是通过任何数学方法。在原始数据基础上，通过该工程得到的优化特征可以更好的描述数据关系。从数学的角度上就是优化自变量矩阵\\(\\mathbf{X}\\)。 特征工程在机器学习中起着举足轻重的作用，如果你能找到有效的特征，其实未必需要复杂的算法。很遗憾，大多数的书中并没有专门花一章来讲特征工程，更常提到的是特征选择（Feature Selection）。很多机器学习的书都是以介绍算法为主，目的在于理解算法本身，所以特征工程通常不是重点。这和特征工程在实际应用中的重要性极不相称。所以在这里，我们专门花一章来介绍特征工程。在特征工程下面有3个主要的子问题，我们会一次讨论这三个问题： 特征构建（Feature Construction）：从原始数据中构建新变量 特征提取（Feature Extraction）：将原始变量按照某种标准变换得到能够更好反映数据关系的变量 特征选择（Feature Selection）：在整个自变量集中找到和因变量有关的变量子集，从而达到降维且增加模型估计稳定性和可解释性的效果 这三者的大概顺序是：特征构建 -&gt; 特征提取 -&gt; 特征选择。如果特征构建做的不好，那么它会直接影响特征提取，进而影响了特征选择，最终影响模型的性能。事实上，特征工程是一个迭代过程，我们需要不断的设计特征、选择特征、建立模型、评估模型，然后得到反馈在回头优化特征的设计。 9.1 特征构建 在实际应用中，显然是不可能凭空而来的，需要我们手工去构建特征。关于特征构建的定义，可以这么说：特征构建指的是从原始数据中人工的构建新的特征。我们需要人工的创建它们。这需要我们花大量的时间去研究真实的数据样本，思考问题的潜在形式和数据结构，同时能够更好地应用到预测模型中。 特征构建需要很强的洞察力和分析能力，要求我们能够从原始数据中找出一些具有物理意义的特征。假设原始数据是表格数据，一般你可以使用混合属性或者组合属性来创建新的特征，或是分解或切分原有的特征来创建新的特征。比如之前在数据预处理那章中讲编码名义变量时，我们将分类变量gender(性别)转化为两个名义变量：Female和Male。之后考虑收入和性别的交互效应(income:gender)也是一种特征构建。另外再举一个例子，假设你有一个日期时间 (2006-04-01 02:26:00)该如何转换呢？对于这种时间的数据，我们可以根据需求提取出多种属性。比如下面这个从农业论坛爬取的文本数据。 library(readr) library(dplyr) topic&lt;-read_csv(&quot;/Users/happyrabbit/Documents/Hui/Project/NLP/RawData/newagtalk-20150607/topic.csv&quot;) glimpse(topic) ## Observations: 209,607 ## Variables: 6 ## $ tid &lt;int&gt; 242, 259, 270, 281, 301, 312, 333, 367, 386, 387... ## $ fid &lt;int&gt; 5, 5, 5, 5, 5, 5, 5, 7, 5, 5, 5, 5, 5, 5, 5, 7, ... ## $ title &lt;chr&gt; &quot;0&quot;, &quot;Adobe Reader&quot;, &quot;? about this forum veiw?&quot;,... ## $ posted_at &lt;time&gt; 2006-04-01 02:26:00, 2006-04-01 08:39:00, 2006-... ## $ user_name &lt;chr&gt; &quot;Rich&quot;, &quot;Larry NCKS&quot;, &quot;Hay Hud Ohio&quot;, &quot;jakescia&quot;... ## $ user_location &lt;chr&gt; &quot;Kansas&quot;, &quot;Washington, Kansas &amp; Lincoln, Nebras... # 将发帖时间提取出来，存在posted_at2对象中 posted_at2&lt;-topic$posted_at 数据由6列，这里只解释其中2列。posted_at是发帖时间，user_name是论坛用户名。对于发帖时间，我们可以将其拆分成不同的变量，这个过程类似于探索性数据分析。这里时间观测是按照年、月、日、时、分、秒的顺序，以这样的格式2006-04-01 02:26:00排列的。在具体分析中，我们通常希望知道用户的发帖规律。在事先不知道怎样划分能够看到规律的情况下该怎么办？尝试不同的划分方法。比如我们可以研究每年的发帖量，可以通过substr()函数截取字符串的固定位置得到年份： # 将截取的年份存在名为year的列中 topic$year&lt;-substr(posted_at2,1,4) # 看下结果如何 car::some(topic$year) ## [1] &quot;2011&quot; &quot;2012&quot; &quot;2013&quot; &quot;2013&quot; &quot;2013&quot; &quot;2013&quot; &quot;2014&quot; &quot;2014&quot; &quot;2014&quot; &quot;2015&quot; 接下来我们可以查看下每年发帖数目的变化情况： barplot(table(topic$year),family =&quot;Songti SC&quot;, main=&quot;年度发帖数目频数直方图&quot;) 图中我们可以看到，从2006年论坛创建以来，帖子的数目几乎呈指数上升，2015年貌似不符合规律，其实是因为当前数据中只饱含到2015年5月的论坛数据，也就是说如果我们由所有2015年的数据，最后的直方条应该会超过2014年。这么简单的统计能够告诉我们什么呢？这样的统计量对于公司的市场预算是很重要的。假如某农业公司要决定是否投入人力和财力去挖掘这个农业论坛数据，首先需要明确的就是这些数据是不是有代表性，该论坛是不是活跃。上图就表明该论坛是处在高速发展阶段的，而且考虑到论坛的用户是农民，这样的活跃度是非常高的。所以相关决策人员或许可以将该论坛当作一个消费者评论信息的来源。只考虑年是不够的，我们可能还想类似的检查下月度发帖分布，我们可以类似的截取时间字符串中的月份，绘制直方图： topic$month&lt;-substr(posted_at2,6,7) barplot(table(topic$month),family =&quot;Songti SC&quot;, main=&quot;月度发帖数目频数直方图&quot;) 大家可以看到明显的季节效应。通常北半球12-4月是农闲时节，这个时候发帖数远高于开春（5月）之后。基于此，我们可以假设12-4月的帖子或许更多的是关于去年购买的种子收获情况以及一些下一年的耕种计划的信息，5月到11月间或许更多的是当下遇到的问题，比如播种时种子发芽情况，生长中雨水，作物抗旱性能等等。这些为农业公司提供了消费者对其产品的体验信息。这样简单的统计能够给我们之后的分析指引放下。再次强调，分析的整个流程是个渐进且协同的过程，前一步中获得的信息可能有助于我们明确之后要如何进行分析，下一步分析的结果可能又让我们返回之前的步骤，比如收集构建新变量等等。 再看看每天发帖的规律： topic$time&lt;-substr(posted_at2,12,13) barplot(table(topic$time),family =&quot;Songti SC&quot;,main=&quot;每日不同时间段发帖频数直方图&quot;) 可以看到，每天有两个发帖高峰，中午一阵，晚上一阵（这里不考虑北美不同纬度的时差影响）。这能给我们什么信息呢？很有可能农民习惯于在这两个时间段上网查询信息等。如果农业公司想要发营销广告，这或许是最佳的时间。 上面是一个简单的关于构建特征的例子，在具体应用中特征构建的方法可以非常灵活，没有一个黄金标准。随着分析经验的增长，以及对相关领域的了解加深，构建有意义特征的能力也会随之提高。特征构建是特征工程中艺术成分最高的部分，接下来我们要介绍的特征提取和特征选择就有更强的技术性。 9.2 特征提取 特征提取是一项用不同变量的组合代替原变量的技术。它的目的是自动地构建新的特征，将原始特征转换为一组具有明显物理意义或者统计意义的特征。比如通过变换特征取值来减少原始数据中某个特征的取值个数等。我们考虑3个常用的可以对数据降维的特征提取方法。主成分分析（PCA）试图找到原变量的不相关线性组合，这些线性组合能够最大限度的解释原数据中变量方差。解释性因子分析（EFA）同样试图在尽量小的维度上解释原数据中尽可能多的方差。高维标度化（MDS）将观测见的相似度映射到低维度上，如2维平面。MDS能够作用于非数值型变量，如分类变量或者有序数据预测变量。接下来我们通过模拟的航空公司数据集来展示不同的特征提取方法。在市场营销中这类消费者调查问卷中，虽然初始问题很多，但通常存在多个调查项共同反应少数几个潜在因子。比如航空公司满意度调查数据中下面四个问题：购票容易度（Easy_Reservation）、座椅选择（Preferred_Seats）、航班选择（Flight_Options）和票价（Ticket_Prices）都和购票体验有关。 9.2.1 初步探索数据 我们先读入该数据： # 可以从网站下载该数据 airline&lt;-read_csv(&quot;/Users/happyrabbit/Documents/GitHub/DataScientistR/Data/AirlineRating.csv&quot;) 可以用glimpse()函数检查该数据： glimpse(airline) ## Observations: 3,000 ## Variables: 17 ## $ Easy_Reservation &lt;int&gt; 6, 5, 6, 5, 4, 5, 6, 4, 6, 4, 5, 5, 6, 5, 5, ... ## $ Preferred_Seats &lt;int&gt; 5, 7, 6, 6, 5, 6, 6, 6, 5, 4, 7, 5, 7, 6, 6, ... ## $ Flight_Options &lt;int&gt; 4, 7, 5, 5, 3, 4, 6, 3, 4, 5, 6, 6, 6, 5, 6, ... ## $ Ticket_Prices &lt;int&gt; 5, 6, 6, 5, 6, 5, 5, 5, 5, 6, 7, 7, 6, 7, 7, ... ## $ Seat_Comfort &lt;int&gt; 5, 6, 7, 7, 6, 6, 6, 4, 6, 9, 7, 7, 6, 6, 6, ... ## $ Seat_Roominess &lt;int&gt; 7, 8, 6, 8, 7, 8, 6, 5, 7, 8, 8, 9, 7, 8, 6, ... ## $ Overhead_Storage &lt;int&gt; 5, 5, 7, 6, 5, 4, 4, 4, 5, 7, 6, 6, 7, 5, 4, ... ## $ Clean_Aircraft &lt;int&gt; 7, 6, 7, 7, 7, 7, 6, 4, 6, 7, 7, 7, 7, 7, 6, ... ## $ Courtesy &lt;int&gt; 5, 6, 6, 4, 2, 5, 5, 4, 5, 6, 4, 6, 4, 5, 5, ... ## $ Friendliness &lt;int&gt; 4, 6, 6, 6, 3, 4, 5, 5, 4, 5, 6, 7, 5, 4, 4, ... ## $ Helpfulness &lt;int&gt; 6, 5, 6, 4, 4, 5, 5, 4, 3, 5, 5, 6, 5, 4, 5, ... ## $ Service &lt;int&gt; 6, 5, 6, 5, 3, 5, 5, 5, 3, 5, 6, 6, 5, 5, 4, ... ## $ Satisfaction &lt;int&gt; 6, 7, 7, 5, 4, 6, 5, 5, 4, 7, 6, 7, 6, 4, 4, ... ## $ Fly_Again &lt;int&gt; 6, 6, 6, 7, 4, 5, 3, 4, 7, 6, 8, 6, 5, 4, 6, ... ## $ Recommend &lt;int&gt; 3, 6, 5, 5, 4, 5, 6, 5, 8, 6, 8, 7, 6, 5, 6, ... ## $ ID &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14... ## $ Airline &lt;chr&gt; &quot;AirlineCo.1&quot;, &quot;AirlineCo.1&quot;, &quot;AirlineCo.1&quot;, ... 数据的前15列都是问卷调查的各种问题，问题格式如下：对该航空公司的你的满意度是？从1到9，分值越大满意度越高。可以看到前15列评分在1-9之间，是整数型了。ID代表受访者编号，不同编号代表不同受访者。每个受访者需要评估3家航空公司，列Airline指出相应的航空公司。一共有1000名受访者，因此观测的总行数为1000x3=3000。关于数据各列变量的解释，大家可以参考“数据集模拟和背景介绍”中相关小节。 我们用corrplot()函数检查问卷调查问题的相关性： library(corrplot) # 选取其中的问卷调查项 select(airline,Easy_Reservation:Recommend)%&gt;% # 得到相关矩阵 cor()%&gt;% # 用corrplot()绘制相关图 # 选项order=&quot;hclust&quot;按照变量的相似度，基于系统聚类的结果对行列进行重新排列 corrplot(,order=&quot;hclust&quot;) 由相关矩阵图可以看到，这些问卷项大致分成3类： 空航服务相关 礼貌（Courtesy） 友善（Friendliness） 能够提供需要的帮助（Helpfulness） 食物饮料服务（Service） 购票体验相关 购票容易度（Easy_Reservation） 座椅选择（Preferred_Seats） 航班选择（Flight_Options） 票价（Ticket_Prices） 机舱设施和总体评估指数 座椅舒适度（Seat_Comfort） 位置前后空间（Seat_Roominess） 随机行李存放（Overhead_Storage） 机舱清洁（Clean_Aircraft） 总体满意度（Satisfaction） 再次选择次航空公司（Fly_Again） 向朋友推荐此航空公司（Recommend） 而且机舱设施和总体满意度相关性较高。空航服务和购票体验貌似负相关，也就是说航空公司目前没有做到让乘客对这两类体验都感到满意，这也可能是潜在需要提高的地方。这里简单的检查数据能够给我们一些基本的信息，让我们能够做出一些假设，然后在之后的分析中尝试证实这些假设。 对于这样的数据初步探索，一个非常自然的问题是：每个航空公司对应的各项评分均值是多少？我们可以用之前介绍的dplyr包中的各种函数，以及使用之前讲到的管道操作%&gt;%让代码更易读： # 选取其中的问卷调查项和航空公司因子信息 # 即删除ID项 airline.mean&lt;-select(airline,-ID)%&gt;% # 按Airline对数据进行分组总结 group_by(Airline)%&gt;% # 对每个数值 summarise_each(funs(mean))%&gt;% # 显示数据 glimpse() ## Observations: 3 ## Variables: 16 ## $ Airline &lt;chr&gt; &quot;AirlineCo.1&quot;, &quot;AirlineCo.2&quot;, &quot;AirlineCo.3&quot; ## $ Easy_Reservation &lt;dbl&gt; 5.031, 2.939, 2.038 ## $ Preferred_Seats &lt;dbl&gt; 6.025, 2.995, 2.019 ## $ Flight_Options &lt;dbl&gt; 4.996, 2.033, 2.067 ## $ Ticket_Prices &lt;dbl&gt; 5.997, 3.016, 2.058 ## $ Seat_Comfort &lt;dbl&gt; 6.988, 5.009, 7.918 ## $ Seat_Roominess &lt;dbl&gt; 7.895, 3.970, 7.908 ## $ Overhead_Storage &lt;dbl&gt; 5.967, 4.974, 7.924 ## $ Clean_Aircraft &lt;dbl&gt; 6.947, 6.050, 7.882 ## $ Courtesy &lt;dbl&gt; 5.016, 7.937, 7.942 ## $ Friendliness &lt;dbl&gt; 4.997, 7.946, 7.914 ## $ Helpfulness &lt;dbl&gt; 5.017, 7.962, 7.954 ## $ Service &lt;dbl&gt; 5.019, 7.956, 7.906 ## $ Satisfaction &lt;dbl&gt; 5.944, 3.011, 7.903 ## $ Fly_Again &lt;dbl&gt; 5.983, 3.008, 7.920 ## $ Recommend &lt;dbl&gt; 6.008, 2.997, 7.929 上面的数值结果可以看到乘客对各个航空公司的满意度情况有明显的区别。总的来说购票体验相关的项满意度偏低（购票容易度（Easy_Reservation）、座椅选择（Preferred_Seats）、航班选择（Flight_Options）和票价（Ticket_Prices）），相较而言第1个航空公司在购票体验方面优于竞争对手，但在其它方面并没有优势。第2个航空公司在空航服务方面做的比较好，在其它方面也没有优势。第3个航空公司除了购票体验较差以外，在其它方面都至少和竞争对手相当，或者优于竞争对手。这里的数据分类和之前相关矩阵图展示出的信息有一致性，但也提醒我们各个航空公司对应的问卷回复项之间的关系可能不一样。对于上面各航空公司评分均值结果使用热图进行可视化是很好的方式。我们用gplots包中的heatmap.2()函数绘制热图，用RColorBrewer包对图形着色： # gplots是可视化包 library(gplots) # RColorBrewer包用于设计图形的调色盘 # 相关信息见：http://colorbrewer2.org library(RColorBrewer) # 将航空公司设置成行名称然后将对应的字符列删除 row.names(airline.mean)&lt;-airline.mean$Airline airline.mean&lt;-select(airline.mean,-Airline) # 绘制热图 heatmap.2(as.matrix(airline.mean), col=brewer.pal(9,&quot;YlGn&quot;),trace=&quot;none&quot;,key=FALSE,dend=&quot;none&quot;,cexCol=1,cexRow = 1) title(family =&quot;Songti SC&quot;, main=&quot;航空公司问卷调查均值热图&quot;) 在上面代码中，我们将数据框airline.mean转化成矩阵传递给heatmap.2()，因为函数要求。我们通过RColorBrewer包中的YlGn调色盘，用黄色和绿色对热图着色，并且取消了一些将热图变得复杂的选项（trace、key和dend）。在结果图中，绿色表示高观测值，黄色表示低观测值，处于中间的值对应的颜色较浅。乘客对不同航空公司的满意度分值分布很显然有区别。航空公司3和2有明显让人满意和不满意的地方，而公司1总体来说比较平均，除了乘客对其位置前后空间（Seat_Roominess）特别满意。如果考虑和竞争对手的差距，1和2需要改进的地方显然比3要多。 通过观测目前得到的一些探索性结果，我们可以猜测各个问题可能的聚类情况，以及它们之间的关系。但我们最好使用更严格正规的统计模型验证这些猜测。接下来我们就开始介绍模型。 9.2.2 主成分分析 主成分分析（PCA）是寻找变量的线性组合，得到新的组合变量叫做“成分”，其指导思想是尽量捕捉原数据中方差。在统计学中，方差通常也被认为是信息。第一个成分捕捉的方差最大。第二个成分尽可能多的捕捉前一个成分没有解释的方差。依次类推，直到成分的数目和原变量数目一样多。我们通过使用前几个成分取代原变量来达到降维的目的，同时要保证这些成分能够解释大部分原始数据集中的方差。这里要提醒一点，如果数据观测不在一个标度上时需要对数据进行标准化，因为PCA是基于变量方差矩阵。这里因为各个变量的观测分布没有很大差异，是不是标准化没有太大的影响。 airline.pc&lt;-select(airline,Easy_Reservation:Recommend)%&gt;% prcomp() summary(airline.pc) ## Importance of components: ## PC1 PC2 PC3 PC4 PC5 PC6 ## Standard deviation 4.693 4.2836 1.68335 1.03625 0.88896 0.82333 ## Proportion of Variance 0.435 0.3624 0.05596 0.02121 0.01561 0.01339 ## Cumulative Proportion 0.435 0.7974 0.85338 0.87458 0.89019 0.90358 ## PC7 PC8 PC9 PC10 PC11 PC12 ## Standard deviation 0.80349 0.78694 0.77536 0.77020 0.74612 0.71831 ## Proportion of Variance 0.01275 0.01223 0.01187 0.01172 0.01099 0.01019 ## Cumulative Proportion 0.91633 0.92856 0.94043 0.95215 0.96314 0.97333 ## PC13 PC14 PC15 ## Standard deviation 0.69417 0.66650 0.65131 ## Proportion of Variance 0.00952 0.00877 0.00838 ## Cumulative Proportion 0.98285 0.99162 1.00000 由上面输出结果的第一行是主成分的标准差，第二行对应成分单独解释方差的比例，第三行是累计解释方差。可见，前两个主成分解释的大部分原变量方差（80%）。plot()函数作用在PCA结果上默认绘制陡坡图，该图展示每个成分额外解释的方差。我们可用累计解释方差比例或者陡坡图来判断需要的主成分。我们通过下面代码绘制图形： plot(airline.pc,type=&quot;l&quot;,family =&quot;Songti SC&quot;,main=&quot;主成分分析陡坡图&quot;) 图中由陡到缓的转折点能告诉我们从哪个主成分开始，继续添加更多的成分对解释更多方差没有太大帮助，反而增加模型复杂度。关于拐点的判断是主观的。图中显示拐点大约在2或者3。这表明前2到3个主成分解释了大部分方差。将各个原始变量映射到前两个主成分张成的平面上能够揭示这些变量之间的关系。这样的图称为“双标图”，可以用biplot()函数绘制： biplot(airline.pc,family =&quot;Songti SC&quot;,main=&quot;PCA结果双标图&quot;,cex=c(0.5,1),xlim=c(-0.06,0.04)) 满意度评分PCA结果的双标图中红色的箭头中看到不同调查问题的聚类情况，并且可以大致感觉样本的聚类情况，大致成为3类，一个合理的猜测是针对3个不同的航空公司。从之前的热图可以看到，3个公司有各自的优势和劣势，评分分布显然是不同的。但这样的图有个问题：基于所有评分样本导致图形非常稠密，难以识别。如果是基于各个公司聚合后的数据，得到的图或许会更清晰。 airline.mean.pc&lt;-select(airline.mean,Easy_Reservation:Recommend)%&gt;% prcomp() biplot(airline.mean.pc,family =&quot;Songti SC&quot;,main=&quot;聚合后PCA结果双标图&quot;, cex=0.7, expand=2,xlim=c(-0.8, 1),ylim=c(-0.7,0.8)) 按航空公司聚合后的结果双标图提供了可解释的乘客感知图，该图展示了各个航空公司在前两个主成分上的定位。我们先和聚合前后的感知图进行比较。注意，感知图的空间旋转是任意的，重要的是箭头的相对位置。比如，在两个感知图中，Courtesy、Friendliness、Service和Helpfulness都几乎重叠。 Seat_Comfort、Seat_Roominess、Overhead_Storage、Clean_Aircraft、Satisfaction、Fly_Again和Recommend大致指向相同的方向。剩下的Easy_Reservation、Preferred_Seats、Flight_Options和Ticket_Prices紧密相连。因此聚合后变量在主成分纬度上的分布位置和用原始观测得到的一致，但基于公司平均分值的结果更清晰的展示了公司相对定位情况。航空公司3在机舱设施、总体满意度和空航服务上得分都较高。航空公司2在空航服务方面得分较高。航空公司1在购票体验上表现较好。且在购票体验上满意度高的乘客更不满空航服务。如果你是航空公司3的商业数据分析师，看到这样的结果你可以得到什么结果？ 公司在很多方面具有竞争优势，客户满意度总体高于竞争对手 公司在购票体验上有明显劣势，这是需要努力改进的地方 我们什么在购票体验上满意度高的乘客更不满空航服务？是因为乘客本身的特质，或是由于某种原因重视空航服务的公司容易忽视购票体验？ 需要进一步研究购票体验差的原因，以及评估其可能带来的影响：如果购票体验差并不会影响当前总体满意度以及票的销售情况，那我们需要投入多少改进该问题？ 如果航空公司1只是一个很小的公司，并不是主要竞争对手。你的主要竞争对手是航空公司3，那你可以进一步检查你们公司和航空公司2的得分差别： airline.mean[3,]-airline.mean[1,] ## Easy_Reservation Preferred_Seats Flight_Options Ticket_Prices ## 1 -2.993 -4.006 -2.929 -3.939 ## Seat_Comfort Seat_Roominess Overhead_Storage Clean_Aircraft Courtesy ## 1 0.93 0.013 1.957 0.935 2.926 ## Friendliness Helpfulness Service Satisfaction Fly_Again Recommend ## 1 2.917 2.937 2.887 1.959 1.937 1.921 从上面结果可以看出，和主要竞争对手相比，我们主要的劣势在于购票容易度（Easy_Reservation）、座椅选择（Preferred_Seats）、航班选择（Flight_Options）和票价（Ticket_Prices）上。在座椅舒适度（Seat_Comfort）、座椅空间（Seat_Roominess）、机舱清洁（Clean_Aircraft）和随机行李存放（Overhead_Storage）上两者相当。在剩余方面我们有优势。 通常情况下，对这样的问卷调查数据，你需要在不同维度上比较各个公司。可以通过陡坡图或者直接观察累计方差来决定该用多少的主成分。两个主成分张成的平面上绘制感知图能够解释观测在主成分维度上的分布。对这样多维度的市场调查数据，用PCA进行可视化是理解各个公司或者品牌在消费者认知中的分布的有效手段。关于该方法有几个需要注意的地方： 这里我们选择用均值对各个公司的评分进行聚合。但这并不是唯一的方式，取决于你的数据和问题，也可以使用中位数。在解释聚合后结果双标图之前，应该先确保聚合前后双标图上主成分相对位置分布一致。 这里所说的位置分布都是只相对位置，而不是具体的位置。主成分是基于所有变量的线性组合，因此从图上无法看出某个公司在特定问卷调查项上的具体强度。比如由图可以看出，公司3和公司2在机舱设施和总体满意度这个大方向上分布不同。检查具体的平均评分你会发现，总体上3确实在这两个方面好于2，但这并不代表3在其中每一个问卷调查项上都由明显优势，事实上，这三个公司在Clean_Aircraft这个选项上差别都不是很大。这里我们研究的是在一个更高层面上的消费者满意度分布。 这里得到的各项相对分布位置和你考虑的公司，还有问卷调查项有关。如果对另外3个不同的公司进行同样的问卷调查，可能结果会不同，或者添加新的调查项也可能改变原来调查项的相对位置。一个评估模型敏感性的方法是抽取一个样本子集进行类似分析，或者删除一些问卷调查项，看看结果是不是有很大变化。如果这些随机干扰下得到的双标图中各项的相对位置相似的话，你对在该项目中使用这个模型就更加自信。 最后一点和建模没有直接关系，但是在应用中很重要。问题出现在问卷调查上不代表问题就重要。取决于不同公司对定义“重要”的定义。通常情况下，在有限的市场研究经费下我们希望问和用户购买行为最相关的问题。比如在购票体验方面，对购买决定影响最大的可能是票价，而了解乘客是否对座椅选择满意本身当然没有坏处，但是考虑到进行调查研究的成本，我们不得不问：是不是需要问这个问题？有没有更好的问题取代当前问题？这需要你首先定义一个衡量“重要性”的标准（如，总体满意度，购买行为），然后据此尽量寻找对该标准影响最大的问题。 从上面的例子中我们可以看到调查项大致分成几个类，每类问题对应一个可能的潜在变量。比如购票容易度（Easy_Reservation）、座椅选择（Preferred_Seats）、航班选择（Flight_Options）和票价（Ticket_Prices）就和购票体验有关。接下来我们可能会想知道如何用更科学的方式分析出调查项背后的潜变量，以及衡量受访者对某公司或品牌针对某个潜变量的总体得分（比如对总体购票体验），如果需要提高消费者对某方面的认知，商家需要关注哪些具体问题（比如，提高价格竞争力可能对购票体验的提高帮助最大）。下面我们要讲的探索性因子分析就可以帮助我们实现这一点。 9.2.3 探索性因子分析 探索性因子分析（EFA）可以用来获取抽样调查中问题之间的构造。这里的因子就是无法观测到的潜变量，或者隐变量。关于因子分析的经典案例是心理学和教育学中的测试。例如“智力”，人格依恋类型（安全型，焦虑－矛盾型和回避型），人格特点（外倾性，宜人性，尽责性，神经质和开放性），这些都是抽象的概念或者说构造，它们都是无法直接观测到的。取而代之的，我们可以用不同的行为反映这些变量。这些观测到的行为变量称为显变量，比如测试分数，问卷调查回复以及其它观测到的行为。EFA的目标是找到能最大限度解释显变量方差的隐因子（即潜变量）。 比如在此例中，我们不能直接观测到客户总体满意度的构成，但我们可以通过问卷获知客户对各项具体活动的满意度，然后通过数据分析，尽可能的揭示导致客户总体印象背后的原因，这样可以将资金投入到能最有效改善客户满意度的项目。在本小节中，我们通过EFA进一步探索评分数据下的潜在机制，然后根据得到的隐因子估计比较不同的公司。 EFA的结果是一个因子矩阵，其目标是使一小部分变量对应较高的因子载荷，其余的因子载荷都很低。这样的因子能由少数几个变量解释。其通过旋转正交矩阵改变变量对应的因子载荷，在旋转过程中不改变解释方差。好比小时候切生日蛋糕，总想尽量保持蛋糕上图样的完整，坚决不要把奶油塑成的花对半切开，当然还有生肖图案，每次不得不切开的时候都无比纠结。这个纠结权衡的过程就喝EFA类似，这样旋转切分后得到的结果比随机切分要让强迫症患者舒服的多。其中一些每块蛋糕上面一朵完整的花，有一大块表面是完整的的生肖图案，有的上面是水果等等。这一块块蛋糕就好比隐因子，这些因子的定义不是唯一的，没有一种切分方式严格优于另外一种，但是总有一些比另外一些让你更舒服。对于EFA的结果，总有一些因子结果比另外一些更有用，能更好的解释实际现状。 和PCA相比，能产生可解释和实践的结果是EFA的一大优势。如果我们只对EFA得出的某因子感兴趣，可以保留该因子对应载荷高的问题，舍去其它问题，优化问卷调查。之后在例子中我们会展示EFA还能用来探索调查项相互联系的方式是不是符合我们的期待。可能真实的维度比我们想的少，也可能因子分析结果反映了一些我们不知道的维度。由于EFA是探索性的，没有非黑即白的标准答案，这里一定要特别关注结果的可解释性。如果结果难以解释，得到的因子对实际工作也没有作用。 下面我们还是以航空公司满意度调查的数据为例展示如何应用EFA。其中第一步就是决定要估计的因子数目。通常的方法是通过之前提到的陡坡图，或者根据对应特征值来决定因子数目（大于1的特征值数目）。R包nFactors中的函数nScree()能够应用几种方法通过陡坡检验估计因子数目。 library(nFactors) subset(airline,select=Easy_Reservation:Recommend)%&gt;% # 转化成数据框格式传递给nScree()函数 data.frame()%&gt;% nScree() ## noc naf nparallel nkaiser ## 1 2 2 2 2 结果显示，当前的4个方法建议的因子数目都是2。我们也可以通过特征值来决定因子数目。eigen()函数可以用来得到特征值： # 得到变量相关矩阵的特征值 eigenvalue&lt;-subset(airline,select=Easy_Reservation:Recommend)%&gt;% cor()%&gt;% eigen() eigenvalue$values ## [1] 6.13519681 5.47501966 0.94251965 0.46623604 0.26506447 0.22711198 ## [7] 0.21221230 0.21044797 0.19462556 0.17989084 0.16075997 0.15678382 ## [13] 0.15513097 0.13273209 0.08626788 可以看到开始2个特征值都大于1，因此，这里给出的因子数目建议还是2。再次强调，最后模型是不是有效取决于其可解释性。在实际应用中，最好检测多个因子数目。比如，2个或者3个。这里我们使用2个因子。接下来通过factanal()函数拟合EFA模型： airline%&gt;% subset(select=Easy_Reservation:Recommend)%&gt;% factanal(factors=2) ## ## Call: ## factanal(x = ., factors = 2) ## ## Uniquenesses: ## Easy_Reservation Preferred_Seats Flight_Options Ticket_Prices ## 0.428 0.307 0.335 0.327 ## Seat_Comfort Seat_Roominess Overhead_Storage Clean_Aircraft ## 0.251 0.164 0.252 0.495 ## Courtesy Friendliness Helpfulness Service ## 0.253 0.244 0.248 0.240 ## Satisfaction Fly_Again Recommend ## 0.152 0.112 0.113 ## ## Loadings: ## Factor1 Factor2 ## Easy_Reservation -0.754 ## Preferred_Seats -0.831 ## Flight_Options -0.801 0.155 ## Ticket_Prices -0.819 ## Seat_Comfort 0.863 ## Seat_Roominess -0.346 0.846 ## Overhead_Storage 0.233 0.833 ## Clean_Aircraft 0.707 ## Courtesy 0.864 ## Friendliness 0.869 ## Helpfulness 0.867 ## Service 0.872 ## Satisfaction 0.921 ## Fly_Again 0.942 ## Recommend 0.941 ## ## Factor1 Factor2 ## SS loadings 5.769 5.309 ## Proportion Var 0.385 0.354 ## Cumulative Var 0.385 0.739 ## ## Test of the hypothesis that 2 factors are sufficient. ## The chi square statistic is 5416.28 on 76 degrees of freedom. ## The p-value is 0 其中factors=2设置因子数目为2. 我们看看结果输出的载荷Loadings，这是最需要解释的部分。结果中没有显示接近0的因子载荷。在此方案中，因子1对应载荷高的变量为： 购票体验相关变量：Easy_Reservation，Preferred_Seats，Flight_Options，Ticket_Prices 机舱服务相关变量：Courtesy，Friendliness，Helpfulness，Service 因子2对应载荷高的变量为： 机舱设施相关变量：Seat_Comfort，Seat_Roominess，Overhead_Storage，Clean_Aircraft 总体满意度相关变量：Satisfaction，Fly_Again，Recommend 也就是说这些结果大致反映出两类因子，其中一类和购票体验和机舱服务相关，另外一类和机舱设施和总体满意度相关。回忆之前得到的相关矩阵图，购票体验和机舱服务在图像强烈负相关，这里因子分析将它们看作受同一个因子影响，但载荷符号相反。这里很自然的提出一个实际问题，为什么对购票体验满意度高时对机舱服务满意度就低？这是需要和营销人员沟通的地方。同时，从统计的角度，我们可能会想，如果我们设置3个因子，是否会将购票体验和机舱服务分开呢？下面我们试着将因子个数设置成3个： airline%&gt;% subset(select=Easy_Reservation:Recommend)%&gt;% factanal(factors=3) ## ## Call: ## factanal(x = ., factors = 3) ## ## Uniquenesses: ## Easy_Reservation Preferred_Seats Flight_Options Ticket_Prices ## 0.233 0.157 0.222 0.173 ## Seat_Comfort Seat_Roominess Overhead_Storage Clean_Aircraft ## 0.251 0.165 0.253 0.495 ## Courtesy Friendliness Helpfulness Service ## 0.219 0.191 0.153 0.161 ## Satisfaction Fly_Again Recommend ## 0.151 0.111 0.113 ## ## Loadings: ## Factor1 Factor2 Factor3 ## Easy_Reservation -0.318 0.814 ## Preferred_Seats -0.422 0.815 ## Flight_Options 0.163 -0.419 0.759 ## Ticket_Prices -0.406 0.813 ## Seat_Comfort 0.864 ## Seat_Roominess 0.847 -0.286 0.189 ## Overhead_Storage 0.832 0.160 -0.172 ## Clean_Aircraft 0.706 ## Courtesy 0.786 -0.403 ## Friendliness 0.813 -0.385 ## Helpfulness 0.854 -0.342 ## Service 0.842 -0.362 ## Satisfaction 0.921 ## Fly_Again 0.942 ## Recommend 0.941 ## ## Factor1 Factor2 Factor3 ## SS loadings 5.309 3.452 3.192 ## Proportion Var 0.354 0.230 0.213 ## Cumulative Var 0.354 0.584 0.797 ## ## Test of the hypothesis that 3 factors are sufficient. ## The chi square statistic is 769.65 on 63 degrees of freedom. ## The p-value is 3.9e-122 结果如我们所料，将之前机舱服务和购票体验对应的因子进一步分开成为两个因子。这个结果和之前基于主成分分析得到的双标图中问题对应箭头方向的聚类情况一致。 了解一些关于EFA背后理论的读者应该知道，因子载荷估计并不是唯一的，我们可以通过旋转产生新的载荷，所谓旋转，就是只用一个矩阵右乘因子载荷矩阵。当你使用的是正交矩阵时，就是正交旋转，否者是斜交旋转。比如有一种常用的正交旋转叫做方差最大正交旋转（varimax rotation），其目的是使得旋转后的各因子载荷的平方按列向0和1两级分化，以便每个因子具有实际的解释，该方法寻找的是不相关的因子。斜交旋转的方法允许因子之间相关。基于之前的讨论可知，购票体验和机舱服务是负相关的，更合理的方法是允许各因子间相关。下面我们用一种常见的斜交变换（oblimin）重复3因子模型： library(GPArotation) airline%&gt;% subset(select=Easy_Reservation:Recommend)%&gt;% factanal(factors=3,rotation=&quot;oblimin&quot;) ## ## Call: ## factanal(x = ., factors = 3, rotation = &quot;oblimin&quot;) ## ## Uniquenesses: ## Easy_Reservation Preferred_Seats Flight_Options Ticket_Prices ## 0.233 0.157 0.222 0.173 ## Seat_Comfort Seat_Roominess Overhead_Storage Clean_Aircraft ## 0.251 0.165 0.253 0.495 ## Courtesy Friendliness Helpfulness Service ## 0.219 0.191 0.153 0.161 ## Satisfaction Fly_Again Recommend ## 0.151 0.111 0.113 ## ## Loadings: ## Factor1 Factor2 Factor3 ## Easy_Reservation 0.941 ## Preferred_Seats 0.880 ## Flight_Options 0.167 0.803 ## Ticket_Prices 0.887 ## Seat_Comfort 0.865 ## Seat_Roominess 0.844 -0.242 ## Overhead_Storage 0.833 0.137 -0.142 ## Clean_Aircraft 0.708 ## Courtesy 0.818 ## Friendliness 0.868 ## Helpfulness 0.953 ## Service 0.922 ## Satisfaction 0.921 ## Fly_Again 0.943 ## Recommend 0.942 ## ## Factor1 Factor2 Factor3 ## SS loadings 5.316 3.285 3.135 ## Proportion Var 0.354 0.219 0.209 ## Cumulative Var 0.354 0.573 0.782 ## ## Factor Correlations: ## Factor1 Factor2 Factor3 ## Factor1 1.0000 -0.0494 0.0188 ## Factor2 -0.0494 1.0000 -0.7535 ## Factor3 0.0188 -0.7535 1.0000 ## ## Test of the hypothesis that 3 factors are sufficient. ## The chi square statistic is 769.65 on 63 degrees of freedom. ## The p-value is 3.9e-122 从载荷中可以看到，允许因子之间相关更好的区分了购票体验和机舱服务，这样能够更加清晰的解释这两个感知项。这里就牵扯到一个问题，我们如何决定因子是该独立还是相关呢？你可能觉得这是数据决定的。但其实各因子间的相关性不是一个数据问题，而是你对潜因子的构想，或者假设。从认知概念的角度，你觉得在当前语境下因子独立有意义还是相关有意义？ 因子分析在特征提取方面的优势体现在下面3个方面： 我们可以用维度更低的因子分值取代大量原始调查问题 将反映相同认知维度的问卷分值联合起来比使用其中任何一项含有更多的信息。也在某种程度上缓解单个变量评分中的不确定性。从人脑认知的角度看，我们的购买决策常取决于一个总体的印象，而我们自己也无法解释其中具体的组成成分。所以在比较不同的公司或品牌时，应该关注总体感知。 通常人们会有一些感兴趣的因子，通过因子载荷，我们可以删除贡献小的变量从而简化收集过程。 9.2.4 理论背景 之前只介绍如何应用主成分分析和因子分析。虽然我尽量用非技术的语言解释模型背后的思想，但要很好的解释模型结果，还是需要对理论背景有所了解。本小节主要介绍这两种方法背后的数学理论，建议有一定数学基础的读者能够花些时间理解这些知识。 这里沿用之前在基础建模技术那个章节开头定义的数学公式表达。观测矩阵\\(\\mathbf{X}\\)： \\[ \\mathbf{X}=\\left[\\begin{array}{cccc} x_{11} &amp; x_{12} &amp; \\cdots &amp; x_{1p}\\\\ x_{21} &amp; x_{22} &amp; \\cdots &amp; x_{2p}\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ x_{n1} &amp; x_{n2} &amp; \\cdots &amp; x_{np} \\end{array}\\right]=\\left[\\begin{array}{c} \\mathbf{x_{1.}^{T}}\\\\ \\mathbf{x_{2.}^{T}}\\\\ \\vdots\\\\ \\mathbf{x_{n.}^{T}} \\end{array}\\right]=\\left[\\begin{array}{cccc} \\mathbf{x_{.1}} &amp; \\mathbf{x_{.2}} &amp; \\ldots &amp; \\mathbf{x_{.p}}\\end{array}\\right] \\] 其中，\\(\\mathbf{x_{.j}}\\)代表第j个变量的所有样本观测组成的向量： \\[ \\mathbf{x_{.j}}=\\left[\\begin{array}{c} x_{1j}\\\\ x_{2j}\\\\ \\vdots\\\\ x_{nj} \\end{array}\\right] \\] 主成分分析的目的就是通过对原变量进行线性组合找到彼此不相关的新变量 \\((\\mathbf{z_{1}} , \\mathbf{z_{2}} , \\ldots , \\mathbf{z_{p}})\\)，依次最大化每个变量的方差: \\[ \\begin{array}{ccccc} \\mathbf{z_{1}} &amp; = &amp; \\mathbf{a_{1}^{T}} &amp; = &amp; a_{11}\\mathbf{x_{.1}}+a_{12}\\mathbf{x_{.2}}+\\ldots+a_{1p}\\mathbf{x_{.p}}\\\\ \\mathbf{z_{2}} &amp; = &amp; \\mathbf{a_{2}^{T}} &amp; = &amp; a_{21}\\mathbf{x_{.1}}+a_{22}\\mathbf{x_{.2}}+\\ldots+a_{2p}\\mathbf{x_{.p}}\\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots\\\\ \\mathbf{z_{p}} &amp; = &amp; \\mathbf{a_{p}^{T}} &amp; = &amp; a_{n1}\\mathbf{x_{.1}}+a_{n2}\\mathbf{x_{.2}}+\\ldots+a_{np}\\mathbf{x_{.p}} \\end{array} \\] 其中： \\[Var(\\mathbf{z_{i}})=\\mathbf{a_{i}^{T}\\Sigma a_{i}},\\ \\ \\ \\ \\ \\ \\ \\ \\ i=1...p\\] \\[Cov(\\mathbf{z_{i}},\\mathbf{z_{k}})=\\mathbf{a_{i}^{T}\\Sigma a_{k}},\\ \\ \\ \\ \\ \\ \\ \\ \\ i,k=1...p\\] 从几何学上讲，主成分分析中生成各个主成分的过程可以看成是将\\((\\mathbf{x_{.1}} , \\mathbf{x_{.2}} , \\ldots , \\mathbf{x_{.p}})\\)定义的坐标系旋转成新坐标系。新坐标的坐标轴代表了观测方差最大的方向。主成分的推导依赖于 \\((\\mathbf{x_{1}} , \\mathbf{x_{2}} , \\ldots , \\mathbf{x_{p}})\\)的相关矩阵\\(\\Sigma\\)，这是典型的无监督学习，并且不要求变量服从多元正态分布。PCA过程按如下依次寻找主成分： 在控制条件\\(\\mathbf{a_{1}^{T}a_{1}}=1\\)下，通过最大化方差\\(Var(\\mathbf{z_{1}})=\\mathbf{a_{1}^{T}\\Sigma a_{1}}\\)得到第一个主成分\\(\\mathbf{z_{1}}\\) 在控制条件\\(\\mathbf{a_{2}^{T}a_{2}}=1\\)和\\(Cov(\\mathbf{z_{1},z_{2}})=0\\)下，通过最大化\\(Var(\\mathbf{z_{1}})=\\mathbf{a_{1}^{T}\\Sigma a_{1}}\\)得到第二个主成分\\(\\mathbf{z_{2}}\\) 依次类推，对第i个主成分，在控制条件\\(\\mathbf{a_{i}^{T}a_{i}}=1\\)和\\(Cov(\\mathbf{z_{i},z_{k}})=0\\)（任意\\(k&lt;i\\)）下，通过最大化\\(Var(\\mathbf{z_{i}})=\\mathbf{a_{i}^{T}\\Sigma a_{i}}\\)得到第二个主成分\\(\\mathbf{z_{i}}\\) 9.2.5 高维标度化 9.3 变量选择 我们建立统计模型常常会对下面的几个问题感兴趣： 模型拟合情况如何？ 模型在新样本上预测的情况如何？ 所有的自变量都有助于解释应变量（\\(\\mathbf{y}\\)），还是只有其中部分重要的自变量？ 回答这三个问题的共同前提是得先有一个评判模型“好”和“坏”的标准。前两个问题在建模技术那章已经给出了一般性的解答，通常使用数据划分和再抽样的方式，根据建模目的不同选择不同的评判标准，在此基础上检验模型拟合和预测情况。这章我们主要回答第3个问题。虽然所有自变量对于解释因变量来说都是重要的这样的情况可能发生，但更常见的是因变量只和一部分自变量有关。 |t|) ## (Intercept) 67274.7949 6449.7606 10.431 "],
["section-10.html", "第10章 线性回归极其衍生 10.1 普通线性回归 10.2 收缩方法 10.3 分层线性回归 10.4 贝叶斯线性回归 10.5 贝叶斯分层线性回归", " 第10章 线性回归极其衍生 本章主要讲线性回归和它的衍生，顺序由易到难。先介绍普通线性回归（也称为最小二乘回归），这是非常简单的（可能是最简单的）有监督方法，相对于其它方法，普通线性回归可谓历史悠久，声名远扬。很多非理工科专业的小伙伴也都听过或者用过该模型。虽然和很多我们将要介绍的更新的模型比起来，普通线性回归太过低端，但它依旧是有用并且被广泛使用，此外，很多新模型其实是普通线性模型的衍生。因此理解普通线性模型对理解后面更加复杂的模型非常重要。之后我们会讲到两个收缩方法：岭回归和Lasso回归。和普通最小二乘估计相比，收缩方法可以将参数估计向0“收缩”，当观测量少时（相对于变量个数而言），这种方法有助于减少估计方差，稳定参数估计。接下来我们会介绍分层线性回归，和贝叶斯框架下的分层线性回归。R有强大的拟合线性模型的功能。我们先回顾一些基本知识，展示如何用R展示拟合相应模型，但是本章不会介绍所有实践中需要知道的知识。我们鼓励想进一步了解模型的读者参考我们在介绍该部分时列出的参考资料。本章中我们还是使用服装消费者数据解释线性模型。我们需要回答类似这样的问题：“那些变量是总消费量（线上和实体店消费额之和）的驱动因子？”这个问题的答案可以帮助公司知道需要将钱投到产品的哪个方面（如服装的设计，服装质量等）。 这里特别要注意的一点是，驱动因子不意味着原因。线性模型只假设变量之间存在关联性。如果某汽车客户问卷调查结果显示满意度和价格之间正相关，难道商家为了提高消费者满意度而刻意提高汽车价格？貌似不符合常识。更可能的情况是因为价格更高的汽车质量也更好，客户真正满意的是汽车的质量。因果关系在分析实践中是个很大的坑，在解释结果的时候一定要小心再小心，一定要将问题放在相应的语境中。 10.1 普通线性回归 虽然最小二乘线性回归看起来太过简单粗暴，但现在很多更复杂的模型其基本形式也是线性的。比如逻辑回归，就是对因变量的均值进行逻辑变换后再拟合线性模型。通常我们都将神经网络模型归于非线性模型，但神经网络中的每个潜变量都是某些预测变量的线性组合。日光之下，并无新事。在大量新技术不断涌入更新换代的今天，人的思维更加容易见树不见林。很多事物本质上是有相似性的，找到光怪陆离的表象下的实质是一种重要的能力。在学习了很多不同的方法之后应该退后一步，看看这些方法的演变联系，对背后的知识进行提取抽象，触及本质，时不时停下来问自己：这些模型背后的根本思想是什么？ 在R实现这些模型也类似，不同的模型在R中的表达方法都是模仿线性模型的拟合语句。因此，只要理解了如何使用R拟合，解释和诊断线性模型，你能够举一反三的应用其它更加复杂的模型。本节主要介绍用R中lm()拟合最小二乘线性模型，以及该函数中的不同选项。然后我们将会讲到线性模型的诊断方法，它们用于检测模型的假设是否成立，或者我们拟合的结果是否充分。 10.1.1 最小二乘线性模型 在线性模型中， \\[f(\\mathbf{X})=\\mathbf{X}\\mathbf{\\beta}=\\beta_{0}+\\sum_{j=1}^{p}\\mathbf{x_{.j}}\\beta_{j}\\] 其中\\(\\mathbf{\\beta}\\)是长度为\\(p+1\\)的参数向量。这里的数学公式表达和之前6.1中介绍的一致。最小二乘估计就是选择\\(\\mathbf{\\beta^{T}}=(\\beta_{0},\\beta_{1},...,\\beta_{p})\\)最小化下面残差平方和： \\[RSS(\\beta)=\\sum_{i=1}^{N}(y_{i}-f(\\mathbf{x_{i.}}))^{2}=\\sum_{i=1}^{N}(y_{i}-\\beta_{0}-\\sum_{j=1}^{p}x_{ij}\\beta_{j})^{2}\\] 我们还是从载入数据开始。 dat&lt;-read.csv(&quot;/Users/happyrabbit/Documents/GitHub/DataScientistR/Data/SegData.csv&quot;) 在我们开始之前，还需要对数据进行一些清理，删除错误的样本观测，消费金额不能为负数。 dat&lt;-subset(dat,store_exp&gt;0 &amp; online_exp&gt;0) 我们将10个问卷调查变量当作自变量。 modeldat&lt;-dat[,grep(&quot;Q&quot;,names(dat))] 将实体店消费量和在线消费之和当作应变量。 # 得到总消费量=实体店消费+在线消费 modeldat$total_exp&lt;-dat$store_exp+dat$online_exp 我们先检查一下数据，看是不是有缺失值或者离群点： # 这里没有展示输出结果 summary(modeldat) par(mfrow=c(1,2)) hist(modeldat$total_exp,main=&quot;&quot;,xlab=&quot;total_exp&quot;) boxplot(modeldat$total_exp) 数据集modeldat中没有缺失值，但是明显有离群点，而且应变量total_exp分布明显偏离正态。我们删除离群点，然后对应变量进行对数变换。 我们用之前数据预处理章节介绍的Z分值的方法查找并删除离群点。这里不重复解释，不明白的读者可以返回复习相应的章节。 y&lt;-modeldat$total_exp # 求Z分值 zs&lt;-(y-mean(y))/mad(y) # 找到Z分值大于3.5的离群点，删除这些观测 modeldat&lt;-modeldat[-which(zs&gt;3.5),] 这里我们先不对应变量进行对数变换，之后在回归函数的公式里对应变量进行变换。接下来检查变量的共线性： library(corrplot) correlation&lt;-cor(modeldat[,grep(&quot;Q&quot;,names(modeldat))]) corrplot.mixed(correlation,order=&quot;hclust&quot;,tl.pos=&quot;lt&quot;,upper=&quot;ellipse&quot;) Figure 10.1: 自变量相关矩阵图 由图10.1 可以看到，变量之间有很强的相关性。我们用之前在预处理章节中提到的删除高度相关变量的算法，设置阈值为0.75： library(caret) highcor&lt;-findCorrelation(correlation,cutoff=.75) modeldat&lt;-modeldat[,-highcor] 现在我们可以拟合线性模型。标准的模型公式表达是在“~”号的左边指定因变量，右边指定自变量。“.”表示数据集modeldat中除了因变量之外的所有变量都被当作自变量。这里我们没有考虑交互效应，如果要添加Q1和Q2的交互效应，只要在“~”右边加上“Q1*Q2”即可。注意下面的代码中我们对原始变量进行了对数变换（log(total_exp)）。 lmfit&lt;-lm(log(total_exp)~.,data=modeldat) summary(lmfit) ## ## Call: ## lm(formula = log(total_exp) ~ ., data = modeldat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.17494 -0.13719 0.01284 0.14163 0.56227 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 8.098314 0.054286 149.177 &lt; 2e-16 *** ## Q1 -0.145340 0.008823 -16.474 &lt; 2e-16 *** ## Q2 0.102275 0.019492 5.247 1.98e-07 *** ## Q3 0.254450 0.018348 13.868 &lt; 2e-16 *** ## Q6 -0.227684 0.011520 -19.764 &lt; 2e-16 *** ## Q8 -0.090706 0.016497 -5.498 5.15e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2262 on 805 degrees of freedom ## Multiple R-squared: 0.8542, Adjusted R-squared: 0.8533 ## F-statistic: 943.4 on 5 and 805 DF, p-value: &lt; 2.2e-16 从模型结果总结中我们可以看到各个自变量的参数估计（Estimate列）、标准差（Std. Error），t统计量（t value）和p值（Pr(&gt;|t|)）。在输出的底部包含了残差标准误，即RMSE（Residual standard error），\\(R^2\\)（Multiple R-squared）和调整后的\\(R^2\\)（Adjusted R-squared），模型的F统计量（F-statistic）以及相应F检验的显著性p值（p-value）。 关于p值的讨论 谈到p值，不能不提美国统计协会在2016年2月发表的关于P值的声明 “Position on p-values: context, process, and purpose” (Ronald L. Wassersteina 2016) ，统计之都有一篇对该声明的中文总结1。关于p值弊端的讨论在统计学领域已经不是新鲜事。其中一些抨击言辞比较激烈的是 Siegfried： 这是科学中最肮脏的秘密：使用统计假设检验的“科学方法”建立在一个脆弱的基础之上。——ScienceNews（Siegfried, 2010） 假设检验中用到的统计方法……比Facebook隐私条款的缺陷还多。——ScienceNews（Siegfried, 2014） 尽管争议已经持续了很久，但这是第一次统计协会对该话题给出郑重的声明，其主要目的不是解决该问题，而是对这些批评和讨论作一个回应，讨论发表一些关于p值的普遍共识，唤起大家对科学研究可重复性的重要性。声明中概括了关于p值的6个准则： P值可以表达的是数据与一个给定模型不匹配的程度。 P值并不能衡量某条假设为真的概率，或是数据仅由随机因素产生的概率。 科学结论、商业决策或政策制定不应该仅依赖于P值是否超过一个给定的阈值。 合理的推断过程需要完整的报告和透明度。 P值或统计显著性并不衡量影响的大小或结果的重要性。 P值就其本身而言，并不是一个非常好的对模型或假设所含证据大小的衡量。 在文章末尾列举了一些其他替代手段，其中之一就是报告置信区间而非p值。 回到当前的例子，这里我们不去讨论参数估计的对应p值，而是使用各个参数估计的置信区间。在R中可以用下面代码得到置信区间： confint(lmfit,level=0.9) ## 5 % 95 % ## (Intercept) 8.00891811 8.18771037 ## Q1 -0.15986889 -0.13081186 ## Q2 0.07017625 0.13437445 ## Q3 0.22423576 0.28466333 ## Q6 -0.24665434 -0.20871330 ## Q8 -0.11787330 -0.06353905 上面的输出就是参数的90%置信区间。其中level=0.9将置信度设置为0.9。 拟合线性模型是非常容易的，以致于很多分析师拟合了模型之后不考虑模型是否合理，直接撰写结果报告。其实我们可以很容易用R从不同方面检查模型的拟合情况和假设条件。下面的几个小节中我们将介绍一些常用的线性模型诊断方法。 10.1.2 回归诊断 拟合线性模型是非常容易的，以致于很多分析师拟合了模型之后不考虑模型是否合理，直接撰写结果报告。其实我们可以很容易用R从不同方面检查模型的拟合情况和假设条件。下面的几个小节中我们将介绍一些常用的线性模型诊断方法。我们希望需要最小二乘估计（OLS）同时也是最优线性无偏估计（BLUE）。换句话说，我们希望得到的估计的期望即为真实值（无偏），且最小化残差方差（最优）。根据高斯-马尔可夫定理（Gauss-Markov theorem），OLS在下面条件满足时是BLUE: 自变量（\\(\\mathbf{x_{.j}}\\)）和随机误差（\\(\\mathbf{\\epsilon}\\)）不相关，即：\\(cov(\\mathbf{x_{.j},\\epsilon})=0\\) 对 \\(\\forall j=j\\in1...p\\) 随机误差均值为0：\\(E(\\mathbf{\\epsilon|X})=0\\) 随机误差方差一致且相互独立：\\(Var(\\mathbf{\\epsilon})=\\sigma^{2}I\\)，其中\\(\\sigma\\)是正实数，\\(I\\)是\\(n\\times n\\)的单位矩阵 下面介绍4种图形诊断。 残差图（Residuals vs Fitted） 残差图分析法是一种直观、方便的分析方法。它以残差\\(\\epsilon_{i}\\)为纵坐标，以样本拟合值为横坐标画散点图（也可以绘制横坐标为任意自变量的残差散点图）。正常情况下残差分布应该是随机的。我们要检查残差图的如下几个方面： 残差是否在0附近分布 残差分布是否随机，如果呈现出某种特定分布模式（如：随横坐标的增大而增大或减小）的话，说明当前模型关系的假设不充分 残差是否存在异方差性，比如随着拟合值增大残差分布方差增加，这就说明残差分布有异方差性。如前所述，当存在异方差时，参数估计值虽然是无偏的，但不是最小方差线性无偏估计。由于参数的显著性检验是基于残差分布假设的，所以在该假设不成立的情况下该检验也将失效。如果你用该回归方程来预测新样本，效果很可能极不理想。 Q-Q图（Norm Q-Q） Q-Q图是一种正态分布检测。对于标准状态分布，Q-Q图上的点分布在Y=X直线上，点偏离直线越远说明样本偏离正态分布越远。 标准化残差方根散点图（Scale-Location） 和残差图类似，横坐标依旧是样本拟合值，纵坐标变为了标准化残差的绝对值开方。 Cook距离图（Cook’s distance） 该图用于判断观测值是否有异常点。一般认为 当D&lt;0.5时认为不是异常值点；当D&gt;0.5时认为是异常值点。 对回归结果应用plot()函数可以得到不同的图形诊断。 par(mfrow=c(2,2)) plot(lmfit,which=1) plot(lmfit,which=2) plot(lmfit,which=3) plot(lmfit,which=4) Figure 10.2: 一般线性回归残差图 从回归的四个图形结果（图10.2）来看： 残差图：数据点都基本均匀地分布在直线y=0的两侧, 无明显趋势，满足线性假设。 标准Q-Q图：图上的点基本都在y=x直线附件，可认为残差近似服从正态分布； 标准化残差方根散点图：若满足不变方差假设，则在该图中水平线周围的点应随机分布，最高点为残差最大值点。该图显示基本符合方差齐性的要求。 Cook距离图：最大的Cook距离为0.05左右，可以认为没有异常值点。 10.1.3 离群点，高杠杆点和强影响点 关于一般线性回归，最好检查下是否有观测会强烈影响线性模型拟合结果。如果一个或者几个观测对模型结果有决定性的影响，那么用这些观测得到的模型是具有误导性的。这里我们介绍这三类观测点的检测：离群点，高杠杆点和强影响点。 离群点 刚才介绍的Cook距离图，以及之前讲到的Z分值都可以用来检测线性模型中的离群点。注意，Z分值仅仅是针对应变量观测而言，和使用的模型无关，即其并未考虑模型的拟合情况。下面我们用car包(John Fox and Weisberg 2011)中的outlierTest()函数对拟合模型对象检测是否存在离群点，和Z分值方法鉴别的离群点不同，这里的离群点指的是那些模型预测效果不佳的观测点，通常有很大的、或正或负的残差，正残差说明模型低估了响应值，负残差说明高佑了响应值。这里使用的是Bonferroni离群点检验，该检验也可作用于广义线性模型。对于一般线性模型使用的是t检验，对于广义线性模型使用的是正态检验。关于该检验相关知识见 (Williams 1987; J. Fox 2008; Cook and Weisberg 1982; S. Weisberg 2014)。 library(car) outlierTest(lmfit) #Bonferroni离群点检验 ## rstudent unadjusted p-value Bonferonni p ## 960 -5.295504 1.533e-07 0.00012432 outlierTest()函数是根据单个最大（或正或负）残差值的显著性来判断是否有离群点，若不显著，则说明数据集中没有离群点，若显著，则建议删除该离群点，然后再检验是否还有其他离群点存在。这里我们删除第960个被认为是离群点的观测。 outlierTest(lmfit) ## rstudent unadjusted p-value Bonferonni p ## 960 -5.295504 1.533e-07 0.00012432 # 这里数据modeldat的行名是原数据集的行号，所以是字符类型 # 找到相应的观测 idex&lt;-which(row.names(modeldat)==&quot;960&quot;) # 删除离群观测 modeldat=modeldat[-idex,] 接下来我们再拟合一次模型然后检测看看是否还有离群点： lmfit&lt;-lm(log(total_exp)~.,data=modeldat) outlierTest(lmfit) ## ## No Studentized residuals with Bonferonni p &lt; 0.05 ## Largest |rstudent|: ## rstudent unadjusted p-value Bonferonni p ## 155 -3.818112 0.00014483 0.11731 可以看到现在没有检测出显著离群点。 高杠杆值点是与其他预测变量有关的离群点，即它们是由许多异常的预测变量组合起来的，与响应变量值没有关系。 高杠杆值的观测点可通过帽子矩阵的值（hat statistic）判断。对于一个给定的数据集，帽子均值为\\(p/n\\)，其中p是模型估计的参数数目（包含截距项），n是样本量。一般来说，若观测点的帽子值大于帽子均值的2或3倍，则可认定为高杠杆值点。 10.2 收缩方法 10.3 分层线性回归 我们 已经讨论过简单线性模型，下面讨论该模型的扩展分层线性模型。什么时候需要使用分层模型呢？需要考虑分层结构的常见情况有两种。（1）当你的数据有嵌套结构，这是指一些观测可能属于更高一层级单位。比如在教育学分析当中通常需要研究学生的学习情况，同一所学校或者同一班级的学生相似度更高，这里我们就需要考虑学生学习情况的个体观测可能嵌套在班级，或者学校这个更高一层级单位中。市场营销中，地理位置通常是一个更高的层极单位。北上广的消费者和一些二三线城市的消费者可能不同，东南沿海和东北地区的消费者差异可能更大。这些情况中我们都需要考虑分层结构。（2）还有一种情况是针对纵向数据。比如对一些人年收入连续10年的观测，那么研究观测就该考虑个人的随机效应。类似的还有我们的航空公司满意度调查数据，每个受访者针对每项对3个航空公司进行评分，这里也需要考虑受访者的个体随机效应，可能的情况是有的人倾向于给高分，有的人倾向于给低分，这样的倾向和问卷问题以及哪家航空公司无关。 10.4 贝叶斯线性回归 10.5 贝叶斯分层线性回归 References "],
["section-11.html", "第11章 树模型 11.1 基本树模型 11.2 装袋树 11.3 随机森林 11.4 其它树话题", " 第11章 树模型 本章讲介绍基于树的模型。树模型可以用于回归和判别。这类模型常被称为决策树（或分类回归树），是经典的机器学习算法，也是最广泛使用的工具。不太大的决策树简洁明了，容易解释。但是简单的树表现不一定好，复杂的树（比如之后会讲到的集成方法）效果大为提高，但过程如同黑箱，无法解释。所以建模者需要在解释性和精确性之间进行权衡。之所以称其为“树”当然因为结构有类似之处，只是决策树的方向和真实的树相反，根在上，叶在下。一棵决策树从单个根节点开始划分为几个不同的枝桠，从⽽产生更多的节点,在每个节点都可以决定是不是要继续划分，如果停止则该节点就是叶节点，若继续就是枝节点接着分裂产生下一层新节点。每个⾮叶节点都牵扯到决定接下来选择哪根树枝。叶节点包含 最终“决定”，讲该样本归于哪个最终类或者取值。 11.1 基本树模型 11.2 装袋树 Bootstrap 样本是对数据进行有放回随机抽样得到的样本（Efron 和 Tibshirani 1986）。这意味着，当一个样本点被选中时，它有可能会在将来的抽取中继续被选中。Bootstrap 样本和原数据的样本量一样。因此，一些样本可能被抽到过很多次，而另一些则可能没有被选到。没有被选到的样本被称为“袋外样本“（out-of-bag）。在一次 Bootstrap 重抽样迭代中，选中的样本点被用来建立模型，而袋外样本则被用于预测。 在20世纪90年代，集成方法（即将许多模型组合起来进行预测的模型）开始出现。 装袋法（Bagging，bootstrap aggregation 的缩写）最初由 Leo Breiman 提出，它是最早发展起来的集成方法之一（Breiman 1996a）。 装袋法是一种利用 bootstrap的通用方法，可用于任何回归（或分类）模型来构建集成组合。这种方法的构建非常简单，它包含算法 8.1 中所述的步骤。 集成组合中的每一个模型都对新样本进行一次预测，然后这 个预测将进行平均， 来给出装袋法模型的预测值。 算法：装袋法 对 i=1 到 m 执行 从原数据中生成bootstrap样本 在生成的bootstrap样本上建立未修剪的树 终止 装袋法模型相对于没有装袋的模型具有若干优势。首先， 装袋法通过模型的聚集过程有效地降低了预测的方差（参见之前关于偏差-方差权衡的讨论）。对于那些预测值不稳定的模型，例如回归树，将不同版本的训练集进行聚集，可以减小预测的方差，从而使得预测值更加稳定。假设我们有10个 bootstrap 样本各自生成了一棵最大深度的树。这些树在结构上有所差异，每棵树对新样本的预测都有所不同。 如果将这10棵树的预测结果进行平均作为新样本的预测， 那么这一平均值将比单棵树的预测方差更小。这意味着， 如果我们产生另一组 bootstrap 样本，在其中每一个样本上建立一个模型， 然后对所有模型的预测值进行平均，那么得到的结果将与前一个装袋模型的结果相类似。这一特性还使得装袋模型比未装袋的模型具有更好的预测效能。 如果建模的目标是得到最优的估计而不是解释树的结构，那么装袋法更有优势。 另一方面，对稳定、方差小的模型（如回归，MARS）进行装袋则只会对其预测效能带来较小的改进。 在预测结果具有内在的不稳定性的情况下，可以用装袋法进行改进。 装袋法模型的另一个优势是它可以提供内在的预测效能估计， 而且这一估计可以与交叉验证估计或测试集估计很好地对应上。原因如下： 构建集成组合中的每个 bootstrap 样本时，会有部分的观测被排除在外。 这部分样本称为袋外样本，它们扮演着测试集的角色，可以用来评估模型的预测效能， 因为这批样本并没有参与建模。因此，集成组合中的每个模型都可以通过袋外样本计算得到一个预测效能的估计， 而对所有袋外效能估计进行平均就能计算出整个集成组合的预测效能。 这一结果通常与交叉验证或测试集验证的结果非常吻合， 该误差估计称为袋外估计。 对于基本的装袋法，用户可以选择bootstrap样本的数目。 通常模型的预测效能与迭代次数之间会出现指数递减的关系； 大部分预测效能的提升是由少数几棵树实现的。 ［加一个例子，］。《应用预测模型（Applied Predictive Modeling）》(Max Kuhn 2013)的作者指出，根据他们的经验，一直到50个bootstrap样本，模型都可能还会有微小的改进， 如果迭代了50次代后模型的表现还是不令人满意，那就需要尝试使用其他更强大的集成预测方法，如随机森林和 boosting。 尽管装袋法通常都能改进不稳定模型的预测效能，但它同样有一些缺陷。 首先，是计算量。随着 bootstrap 样本数目的增多，计算成本和内存需求也会相应增加。这一劣势可以通过并行计算来得到大部分的消减，原因是装袋的过程是非常容易并行化的。 回顾之前的介绍可以发现，每一个 boostrap 样本和相应的模型都是独立于其他样本和模型的。 这意味着每个模型都可以单独进行建模， 而只需在最后将所有模型的结果组合起来以生成最终的预测。 装袋法的另一个劣势在于解释性差， 我们无法在装袋法中得到之前单棵回归树给出的简洁的规则。然而，变量重要性依旧可以通过将单个模型的重要性得分组合起来进行构建。 下一节介绍的随机森林将进一步讨论变量重要性。 11.3 随机森林 11.4 其它树话题 References "],
["section-12.html", "第12章 聚类判别分析 12.1 聚类分析 12.2 判别分析 12.3 案例：客户分组", " 第12章 聚类判别分析 12.1 聚类分析 聚类是无监督学习，主要着眼于梳理数据结构。林林总总的聚类方法的目的都是将数据划分成不同的部分（类），使得同一类的观测尽可能相似，不同类的样本尽可能相异。 What makes clustering different from supervised learning is that there is no number or name that tells you what group each point belongs to, what the groups represent, or even how many groups there should be. If supervised learning is picking out planets from among the stars in the night sky, then clustering is inventing constellations. Clustering tries to separate out data into natural “clumps,” so that a human analyst can more easily interpret it and explain it to others. Clustering always relies on a definition of closeness or similarity, called a distance metric. The distance metric can be any measurable quantity, such as difference in IQ, number of shared genetic base pairs, or miles-as-the-crow-flies. Clustering questions all try to break data into more nearly uniform groups. Which shoppers have similar tastes in produce? Which viewers like the same kind of movies? Which printer models fail the same way? During which days of the week does this electrical substation have similar electrical power demands? What is a natural way to break these documents into five topic groups? 12.2 判别分析 12.2.1 逻辑回归 12.2.1.1 普通逻辑回归 12.2.1.2 LASSO逻辑回归 12.2.1.3 分组LASSO逻辑回归 12.2.1.4 疾病预测案例 Risk scoring systems for predicting disease are widely used in medicine. Such scoring systems are usually derived from multivariate logistic regression models with disease as the response variable. Typical approaches in the literature select potential explanatory variables (risk factors) based on variable significance , with risk scores of selected variables assigned based on estimated regression coefficients . However, when the number of potential explanatory variables is large, such approaches may fail to produce a risk scoring system with the greatest power for predicting disease. This paper is motivated by the need to develop a risk scoring system for porcine reproductive and respiratory syndrome (PRRS) based on survey data. PRRS, caused by the PRRS virus, is a major disease, production and financial problem for swine producers in nearly every country. PRRS costs the United States swine industry around $560 million annually . PRRS outbreaks in China caused pork prices to increase by 85 percent in 2006 . For breeding herds, costs of clinical outbreaks of PRRS result from lost production due to abortion, mummies, stillborns, pre-wean mortality and sow deaths and increased costs for treatment and control. Performance of observational studies to better understand the relative importance of risk factors for PRRS outbreaks have been limited by the availability of good data on a large set of farms over a relatively long period of time. In human medicine, large datasets of information on risk factors, prevalence, incidence and clinical outcomes of disease are common. In veterinary medicine, until recently, there have been no parallel efforts to create epidemiological databases on a similar scale. The American Association of Swine Veterinarians (AASV) Production Animal Disease Risk Assessment Program (PADRAP) is a program through which a set of web-based risk assessment surveys are delivered(please visit: http://vdpambi.vdl.iastate.edu/padrap/default.aspx). It is used by veterinarians who are members of the AASV. Each of the surveys consists of a set of questions about potential risk factors for clinical outbreaks of PRRS in swine. Each question may have up to 6 possible responses. Members of the AASV use PADRAP to help producers systematically assess biosecurity factors that may be associated with clinical outcomes. As assessments are performed by veterinarians they are added to the database of completed assessments. Version 2 of the PRRS Risk Assessment for the Breeding Herd survey was introduced in 2005. The survey instrument was developed using expert opinion with the aid of the PRRS Risk Assessment Working Group composed of 21 veterinarians and researchers with expertise in PRRS. Initial estimates of the risk scores associated with each response were based on the consensus of expert opinion and equal weight is assigned to each question. The aim of this study is to use the survey data that has been collected to develop a risk scoring system with 127 survey questions (categorical explanatory variables) that outperforms the current risk scoring system based on expert opinion when multivariate logistic regression is used in similar studies with variables selected by significance. Quasi-complete-separation may result when there are a large number of explanatory variables which makes estimation of the coefficients unstable. To stabilize the estimation of parameter coefficients, one popular approach is the lasso algorithm with \\(l_{1}\\)-norm penalty proposed by Tibshirani . Since the lasso algorithm can estimate some variable coefficients to be 0, it can also be used as a variable selection tool. For models with categorical survey questions (explanatory variables), however, original lasso algorithm only selects individual dummy variables instead of sets of the dummy variables grouped by question in the survey. Another disadvantage of applying lasso to grouped variables is that the estimates are affected by the way dummy variables are encoded. Thus the group lasso method has been proposed to enable variable selection in linear regression models on groups of variables, instead of on single variables. For logistic regression models, the group lasso algorithm was first studied by Kim et al. . They proposed a gradient descent algorithm to solve the corresponding constrained problem, which does, however, depend on unknown constants. Meier et al. proposed a new algorithm that could work directly on the penalized problem and its convergence property does not depend on unknown constants. The algorithm is especially suitable for high-dimensional problems. It can also be applied to solve the corresponding convex optimization problem in generalized linear models. The logistic group lasso involves selection of a penalty (tuning) parameter \\(\\lambda\\) which can be determined by cross-validation. The group lasso estimator proposed by Meier et al. for logistic regression has been shown to be statistically consistent, even with large number of categorical predictors. In this paper, we propose to use the logistic group lasso algorithm to construct risk scoring systems for predicting clinical PRRS outbreaks in swine herds. The paper is organized as follows. In Section 2, we introduce the group lasso method for logistic regression to construct risk scoring system for clinical PRRS outbreaks. The penalty parameter \\(\\lambda\\) for group lasso is selected through leave-one-out cross validation, using the criterion of the area under the receiver operating characteristic curve (AUC). Section 3 presents a simulation study to evaluate the performance of each method. In Section 4, we discuss the application to the PRRS survey data from 896 swine breeding herd sites in the United States and Canada. We show our scoring system for PRRS is superior to both the current scoring system based on expert opinion and that developed by using logistic regression with model selection based on variable significance. Finally results and conclusions presented in Section 5 and 6. 12.2.1.5 Models for risk scoring systems Consider risk scoring system construction using a samples of \\(n\\) observations, with information collected for \\(G\\) categorical predictors and one binary response variable for each observation. Let \\(\\bx_{i,g}\\) be the vector of dummy variables associated with the \\(g\\)th categorical predictor for the \\(i\\)th observation, and let \\(y_i\\) (= 1, diseased; or 0, not diseased) be the binary response for the \\(i\\)th observation, \\(i=1,\\cdots,n\\), \\(g=1,\\cdots,G\\). Denote the degrees of freedom of the \\(g\\)th predictor by \\(df_g\\), which is also the length of vector \\(\\bx_{i,g}\\), \\(i=1,\\cdots,n\\). 12.2.1.6 Multivariate logistic regression model Multivariate logistic regression has been used to construct risk scoring systems for predicting disease . Denote the probability of disease for \\(i\\)th subject by \\(\\theta_i\\), the model can be formulated as \\begin{equation} y_{i}\\sim Bounoulli(\\theta_{i}), \\end{equation} with \\begin{equation} \\log\\left(\\frac{\\theta_{i}}{1-\\theta_{i}}\\right)=\\eta_{\\bbeta}(x_{i})=\\beta_{0}+\\sum_{g=1}^{G}\\bx_{i,g}^{T}\\bbeta_{g}, \\end{equation} where \\(\\beta_{0}\\) is the intercept and \\(\\bbeta_{g}\\) is the parameter vector corresponding to the \\(g\\)th predictor. Construction of risk scoring systems using logistic regression usually consists of two steps: selection among the \\(G\\) risk factors, and estimation of the selected factor parameters. For model selection, significance has been used as a criterion for inclusion and exclusion of risk factors . Some researchers use univariate logistic regression to screen factors by significance before putting them into a multivariate logistic regression model , whereas others don’t. Traditional estimation of logistic parameters \\(\\bbeta=(\\beta_{0}^{T},\\bbeta_{1}^{T},\\bbeta_{2}^{T},...,\\bbeta_{G}^{T})^{T}\\) is done through maximizing the log-likelihood \\begin{eqnarray*} l(\\bbeta)&amp;=&amp;log[\\prod_{i=1}^{n}\\theta_{i}^{y_{i}}(1-\\theta_{i})^{1-y_{i}}]\\\\ &amp;=&amp;\\sum_{i=1}^{n}\\{y_{i}log(\\theta_{i})+(1-y_{i})log(1-\\theta_{i})\\}\\\\ &amp;=&amp;\\sum_{i=1}^{n}\\{\\ y_{i}\\eta_{\\bbeta}(\\bx_{i})-log[1+exp(\\eta_{\\bbeta}(\\bx_{i}))]\\ \\}. \\end{eqnarray*} For logistic regression analysis with a large number of explanatory variables, complete- or quasi-complete-separation may result which makes the maximum likelihood estimation unstable. 12.2.1.7 Group lasso for logistic regression} In this paper, we propose to perform model selection and parameter estimation for risk scoring system construction by using the group lasso algorithm of Meier et al. . Instead of minimizing the negative log-likelihood \\(-l(\\bbeta)\\) in the maximum likelihood method, the logistic group lasso estimates are calculated by minimizing the covex function \\begin{equation} S_{\\lambda}(\\beta)=-l(\\bbeta)+\\lambda\\sum_{g=1}^{G}s(df_{g})\\parallel\\bbeta_{g}\\parallel_{2}, \\end{equation} where \\(\\lambda\\) is a tuning parameter for the penalty and \\(s(\\cdot)\\) is a function to rescale the penalty. In lasso algorithms, choice of \\(\\lambda\\) is usually determined by cross-validation using data. For \\(s(\\cdot)\\), we use the square root function \\(s(df_g)=df_g^{0.5}\\) as suggested in Meier et al. . %### Choosing \\(\\lambda\\) through leave-one-out cross validation} Here we consider selection of the tuning parameter \\(\\lambda\\) from a multiplicative grid of 148 values \\(\\{0.96\\lambda_{max},0.96^{2}\\lambda_{max},0.96^{3}\\lambda_{max},...,0.96^{148}\\lambda_{max}\\}\\), as in Meier et al. . Here \\(\\lambda_{max}\\) is defined as \\begin{equation} \\lambda_{max}=\\underset{g\\in\\{1,...,G\\}}{max}\\left\\{\\frac{1}{s(df_{g})}\\parallel \\bx_{g}^{T}(\\by-\\bar{\\by})\\parallel_{2}\\right\\}, \\end{equation} such that when \\(\\lambda=\\lambda_{max}\\), only the intercept is in the model. When \\(\\lambda\\) goes to \\(0\\), the model is equivalent to regular logistic regression. The optimal value of \\(\\lambda\\) is determined through leave-one-out cross validation, which is a special case of K-fold cross-validation with K being equal to \\(n\\), the number of observations in the sample. In each fold, leave-one-out cross validation uses a single observation from the original sample as the validation data, and the remaining observations as the training data. This step is repeated until each observation in the sample is used once as the validation data. Predicted probabilities of disease are calculated from cross-validation and are compared to true observed disease status to assess the predictive power of model. In this paper, we assess the predictive power of each model through ROC analysis. ROC curve is a graph of pairs of the true positive rate (sensitivity) and false positive rate (1-specificity) that result as the cutoff value for the predicted probability of disease is varied. %The cutoff value varied in order from minimum to maximum of the resulted score. Theoretically, cutoff values can be any values on the real line. The practical cutoff values are determined from resulting scores based on our data. The value of AUC is used as the criterion to evaluate the predictive power of the logistic model. AUC as well as the confidence interval are estimated through the approach proposed by DeLong et al. . The AUC can be interpreted as the probability that a random diseased individual has larger predicted probability of disease than a random non-diseased individual and it has been used to assess predictive power of risk scoring systems . We calculate the AUCs for cross-validations of all \\(\\lambda\\)s, and the value of \\(\\lambda\\) with the largest AUC is chosen as the \\(\\lambda\\) used in constructing the final scoring system. Besides AUC, two other goodness-of-fit criteria for: the log-likelihood score used in Meier et al. , and the maximum correlation coefficient in Yeo and Burge were also considered. The log-likelihood is taken as the average over all cross-validation sets. The maximum correlation coefficient is defined as \\begin{equation} \\rho_{max}=max\\{\\rho_{\\tau}|\\tau\\in(0,1)\\}, \\end{equation} where \\(\\tau\\in(0,1)\\) is a threshold to classify the predicted probability into a binary disease status and \\(\\rho_\\tau\\) is the Pearson correlation coefficient between the true binary disease status and the predictive disease status with threshold \\(\\tau\\). \\section{Simulation Study} A simulation study to %use simulation to demonstrate group lasso logistic regression and compare it to ordinary forward stepwise logistic regression is performed. \\section{Application to PRRS Data} In this section, we apply the proposed method to construct a scoring system for PRRS survey data of swine breeding herd sites in the United States and Canada. 12.2.1.8 Data Description Surveys in the database completed between March 2005 and March 2009 are candidates for inclusion in the analysis. To avoid multiple surveys from a single swine breeding herd site, the study dataset is limited to responses obtained from the most recently completed survey for each site. Surveys meeting these criteria are extracted from the database, and identity information is removed. Incomplete surveys are excluded. %The outcome of interest is whether a breeding herd site reported %a clinical PRRS outbreak in the 3 years prior to when the assessment %was completed. The outcome of interest is whether a site is positive or not. Positive sites are sites with clinical PRRS outbreak in the 3 years prior to when the assessment was completed, negative sites otherwise. The information to determine the outcome was obtained from the survey. A clinical PRRS outbreak is described in the survey as an increase in one or more reproductive performance measures that exceeds normal variation with diagnostic confirmation of PRRS virus involvement. Of the 896 sites in the United States and Canada included in the study, 499 (56%) became positive during the past 3 years. 127 survey questions are considered potential explanatory variables in the analysis. The survey questions are first converted to dummy indicator variables. All of the responses for each survey question are defined as a group of variables. 12.2.1.9 Application of logistic group lasso First, leave-one-out cross validation is used to choose tuning parameter \\(\\lambda\\), as described in Section 2.3. % In each fold of cross-validation, one of the 896 farms is excluded and the other 895 farms are used as a training data set on which the % group lasso logistic regression is applied. The resulting model is used to calculate a predicted probability for PRRS outbreak for the excluded farm. This procedure is repeated for all 896 farms. For each \\(\\lambda\\) in the grid \\(\\{0.96\\lambda_{max},0.96^{2}\\lambda_{max},0.96^{3}\\lambda_{max},...,0.96^{148}\\lambda_{max}\\}\\), the values of three evaluation criteria are calculated based on cross validation. The penalty parameter for final risk scoring system is selected to be the one that optimizes AUC. %### Comparison with other risk scoring systems} The logistic group lasso based scoring system is compared with two other systems: \\begin{enumerate} \\item The current risk scoring system used in versions 2 of the PRRS risk assessment for the breeding herd that is based on expert opinion, \\item A risk scoring system based on multivariate logistic regression model selected by variable significance. \\end{enumerate} We construct the significance based logistic model by following the method used by Van Zee et al. . Specifically, we use forward stepwise variable selection to construct the logistic regression model with 0.05 significant level. Leave-one-out cross validation is applied to the model construction by variable significance, in the same manner as described for logistic group lasso. ROC curves are plotted for the three risk scoring systems. A point estimate as well as the 95% confidence interval for the AUC are provided. The estimated AUCs are compared by using the nonparametric approach of DeLong et al. and p-values are calculated. R package {}``grplasso’’ is used to perform group lasso logistic regression. Significance-based logistic model selection is performed using the LOGISTIC procedure in SAS. All other algorithms and calculations are programmed in R language. 12.2.1.10 Determination of penalty parameter \\(\\lambda\\) The AUC, maximum correlation coefficient and log-likelihood are calculated based on leave-one-out cross validation and are plotted against the penalty parameter \\(\\lambda\\) in Figure . The trends for all three criteria are similar with a sharp increase for small values of \\(\\lambda\\) and gradual decrease after reaching the maximum. The optimal values of \\(\\lambda\\) selected to maximize the three criteria are 11.72, 4.22 and 11.72 for AUC, maximum correlation coefficient and log-likelihood respectively. \\begin{figure} \\resizebox*{15cm}{!}{\\includegraphics{ThreeCri}}% \\caption{Three criteria for choice of penalty parameter $\\lambda$} \\label{Flo:penalty} \\end{figure} 12.2.1.11 Logistic group lasso based PRRS risk scoring system The penalty parameter maximizing AUC (i.e. \\(\\lambda=11.72\\)) from the leave-one-out cross validation is used for the group lasso estimation of the logistic regression parameters. Figure~ show the distributions of the predicted probabilities based on cross validation for both negative and positive farms. It can be observed that the predicted probability for positive farms is larger than that of negative farms in stochastic order. The actual risk score can take the value of the predicted probability, the linear predictor in the logistic regression model, or any strictly increasing function of the predicted probability. This is because the ROC curve for a predictor is invariate to such transformation. \\begin{figure} \\resizebox*{15cm}{!}{\\includegraphics{histograms}}% \\caption{Distributions of estimated probabilities for both negative and positive groups} \\label{histogram} \\end{figure} In the resulting scoring system, 74 out of 127 survey questions are estimated with 0 coefficients and are excluded from the system. PADRAP questions target internal risks (bio-management of virus already present) and external risks (bio-exclusion of virus not present). A summary of the number of questions included in the final risk scoring system in each category of risk factors in the PRRS Risk Assessment for the Breeding Herd is shown in Table 1. \\begin{table} \\caption{Summary of number of questions in the final risk scoring system by category of risk factors } \\begin{center} \\begin{tabular}{lrrrr} \\hline Category of risk factors &amp; \\multicolumn{2}{r}{Questions} &amp; \\multicolumn{2}{r}{Dummy Variables} \\\\ \\hline INTERNAL RISKS &amp; Included &amp; Total &amp; Included &amp; Total \\\\ Circulation Risk &amp; &amp; &amp; &amp; \\\\ Characteristics of the herd &amp; 3 &amp; 4 &amp; 9 &amp; 11 \\\\ Characteristics of the site &amp; 0 &amp; 2 &amp; 0 &amp; 5 \\\\ Management practices &amp; 0 &amp; 2 &amp; 0 &amp; 9 \\\\--------- &amp;-------&amp;------&amp;------&amp;------\\\\ \\textbf{\\textit{Total}} &amp; \\textbf{\\textit{3}} &amp; \\textbf{\\textit{8}} &amp; \\textbf{\\textit{9}} &amp; \\textbf{\\textit{25}}\\\\ &amp; &amp; &amp; &amp; \\\\ EXTERNAL RISKS &amp; &amp; &amp; &amp; \\\\ Pig Related &amp; &amp; &amp; &amp; \\\\ Entry of replacement animals into the breeding herd &amp; 4 &amp; 12 &amp; 18 &amp; 40 \\\\ Entry of semen into the breeding herd &amp; 13 &amp; 31 &amp; 47 &amp; 104 \\\\ &amp; &amp; &amp; &amp; \\\\ Non-Pig Related &amp; &amp; &amp; &amp; \\\\ Transportation of live animals &amp; 13 &amp; 29 &amp; 38 &amp; 71 \\\\ Transportation of feed &amp; 1 &amp; 1 &amp; 2 &amp; 2 \\\\ Employee and service vehicles &amp; 1 &amp; 2 &amp; 3 &amp; 6 \\\\ Disposal of dead animals and waste management &amp; 2 &amp; 8 &amp; 3 &amp; 10 \\\\ Employees and visitors &amp; 5 &amp; 9 &amp; 15 &amp; 19 \\\\ Entry of supplies &amp; 1 &amp; 1 &amp; 3 &amp; 3 \\\\ Facilities &amp; 0 &amp; 4 &amp; 0 &amp; 11 \\\\ Biovectors &amp; 1 &amp; 1 &amp; 2 &amp; 1 \\\\ Density of pig farms in the area &amp; 3 &amp; 3 &amp; 10 &amp; 10 \\\\ Neighboring pig farms &amp; 3 &amp; 13 &amp; 12 &amp; 28 \\\\ Distance to pork industry infrastructure &amp; 2 &amp; 4 &amp; 5 &amp; 11 \\\\ Topography and forestation of surrounding area &amp; 1 &amp; 1 &amp; 3 &amp; 3 \\\\ --------- &amp;-------&amp;------&amp;------&amp;------\\\\ \\textbf{\\textit{Total}} &amp; \\textbf{\\textit{50}} &amp; \\textbf{\\textit{119}} &amp; \\textbf{\\textit{161}} &amp;\\textbf{\\textit{319}}\\\\\\hline \\end{tabular} \\end{center} \\end{table} Three out of eight questions regarding internal risk factors remain in the scoring system, and they are all factors concerning characteristics of the herd. Fifty questions remain in external risk factor section out of the total 119 questions. %Sixty nine questions are excluded out of the external risk factors section and fifty remain. In the external risks section, all of the 14 categories have at least one question remaining in the final scoring system, except all 4 questions concerning facilities are excluded. Several categories have a large number of questions removed. In particular, 8 of 12 (66.7%) questions concerning entry of animals into the breeding herd, 18 of 31 (58.1%) questions concerning entry of semen into the breeding herd, 16 of 29 (55.2%) questions concerning transportation of live animals, and 10 of 13 (76.9%) questions concerning neighboring pig farms are excluded. 12.2.1.12 Comparison among risk scoring systems The ROC curves for the three risk scoring systems are plotted in Figure . The ROC curves for the two scoring systems based on logistic regression analyses of the data are constructed using the results of leave-one-out cross validation. The ROC curve of logistic group lasso apparently dominates the other two scoring systems. \\begin{figure} \\includegraphics{roc_plot_diff_model} \\caption{ROC curves for three risk scoring systems} \\label{ROC} \\end{figure} Point and 95% interval estimates of AUC are reported in Table 2. The risk scoring system based on the has the largest AUC = 0.848. This AUC estimate is significantly higher than those based on either expert opinion (AUC = 0.696, p-value \\(&lt;\\) 0.001) or logistic regression model selected by variable significance (AUC = 0.807, p-value \\(&lt;\\) 0.001). \\begin{table} \\caption{AUC estimations for three risk scoring systems} \\begin{center} \\begin{tabular}{lcc} \\hline Model Names &amp; AUC &amp; 95\\% CI \\\\ \\hline Group Lasso &amp; 0.848 &amp; (0.822, 0.873) \\\\ Significance Based Method &amp; 0.807 &amp; (0.773, 0.841) \\\\ Expert Opinion &amp; 0.696 &amp; (0.661, 0.731) \\\\ \\hline \\end{tabular} \\end{center} \\end{table} Results for the simulation study are shown in Table 3. The mean AUC is increasing with the value of \\(\\gamma\\) for both methods. The Wilcoxon signed-rank test result in the last column of Table 3 shows that AUC’s from group lasso are significant larger than those from logistic regression, especially for \\(\\gamma \\geq 0.25\\). \\begin{table} \\caption{Simulation study result with various values of coefficient $\\gamma$ with mean and standard deviation for both method, mean difference and p value from Wilcox signed rank test } \\begin{center} \\begin{tabular}{|c|c|c|c|c|} \\hline Coefficient $\\gamma$ &amp; Group Lasso (mean$\\pm$sd ) &amp; Logistic Regression (mean$\\pm$sd ) &amp; Difference &amp; p value \\tabularnewline \\hline 0.1 &amp; $0.57\\pm0.03$ &amp; $0.54\\pm0.06$ &amp; $0.03$ &amp; $0.040$\\tabularnewline \\hline 0.25 &amp; $0.71\\pm0.02$ &amp; $0.64\\pm0.04$ &amp; $0.07$ &amp; $&lt;0.001$\\tabularnewline \\hline 0.5 &amp; $0.91\\pm0.03$ &amp; $0.78\\pm0.03$ &amp; $0.13$ &amp; $&lt;0.001$\\tabularnewline \\hline 1 &amp; $0.92\\pm0.01$ &amp; $0.82\\pm0.02$ &amp; $0.1$0 &amp; $&lt;0.001$\\tabularnewline \\hline 2 &amp; $0.95\\pm0.01$ &amp; $0.84\\pm0.02$ &amp; $0.11$ &amp; $&lt;0.001$\\tabularnewline \\hline \\end{tabular} \\end{center} \\end{table} 12.2.1.13 Discussion The risk scoring system for disease developed using the logistic group lasso algorithm significantly improves upon the current risk scoring system based on expert opinion for predicting whether a swine breeding site experienced a PRRS outbreak. %We introduce the logistic group lasso algorithm to develop risk scoring systems for diseases. Choice of penalty parameter \\(\\lambda\\) is determined by leave-one-out cross validation with criterion %of AUC. %We apply our method to construct a new risk scoring system for PRRS outbreak in swine farms. Our scoring system significantly improves the current risk scoring system based on expert %opinion with respect to informing us about the contribution of certain category to the probability of outbreak. The simuation study explores the performance of the scoring systems with different settings of coefficients. The logistic group lasso based scoring system is superior to the scoring system constructed through logistic regression selected by variable significance. One advantage of group lasso is that it can be used as variable selection tool by setting 0 coefficients to parameters. It not only helps to find important explanatory factors in predicting the response variables but also identifies questions that could be removed from the survey without affecting the survey’s ability for classifying herds according to whether they report clinical PRRS outbreaks in the previous 3 years. Seventy-four of the 127 questions analyzed are excluded from the final risk scoring system based on logistic group lasso. The analysis and results demonstrate how a program like PADRAP, that is supported by a professional association and used by a community of veterinarians, can generate valuable data that contributes to our understanding of the relative importance of risk factors and areas of risk factors for clinical outcomes. The results may also be used to decrease the reliance upon expert opinion to identify questions that should remain in the survey and those that may be eliminated to iteratively increase the value of the program and the data. 关于100多种判别分析的选择。 12.2.2 线性判别分析 12.2.3 最小二乘判别分析 12.2.4 朴素贝叶斯 朴素贝叶斯（Naive Bayes）是基于概率的判别法。 12.3 案例：客户分组 "],
["section-13.html", "第13章 关联法则分析 13.1 关联法则简介 13.2 案例：商业购物篮分析 13.3 关联法则可视化", " 第13章 关联法则分析 13.1 关联法则简介 关联法则挖掘的基本想法是：当若干事件共同发生的频率大于某人仅从它们各自单独发生的频率出发预期的共同发生率时，此共同发生的情况即为一个令人感兴趣的模式。 基本度量： - 支持 - 信用 - 提升 13.2 案例：商业购物篮分析 对于销售产品纪录，由于大部分东西不会和其它一些东西在一个交易中同时出现，因为可能的组合太多了。这就导致有大量的数据点但每个观测包含的信息相对较少。在这种情况下，许多分析都不起作用，如相关性分析和线性回归。关联法则挖掘关联法则分析试图从大量的稀疏数据中搜寻有信息量的不同模式。 13.3 关联法则可视化 "],
["section-14.html", "第14章 数据可视化和结果展示 14.1 R Markdown", " 第14章 数据可视化和结果展示 你不需要太久就能够发现，分析行业不仅仅是脑力活，你在一个公司呆了一年之后就会发现，很多脑力活慢慢的转变为体力活，第二年有很大一部分的工作是重复第一年的分析，相似的方法，只是换了新的数据。这个世界上没有完美的职业，就像没有完美的人一样，至少我还没有发现。我们能够做的，就是将这不完美的部分尽力降至最低。 主要介绍可重复报告 清晰的展现从数据到代码到最后的结果报告这整个过程，便于日后查询，与其他分析师合作，以及项目交接 如果数据有变化，可以很容易重新分析并得出类似报告，省去无数琐碎的复制，黏贴，修改工作。 你可以自行定义报告格式，选择显示哪些代码，哪些输出，建立完善自己的工作流程。有一定分析经验的人会知道，很多项目具有相似性，比如一般都会从基本的数据检查开始，看看变量的分布。然后对数据进行类似的可视化。此外，某一类问题（比如判别分析）使用的模型比较固定，因此建模过程的代码也很相似，使用可重复报告让你处理相似的问题时能最大的发挥协同效应，尽可能减少重复性的工作。 knitr is ideal for this effort. It’s a system for combining code and text into a single document. Process the document, and the code is replaced with the results and figures that it generates. 14.1 R Markdown 14.1.1 什么是R Markdown? Markdown是一种轻量级的标记语言，意在使书写更加方便。它最早由John Gruber 于2004年12月推出1.0.1版本。和一些更加复杂的标记语言（如LyX，XML，HTML和LaTeX）不同，Markdown中的语法非常简单，标准的Markdown中只是定义了10多种格式，这些格式定义都很简单以至于不需要特别的记忆，使用一段时间自然就上手了。 Markdown看上去就像简单的邮件文本。和其它标记语言相比，Markdown还很年轻。它的优势就在于简洁优美，不仅机器能读，人也能轻松的阅读。 有的人可能会怀疑格式这么少的标记语言是不是太过简化了？单纯的Markdown确实很局限，但后来Markdown的几种扩展大大增加了其功能。目前比较流行的扩展有pandoc和GitHub扩展。如我的个人网页就是用Markdown通过GitHub建立的。许多GitHub的用户都使用这样的方式生成简洁，可控性强的个人网页。pandoc是一个强大的格式转换工具，基本上支持各类常见格式之间的转换，其中就包括有markdown。而markdown的格式超级简单，这就使得人们能够用超级简单的markdown来生成看起来复杂的其他格式，比如word、html、pdf等。这样，因为pandoc使得人们能够使用markdown来生成doc和pdf文档，因此受到广泛的欢迎。随着pandoc的流行，出现了很多以pandoc为基础的外围工具，以此来方便人们对pandoc的使用，比如在R语言中的几个软件包Rmarkdown，knitr等就使用pandoc作为底层的支持来实现从markdown到各种其他格式的转换。这里我们主要介绍R Markdown。 R Markdown combines the core syntax of markdown and embedded R code chunks that are run so their output can be included in the final document. R Markdown documents are fully reproducible and the most important, it is simple! 14.1.2 How to Start? Please install R if you haven’t. You can get R from the Comprehensive R Archive Network (CRAN). After installing R, install RStudio. Once you have RStudio, launch it. After that, I encourage those who are new to R Markdown start from this introductory article R Markdown — Dynamic Documents for R which can get you started with R Markdown right away. Once you install R Markdown package and learn the most commonly used markdown constructs, you can go into more depth on customizing the behavior of embedded R code. The “Learning More” section on the bottom of the introductory article has a list of reference you can refer to learn more about R Markdown. 14.1.3 Interactive R Markdown Document There are some packages that can be used to make interactive R Markdown documents. DT: A Wrapper of the JavaScript Library “DataTables” The R package DT provides an R interface to the JavaScript library DataTables. R data objects (matrices or data frames) can be displayed as tables on HTML pages, and DataTables provides filtering, pagination, sorting, and many other features in the tables. You can refer to https://rstudio.github.io/DT/ for more details. Leaflet:Interactive Web-Maps Based on the Leaflet JavaScript Library R package ‘leaflet’ makes it easy to integrate and control Leaflet maps in R. The JaveScript library ‘leaflet’ is for interactive maps. You can embed maps in knitr/R Markdown documents and Shiny apps. See https://rstudio.github.io/leaflet/ for more details. dygraphs: interactive plot for time series data The dygraphs package is an R interface to the dygraphs JavaScript charting library. It provides rich facilities for charting time-series data in R, including highly configurable series and axis display and interactive features like zoom/pan and series/point highlighting. See https://rstudio.github.io/dygraphs/ for more details. networkD3: D3 JavaScript Network Graphs from R Package ‘networkD3’ provides tools for creating D3 JavaScript network graphs from R. threejs: Interactive 3D Scatter Plots and Globes Package threejs provides interactive 3D scatterplots and globe plots. Here is a galary of examples from the package and also the source code http://bwlewis.github.io/rthreejs/. "],
["section-15.html", "第15章 数据科学的科学", " 第15章 数据科学的科学 "],
["references.html", "第16章 References", " 第16章 References "]
]
