[
["index.html", "套路！机器学习 声明", " 套路！机器学习 林荟 2017-10-09 声明 本书的纸质已经由电子工业出版社出版，10月1刚上市：）由于版权问题，电子版的内容和相关博客已经从网站上取下，感谢大家的支持和所有的来信！这里是一个书的提纲。可以在此购买纸版书：http://www.phei.com.cn/module/goods/wssd_content.jsp?bookid=50738 我正在和一个北美资深数据科学家朋友合作写另外一本关于数据科学的英文书，全书免费在线公开：http://scientistcafe.com/CE_JSM2017/ 感兴趣的小伙伴可以参考。因为书还在写作过程当中，所以先不做过多介绍，欢迎大家的意见和建议！ 如有任何关于本书或者数据科学行业的问题或者建议，请联系邮箱：longqiman@gmail.com 我会尽量答复大家：)谢谢！ --> "],
["e4bd9ce88085e887aae5ba8f.html", "作者自序", " 作者自序 首先，感谢你翻开这本书！ 这是一本什么书？ 这是一本关于数据的科学和艺术的书。书中介绍了数据科学这个行业，数据科学家需要的技能，以及“分析哲学”。书中对最常用，最有效的模型进行了展开。数据科学这个行业的本质是通过分析数据解决实际问题，所以本书很看重读者能够真正将书中介绍的知识付诸实践。书中的数据全部都是公开的，书中的代码，建模过程都可以重复。一切不能重复的的分析都是耍流氓！ 为什么写这本书？ 当前关于大数据，人工智能的炒作着实令人眼花缭乱，如大数据平台（如Hadoop、Spark），以及一些黑箱模型（如神经网络，深度学习【实际上就是多层神经网络】）。各路媒体和砖家深谙吃瓜群众不明觉厉的心态，所以就像个妓院头牌似的越发摆谱。曾今的我也是吃瓜群众中的一员，妥妥的迷失在这信息时代造成的漫天泡沫中，仿佛卡在一扇旋转门里，转了很久不知道去哪。了解一件事情最有效的方法就是实践。很幸运的是，在过去的4年里，我主导了大大小小各种分析项目。正是这些实践经验造就了这本书。我并没有打算写一本数据科学的圣经，告诉你所有关于数据科学的一切。只想尽我所能的给大家还原一个真实的数据科学和数据科学家。希望能为后来者提供一些信息，使得你们能够少走弯路。 为什么学习数据科学？ 这个问题的答案因人而异。从事某个行业和同某人结婚一样，都有很大的随机性和主观性。所以下面只是我个人喜欢这个行业的理由。 我把数据科学家定义为匠人。个人很享受作为一个匠人，统帅三军之能不如薄技在身。当你相信自己在某些领域有专长并且因此产生自我价值感时，就会有激情。激情是有吸引力的，就像爱一样，这是一种值得为之奋斗的感觉。 这个世界上的手艺很多，为什么是数据科学？因为我觉得数据科学这门手艺能够帮你培养在当今信息海啸中独善其身的技能——独立思考的能力。用数据进行决策能够让你看问题更清晰，有逻辑，理性客观。这种能力不是只有数据分析师才需要掌握，理性思考是贯穿很多人一生的必修课，尤其在互联网时代，通过理性思考甄别过滤信息比之前任何时候都重要。此外，人的大脑是有连贯性的，已经习得过某项技能的人，再学另外一个技能的时候，学得会比上一次快一些，因为学习经验在起作用。而若是习得的基础知识是可积累、可扩展的，那么随后可能习得的技能可变现价值就会越来越高。通过数据分析进行决策就是一门可扩展性极高的技能，几乎可以扩展到这个数据时代的方方面面，而且随着社会的数据化趋势，这种可扩展性产生的“复利效应”将越来越大 —— 有着可怕的潜力。 数据科学是美的，美只有爱知道，所以热爱是选择这个行业的主要理由。不知道从什么时候开始，中国互联网上开始流传一句话：生活不止眼前的苟且，还有诗和远方。其实问题不在于缺少诗和远方，而在于你以为眼前的是苟且。如果你热爱自己当前所做的事情，那就是诗，就是远方。如果你不热爱自己所做的事情，在你找到自己真正热爱的事情之前，到哪里都是苟且。我希望阅读这本书的所有人都能够在数据分析中找到乐趣。归根结底，快乐并不是什么深奥的事情，无非是猫吃鱼，狗吃肉，奥特曼打小怪兽。 最后，感谢父母的爱和支持，感谢你们帮助我找到自己热爱的东西。感谢Scott Iverson，他是我在市场营销领域的导师，没有他，我无法将数据科学很好的应用于市场营销。感谢王正林以及所有为本书出版做出努力的人，没有你们就没有本书的问世。再次感谢你选择本书！ "],
["1.html", "推荐序1", " 推荐序1 又来一个找我写序的……感觉自己都快成了写序专业户，惭愧惭愧。以前叫我写序的作者我一般都不熟，但这次这位我还算熟，所以终于可以说点电视上不让播的内容。八年前我们一同进入爱荷华州立大学（俗称 Ames 村办大学）统计系读博，当时我们的背景完全相反：我在测度论课上奄奄一息、在 R 里如鱼得水，林荟在 R 入门课上死去活来、在理论课上羽化登仙。毫不脸红地吹个牛：要不是我当年的提携，她早就能写出这本书了。 玩笑归玩笑。总的来说，看到这本书时我还是吃了一斤。我读博的时候一定是遇到了一个假林荟。尽管上学的时候她在村办大学的牲口学院（好吧，兽医学院）有一些科研经历，但我记得也就是画画 ROC 曲线、跑跑逻辑回归而已。士别三年，竟然已经成了一只 R 语言老司机，而且还写出一本主题这么宏大的书。书里举的例子都是种子、生猪、农业论坛，鬼知道她这几年都经历了写什么。以前她抗拒写代码，主要原因是对着电脑时间长了怕脸上长痘，看来后来还是决定为（数据）科学献身了。我们假装感动三秒。 书的内容我大致看了一遍，因为都是熟人，我评价起来也就不客套了；按书的标题，分两方面说：R 语言和数据科学。 一般来说，我不在乎别人的 R 代码写得好不好，因为反正写得再好也没我写得好（明年请在我的坟头多烧两张纸）。我对计算机相关书籍的最低标准是不要把“阈值”写成“阀值”，我仔细看过了，本书作者写的是对的。看 R 相关的书籍时，我也有个怪癖，就是找有没有 if (x == TRUE) 或者 y[which(y &gt; 3)] 这样的语句，其实语句都没错，只是看看作者的强迫癌是不是到了晚期（if (x) 和 y[y &gt; 3] 就已足够）。本书作者似乎没有患强迫症。不过这也无妨，很多时候我觉得对代码吹毛求疵反而影响效率，而且不太老的司机分享的经验对新司机可能更有用。在我眼中，这本书在 R 方面有两个亮点：一是里面介绍了很多 R 的附加包，例如 caret，读者拿起来应该能很快上手；二是用几乎以假乱真的模拟数据，这一点可能会为人诟病（不是真实数据），但我觉得模拟数据有其独特的价值，就是你掌控着整个小宇宙，数据从生成到建模到解释，一路的过程你都可以看清楚，而且可以变着法子换新数据玩，学习模型使用方法。 数据科学我就不敢妄言了，毕竟我毕业之后已经转向纯码农，很少做有关统计或数据的一线工作。就我的快速粗读来看，我感觉话题的覆盖范围很广，但深度也比较适宜。广度和深度通常只能二选一，也没有优劣之分。我读书少，也限于篇幅，就随意翻两页点评两个例子吧，从我自己的视角管窥一下本书的价值。比如多年前我就坚信，讲主成分分析的人如果不马上讲偏最小二乘就是耍流氓，尤其是主成分回归，是流氓中的流氓，而本书作者很明确地指出了主成分回归的弊病。再比如 Bootstrap 方法，作者讲，“假如你只有 1 个样本，难道你不停有放回抽样就能得到大样本了？”这是很漂亮的一拳。很多方法因为实施简单，所以很容易让人忘了它们的先决条件。我非常反对迷信模型或方法甚至软件，世上没那么多万金油。基于同样的原因，我很欣慰看到本书不是清一色 ggplot2 图形（虽然有些图可能长得略丑，但想得美就好了）。 仔细看完本书的话应该能看出作者是苹果粉（某一页上画图时字体用的是 Songti SC）以及段子狗。都读完了博士，选电脑还这么看脸，还整天为各种段子操碎了心，这位数据科学家也是蛮拼的。 谢益辉 写于奥马哈 "],
["2.html", "推荐序2", " 推荐序2 伴随着计算机硬件，数据获取和存储技术，分布式算法的飞速发展， 以及海量数据的积累，数据科学成为近几年来飞速发展的学科。但确切的说，数据科学还不是一门定义完善的学科。直到最近两年，大学里才慢慢开始建立数据科学相关的项目和学位。林荟博士的著作及时的填补了“如何成为成功的数据科学家”领域的空白。由于数据科学家的就业市场非常火热，很多领域的人才都想通过提升自身技术水平和经验成为真正的数据科学家。但正如林博士在书中指出的数据科学家=数据+科学+艺术家，想成为成功的数据科学家，各个领域的人才需要通过大量的学习和实践来弥补自身的欠缺。比如传统的统计学家和计量经济师需要熟悉编程，数据库操作和大数据分布式计算架构。对于刚刚毕业的理工科硕士和博士，积累利用真实数据解决实际问题的经验，提高高效的书面和口头表达能力，提升团队协作能力和自身的影响力是至关重要的。 林博士的著作首先系统的阐述了什么是数据科学以及成为成功数据科学家的必要条件。然后通过具体的数据和例子来引导读者一步步的理解和学习如何获取这些必要的条件成为真正的数据科学家。本书中各个章节的数据和具体操作都由开源系统的R语言来实现。读者可以下载所有的数据和代码，通过自己运行这些代码来加深对每个章节知识的理解，并且可以很快灵活的学以致用来解决学习和工作中遇到的数据科学相关的项目。对数据科学家而言，很大一部分精力是要花在数据的理解，整合和预处理上面。林博士通过自己在数据科学领域多年的经验来仔细的讲解了如何理解和预处理数据，这是本书的亮点之一。没有很好的理解数据，没有透彻的了解具体要解决的问题，就不可能找到好的解决方法。接着林博士用生动的语言的诙谐的例子介绍了在数据科学中常见的模型和方法。读者可以通过相关例子和代码来高效的理解这些模型和方法，并可以快速学以致用。虽然几乎所有的算法都有相应的程序包来实现，但作为成功的数据科学家，理解模型的理论背景和基础是必须的。因为只有理解了这些程序包的理论基础，才能有效的对不同数据不同问题来选择解决的方法并且设置合理的参数。本书对常用模型和方向进行了介绍和引申，可以帮助读者了解各个模型和方法背后的理论。简言之，本书系统的阐述了如何成为成功的数据科学家，读者可以通过本书的数据和代码，高效的学习并能很快的应用到实际项目中去。 伴随着大数据应用从互联网科技公司普及到传统商业领域诸如零售，制造，交通，电力和能源，航空航天，金融，医疗保健，以及大数据在各级政府部门政策制定和实施中的应用，数据科学家的需求还会逐年增高。尤其是大数据在新兴领域如工业互联网，物联网，智能家居和传感器网络的重要应用，很多相应的数据科学家的职位也会有新的需求。比如在制造业工业物联网领域的数据科学家岗位，除了上述提到的知识和经验，通常还会要求对制造业背后的物理和工程原理有所了解。具备了相应工业的基础知识和原理，数据科学家才能更好的理解数据并建立有效的模型和应用。这也对各理工科背景的人才敞开了数据科学的大门。同时通过大量用户数据的积累，数据科学家也对人文学科的人才敞开了大门。数据科学是一个飞速发展的学科，它通过数据和模型来影响各个学科和领域从而产生价值。数据科学家使得采集的数据有了真正的用武之地。对数据科学感兴趣的人才们，请从本书开始，不断提升自己的技术和经验，成为真正的成功数据科学家，为各行各业带来颠覆性的创新吧！ 李明写于默瑟岛●西雅图 二零一七月年四月 序言作者简介：李明博士，毕业于爱荷华州立大学（Iowa State University ）拥有物理和统计背景。曾任通用电气全球研发中心（GE Global Research Center）统计方向负责人（Statistical Leader），沃尔玛技术部（Walmart Technology）数据科学家（Data Scientist）。现任美国亚马逊（Amazon）资深数据科学家 （Senior Data Scientist）。李博士还担任美国统计学会（American Statistical Association）质量和生产力分会（Quality and Productivity Section）2017年度主席，以及统计在物理和工程应用年度奖评选委员会主席（SPES Award，one of American Statistical Association annual awards）。李博士的职业生涯中曾涉及金融，零售，制造，电力和能源，交通，医疗保健，和航空航头 等多个产业及相关跨产业领域。 "],
["e5898de8a880.html", "前言", " 前言 数据科学家目前是北美最热门的职业之一，平均年薪突破10万美元。但数据科学并不是一个低门槛的行业，除了对数学，统计，计算机等相关学科技术的要求以外，还要需要相关应用领域的知识。这个职业听起来很酷，但如果你对数据分析没有兴趣的话，你也会觉得这个行业很苦。这里我默认本书的读者都至少是对这个行业有兴趣和激情的。本书的写作对象是那些现在从事数据分析相关行业，或者之后想从事数据分析行业的人，意在为实践者提供数据科学家这门职业的相关信息。读者可以从阅读中了解到数据科学家需要的技能，及背后的“分析哲学”。书中会对部分最常用，有效的模型加以展开。关于模型技术部分，我希望读者有初步统计知识，最好知道线性回归。 数据科学家这个行业的本质是应用。市面上有很多文章，出版物介绍各种数据模型，大多数此类书籍并不能让读者重复书中所述的分析过程，对于书中介绍的知识，读者真正实践起来会遇到很多困难。本书着重在于数据科学的实际应用，让读者能够重复书中的结果，这也用到了统计软件R的自动化报告功能。可能有读者会问，为什么要可重复？根据个人经验，学习数据分析技能最好的方式是实践：动手重复分析的过程，检查分析结果，发现问题后再去查询相关模型的背景技术知识。这一过程得到的学习效果远远超过死磕一本大部头的技术理论书籍。磕了一年之后发现碰到实际问题不知道该用什么工具实践这些书中讲到的模型方法。而且对于新手而言，一开始就直奔艰深的理论，很容易因为困难而失去兴趣最终放弃。本书倡导的是一种循序渐进的启发性教学路径，从实际问题入手，抽丝剥茧进入技术内核。 本书主要部分将避免过多的数学公式，但难免有例外。我们在一些地方提到方法背后的技术细节是为了帮助读者理解模型的长处和弱点，而非单纯的介绍数理统计知识。这并不意味着这些数理背景知识不重要，相反尽可能多的了解模型背后的数学重要且有意义，为了平衡理论和应用，我们会在有的章中加一些选学小节，用来介绍更多的模型数理背景或给出必要的参考资料来源，如果不感兴趣的读者可以跳过这些小节，不会影响本书主要部分的阅读。书中的每一章都只是冰山一角，我并不试图彻底的介绍模型，而是有选择性的解释其中部分我觉得重要的地方。我会尽量将想要强调的概念和内容在分析数据的过程中体现出来，而不仅仅是数学公式符号表达。想要成为数据科学家，仅靠阅读本书是远远不够的，读者需要进一步查阅书中提到的参考资料，或者选修相关课程。 随着计算机科学的发展，不仅收集存储的数据增加了，分析数据的软件包也不断推陈出新，这极大的降低了应用统计学习方法的壁垒。现在不管会建模的不会建模的，大都听过线性回归，这个经典统计模型可追根溯源至19世纪Legendre和Gauss发表的若干关于最小二乘的论文。现在你要通过最小二乘拟合一个线性模型那就动动指头两秒钟的事情。可在那个计算器都没有的时代，能优化误差平方和这样的东西的大牛都会被认为是火星人。那会美国宪法规定每十年必须进行一次人口普查，1880年排山倒海的普查资料花了8年时间处理分析，一个名叫Herman Hollerith的品学兼优的美国少年跳出来，在1890年发明了一种排序机，利用打孔卡储存资料，再由机器感测卡片，协助人口调查局对统计资料进行自动化制表，结果不出3年就完成了户口普查工作，Herman同学也顺带用这个发明拿个了工程学博士学位。你可能要问，计算能力这么落后那这伙数学家捣鼓出来的方法谁用？天文学家用。线性模型最早用在天文学研究中。研究使用统计方法的，那会绝对是小众边缘群体，全都可以贴上火星制造的标签。然后盼星星盼月亮我们终于在1912年6月等到了图灵这个天才的降临。 若不是图灵这个孩子被性取向拖了后腿，数据科学家这个行业早几十年可能就火了。当然，统计泰斗们也没有闲着，Fisher在1936年提出了线性判别分析。在1940s，又一家喻户晓的经典统计模型——逻辑回归——问世了！在1970s早期，Nelder和Wedderburn发明了广义线性模型这个词，这是一个更大的统计模型框架，它将随机分布函数和系统效应（非随机效应）通过一个连接函数（link function）连起来，之前的线性模型和逻辑回归都是该框架下的特例。到1970s末，可以用来分析数据的方法已经有好些了，但这些方法几乎都是线性模型，因为在那时，拟合非线性关系的计算量相对当时计算机水平来说还是太大了。等到1980s，计算机技术终于发展到可以使用非线性模型了。Breiman, Fridman, Olshen和Stone提出了分类回归树。随后的一些机器学习方法进一步丰富了数据科学家可以使用的工具集。计算机软件的飞速发展使得这些方法模型得以应用在更加广泛的领域，应用涵盖了商业，健康，基因，社会心理学研究和政策分析等等。数据科学家这个行业随着数据量的增加和分析软件的进步不断向前发展。 关于分析软件，本书使用R。选择R语言的原因如下： R免费，且可以在不同操作系统上使用。 R开源、可扩展：它在通用公共许可（General Public License）下发行，在此构架下任何人可以检查修改源程序。含有很多最新的模型。 R有强大图形可视化和自动化报告功能 笔者10年使用R的经验证明无论在学术还是业界，这都是非常有效的工具。 网上有大量的R入门教程，关于用R进行数据分析的书也有好些，所以这里就不重复造轮子，不熟悉R的读者可以先学习相关资料，这里我假设读者已经有一定R语言基础。 本书布局如下，本书先介绍数据科学家这个行业和“分析哲学”和数据分析的一般流程。这是非技术的部分，但对于从业者来说非常重要，它帮助你对这个职业设定一个合理的预期。其中会讨论数据科学家需要的技能。之后的章节会对这里提到的部分我觉得重要的技能进一步展开讨论，由于篇幅所限，不可能详细讨论开始这几章中提到的所有技能。随后开始进入技术部分，讲分析环节的第一步——数据预处理，这一步虽然不是正式建模，但却是整个分析过程中最耗时的一个环节。这步没有到位将严重影响模型质量。也正是因为预处理重要，所以单独作为一个章节，没有和章其它建模技术合并起来。“基础建模技术”这个章节介绍的是一些在建模过程中需要的辅助性的技术以及建模需要注意的问题。之后正式介绍各种笔者在从业过程中经常用到的模型。 本书用来展示模型的数据大部分是通过R得到的模拟数据集。为什么用模拟数据而不是真实数据呢？原因如下： 你可以控制数据生成过程，免去了传输下载数据的麻烦。 你可以根据需要改变生成数据的代码，得到新的数据，观察数据变化对模型结果的影响。 对于自己创建的数据，我们知道数据要表达的真实信息，那么就可以评估分析使用的模型的准确性，然后再用于真实数据。 可以通过使用模拟数据在拿到真实数据前准备好代码模板，这样，当你有真实数据时就可以迅速进行分析。 通过重复数据模拟的过程可以加深对模型假设的理解。 同一章后面的代码通常建立在之前代码上，但每章的代码自成系统，也就是说你不需要以其它章节代码运行结果为前提重复某章的代码。有一定R语言基础的读者可以通过学习生成数据的代码了解数据的结构，以及模型假设。R语言的新手学习这些代码可能会觉得太困难，没有关系，你们可以跳过生成数据的细节，只需要了解数据的语境，都有哪些变量以及变量类型。你可以直接从网站上读取这些数据。书中的代码和数据可以在这个github页面上找到：https://github.com/happyrabbit/DataScientistR 现在开始我们的旅程吧！ "],
["section-1.html", "第1章 数据科学", " 第1章 数据科学 "],
["section-2.html", "第2章 数据集模拟和背景介绍", " 第2章 数据集模拟和背景介绍 "],
["section-3.html", "第3章 数据分析一般流程", " 第3章 数据分析一般流程 "],
["section-4.html", "第4章 数据预处理", " 第4章 数据预处理 "],
["section-5.html", "第5章 数据操作", " 第5章 数据操作 "],
["section-6.html", "第6章 基础建模技术", " 第6章 基础建模技术 "],
["section-7.html", "第7章 模型评估度量", " 第7章 模型评估度量 "],
["section-8.html", "第8章 特征工程", " 第8章 特征工程 "],
["section-9.html", "第9章 线性回归极其衍生", " 第9章 线性回归极其衍生 "],
["section-10.html", "第10章 广义线性模型压缩方法", " 第10章 广义线性模型压缩方法 "],
["section-11.html", "第11章 树模型", " 第11章 树模型 "],
["section-12.html", "第12章 神经网络", " 第12章 神经网络 "],
["e98197e5a4b1e79a84e280a6e280a6e5b9b2e8b4a73f.html", "遗失的……干货?", " 遗失的……干货? 忘记什么时候从互联网上学会干货这个词，按照我的理解，如果别人说你的内容是“干货”，这是一种表扬。很多文章标题也用“干货”来博人眼球。信息时代下的人们对速度空前的执着，即使是知识也希望高效精华，满满的干货。这种对速度的追求让我深感不安。 很多评论说碎片化正在侵蚀我们系统性思考的能力，但我觉得这只不过是替罪羊罢了。真正的罪魁祸首是根植于人性中的懒惰和贪婪，我们不想花太多努力，但又想得到最大的回报。而移动互联网时代的便捷，和各种社交媒体导致时间的碎片化只不过放大了人的劣根性。因为每次只有一小块的时间，所以大家希望得到的是在短时间内能够读完，然后立马get到新技能的“干货”。如果这是你想要的，本书要让你失望了。本书的目的是帮你绕开一些弯路，但数据科学家没有捷径可走。 平滑的大脑比褶皱的肌肤还要令人恐惧。大脑上每一道褶痕都需要付出足够的努力和汗水。大脑除了灰质以外还有近50%由“白质”构成，英文叫做myelin，中文翻译成髓磷脂或者髓鞘质，其产生并且包裹大脑神经元轴突的过程叫作神经髓鞘化（myelination），这个过程从出生3个月就开始发生。神经髓鞘化能使神经传导速度变快，因而对学习新技能而言非常关键。在婴幼儿阶段，神经髓鞘化发生极其迅速，因为这个时期婴儿需要习得大量新的技能，从爬到跑，从哭闹到能够说话。虽然随着年龄的增长，这个过程逐渐减弱，但大脑的可塑性伴随我们一生，也就是说只要你努力练习，神经髓鞘化的过程会一直继续帮助我们不断学习新的技能，只是这需要大量、反复、有针对性的练习。所以再次强调大家动手重复书中的代码，尽量找机会将书中介绍的知识付诸实践。只有练习才能搭建数据科学的大脑回路。【注：关于成年人如何发展大脑，神经干细胞研究员Sandrine Thuret在TED有个很有意思的演讲：You can grow new brain cells. Here’s how。关于大脑工作的一些书有：Mind Wide Open，The Human Advantage，Thinking Fast and Slow，What intelligence tests miss。这些书都很有意思，可以帮助大家了解我们的大脑和思考模式，有助于找到有效的学习方法。Coursera上还有一门基于脑科学研究总结出的教你如何学习的课程，很推荐大家听听，课程的名字叫做Learning How to Learn。】 这自然就引出另外特别重要的一点：做一个终生学习者，这是匠人精神的精髓所在。这里需要澄清的是，我并不反对碎片化学习，事实上，我觉得碎片化学习和终生学习一样是一个必然的趋势。我反对的是速食学习，不假思索，不经过思考过滤的信息大采购，这种肤浅的学习方式给你的只是一种觉得自己无所不知的幻觉。你完全可以利用碎片化的时间深入思考一个问题。得到的信息最后需要通过反刍，过滤，实践才能真正转化成知识。 不断学习的目的在于认知升级，而非单纯的记忆信息。认知升级是指你能在各种信息中把握重要的信息，在遇到问题的时候把握解决的关键点，然后基于已储存的知识找到当前较优的解决方法。为什么不是最优的？因为这通常很难实现，当一个解决方案到一定程度能够满足当前需求的时候，你就需要考虑继续优化结果的成本和带来的收益是不是匹配。比如你用一个简单的逻辑回归可以达到90%的精确度，这个时候你尝试随机森林，得到的精确度是93%，而前者只需要运行不到一分钟，后者却要运行5个小时模型可解释性还很差，这个时候你就要考虑使用更复杂的模型提高精确度是不是值得。如果是在医疗方面的研究，人命关天的事情，当然是越精确越好。但如果是商业决策，这点精确度差异几乎可以忽略不计，而且在这个场合中，可解释性是非常重要的。所以，脑子里的知识应该是活的，需要懂得融会贯通。学习大量的知识是聪明，知道如何使用知识是智慧。涵养智慧的过程，就是认知升级。在信息海啸中保持独立和清醒，是一件非常困难，需要长时间刻意练习的终极技能。 我真心希望对这个行业感兴趣的伙伴能够在这三点上达成共识： 数据科学家之路没有捷径 终身学习的必要性 学习以认知升级为目的 在此基础上，我想对全书做一个总结，指出一些没有讲到的模型方法。书中对最常用，最有效的模型进行了展开。当然，还有很多模型技术书中并没有展开。下面是对遗漏部分的补充说明。 书中对核函数类的算法没有介绍，比如支持向量机。支持向量机（SVM）和广义线性模型类似，是一个框架。由核函数和损失函数组成。你可以通过使用不同的核函数和损失函数来得到不同的模型，但这些都可以叫做支持向量机。所以这是一个灵活度较高的模型框架。在实际应用中，当SVM模型使用的是线性核函数时，其和逻辑回归差不多。SVM的长处在于： 通常在非线性可分的问题中，SVM要优于逻辑回归。在这类问题中，你可以使用非线性核函数的SVM。 本身自带的变量选择功能，这来自于其损失函数，所以在一些高维度问题上效果优于传统的逻辑回归，比如文本归类问题，这类问题中通常面对的是非常稀疏的词频矩阵。 在一些自变量矩阵非稀疏的问题中，基于罚函数的逻辑回归不管实际精确性还是在可解释性上都要优于SVM。SVM的一个劣势在于对计算机要求高，这点也是一些集成模型和神经网络类模型的弊端。所以，在用这类模型之前，建议大家先抽取一个小的样本，然后尝试一下。看看效果如何，好的话再用于所有的样本。 用于判别的模型非常多，但在实际应用中不可能尝试所有的模型，也没有那个必要。有很多关于比较各种判别模型的论文，推荐大家搜一些看看，这些文章中会将不同的模型用于不同的数据集，比较模型结果。选择模型是一个很复杂的话题，而且和具体情况有关。SVM在基因分类研究中效果可能不错，但是我在市场数据环境中确发现效果并不佳，所以这里并没有进行详细的介绍，关于这个模型，有很多文献资料，所以对于想要了解模型的读者，推荐大家花时间自己去学习。 当然，对算法的选择也非完全随机，还是有迹可循的。你可以考虑下面几点： 训练样本量 特征空间的维度 凭直觉判断这个问题是线性还是非线性 各个变量之间独立性如何 自变量和应变量之间是不是可能存在线性相关 过度拟合的风险有多大 模型对计算机计算能力和存储的要求是什么 上面的这些点可能让你有点抓狂，考虑的维度好像有点多，而且有些貌似不是一下就能够很好回答的。学习是个螺旋上升的过程。就好像你开始学习钢琴的时候，觉得同时要注意音调，节奏，左右手还要配合，居然还有情感的处理、弹奏的力度，简直应接不暇。但是人的手指有记忆性，大脑也有记忆性，当你熟悉以后，对之前学过的东西大脑会自然将其组块，形成条件反射般的直觉。于是之后对类似的问题，你就不需要占用太多脑力，而将节省出来的工作记忆花在新的，需要更多思考的问题上。随着经验的丰富，你会对各种模型的优劣，对计算机的需求，问题的性质产生更深的理解。到那个时候，这个过程看起来就不那么抓狂了。 这个世界是复杂的，但是我们终归要找到一个简单的应对措施。如果非得要给一个一般性的简化版的模型选择建议，对分类回归问题（因为这些问题占了主要部分）我会建议大家： 首先尝试随机森林模型，看看大概的精确度有多少； 然后至少在此基础上尝试随机助推，看看是否会进一步提高精确度； 对数据进行更加仔细的预处理，使用罚函数模型，研究选出的变量，以及参数的估计； 比较不同模型的精确度以及变量的重要性排序，看看是不是一致； 如果时间精力允许，可以尝试下SVM； 最后在模型的精确度和可解释性之间进行一些权衡，选择最后的模型。 上面只是一个一般性的建议，不是固定的流程。如果你遇到的是极其稀疏的自变量矩阵，那么这个流程是不适用的，因为矩阵永远都是奇异的，你用罚函数模型的时候报错会让你烦到吐血…… 深度挖掘。这好像是T台上的衣服，大家都在谈论，但你几乎看不到有人真正穿出门。我再说一次，深度挖掘其实就是神经网络，是穿了好几件马甲的神经网络。我个人是坚决不建议在一般情况下使用这类模型。如果你在谷歌人工智能团队工作，那你可能用到神经网络。但我想这类人也不会来读这本书。而且这是计算机工程师，而不是数据科学家。在实际生活中的决策问题不是下棋，大家能够理解这一点么？当你有上百万的样本而只有100个变量，其中大部分变量还高度相关，加上面对的不是棋牌，图片识别这类值域定义相对明确的问题的时候，你用个100层神经网络也是 然 并 卵！作为数据科学家，你能做的只能是从数据中挖掘信息，而没法创造出信息来。 如果大家对复杂模型真的那么执着的话，推荐大家看下另外一种集成模型的方法，叫做超级学习器（Super Learner）。和之前集成模型的不同之处在于，这种模型是将不同的模型进行集成，比如随机森林，lasso，svm等。好比从不同的角度看待数据，然后中和大家的结果。这个模型还不是非常流行，但个人感觉能够在不同模型结果中折中的想法很好。 此外时间序列模型也没有讲到。这类方法适用的范围比较窄，我在工作中从来都没有遇到适用这类模型的情况。而且这类模型比较传统，相关的资料教材实在太多，想要了解这方面知识的伙伴可以很容易找到相关资料进行学习。 贝叶斯网络属于概率图模型，这类模型适用于无法严格实验的问题。个人很喜欢这类模型，因为市场营销问题基本无法进行实验，环境具有很强的不可控性。比如你想了解不同的商业策略如折扣、服务、支付方式（如分期付款）以及客户行为之间的关系，这类问题的因素之间通常相互影响，而且没有一个真正所谓的“因变量”。你感兴趣的是其中一个或多个变量的变化对其它变量分布可能造成的影响。当前能够应对这种问题的最好方法也就是概率图模型了。由于本书篇幅限制，没有办法对这个有用的模型进行详细展开。关于贝叶斯，还想补充一点，真正的贝叶斯模型应用的还不是很多（至少在业界）。大家耳熟能详的所谓贝叶斯网络，朴素贝叶斯其实只是用到了贝叶斯定理，而非严格意义上的贝叶斯模型。贝叶斯模型应该有先验分布和似然函数，而贝叶斯定理只是一个概率统计常用的定理。个人觉得真正的贝叶斯在业界的应用还是有很大潜力的，因为很多实际问题很符合贝叶斯的逻辑，通常我们都有一些专家知识（先验信息），然后获取数据结合之前的先验信息得到后验分布。当然，这要展开讲就太大了。 关于可视化，虽然没有单独的讲，但是在书中各个章节案例代码中频繁的用到。可视化不是一个适合单独讲的话题，更好的方式是结合具体问题。因为这是一个辅助的工具，仅仅列出一大串的工具而脱离具体场景是没有意义的，大家也很难记住。 最后需要讲到自动化报告。这个我犹豫了很久，因为这是也是我每天都会使用的利器。按道理是应该讲的，但最后还是决定不讲。一来这个话题本身和可视化一样，最好还是结合案例讲。二来，这个话题和可视化，以及围绕Rmarkdown的一系列生成交互html页面的工具联系紧密，所以最好是将这些话题放在一块结合案例介绍。这样一来其实牵扯到的是一个围绕自动化报告的生态系统，内容就太多了。想要了解相关知识的小伙伴可以关注Rstudio的文章以及谢益辉的书Dynamic Documents with R and knitr. 不知不觉，居然又碎碎念了这么多。希望你觉得本书对你有一点帮助。不管你的职业选择是什么，尊重你正在做的事情，享受你正在做的事情，认真对待你正在做的事情的一种状态，它里面包含着对生活的一份热爱和对未来的美好的预期。 "],
["references.html", "References", " References "]
]
