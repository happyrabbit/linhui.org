---
title: "The Art and Science of Data---Data Wrangling"
author: "Hui Lin (@gossip_rabbit)"
date: "`r Sys.Date()`"
output: 
 html_document:
  theme: united
  highlight: tango
  toc: true
  toc_depth: 5
  number_sections: true
---

This blog focuses on some of the most frequently used data manipulations and shows how to implement them in R. It is important to explore the data set with descriptive statistics (mean, standard deviation, etc.) and data visualization prior to analysis. Transform data so that the data structure is in line with the requirements of the model. You also need to summarize the results after analysis. 

Here we assume the readers are already familiar with some of the traditional R data operations, such as subsetting data frame, deleting variables, read and write functions (`read.csv ()`, `write.csv ()`, etc.) in base R. We will also skip some basic descriptive functions in R. For example, for discrete variables, we often use the frequency table to look at the frequency (`table ()`) of the variable at various levels as needed, or a crosstab of two variables. You can also draw a bar chart for discrete variables (`bar()`). For continuous variables, we need to look at the mean (`mean ()`), standard deviation (`sd()`), quantile (`quantile()`) of a variable from time to time. There are also functions like `summary()`, `str()` and `describe()` (a functions in the 'psych' package) that give a summary of a data frame.

<!--
These are some of the most basic methods of exploring data at all stages (including model results). But only these are not enough, the flexibility of these methods is not high, the output of the information is fixed. We may not want the `summary ()` function to output all of the information, and some of the information we want is not. For example, if we want to know the mean of customer revenue, online spend, and so on for each category, or if we want to find the highest-earning people in each category, then extract them together, or we want a The new variable indicates whether the channel was purchased (online or in-store) and used this for modeling, where the data needs to be reshaped, the records for online purchases and the records for physical store purchases are row by row rather than now Of the column. These operations can be cumbersome and inefficient when used only with primitive functions. There are some other packages in R that can be very efficient and succinct to accomplish these seemingly complex tasks. The first contact with these functions of the small partners will feel that they are not very studious, it is natural, more flexible tools need to learn to master the natural longer, but this is a ** practice makes perfect process.
-->

The focus here is to introduce some of the more efficient data wrangling methods in R.

## Read and write data

### `readr`

You must be familar with `read.csv()`, `read.table()` and `write.csv()` in base R. Here we will introduce a more efficient package from RStudio in 2015 for reading and writing data: `readr` package. The corresponding functions are `read_csv()`, `read_table()` and `write_csv()`. The commands look quite similar, but `readr` is different in the following respects:

1. It is 10x faster. The trick is that `readr` uses C++ to process the data quickly. 

1. It doesn't change the column names. The names can start with number and "`.`"  will not be substitued to "`_`". For example:  
    
    ```{r}
    library(readr)
    read_csv("2015,2016,2017
    1,2,3
    4,5,6")
    ```

1. `readr` functions do not convert strings to factors by default, are able to parse dates and times and can automatically determine the data types in each column. 

1. The killing character in my opinion is that `readr` provides **progress bar**. What makes you feel worse than waiting is not knowing how long you have to wait. Without "progress bar" might be the No.1 reason that people break up with the one they have been dating.

(add progress bar)

The major functions of readr is to turn flat files into data frames:

- `read_csv()`: reads comma delimited files
- `read_csv2()`: reads semicolon separated files (common in countries where  `,`  is used as the decimal place)
- `read_tsv()`: reads tab delimited files
- `read_delim()`: reads in files with any delimiter
- `read_fwf()`: reads fixed width files. You can specify fields either by their widths with `fwf_widths()`  or their position with  `fwf_positions()`  
- `read_table()`: reads a common variation of fixed width files where columns are separated by white space 
- `read_log()`: reads Apache style log files

The good thing is that those functions have similar syntax. Once you learn one, the others become easy. Here we will focus on `read_csv()`.

The most important information for `read_csv()` is the path to your data:

```{r, message=FALSE}
library(readr)
sim.dat <- read_csv("https://raw.githubusercontent.com/happyrabbit/DataScientistR/master/Data/SegData.csv ")
head(sim.dat)
```

The function reads the file to R as a `tibble`. You can consider `tibble` as next iteration of data frame. They are different with data frame in the following aspects:

- It never changes an input’s type (i.e., no more `stringsAsFactors = FALSE`!)
- It never adjusts the names of variables
- It has a refined print method that shows only the first 10 rows, and all the columns that fit on screen. You can also control the default print behavior by setting options.

Refer to http://r4ds.had.co.nz/tibbles.html for more information about ‘tibble’.

When you run `read_csv()`  it prints out a column specification that gives the name and type of each column. In order to better understanding how `readr` works, it is helpful to type in some baby data set and check the results:

```{r}
dat=read_csv("2015,2016,2017
100,200,300
canola,soybean,corn")
print(dat)
```

You can also add comments on the top and tell R to skip those lines:

```{r}
dat=read_csv("# I will never let you know that
          # my favorite food is carrot
          Date,Food,Mood
          Monday,carrot,happy
          Tuesday,carrot,happy
          Wednesday,carrot,happy
          Thursday,carrot,happy
          Friday,carrot,happy
          Saturday,carrot,extremely happy
          Sunday,carrot,extremely happy", skip = 2)
print(dat)
```

If you don't have column names, set `col_names = FALSE` then R will assign names "`X1`","`X2`"... to the columns:

```{r}
dat=read_csv("Saturday,carrot,extremely happy
          Sunday,carrot,extremely happy", col_names=FALSE)
print(dat)
```

You can also pass `col_names`  a character vector which will be used as the column names. Try to replace `col_names=FALSE` with `col_names=c("Date","Food","Mood")` and see what happen.

As mentioned before, you can use `read_csv2()` to read semicolon separated files:

```{r}
dat=read_csv2("Saturday; carrot; extremely happy \n Sunday; carrot; extremely happy", col_names=FALSE)
print(dat)
```

Here "`\n`" is a convenient shortcut for adding a new line. 

You can use `read_tsv()` to read tab delimited files：

```{r}
dat=read_tsv("every\tman\tis\ta\tpoet\twhen\the\tis\tin\tlove\n", col_names = FALSE)
print(dat)
```
Or more generally, you can use `read_delim()` and assign separating character：

```{r}
dat=read_delim("THE|UNBEARABLE|RANDOMNESS|OF|LIFE\n", delim = "|", col_names = FALSE)
print(dat)
```

Another situation you will often run into is missing value. In marketing survey, people like to use "99" to represent missing. You can tell R to set all observation with value "99" as missing when you read the data:

```{r}
dat=read_csv("Q1,Q2,Q3
               5, 4,99",na="99")
print(dat)
```

For writing data back to disk, you can use `write_csv()` and `write_tsv()`. The following two characters of the two functions increase the chances of the output file being read back in correctly:

- Encode strings in UTF-8
- Save dates and date-times in ISO8601 format so they are easily parsed elsewhere

For example:

```r
write_csv(sim.dat, "sim_dat.csv")
```

For other data types, you can use the following packages: 

- `Haven`: SPSS, Stata and SAS data
- `Readxl` and `xlsx`: excel data(.xls and .xlsx)
- `DBI`: given data base, such as RMySQL, RSQLite and RPostgreSQL, read data directly from the database using SQL

Some other useful materials:

- For getting data from internet, you can refere to the book “XML and Web Technologies for Data Sciences with R”. 
- [R data import/export manual](https://cran.r-project.org/doc/manuals/r-release/R-data.html#Acknowledgements)
- `rio` package：https://github.com/leeper/rio


### `data.table`--- enhanced `data.frame`

What is `data.table`? It is an R package that provides an enhanced version of `data.frame`.  The most used object in R is `data frame`.  Before we move on, let's briefly review some basic characters and manipulations of data.frame:

- It is a set of rows and columns.
- Each row is of the same length and data type
- Every column is of the same length but can be of differing data types
- It has characteristics of both a matrix and a list
- It uses `[]` to subset data

I will use the clothes customer data to illustrate. There are two dimensions in `[]`. The first one indicates row and second one indicates column. It uses comma to separate them.


```{r}
# read data
sim.dat<-read.csv("https://raw.githubusercontent.com/happyrabbit/DataScientistR/master/Data/SegData.csv")
# subset the first two rows
sim.dat[1:2,]
# subset the first two rows and column 3 and 5
sim.dat[1:2,c(3,5)]
# get all rows with age>70
sim.dat[sim.dat$age>70,]
# get rows with age> 60 and gender is Male
# select column 3 and 4
sim.dat[sim.dat$age>68 & sim.dat$gender == "Male", 3:4]
```

Remember that there are usually different ways to conduct the same manipulation. For example, the following code presents three ways to calculate average number of online transactions for male and female:

```{r,message=FALSE}
tapply(sim.dat$online_trans, sim.dat$gender, mean )

aggregate(online_trans ~ gender, data = sim.dat, mean)

library(dplyr)
sim.dat%>% 
  group_by(gender)%>%
  summarise(Avg_online_trans=mean(online_trans))
```

There is no gold standard to choose a specific function to manipulate data. The goal is to solve real problem not the tool itself. So just use whatever tool that is convenient for you.  

The way to use `[]` is straightforward. But the manipulations are limited. If you need more complicated data reshaping or aggregation, there are other packages to use such as `dplyr`, `reshape2`, `tidyr` etc. But the usage of those packages are not as straightforward as `[]`. You often need to change functions.  Keeping related operations together, such as subset, group, update, join etc,  will allow for:

- concise, consistent and readable syntax irrespective of the set of operations you would like to perform to achieve your end goal 
- performing data manipulation fluidly without the cognitive burden of having to change among different functions 
- by knowing precisely the data required for each operation, you can automatically optimize operations effectively 

`data.table` is the package for that. If you are not familiar with other data manipulating packages and are interested in reducing programming time tremendously, then this package is for you. 


Other than extending the function of `[]`, `data.table` has the following advantages:

Offers fast import, subset, grouping, update, and joins for large data files
It is easy to turn data frame to data table
Can behave just like a data frame

You need to install and load the package:

```{r, message=FALSE}
# If you haven't install it, use the code to instal
# install.packages("data.table")
# load packagw
library(data.table)
```

Use `data.table()` to covert the existing data frame `sim.dat` to data table:

```{r}
dt <- data.table(sim.dat)
class(dt)
```

Calculate mean for counts of online transactions:

```{r}
dt[, mean(online_trans)]
```

You can't do the same thing using data frame:

```r
sim.dat[,mean(online_trans)]
```
```html
Error in mean(online_trans) : object 'online_trans' not found
```

If you want to calculate mean by a categories as before, set “`by = `” argument:

```{r}
dt[ , mean(online_trans), by = gender]
```
You can group by more than one variables. For example, group by “`gender`” and “`house`”:

```{r}
dt[ , mean(online_trans), by = .(gender, house)]
```

Assign column names for aggregated variables:

```{r}
dt[ , .(avg = mean(online_trans)), by = .(gender, house)]
```

`data.table` can accomplish all operations that `aggregate()` and `tapply()`can do for data frame.

-  General setting of `data.table`

Different from data frame, there are three arguments for data table:

<center>
![](http://scientistcafe.com/book/Figure/datable1.png)
</center>

It is analogous to SQL. You don't have to know SQL to learn data table. But experience with SQL will help you understand data table.  In SQL, you select column `j` (use command `SELECT`) for row `i` (using command `WHERE`).  `GROUP BY` in SQL will assign the variable to group the observations. 

<center>
![](http://scientistcafe.com/book/Figure/rSQL.png)
</center>

Let's review our previous code:

```{r}
dt[ , mean(online_trans), by = gender]
```

The code above is equal to the following SQL：

```sql
SELECT  gender, avg(online_trans) FROM sim.dat GROUP BY gender
```

R code:
 
```{r}
dt[ , .(avg = mean(online_trans)), by = .(gender, house)]
```

is equal to SQL：

```sql 
SELECT gender, house, avg(online_trans) AS avg FROM sim.dat GROUP BY gender, house
```

R code：

```{r}
dt[ age < 40, .(avg = mean(online_trans)), by = .(gender, house)]
```

is equal to SQL：

```sql
SELECT gender, house, avg(online_trans) AS avg FROM sim.dat WHERE age < 40 GROUP BY gender, house
```

You can see the analogy between `data.table` and `SQL`.  Now let's focus on operations in data table. 

- select row

```{r}
# select rows with age<20 and income > 80000
dt[age < 20 & income > 80000]
# select the first two rows
dt[1:2]
```
- select column

Selecting columns in  `data.table` don't need `$`:

```{r}
# select column “age” but return it as a vector
# the argument for row is empty so the result will return all observations
ans <- dt[, age]
head(ans)
```

To return `data.table` object, put column names in `list()`:

```{r}
# Select age and online_exp columns and return as a data.table instead
ans <- dt[, list(age, online_exp)]
head(ans)
```

Or you can also put column names in `.()`:

```{r}
ans <- dt[, .(age, online_exp)]
# head(ans)
```

To select all columns from “`age`” to “`income`”:

```{r}
ans <- dt[, age:income, with = FALSE]
head(ans,2)
```

Delete columns using `-` or `!`:

```{r}
# delete columns from  age to online_exp
ans <- dt[, -(age:online_exp), with = FALSE]
ans <- dt[, !(age:online_exp), with = FALSE]
```

- tabulation

In data table. `.N` means to count。

```{r}
# row count
dt[, .N] 
```

If you assign the group variable, then it will count by groups:

```{r}
# counts by gender
dt[, .N, by= gender]  
# for those younger than 30, count by gender
 dt[age < 30, .(count=.N), by= gender] 
```

Order table:

```{r}
# get records with the highest 5 online expense:
head(dt[order(-online_exp)],5) 
```
Since data table keep some characters of data frame, they share some operations:
 
```{r}
dt[order(-online_exp)][1:5]
```

You can also order the table by more than one variable. The following code will order the table by `gender`, then order within `gender` by `online_exp`:

```{r} 
dt[order(gender, -online_exp)][1:5]
```

-  Use `fread()` to import dat

Other than `read.csv` in base R, I have introduced 'read_csv' in 'readr'.  `read_csv` is much faster and will provide progress bar which makes user feel much better (at least make me feel better). `fread()` in `data.table` further increase the efficiency of reading data. The following are three examples of reading the same data file `topic.csv`. The file includes text data scraped from an agriculture forum with 209670 rows and 6 columns:


```r
system.time(topic<-read.csv("/Users/happyrabbit/Documents/GitHub/DataScientistR/Data/topic.csv"))
```

```html
  user  system elapsed 
  4.313   0.027   4.340
```

```r
system.time(topic<-readr::read_csv("/Users/happyrabbit/Documents/GitHub/DataScientistR/Data/topic.csv"))
```

```html
   user  system elapsed 
  0.267   0.008   0.274 
```

```r
system.time(topic<-data.table::fread("/Users/happyrabbit/Documents/GitHub/DataScientistR/Data/topic.csv"))
```

```html
   user  system elapsed 
  0.217   0.005   0.221 
```

It is clear that `read_csv()` is much faster than `read.csv()`. `fread()` is a little faster than `read_csv()`. As the size increasing, the difference will become for significant. Note that `fread()` will read file as `data.table` by default. 

## Summarize data 

### `apply()`, `lapply()` and `sapply()` in base R

There are some powerful functions to summarize data in base R, such as `apply()`, `lapply()` and `sapply()`. They do the same basic things and are all from "apply" family: apply functions over parts of data. They differ in two important respects:

1. the type of object they apply to
1. the type of result they will return

When do we use `apply()`? When we want to apply a function to margins of an array or matrix. That means our data need to be structured. The operations can be very flexible. It returns a vector or array or list of values obtained by applying a function to margins of an array or matrix.

For example you can compute row and column sums for a matrix:

```{r}
## simulate a matrix
x <- cbind(x1 =1:8, x2 = c(4:1, 2:5))
dimnames(x)[[1]] <- letters[1:8]
apply(x, 2, mean)
col.sums <- apply(x, 2, sum)
row.sums <- apply(x, 1, sum)
```

You can also apply other functions:

```{r}
ma <- matrix(c(1:4, 1, 6:8), nrow = 2)
ma
apply(ma, 1, table)  #--> a list of length 2
apply(ma, 1, stats::quantile) # 5 x n matrix with rownames
```

Results can have different lengths for each call. This is a trickier example. What will you get? 

```r
## Example with different lengths for each call
z <- array(1:24, dim = 2:4)
zseq <- apply(z, 1:2, function(x) seq_len(max(x)))
zseq         ## a 2 x 3 matrix
typeof(zseq) ## list
dim(zseq) ## 2 3
zseq[1,]
apply(z, 3, function(x) seq_len(max(x)))
```

`lapply()` applies a function over a list, data.frame or vector and returns a list of the same length.
`sapply()` is a user-friendly version and wrapper of `lapply()`. By default it returns a vector, matrix or if  `simplify = "array"`, an array if appropriate. `apply(x, f, simplify = FALSE, USE.NAMES = FALSE)` is the same as `lapply(x, f)`. If `simplify=TRUE`, then it will return a `data.frame` instead of `list`. 

Let's use some data with context to help you better understand the functions.

- Get the mean and standard deviation of all numerical variables in the data set.

```{r}
# Read data
sim.dat<-read.csv("https://raw.githubusercontent.com/happyrabbit/DataScientistR/master/Data/SegData.csv")
# Get numerical variables
sdat<-sim.dat[,!lapply(sim.dat,class)=="factor"]
## Try the following code with apply() function
## apply(sim.dat,class)
## Why is there the error?
```

The data frame `sdat` only includes numeric columns. Now we can go head and use `apply()` to get mean and standard deviation for each column:

```{r}
apply(sdat, MARGIN=2,function(x) mean(na.omit(x)))
```

Here we defined a function using ` function(x) mean(na.omit(x))`. It is a very simple function. It tells R to ignore the missing value when calculating the mean. ` MARGIN=2` tells R to apply function to each column. It is not hard to guess what ` MARGIN=1` mean. The result show that the average online expense is much higher than store expense. You can also compare the average scores across different questions. The command to calculate standard deviation is very similar. The only difference is to change `mean()` to `sd()`:

```{r}
apply(sdat, MARGIN=2,function(x) sd(na.omit(x)))
```

Even the average online expense is higher than store expense, the standard deviation for store expense is much higher than online expense which indicates there are very likely some big/small purchase in store. We can check it quickly:

```{r}
summary(sdat$store_exp)
summary(sdat$online_exp)
```

There are some odd values in store expense. The minimum value is -500 which is a wrong imputation which indicates that you should preprocess data before analyzing it. Checking those simple statistics will help you better understand your data. It then give you some idea how to preprocess and analyze them. How about using `lapply()` and `sapply()`?

Run the following code and compare the results:

```r
lapply(sdat, function(x) sd(na.omit(x)))
sapply(sdat, function(x) sd(na.omit(x)))
sapply(sdat, function(x) sd(na.omit(x)), simplify = FALSE)
```

### `ddply()` in `plyr` package

_`dplyr` is a set of clean and consistent tools that implement the split-apply-combine pattern in R. This is an extremely common pattern in data analysis: you solve a complex problem by breaking it down into small pieces, doing something to each piece and then combining the results back together again. [From package description]_ 

You may find the description sounds familiar. The package is sort of a wrapper of apply family. I will only introduce the main function `ddply()`. Because the package has next iteration which is `dplyr` package. I will introduce `dplyr` in more details. The reason I still want to spend some time on the older version is because they have similar idea and knowing the lineage will deeper your understanding of the whole family.

I will use the same data frame `sim.dat` to illustrate. Run the following command:

```{r}
library(plyr)
ddply(sim.dat,"segment",summarize, Age=round(mean(na.omit(age)),0),
      FemalePct=round(mean(gender=="Female"),2),
      HouseYes=round(mean(house=="Yes"),2),
      store_exp=round(mean(na.omit(store_exp),trim=0.1),0),
      online_exp=round(mean(online_exp),0),
      store_trans=round(mean(store_trans),1),
      online_trans=round(mean(online_trans),1))
```

Now, let's peel the onion in order.

The first argument `sim.dat` is easy. It is the data you want to work on. 

The second argument `"segment"` is the column you want to group by. It is a very standard marketing segmentation problem. The final segment is the result you want to get by designing survey, collecting and analyzing data.  Here we assume those segments are known and we want to understand how each group of customer look like. It is a common task in segmentation: figuring out a profile. Here we only summarize data by one categorical variable but you can group by multiply variables using ` ddply(sim.dat, c("segment","house"), .)`. So the second argument tell the function we want to divide data by customer segment. 

The third argument `summarize` tells R the kind of manipulation you want to do which is to summarize data.  There are other choices for this argument such as `transform` (transform data within each group) and `subset`(subset data within each group). 

Then the rest commands tell R the exact action. For example,  ` Age=round(mean(na.omit(age)),0)` tell R the following things:

1.	Calculate the mean of column `age` ignoring missing value
2.	Round the result to the specified number of decimal places 
3.	Store the result to a new variable named `Age`

The rest of the command above are similar. In the end we calculate the following for each segment:

1.	`Age`: average age for each segment
2.	`FemalePct`: percentage for each segment
1.	`HouseYes`: percentage of people who own a house
2.	`stroe_exp`: average expense in store
3.	`online_exp`: average expense online
4.	`store_trans`: average times of transactions in store
5.	`online_trans`: average times of online transactions 

There is a lot of information you can draw from those simple averages.

- Conspicuous: average age is about 40.  Target for middle-age wealthy people. 1/3 of them are female and 2/3 are male. They may be good target for candy dad. They buy regardless the price. Almost all of them own house (0.86).  It makes us wonder what is wrong with the rest 14%? They may live in Manhattan

- Price: They are older people, average age 60. Nearly all of them own a house(0.94). They are less likely to purchase online (store_trans=6 while online_trans=3). This is the only group that is less likely to purchase online.

- Quality: The average age is 35. They are not way different with Conspicuous in terms of age. But they spend much less. The percentages of male and female are similar. They prefer online shopping. More than half of them don't own a house (0.66).

- Style: They are young people with average age 24. Majority of them are female (0.81). Most of them don't own a house (0.73). They are very likely to be digital natives and definitely prefer online shopping.

You may notice that Style group purchase more frequently online (online_trans=21) but the expense (online_exp=1962) is not higher.  This makes us wondering what is the average expense each time so you have a better idea about the price range the group fall in. 

The analytical process is aggregated instead of independent steps. What you learn before will help you decide what to do next. Sometimes you also need to go backward to fix something in the previous steps. For example, you may need to check those negative expense value.

We continue to use `ddply()` to calculate the two statistics:

```{r}
ddply(sim.dat,"segment",summarize,avg_online=round(sum(online_exp)/sum(online_trans),2),
      avg_store=round(sum(store_exp)/sum(store_trans),2))
```

Price group has the lowest averaged one time purchasing price.  The Conspicuous group will pay the highest price. When we build profile in real life, we will need to look at the survey results too.  Those simple data manipulations can provide you lots of information already. 
As  mentioned before, other than "`summarize`" there are other functions such as "`transform`" and "`subset`". 

For simplicity, I draw 11 random samples and 3 variables (`age`, `store_exp` and `segment`) from the original data according to the different segments. We will explain stratified sampling later. Here we just do it without explanation. 

```{r}
library(caret)
set.seed(2016)
trainIndex<-createDataPartition(sim.dat$segment,p=0.01,list=F,times=1)
examp<-sim.dat[trainIndex,c("age","store_exp","segment")]
```

Now data frame `examp` only has 11 rows and 3 columns. Let's look at the function of `transform`:

```{r}
ddply(examp,"segment",transform,store_pct=round(store_exp/sum(store_exp),2))
```

What "`transform`" does is to transform data within the specified group (segment) and append the result as a new column.

Next let's look at the function of "`subset`":

```{r}
ddply(examp,"segment",subset,store_exp>median(store_exp))
```

You get all rows with ` store_exp ` greater than its group median.

### `dplyr` package

`dplyr` provides a flexible grammar of data manipulation focusing on tools for working with data frames (hence the `d` in the name). It is faster and more friendly:

- It identifies the most important data manipulations and make they easy to use from R
- It performs faster for in-memory data by writing key pieces in C++ using `Rcpp`
- The interface is the same for data frame, data table or database

I will illustrate the following functions in order: 

1. Display
1. Subset
1. Summarize
1. Create new variable
1. Merge

**Display**

- `tbl_df()`: Convert the data to `tibble` which offers better checking and printing capabilities than traditional data frames. It will adjust output width according to fit the current window.

```{r, message=FALSE, echo=FALSE}
library(dplyr)
```


```r
library(dplyr)
tbl_df(sim.dat)
```

- `glimpse()`: This is like a transposed version of `tbl_df()`

```r
glimpse(sim.dat)
```

**Subset**

Get rows with `income` more than 300000:

```{r}
library(magrittr)
filter(sim.dat, income >300000) %>%
  tbl_df()
```

Here we meet a new operator `%>%`. It is called "Pipe operator" which pipes a value forward into an expression or function call. What you get in the left operation will be the first argument or the only argument in the right operation.

```r
x %>% f(y) = f(x, y)
y %>% f(x, ., z) = f(x, y, z )
```
It is an operator from `magrittr` which can be really beneficial. Look at the following code. Can you tell me what it does?

```r
ave_exp <- filter( 
  summarise(
    group_by( 
      filter(
        sim.dat, 
        !is.na(income)
      ), 
      segment
    ), 
    ave_online_exp = mean(online_exp), 
    n = n()
  ), 
  n > 200
) 
```
Now look at the identical code using "`%>%`":

```r
avg_exp <- sim.dat %>% 
 filter(!is.na(income)) %>% 
 group_by(segment) %>% 
 summarise( 
   ave_online_exp = mean(online_exp), 
   n = n() ) %>% 
  filter(n > 200)
```

Isn't it much more straight forward now? Let's read it:

1.	Delete observations from `sim.dat` with missing income values 
2.	Group the data from step 1 by variable `segment`
3.	Calculate mean of online expense for each segment and save the result as a new variable named `ave_online_exp`
4.	Calculate the size of each segment and saved it as a new variable named `n`
5.	Get segments with size larger than 200

You can use `distinct()` to delete duplicated rows. It is a generalization of `unique()` from vector to data frame.

```r
dplyr distinct(sim.dat)
```
`sample_frac()` will randomly select some rows with specified percentage. `sample_n()` can randomly select rows with specified number.

```r
dplyr::sample_frac(sim.dat, 0.5, replace = TRUE) 
dplyr::sample_n(sim.dat, 10, replace = TRUE) 
```

`slice()` will select rows by position:

```r
dplyr::slice(sim.dat, 10:15) 
```

It is equivalent to `sim.dat[10:15,]`. 

`top_n()` will select the order top n entries:

```r
dplyr::top_n(sim.dat,2,income)
```

If you want to select columns instead of  rows, you can use `select()`. The following are some sample codes:

```r
# select by column name
dplyr::select(sim.dat,income,age,store_exp)

# select columns whose name contains a character string
dplyr::select(sim.dat, contains("_"))

# select columns whose name ends with a character string
# similar there is "starts_with"
dplyr::select(sim.dat, ends_with("e"))

# select columns Q1,Q2,Q3,Q4 and Q5
select(sim.dat, num_range("Q", 1:5)) 

# select columns whose names are in a group of names
dplyr::select(sim.dat, one_of(c("age", "income")))

# select columns between age and online_exp
dplyr::select(sim.dat, age:online_exp)

# select all columns except for age
dplyr::select(sim.dat, -age)
```

**Summarize**

The operations here are similar what we did before with `apply()` and `ddply()`.

```{r}
dplyr::summarise(sim.dat, avg_online = mean(online_trans)) 
```

```{r}
# apply function anyNA() to each column
# you can also assign a function vector such as: c("anyNA","is.factor")
dplyr::summarise_each(sim.dat, funs_(c("anyNA")))
```

You can use `group_by()` to indicate the variables you want to group by as before:

```{r}
sim.dat %>% group_by(segment) %>% summarise_each(funs_(c("anyNA")))
```

**Create new variable**

`mutate()` will  compute and append one or more new columns:

```r
dplyr::mutate(sim.dat, total_exp = store_exp + online_exp)
```

It will apply **window function** to the columns and return a column with the same length. It is a different type of function as before.

```r
# min_rank=rank(ties.method = "min")
# mutate_each() means apply function to each column
dplyr::mutate_each(sim.dat, funs(min_rank)) 
```

The other similar function is `transmute()`. The differece is that `transmute()` will delete the original columns and only keep the new ones.

```r
dplyr::transmute(sim.dat, total_exp = store_exp + online_exp) 
```

**Merge**

We create two baby data sets to show how the functions work.

```{r, warning=FALSE}
(x<-data.frame(cbind(ID=c("A","B","C"),x1=c(1,2,3))))
(y<-data.frame(cbind(ID=c("B","C","D"),y1=c(T,T,F))))
```

```{r, warning=FALSE}
# join to the left
# keep all rows in x
left_join(x,y,by="ID")
```

```{r, warning=FALSE}
# get rows matched in both data sets
inner_join(x,y,by="ID")
```

```{r, warning=FALSE}
# get rows in either data set
full_join(x,y,by="ID")
```

```{r, warning=FALSE}
# filter out rows in x that can be matched in y 
# it doesn't bring in any values from y 
semi_join(x,y,by="ID")
```

```{r, warning=FALSE}
# the opposite of  semi_join()
# it gets rows in x that cannot be matched in y
# it doesn't bring in any values from y
anti_join(x,y,by="ID")
```

There are other functions(`intersect()`, `union()` and `setdiff()`). Also the data frame version of `rbind` and `cbind` which are `bind_rows()` and `bind_col()`. We are not going to go through them all. You can try them yourself. If you understand the functions we introduced so far. It should be easy for you to figure out the rest.

## Tidy and Reshape Data 

 "Tidy data" represent the information from a dataset as data frames where each row is an observation and each column contains the values of a variable (i.e. an attribute of what we are observing). Depending on the situation, the requirements on what to present as rows and columns may change. In order to make data easy to work with for the problem at hand, in practice, we often need to convert data between the "wide" and the "long" format. The process feels like playing with a dough. 
 
There are two commonly used packages for this kind of manipulations: `tidyr` and `reshape2`. We will show how to tidy and reshape data using the two packages. By comparing the functions to show how they overlap and where they differ.

### ` reshape2` package

It is a reboot of previous package `reshape`. Why? Here is what I got from Stack Overflow:

_"`reshape2` let Hadley make a rebooted reshape that was way, way faster, while avoiding busting up people's dependencies and habits."_

Take a baby subset of our exemplary clothes consumers data to illustrate: 

```{r}
(sdat<-sim.dat[1:5,1:6])
```

For the above data `sdat`, what if we want to have a variable indicating the purchasing channel (i.e. online or in-store) and another column with the corresponding expense amount? Assume we want to keep the rest of the columns the same. It is a task to change data from "wide" to "long". There are two general ways to shape data:

- Use `melt()` to convert an object into a molten data frame, i.e. from wide to long
- Use `dcast()` to cast a molten data frame into the shape you want, i.e. from long to wide

```{r}
library(reshape2)
(mdat <- melt(sdat, measure.vars=c("store_exp","online_exp"),
              variable.name = "Channel",
              value.name = "Expense"))
```

You melted the data frame `sdat` by two variables:  `store_exp` and `online_exp` (`measure.vars=c("store_exp","online_exp")`).  The new variable name is `Channel` set by command `variable.name = "Channel"`. The value name is `Expense` set by command `value.name = "Expense"`. 

You can run a regression to study the effect of purchasing channel: 

```{r}
# Here we use all observations from sim.dat
mdat<-melt(sim.dat[,1:6], measure.vars=c("store_exp","online_exp"),
            variable.name = "Channel",
              value.name = "Expense")
fit<-lm(Expense~gender+house+income+Channel+age,data=mdat)
summary(fit)
```

You can `melt()`  list, matrix, table too. The syntax is similar and we won't go through every situation. Sometimes we want to convert the data from "long" to "wide". For example, **you want to compare the online and in store expense between male and female based on the house ownership. **

```{r}
dcast(mdat, house + gender ~ Channel, sum)
```

In the above code,  what is the left side of `~` are variables that you want to group by. The right side is the variable you want to spread as columns. It will use the column indicating value from `melt()` before. Here is "`Expense`" .  

### `tidyr` package

The other package that will do similar manipulations is `tidyr`. Let's get a subset to illustrate the usage. 

```{r}
library(dplyr)
# practice functions we learnt before
sdat<-sim.dat[1:5,]%>%
  dplyr::select(age,gender,store_exp,store_trans)
sdat %>% tbl_df()
```

`gather()` function in `tidyr` is analogous to `melt()` in `reshape2`. The following code will do the same thing as we did before using `melt()`: 

```{r, message=FALSE}
library(tidyr)
msdat<-tidyr::gather(sdat,"variable","value",store_exp,store_trans)
msdat %>% tbl_df()
```

Or if we use the pipe operation, we can write the above code as:

```r
sdat%>%gather("variable","value",store_exp,store_trans)
```

It is identical with the following code using `melt()`:

```r
melt(sdat, measure.vars=c("store_exp","store_trans"),
            variable.name = "variable",
              value.name = "value")
```

The opposite operation to `gather()` is `spread()`. The previous one stacks columns and the latter one spread the columns.

```{r}
msdat %>% spread(variable,value)
```

Another pair of functions that do opposite manipulations are `separate()` and `unite()`. 

```{r}
sepdat<- msdat %>% 
  separate(variable,c("Source","Type"))
sepdat %>% tbl_df()
```

You can see that the function separates the original column "`variable`" to two new columns "`Source`" and "`Type`". You can use `sep=` to set the string or regular express to separate the column. By default, it is "`_`". 

The `unite()` function will do the opposite: combining two columns. It is like the generalization of `paste()` to data frame.

```{r}
sepdat %>% 
  unite("variable",Source,Type,sep="_")
```

The reshaping manipulations may be the trickiest part. You have to practice a lot to get familiar with those functions. Unfortunately there is no short cut. 