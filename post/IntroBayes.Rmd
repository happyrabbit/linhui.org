---
title: "Introduction to Bayesian"
author: "[Hui Lin](http://scientistcafe.com)"
date: "`r Sys.Date()`"
output:
  html_document:
    highlight: textmate
    number_sections: true
    toc: true
    toc_depth: 4
    toc_float:
      collapsed: false
      smooth_scroll: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = F, warning =  F)
```


# Why Bayesian?

In an A/B test, we ask the question, "Which version of the product is better?" If you use a frequentist framework, the answer is based on a p-value. The p-value indicates the probability of observing a difference as large or larger than what is observed between versions A and B, assuming that there is no real difference between them and that observed measurements follow a certain distribution. This counter-intuitive way of explaining data often leads to the misuse of the p-value.

## Misuse of P-Value

Misuse of p-value is common in many research fields. There were heated discussions about P-value in the past. Siegfried commented in his 2010 Science News article:

> “It’s science’s dirtiest secret: The scientific method of testing hypotheses by statistical analysis stands on a flimsy foundation.”

American Statistical Association (ASA) released an official statement on p-value in 2016 ([Ronald L. Wassersteina 2016](https://www.tandfonline.com/doi/full/10.1080/00031305.2016.1154108)). It was the first time to have an organization level announcement about p-value. The statement’s six principles, many of which address misconceptions and misuse of the P-value, are the following:

1. P-values can indicate how incompatible the data are with a specified statistical model.
2. P-values do not measure the probability that the studied hypothesis is true or the probability that the data were produced by random chance alone.
3. Scientific conclusions and business or policy decisions should not be based only on whether a p-value passes a specific threshold.
4. Proper inference requires full reporting and transparency.
5. A p-value, or statistical significance, does not measure the size of an effect or the importance of a result.
6. By itself, a p-value does not provide a good measure of evidence regarding a model or hypothesis.

The p=0.05 threshold is not based on any scientific calculation but is an arbitrary number. It means that practitioners can use a different threshold if they think it better fits the problem to solve. However, the p-value is hard to avoid in classical statistical inference.

## Bayesian as an Alternative

In the frequentist framework, inference about a population starts with a sample. The observed data represents only one of many possible data sets, and the uncertainty arises from sampling variation. If we were to resample numerous times, we could collect a list of values (for example, click-through rate) that would exhibit some pattern (i.e. sampling distribution). An assumption of the process generating the observable data is an essential part of developing the desired inference. This means that we can't assign probability distributions to parameters and models because they're considered to be already known, and only measurements can have them. 

Unlike frequentists, bayesians view probability as a degree of belief and uncertainty as a property of information. We are uncertain because we have incomplete knowledge, not because of the sampling process.  A Bayesian can state probabilities about the parameters, which are considered random variables. However, it is not possible in the frequentist paradigm. 

From our perspective, a coin toss is considered "random" because we do not have complete information about the coin or how it is tossed. However, the coin and the toss themselves are not random.


## A Simple Bayesian Model: Let's Toss a Coin

To illustrate the conventional form of a Bayesian statistical model, let's examine a toy example. Suppose you want to determine whether a coin is fair or biased. To make this decision, you toss the coin and obtain HTHHHTHHH (where H denotes heads and T denotes tails). The data story in this case is:

- The true probability of getting a head is p.
- A single toss of the coin has a probability p of producing a head observation. It has 1-p of producing a tail.
- Each toss of the coin is independent of the others.

```{r, out.width = "200px",echo=FALSE, fig.align='center'}
knitr::include_graphics("https://github.com/happyrabbit/BayesianIntro/blob/master/images/garden14.png?raw=true")
```

To frame the data process in the Bayesian framework, we start with prior plausibilities. Then, we update our beliefs based on the data to generate posterior plausibilities. In this case, the bayesian updating process is as follow:

1. Prior to observing any coin toss results, there is no evidence to suggest that the coin is biased towards either heads or tails. Therefore, our Bayesian model initially assigns equal plausibility to heads and tails. This is the dotted straight line in the plot.
2. After seeing the first toss which is a “H”, the model updates the plausibilities to the solid line. The possibility of p = 0 has fallen to exactly zero since we observed at least one “H”.  The possibility of p > 0.5 has increased since there is not yet any evidence that you can get a “T”.
3. As more data comes in, the Bayesian model updates its beliefs accordingly. The posterior distribution is used as the prior distribution for the next toss.

The following plot shows how the model updates after each toss.


```{r, echo= FALSE,  include= FALSE,  results='hide'}
##--------------------- Bayesian Update

# define grid
p_grid <- seq(from=0, to=1, length.out=100 )

par(mfrow = c(3,3), mar=c(2,1,2,1))
##-------------------- Obs 1
# define prior
prior1 <- rep( 1 , 100 )
# compute likelihood at each value in grid
likelihood1 <- dbinom(1 , size=1 , prob=p_grid )
# compute product of likelihood and prior
unstd.posterior1 <- likelihood1 * prior1
# standardize the posterior, so it sums to 1
posterior1 <- unstd.posterior1 / sum(unstd.posterior1)
plot(p_grid, posterior1, type="l", ylab = "", xlab="", 
     labels=F, tick =F,main = "H")
lines(p_grid, prior1-0.99,lty=2)
axis(side = 1, labels = T)

##------------------- Obs 2
likelihood2 <- dbinom( 0 , size=1 , prob=p_grid )
# compute product of likelihood and prior
unstd.posterior2 <- likelihood2 * posterior1
# standardize the posterior, so it sums to 1
posterior2 <- unstd.posterior2 / sum(unstd.posterior2)

plot(p_grid, posterior2, type="l", ylab = "", xlab="", 
     labels=F, tick =F, ylim=c(0,0.03),main = "HT")
lines(p_grid, posterior1,lty=2)
axis(side = 1, labels = T )


##------------------- Obs 3
likelihood3 <- dbinom( 1 , size=1 , prob=p_grid )
# compute product of likelihood and prior
unstd.posterior3 <- likelihood3 * posterior2
# standardize the posterior, so it sums to 1
posterior3 <- unstd.posterior3 / sum(unstd.posterior3)

plot(p_grid, posterior3, type="l", ylab = "", xlab="", 
     labels=F, tick =F, ylim=c(0,0.03),main = "HTH")
lines(p_grid, posterior2,lty=2)
axis(side = 1, labels = T )


##------------------- Obs 4
likelihood4 <- dbinom( 1 , size=1 , prob=p_grid )
# compute product of likelihood and prior
unstd.posterior4 <- likelihood4 * posterior3
# standardize the posterior, so it sums to 1
posterior4 <- unstd.posterior4 / sum(unstd.posterior4)

plot(p_grid, posterior4, type="l", ylab = "", xlab="", 
     labels=F, tick =F, ylim=c(0,0.03),main = "HTHH")
lines(p_grid, posterior3,lty=2)
axis(side = 1, labels = T )


##------------------- Obs 5
likelihood5 <- dbinom( 1 , size=1 , prob=p_grid )
# compute product of likelihood and prior
unstd.posterior5 <- likelihood5 * posterior4
# standardize the posterior, so it sums to 1
posterior5 <- unstd.posterior5 / sum(unstd.posterior5)

plot(p_grid, posterior5, type="l", ylab = "", xlab="", 
     labels=F, tick =F, ylim=c(0,0.03),main = "HTHHH")
lines(p_grid, posterior4,lty=2)
axis(side = 1, labels = T )

##------------------- Obs 6
likelihood6 <- dbinom( 0 , size=1 , prob=p_grid )
# compute product of likelihood and prior
unstd.posterior6 <- likelihood6 * posterior5
# standardize the posterior, so it sums to 1
posterior6 <- unstd.posterior6 / sum(unstd.posterior6)

plot(p_grid, posterior6, type="l", ylab = "", xlab="", 
     labels=F, tick =F, ylim=c(0,0.03),main = "HTHHHT")
lines(p_grid, posterior5,lty=2)
axis(side = 1, labels = T )

##------------------- Obs 7
likelihood7 <- dbinom( 1 , size=1 , prob=p_grid )
# compute product of likelihood and prior
unstd.posterior7 <- likelihood7 * posterior6
# standardize the posterior, so it sums to 1
posterior7 <- unstd.posterior7 / sum(unstd.posterior7)

plot(p_grid, posterior7, type="l", ylab = "", xlab="", 
     labels=F, tick =F, ylim=c(0,0.03),main = "HTHHHTH")
lines(p_grid, posterior6,lty=2)
axis(side = 1, labels = T )

##------------------- Obs 8
likelihood8 <- dbinom( 1 , size=1 , prob=p_grid )
# compute product of likelihood and prior
unstd.posterior8 <- likelihood8 * posterior7
# standardize the posterior, so it sums to 1
posterior8 <- unstd.posterior8 / sum(unstd.posterior8)

plot(p_grid, posterior8, type="l", ylab = "", xlab="", 
     labels=F, tick =F, ylim=c(0,0.03),main = "HTHHHTHH")
lines(p_grid, posterior7,lty=2)
axis(side = 1, labels = T )

##------------------- Obs 9
likelihood9 <- dbinom( 1 , size=1 , prob=p_grid )
# compute product of likelihood and prior
unstd.posterior9 <- likelihood9 * posterior8
# standardize the posterior, so it sums to 1
posterior9 <- unstd.posterior9 / sum(unstd.posterior9)

plot(p_grid, posterior9, type="l", ylab = "", xlab="", 
     labels=F, tick =F, ylim=c(0,0.03),main = "HTHHHTHHH")
lines(p_grid, posterior8,lty=2)
axis(side = 1, labels = T )
```

```{bash}
pip install numpy matplotlib pymc3 scipy pandas
```

```{python}
import numpy as np
import matplotlib.pyplot as plt

# Create the grid of values from 0 to 1
p_grid = np.linspace(0, 1, 100)

# Define prior
prior1 = np.ones(100)/100

# Define the likelihood functions for each observation
def likelihood(prob, observation):
    if observation == 1:
        return prob
    else:
        return 1 - prob

# Initialize empty lists to store posteriors for each observation
posteriors = []

# Loop through the observations and calculate the posteriors
for observation in [1, 0, 1, 1, 1, 0, 1, 1, 1]:
    # Compute likelihood at each value in the grid
    likelihoods = [likelihood(prob, observation) for prob in p_grid]
    
    # Compute product of likelihood and prior
    unstd_posterior = likelihoods * prior1
    
    # Standardize the posterior, so it sums to 1
    posterior = unstd_posterior / np.sum(unstd_posterior)
    
    # Store the posterior and prior in the list
    posteriors.append(posterior)
    
    # Plot the results for the current observation
    plt.subplot(3, 3, len(posteriors))
    plt.plot(p_grid, posterior, 'b-', label='Posterior')
    plt.plot(p_grid, prior1, 'r--', label='Prior')
   # plt.xlabel('Probability (p)')
   # plt.ylabel('Density')
   # plt.title(f'Observation {len(posteriors)}')
    plt.ylim(0, 0.04)
    # Update the prior for the next iteration
    prior1 = posterior

plt.legend()
plt.tight_layout()
plt.show()
```

## Binomial outcome

We have observed how the Bayesian model behaves. Now, let's summarize the model in mathematical format. Let’s define the count of heads as x and the total number of tosses as n. In the example with the coin, we tossed it 9 times (n=9) and observed 7 heads (x=7). Once we add our previous assumptions that (1) every toss is independent of the others, and (2) the probability of getting heads is the same each time, probability theory points to binomial distribution. 

$$x \sim Binomial(N, p)$$ 
$$p \sim Uniform(0,1)$$
We do not have any prior knowledge about the parameter p, so we assign a flat prior over its entire possible range, from zero to one. Every parameter in the Bayesian model requires a prior that represents the information we know so far. As shown in the previous plot, the model updates its beliefs each time it receives new data. 

**How do we choose a prior?** Representing prior information is a complicated process that involves subjective elements. The flat prior (also known as a **non-informative prior**) used here is common, but it is not the only option and may not be the most optimal choice. Another popular method is to use **conjugate priors**, which assume a convenient parametric family and then select a member of that family using carefully elicited summary measures for the desired distribution.

There is a school of Bayesian inference that emphasizes choosing priors based on the personal beliefs of the analyst ([Kadane J. B. 2001](https://www.routledge.com/Principles-of-Uncertainty/Kadane/p/book/9781138052734)). In the natural and social sciences, the prior is considered part of the model that needs to be evaluated and revised (**[Statistical Rethinking](https://xcelab.net/rm/statistical-rethinking/))**. There is no rule stating that you should use only one prior. If you do not have a strong argument for a particular prior, try different ones. You can also perform sensitivity analysis, as with many other classic statistical models.

## Posterior Distribution

Bayesian models can take new data and update prior distributions to their logical consequences, which are known as posterior distributions. The posterior distribution contains information about the relative plausibility of different parameter values, given the data and model. In this case, the posterior distribution represents the probability of each possible value of p, given the specific x that we observed (i.e.Pr(p|x))

The mathematical definition of the posterior distribution is derived from Bayes' Theorem, which is quite straightforward.

$$Pr(x|p) = \frac{Pr(x|p)Pr(p)}{Pr(x)}$$
To translate the above equation to word form:

$$Posterior = \frac{Prob\ of\ the\ data\ given\ certain\ p\ \times Prior}{Average\ probability\ of\ the\ data\ over\ prior}$$
## Continuous Outcome

What if the outcome is not binary but continuous? For example, if you want to measure the difference in time spent per day on a streaming website between two recommendation engines.

$$
minutes\_watched\sim Normal(\mu_i,  \sigma)
$$

$$
\mu_i \sim \alpha_{experiment[i]}
$$

$$
\alpha_j\sim Normal(60, 30)
$$

$$
\sigma \sim Uniform(10, 100)
$$

```{python}
import pandas as pd
import numpy as np
import pymc3 as pm
from scipy.stats import uniform

# Load the data
url = "https://raw.githubusercontent.com/happyrabbit/PublicData/main/watched_min_ab.csv"
dat = pd.read_csv(url)

# Create an experimental dummy variable
dat['exp_dummy'] = (dat['experiment'] == 'A').astype(int)

print(dat.head())
# Define the Bayesian model using PyMC3
with pm.Model() as m2:
    a = pm.Normal('a', mu=60, sd=30, shape=2)  # Using shape=2 to account for two groups (A and B)
    sigma = pm.Uniform('sigma', lower=10, upper=100)
    
    mu = a[dat['exp_dummy']]
    
    minutes_watched = pm.Normal('minutes_watched', mu=mu, sd=sigma, observed=dat['minutes_watched'])

# Perform inference with the model
with m2:
    trace = pm.sample(2000, tune=1000, cores=1, random_seed=42)

# Print the summary statistics for the model parameters
summary = pm.summary(trace, hdi_prob=0.95, round_to=2)
print(summary)

# Extract samples from the posterior
post_samples = pm.trace_to_dataframe(trace)
post_samples['diff_min'] = post_samples['a__0'] - post_samples['a__1']
post_summary = post_samples.describe(percentiles=[0.025, 0.975]).transpose()
print(post_summary[['mean', 'std', '2.5%', '97.5%']])

```

